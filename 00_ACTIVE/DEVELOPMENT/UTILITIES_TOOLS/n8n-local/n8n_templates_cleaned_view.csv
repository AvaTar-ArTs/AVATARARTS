id,template_name,template_description,totalViews_clean,createdAt_parsed,is_ai_related,num_nodes
1951,Scrape and summarize webpages with AI,"This workflow integrates both web scraping and NLP functionalities. It uses HTML parsing to extract links, HTTP requests to fetch essay content, and AI-based summarization using GPT-4o. It's an excellent example of an end-to-end automated task that is not only efficient but also provides real value by summarizing valuable content. Note that to use this template, you need to be on n8n version 1.50.0 or later.",291546,2023-10-02 18:51:45.300000+00:00,True,6
2006,AI agent that can scrape webpages,"‚öôÔ∏èüõ†Ô∏èüöÄü§ñü¶æ This template is a PoC of a ReAct AI Agent capable of fetching random pages (not only Wikipedia or Google search results). On the top part there's a manual chat node connected to a LangChain ReAct Agent. The agent has access to a workflow tool for getting page content. The page content extraction starts with converting query parameters into a JSON object. There are 3 pre-defined parameters: url** ‚Äì an address of the page to fetch method** = full / simplified maxlimit** - maximum length for the final page. For longer pages an error message is returned back to the agent Page content fetching is a multistep process: An HTTP Request mode tries to get the page content. If the page content was successfuly retrieved, a series of post-processing begin: Extract HTML BODY; content Remove all unnecessary tags to recuce the page size Further eliminate external URLs and IMG scr values (based on the method query parameter) Remaining HTML is converted to Markdown, thus recuding the page lengh even more while preserving the basic page structure The remaining content is sent back to an Agent if it's not too long (maxlimit = 70000 by default, see CONFIG node). NB: You can isolate the HTTP Request part into a separate workflow. Check the Workflow Tool description, it guides the agent to provide a query string with several parameters instead of a JSON object. Please reach out to Eduard is you need further assistance with you n8n workflows and automations! Note that to use this template, you need to be on n8n version 1.19.4 or later.",211623,2023-12-06 14:45:11.358000+00:00,True,4
2275,"Automated Web Scraping: email a CSV, save to Google Sheets & Microsoft Excel","How it works: The workflow starts by sending a request to a website to retrieve its HTML content. It then parses the HTML extracting the relevant information The extracted data is storted and converted into a CSV file. The CSV file is attached to an email and sent to your specified address. The data is simultaneously saved to both Google Sheets and Microsoft Excel for further analysis or use. Set-up steps: Change the website to scrape in the ""Fetch website content"" node Configure Microsoft Azure credentials with Microsoft Graph permissions (required for the Save to Microsoft Excel 365 node) Configure Google Cloud credentials with access to Google Drive, Google Sheets and Gmail APIs (the latter is required for the Send CSV via e-mail node).",99001,2024-05-30 06:45:48.037000+00:00,False,5
2384,Chat with local LLMs using n8n and Ollama,"Chat with local LLMs using n8n and Ollama This n8n workflow allows you to seamlessly interact with your self-hosted Large Language Models (LLMs) through a user-friendly chat interface. By connecting to Ollama, a powerful tool for managing local LLMs, you can send prompts and receive AI-generated responses directly within n8n. Use cases Private AI Interactions Ideal for scenarios where data privacy and confidentiality are important. Cost-Effective LLM Usage Avoid ongoing cloud API costs by running models on your own hardware. Experimentation & Learning A great way to explore and experiment with different LLMs in a local, controlled environment. Prototyping & Development Build and test AI-powered applications without relying on external services. How it works When chat message received: Captures the user's input from the chat interface. Chat LLM Chain: Sends the input to the Ollama server and receives the AI-generated response. Delivers the LLM's response back to the chat interface. Set up steps Make sure Ollama is installed and running on your machine before executing this workflow. Edit the Ollama address if different from the default.",77672,2024-08-19 06:04:24.673000+00:00,True,2
2417,Flux AI Image Generator,"Easily generate images with Black Forest's Flux Text-to-Image AI models using Hugging Face‚Äôs Inference API. This template serves a webform where you can enter prompts and select predefined visual styles that are customizable with no-code. The workflow integrates seamlessly with Hugging Face's free tier, and it‚Äôs easy to modify for any Text-to-Image model that supports API access. Try it Curious what this template does? Try a public version here: https://devrel.app.n8n.cloud/form/flux Set Up Watch this quick set up video üëá Accounts required Huggingface.co account (free) Cloudflare.com account (free - used for storage; but can be swapped easily e.g. GDrive) Key Features: Text-to-Image Creation**: Generates unique visuals based on your prompt and style. Hugging Face Integration**: Utilizes Hugging Face‚Äôs Inference API for reliable image generation. Customizable Visual Styles**: Select from preset styles or easily add your own. Adaptable**: Swap in any Hugging Face Text-to-Image model that supports API calls. Ideal for: Creators**: Rapidly create visuals for projects. Marketers**: Prototype campaign visuals. Developers**: Test different AI image models effortlessly. How It Works: You submit an image prompt via the webform and select a visual style, which appends style instructions to your prompt. The Hugging Face Inference API then generates and returns the image, which gets hosted on Cloudflare S3. The workflow can be easily adjusted to use other models and styles for complete flexibility.",76789,2024-09-17 19:50:43.299000+00:00,True,2
2846,AI Voice Chatbot with ElevenLabs & OpenAI for Customer Service and Restaurants,"The ""Voice RAG Chatbot with ElevenLabs and OpenAI"" workflow in n8n is designed to create an interactive voice-based chatbot system that leverages both text and voice inputs for providing information. Ideal for shops, commercial activities and restaurants How it works: Here's how it operates: Webhook Activation: The process begins when a user interacts with the voice agent set up on ElevenLabs, triggering a webhook in n8n. This webhook sends a question from the user to the AI Agent node. AI Agent Processing: Upon receiving the query, the AI Agent node processes the input using predefined prompts and tools. It extracts relevant information from the knowledge base stored within the Qdrant vector database. Knowledge Base Retrieval: The Vector Store Tool node interfaces with the Qdrant Vector Store to retrieve pertinent documents or data segments matching the user‚Äôs query. Text Generation: Using the retrieved information, the OpenAI Chat Model generates a coherent response tailored to the user‚Äôs question. Response Delivery: The generated response is sent back through another webhook to ElevenLabs, where it is converted into speech and delivered audibly to the user. Continuous Interaction: For ongoing conversations, the Window Buffer Memory ensures context retention by maintaining a history of interactions, enhancing the conversational flow. Set up steps: To configure this workflow effectively, follow these detailed setup instructions: ElevenLabs Agent Creation: Create a FREE account on ElevenLabs Begin by creating an agent on ElevenLabs (e.g., named 'test_n8n'). Customize the first message and define the system prompt specific to your use case, such as portraying a character like a waiter at ""Pizzeria da Michele"". Add a Webhook tool labeled 'test_chatbot_elevenlabs' configured to receive questions via POST requests. Qdrant Collection Initialization: Utilize the HTTP Request nodes ('Create collection' and 'Refresh collection') to initialize and clear existing collections in Qdrant. Ensure you update placeholders QDRANTURL and COLLECTION accordingly. Document Vectorization: Use Google Drive integration to fetch documents from a designated folder. These documents are then downloaded and processed for embedding. Employ the Embeddings OpenAI node to generate embeddings for the downloaded files before storing them into Qdrant via the Qdrant Vector Store node. AI Agent Configuration: Define the system prompt for the AI Agent node which guides its behavior and responses based on the nature of queries expected (e.g., product details, troubleshooting tips). Link necessary models and tools including OpenAI language models and memory buffers to enhance interaction quality. Testing Workflow: Execute test runs of the entire workflow by clicking 'Test workflow' in n8n alongside initiating tests on the ElevenLabs side to confirm all components interact seamlessly. Monitor logs and outputs closely during testing phases to ensure accurate data flow between systems. Integration with Website: Finally, integrate the chatbot widget onto your business website replacing placeholder AGENT_ID with the actual identifier created earlier on ElevenLabs. By adhering to these comprehensive guidelines, users can successfully deploy a sophisticated voice-driven chatbot capable of delivering precise answers utilizing advanced retrieval-augmented generation techniques powered by OpenAI and ElevenLabs technologies. Need help customizing? Contact me for consulting and support or add me on Linkedin.",62222,2025-02-04 13:13:55.888000+00:00,True,10
2557,Hacker News to Video Content,"Hacker News to Video Content Overview This workflow converts trending articles from Hacker News into engaging video content. It integrates AI-based tools to analyze, summarize, and generate multimedia content, making it ideal for content creators, educators, and marketers. Features Article Retrieval: Pulls trending articles from Hacker News. Limits the number of articles to process (configurable). Content Analysis: Uses OpenAI's GPT model to: Summarize articles. Assess their relevance to specific topics like automation or AI. Extract key image URLs. Image and Video Generation: Leonardo.ai: Creates stunning AI-generated images based on extracted prompts. RunwayML: Converts images into high-quality videos. Structured Content Creation: Parses content into structured formats for easy reuse. Generates newsletter-friendly blurbs and social media-ready captions. Cloud Integration: Uploads generated assets to: Dropbox Google Drive Microsoft OneDrive MinIO Social Media Posting (Optional): Supports posting to YouTube, X (Twitter), LinkedIn, and Instagram. Workflow Steps 1. Trigger Initiated manually via the ""Test Workflow"" button. 2. Fetch Articles Retrieves articles from Hacker News. Limits the results to avoid processing overload. 3. Content Filtering Evaluates if articles are related to AI/Automation using OpenAI's language model. 4. Image and Video Generation Generates: AI-driven image prompts via Leonardo.ai. Videos using RunwayML. 5. Asset Management Saves the output to cloud storage services or uploads directly to social media platforms. Prerequisites API Keys**: Hacker News OpenAI Leonardo.ai RunwayML Creatomate n8n Installation**: Ensure n8n is installed and configured locally or on a server. Credentials**: Set up credentials in n8n for all external services used in the workflow. Customization Replace Hacker News with any other data source node if needed. Configure the ""Article Analysis"" node for different topics. Adjust the cloud storage services or add custom storage options. Usage Import this workflow into your n8n instance. Configure your API credentials. Trigger the workflow manually or schedule it as needed. Check the outputs in your preferred cloud storage or social media platform. Notes Extend this workflow further by automating social media posting or newsletter integration. For any questions, refer to the official documentation or reach out to the creator. About the Creator This workflow was built by AlexK1919, an AI-native workflow automation architect. Check out the overview video for a quick demo. Tools Used Leonardo.ai** RunwayML** Creatomate** Hacker News API** OpenAI GPT** Feel free to adapt and extend this workflow to meet your specific needs! üéâ",46708,2024-11-19 16:37:48.304000+00:00,True,12
2552,"AI Powered Web Scraping with Jina, Google Sheets and OpenAI : the EASY way","Purpose of workflow: The purpose of this workflow is to automate scraping of a website, transforming it into a structured format, and loading it directly into a Google Sheets spreadsheet. How it works: Web Scraping: Uses the Jina AI service to scrape website data and convert it into LLM-friendly text. Information Extraction: Employs an AI node to extract specific book details (title, price, availability, image URL, product URL) from the scraped data. Data Splitting: Splits the extracted information into individual book entries. Google Sheets Integration: Automatically populates a Google Sheets spreadsheet with the structured book data. Step by step setup: Set up Jina AI service: Sign up for a Jina AI account and obtain an API key. Configure the HTTP Request node: Enter the Jina AI URL with the target website. Add the API key to the request headers for authentication. Set up the Information Extractor node: Use Claude AI to generate a JSON schema for data extraction. Upload a screenshot of the target website to Claude AI. Ask Claude AI to suggest a JSON schema for extracting required information. Copy the generated schema into the Information Extractor node. Configure the Split node: Set it up to separate the extracted data into individual book entries. Set up the Google Sheets node: Create a Google Sheets spreadsheet with columns for title, price, availability, image URL, and product URL. Configure the node to map the extracted data to the appropriate columns.",43837,2024-11-17 03:48:20.277000+00:00,True,4
2508,Generate SQL queries from schema only - AI-powered,"This workflow is a modification of the previous template on how to create an SQL agent with LangChain and SQLite. The key difference ‚Äì the agent has access only to the database schema, not to the actual data. To achieve this, SQL queries are made outside the AI Agent node, and the results are never passed back to the agent. This approach allows the agent to generate SQL queries based on the structure of tables and their relationships, without having to access the actual data. This makes the process more secure and efficient, especially in cases where data confidentiality is crucial. üöÄ Setup To get started with this workflow, you‚Äôll need to set up a free MySQL server and import your database (check Step 1 and 2 in this tutorial). Of course, you can switch MySQL to another SQL database such as PostgreSQL, the principle remains the same. The key is to download the schema once and save it locally to avoid repeated remote connections. Run the top part of the workflow once to download and store the MySQL chinook database schema file on the server. With this approach, we avoid the need to repeatedly connect to a remote db4free database and fetch the schema every time. As a result, we reach greater processing speed and efficiency. üó£Ô∏è Chat with your data Start a chat: send a message in the chat window. The workflow loads the locally saved MySQL database schema, without having the ability to touch the actual data. The file contains the full structure of your MySQL database for analysis. The Langchain AI Agent receives the schema, your input and begins to work. The AI Agent generates SQL queries and brief comments based solely on the schema and the user‚Äôs message. An IF node checks whether the AI Agent has generated a query. When: Yes: the AI Agent passes the SQL query to the next MySQL node for execution. No: You get a direct answer from the Agent without further action. The workflow formats the results of the SQL query, ensuring they are convenient to read and easy to understand. Once formatted, you get both the Agent answer and the query result in the chat window. üåü Example queries Try these sample queries to see the schema-driven AI Agent in action: Would you please list me all customers from Germany? What are the music genres in the database? What tables are available in the database? Please describe the relationships between tables. - In this example, the AI Agent does not need to create the SQL query. And if you prefer to keep the data private, you can manually execute the generated SQL query in your own environment using any database client or tool you trust üóÑÔ∏è üí≠ The AI Agent memory node does not store the actual data as we run SQL-queries outside the agent. It contains the database schema, user questions and the initial Agent reply. Actual SQL query results are passed to the chat window, but the values are not stored in the Agent memory.",38321,2024-10-29 21:14:34.146000+00:00,True,4
2534,Telegram AI bot assistant: ready-made template for voice & text messages,"Free template for voice & text messages with short-term memory This n8n workflow template is a blueprint for an AI Telegram bot that processes both voice and text messages. Ready to use with minimal setup. The bot remembers the last several messages (10 by default), understands commands and provides responses in HTML. You can easily swap GPT-4 and Whisper for other language and speech-to-text models to suit your needs. Core Features Text: send or forward messages Voice: transcription via Whisper Extend this template by adding LangChain tools. Requirements Telegram Bot API OpenAI API (for GPT-4 and Whisper) üí° New to Telegram bots? Check our step-by-step guide on creating your first bot and setting up OpenAI access. Use Cases Personal AI assistant Customer support automation Knowledge base interface Integration hub for services that you use: Connect to any API via HTTP Request Tool Trigger other n8n workflows with Workflow Tool",37867,2024-11-07 15:22:37.896000+00:00,True,5
2563,"‚ú® Vision-Based AI Agent Scraper - with Google Sheets, ScrapingBee, and Gemini","Important Notes: Check Legal Regulations: This workflow involves scraping, so ensure you comply with the legal regulations in your country before getting started. Better safe than sorry! Workflow Description: üòÆ‚Äçüí® Tired of struggling with XPath, CSS selectors, or DOM specificity when scraping ? This AI-powered solution is here to simplify your workflow! With a vision-based AI Agent, you can extract data effortlessly without worrying about how the DOM is structured. This workflow leverages a vision-based AI Agent, integrated with Google Sheets, ScrapingBee, and the Gemini-1.5-Pro model, to extract structured data from webpages. The AI Agent primarily uses screenshots for data extraction but switches to HTML scraping when necessary, ensuring high accuracy. Key Features: Google Sheets Integration**: Manage URLs to scrape and store structured results. ScrapingBee**: Capture full-page screenshots and retrieve HTML data for fallback extraction. AI-Powered Data Parsing**: Use Gemini-1.5-Pro for vision-based scraping and a Structured Output Parser to format extracted data into JSON. Token Efficiency**: HTML is converted to Markdown to optimize processing costs. This template is designed for e-commerce scraping but can be customized for various use cases.",35213,2024-11-21 16:28:54+00:00,True,6
2682,üîç Perplexity Research to HTML: AI-Powered Content Creation,"Transform simple queries into comprehensive, well-structured content with this n8n workflow that leverages Perplexity AI for research and GPT-4 for content transformation. Create professional blog posts and HTML content automatically while maintaining accuracy and depth. Intelligent Research & Analysis üöÄ Automated Research Pipeline Harnesses Perplexity AI's advanced research capabilities Processes complex topics into structured insights Delivers comprehensive analysis in minutes instead of hours üß† Smart Content Organization Automatically structures content with clear hierarchies Identifies and highlights key concepts Maintains technical accuracy while improving readability Creates SEO-friendly content structure Content Transformation Features üìù Dynamic Content Generation Converts research into professional blog articles Generates clean, responsive HTML output Implements proper semantic structure Includes metadata and categorization üé® Professional Formatting Responsive Tailwind CSS styling Clean, modern HTML structure Proper heading hierarchy Mobile-friendly layouts Blockquote highlighting for key insights Perfect For üìö Content Researchers Save hours of manual research by automating the information gathering and structuring process. ‚úçÔ∏è Content Writers Focus on creativity while the workflow handles research and technical formatting. üåê Web Publishers Generate publication-ready HTML content with modern styling and proper structure. Technical Implementation ‚ö° Workflow Components Webhook endpoint for query submission Perplexity AI integration for research GPT-4 powered content structuring HTML transformation engine Telegram notification system (optional) Transform your content creation process with an intelligent system that handles research, writing, and formatting while you focus on strategy and creativity.",34934,2024-12-29 22:20:41.173000+00:00,True,7
2642,"Analyze tradingview.com charts with Chrome extension, N8N and OpenAI","This flow is supported by a Chrome plugin created with Cursor AI. The idea was to create a Chrome plugin and a backend service in N8N to do chart analytics with OpenAI. It's a good sample on how to submit a screenshot from the browser to N8N. Who is it for? N8N developers who want to learn about using a Chrome plugin, an N8N webhook and OpenAI. What opportunity does it present? This sample opens up a whole range of N8N connected Chrome extensions that can analyze screenshots by using OpenAI. What this workflow does? The workflow contains: a webhook trigger an OpenAI node with GPT-4O-MINI and Analyze Image selected a response node to send back the Text that was created after analysing the screenshot. All this is needed to talk to the Chrome extension which is created with Cursor AI. The idea is to visit the tradingview.com crypto charts, click the Chrome plugin and get back analytics about the shown chart in understandable language. This is driven by the N8N flow. With the new image analytics capabilities of OpenAI this opens up a world of opportunities. Requirements/setup OpenAI API key Cursor AI installed The Chrome extension. Download The N8N JSON code. Download How to customize it to your needs? Both the Chrome extension and N8N flow can be adapted to use on other websites. You can consider: analyzing a financial screen and ask questions about the data shown analyzing other charts extending the N8N workflow with other AI nodes With AI and image analytics the sky is the limit and in some cases it saves you from creating complex API integrations. Download Chrome extension",33348,2024-12-16 10:08:04.476000+00:00,True,1
2718,AI agent for Instagram DM/inbox. Manychat + Open AI integration,"Automate Instagram DMs with OpenAI GPT and ManyChat How It Works: Once connected, GPT will automatically initiate conversations with messages from new recipients in Intagram. Who Is This For? This workflow is ideal for marketers, business owners content creators who want to automatically respond to Instagram direct messages using OpenAI GPT. By integrating ManyChat, you can manage conversations, nurture leads, and provide instant replies at scale. What This Workflow Does Captures** incoming Instagram DMs through ManyChat‚Äôs integration. Processes** messages with GPT to generate a relevant response. Delivers** instant replies back to Instagram users, creating efficient, AI-driven communication. Setup Import the Template: Copy the n8n workflow into your workspace. OpenAI Credentials: Add your OpenAI API key in n8n so GPT can generate responses. ManyChat Account: Create (or log in to) your ManyChat account. Connect Instagram: Link your Instagram profile as a channel in ManyChat. ManyChat Custom Field: Create a custom field for storing user input or conversation context. Configure Default Reply: In ManyChat, set up the default Instagram reply flow to point to your n8n webhook. Add External Request: Create an external request step in ManyChat to send messages to n8n. Test the Flow: Send yourself a DM on Instagram to confirm the workflow triggers and GPT responds correctly. Instructions and links: Notion instruction Register in ManyChat",28043,2025-01-12 12:45:33.424000+00:00,True,3
2705,Chat with GitHub API Documentation: RAG-Powered Chatbot with Pinecone & OpenAI,"This workflow demonstrates a Retrieval Augmented Generation (RAG) chatbot that lets you chat with the GitHub API Specification (documentation) using natural language. Built with n8n, OpenAI's LLMs and the Pinecone vector database, it provides accurate and context-aware responses to your questions about how to use the GitHub API. You could adapt this to any OpenAPI specification for any public or private API, thus creating a documentation chatbout that anyone in your company can use. How it works: Data Ingestion: The workflow fetches the complete GitHub API OpenAPI 3 specification directly from the GitHub repository. Chunking and Embeddings: It splits the large API spec into smaller, manageable chunks. OpenAI's embedding models then generate vector embeddings for each chunk, capturing their semantic meaning. Vector Database Storage: These embeddings, along with the corresponding text chunks, are stored in a Pinecone vector database. Chat Interface and Query Processing: The workflow provides a simple chat interface. When you ask a question, it generates an embedding for your query using the same OpenAI model. Semantic Search and Retrieval: Pinecone is queried to find the most relevant text chunks from the API spec based on the query embedding. Response Generation: The retrieved chunks and your original question are fed to OpenAI's gpt-4o-mini LLM, which generates a concise, informative, and contextually relevant answer, including code snippets when applicable. Set up steps: Create accounts: You'll need accounts with OpenAI and Pinecone. API keys: Obtain API keys for both services. Configure credentials: In your n8n environment, configure credentials for OpenAI and Pinecone using your API keys. Import the workflow: Import this workflow into your n8n instance. Pinecone Index: Ensure you have a Pinecone index named ""n8n-demo"" or adjust the workflow accordingly. The workflow is set up to work with this index out of the box. Setup Time: Approximately 15-20 minutes. Why use this workflow? Learn RAG in Action: This is a practical, hands-on example of how to build a RAG-powered chatbot. Adaptable Template: Easily modify this workflow to create chatbots for other APIs or knowledge bases. n8n Made Easy: See how n8n simplifies complex integrations between data sources, vector databases, and LLMs.",22955,2025-01-07 20:05:27.294000+00:00,True,9
3563,"Build an AI Powered Phone Agent üìûü§ñ with Retell, Google Calendar and RAG","This Workflow simulates an AI-powered phone agent with RetellAI with two main functions: üìÖ Appointment Booking ‚Äì It can schedule appointments directly into Google Calendar. üß† RAG-based Information Retrieval ‚Äì It provides answers using a Retrieval-Augmented Generation (RAG) system. For example, it can respond to questions such as store opening hours, return policies, or product details. The guide also explains how to purchase a dedicated phone number (with a +1 prefix) and link it to the AI agent. This setup is cost-effective, as it uses a FREE $10 credit to operate without additional charges in the beginning. ‚ú® Advantages üïê 24/7 Availability** ‚Äì The AI agent can answer calls and assist customers at any time. ü§ñ Automation** ‚Äì It reduces the workload on human staff by handling repetitive tasks like appointment scheduling and FAQ responses. üîå Easy Integration** ‚Äì Built with n8n, it‚Äôs flexible and customizable for various platforms and tools. üí∏ Low-cost Setup** ‚Äì Using the free credit, businesses can get started without an upfront investment. üì¶ Use Cases üõç E-commerce** ‚Äì Answer common product questions or order inquiries. üè¨ Retail Stores** ‚Äì Provide store hours, address info, and return policies. üçΩ Restaurants** ‚Äì Take reservations or share menu information. üíº Service Providers** ‚Äì Book appointments or consultations. üìû Any Local Business** ‚Äì Offer phone support without needing a live operator. How It Works This Workflow simulates an AI-powered phone agent with two primary functions: Appointment Booking The workflow captures call events (e.g., call_ended or call_analyzed) and extracts key details (transcript, caller info, duration, etc.). Using OpenAI, it summarizes the conversation and parses structured data (e.g., names, contact info, dates). For scheduling, it converts user-provided dates into Google Calendar-compatible formats and creates events automatically. RAG-Based Information Retrieval When a query is received (e.g., store hours, product details), the workflow retrieves relevant information from a Qdrant vector store. An AI agent processes the query using the retrieved data and responds via a webhook, ensuring accurate, context-aware answers. Set Up Steps Prepare Qdrant Vector Store Create/refresh a Qdrant collection (via HTTP requests). Upload and vectorize documents (e.g., from Google Drive) using OpenAI embeddings. Configure RetellAI Agent Sign up for RetellAI, create an agent, and set the webhook URLs (n8n_call for call events, n8n_rag_function for RAG queries). Purchase a Twilio phone number and link it to the agent. n8n Workflow Setup Connect OpenAI, Qdrant, Google Calendar, and Telegram nodes with credentials. Customize prompts for summarization, date parsing, and RAG responses. Test the workflow to ensure data flows from call events ‚Üí processing ‚Üí actions (e.g., calendar bookings, Telegram alerts). Deploy Trigger the workflow via RetellAI webhooks during calls. Monitor outputs (e.g., call summaries in Telegram, calendar events). Note: Replace placeholders (e.g., QDRANTURL, COLLECTION, CHAT_ID) with actual values. Need help customizing? Contact me for consulting and support or add me on Linkedin.",22555,2025-04-15 12:55:32.914000+00:00,True,12
2883,Open Deep Research - AI-Powered Autonomous Research Workflow,"Open Deep Research - AI-Powered Autonomous Research Workflow Description This workflow automates deep research by leveraging AI-driven search queries, web scraping, content analysis, and structured reporting. It enables autonomous research with iterative refinement, allowing users to collect, analyze, and summarize high-quality information efficiently. How it works üîπ User Input The user submits a research topic via a chat message. üß† AI Query Generation A Basic LLM generates up to four refined search queries to retrieve relevant information. üîé SERPAPI Google Search The workflow loops through each generated query and retrieves top search results using the SerpAPI API. üìÑ Jina AI Web Scraping Extracts and summarizes webpage content from the URLs obtained via SerpAPI. üìä AI-Powered Content Evaluation An AI Agent evaluates the relevance and credibility of the extracted content. üîÅ Iterative Search Refinement If the AI finds insufficient or low-quality information, it generates new search queries to improve results. üìú Final Report Generation The AI compiles a structured markdown report, including sources with citations. Set Up Instructions üöÄ Estimated setup time: ~10-15 minutes ‚úÖ Required API Keys:** SerpAPI ‚Üí For Google Search results Jina AI ‚Üí For text extraction OpenRouter ‚Üí For AI-driven query generation and summarization ‚öôÔ∏è n8n Components Used:** AI Agents with memory buffering for iterative research Loops to process multiple search queries efficiently HTTP Requests for direct API interactions with SerpAPI and Jina AI üìù Recommended Enhancements:** Add sticky notes in n8n to explain each step for new users Implement Google Drive or Notion Integration to save reports automatically üéØ Ideal for: ‚úîÔ∏è Researchers & Analysts - Automate background research ‚úîÔ∏è Journalists - Quickly gather reliable sources ‚úîÔ∏è Developers - Learn how to integrate multiple AI APIs into n8n ‚úîÔ∏è Students - Speed up literature reviews üîó Completely free and open-source! üöÄ",21345,2025-02-11 16:42:07.823000+00:00,True,6
2768,ü§ñüîç The Ultimate Free AI-Powered Researcher with Tavily Web Search & Extract,"üîç This n8n workflow integrates Tavily's search and extract APIs with AI summarization capabilities to process web content efficiently. Quick Setup Get your Tavily API key from https://app.tavily.com/home Replace tvly-YOUR_API_KEY in the ""Tavily API Key"" node Connect your OpenAI credentials to the ""OpenAI Chat Model"" node Deploy the workflow and start the chat trigger Core Features Search & Extract üéØ Intelligent web searching with relevance filtering Automated content extraction from top results AI-powered content summarization in markdown format User Interaction üí¨ Chat-based search topic input Real-time processing pipeline Structured markdown output The workflow demonstrates practical implementation of Tavily's API endpoints while handling the complete process from search to summarization in a single automated pipeline.",21257,2025-01-21 16:42:02.076000+00:00,True,3
2824,Query Perplexity AI from your n8n workflows,"This workflow illustrates how to use Perplexity AI in your n8n workflow. Perplexity is a free AI-powered answer engine that provides accurate, trusted, and real-time answers to any question. Credentials Setup 1/ Go to the perplexity dashboard, purchase some credits and create an API Key https://www.perplexity.ai/settings/api 2/ In the perplexity Request node, use Generic Credentials, Header Auth. For the name, use the value ""Authorization"" And for the value ""Bearer pplx-e4...59ea"" (Your Perplexity Api Key) AI Model Sonar Pro is the current top model used by perplexity. If you want to use a different one, check this page: https://docs.perplexity.ai/guides/model-cards",21101,2025-01-30 18:22:09.713000+00:00,True,1
2799,"AI-Powered Information Monitoring with OpenAI, Google Sheets, Jina AI and Slack","Check Legal Regulations: This workflow involves scraping, so ensure you comply with the legal regulations in your country before getting started. Better safe than sorry! üìå Purpose This workflow enables automated and AI-driven topic monitoring, delivering concise article summaries directly to a Slack channel in a structured and easy-to-read format. It allows users to stay informed on specific topics of interest effortlessly, without manually checking multiple sources, ensuring a time-efficient and focused monitoring experience. To get started, copy the Google Sheets template required for this workflow from here. üéØ Target Audience This workflow is designed for: Industry professionals** looking to track key developments in their field. Research teams** who need up-to-date insights on specific topics. Companies** aiming to keep their teams informed with relevant content. ‚öôÔ∏è How It Works Trigger: A Scheduler initiates the workflow at regular intervals (default: every hour). Data Retrieval: RSS feeds are fetched using the RSS Read node. Previously monitored articles are checked in Google Sheets to avoid duplicates. Content Processing: The article relevance is assessed using OpenAI (GPT-4o-mini). Relevant articles are scraped using Jina AI to extract content. Summaries are generated and formatted for Slack. Output: Summaries are posted to the specified Slack channel. Article metadata is stored in Google Sheets for tracking. üõ†Ô∏è Key APIs and Nodes Used Scheduler Node:** Triggers the workflow periodically. RSS Read:** Fetches the latest articles from defined RSS feeds. Google Sheets:** Stores monitored articles and manages feed URLs. OpenAI API (GPT-4o-mini):** Classifies article relevance and generates summaries. Jina AI API:** Extracts the full content of relevant articles. Slack API:** Posts formatted messages to Slack channels. This workflow provides an efficient and intelligent way to stay informed about your topics of interest, directly within Slack.",18013,2025-01-25 11:55:56.526000+00:00,True,7
2956,‚ö°üìΩÔ∏è Ultimate AI-Powered Chatbot for YouTube Summarization & Analysis,"üé• YouTube Video AI Agent Workflow This n8n workflow template allows you to interact with an AI agent that extracts details and the transcript of a YouTube video using a provided video ID. Once the details and transcript are retrieved, you can chat with the AI agent to explore or analyze the video's content in a conversational and insightful manner. üåü How the Workflow Works üîó Input Video ID: The user provides a YouTube video ID as input to the workflow. üìÑ Data Retrieval: The workflow fetches essential details about the video (e.g., title, description, upload date) and retrieves its transcript using YouTube's Data API and additional tools for transcript extraction. ü§ñ AI Agent Interaction: The extracted details and transcript are processed by an AI-powered agent. Users can then ask questions or engage in a conversation with the agent about the video's content, such as: Summarizing the transcript. Analyzing key points. Clarifying specific sections. üí¨ Dynamic Responses: The AI agent uses natural language processing (NLP) to generate contextual and accurate responses based on the video data, ensuring a smooth and intuitive interaction. üöÄ Use Cases üìä Content Analysis**: Quickly analyze long YouTube videos by querying specific sections or extracting summaries. üìö Research and Learning**: Gain insights from educational videos or tutorials without watching them entirely. ‚úçÔ∏è Content Creation**: Repurpose transcripts into blogs, social media posts, or other formats efficiently. ‚ôø Accessibility**: Provide an alternative, text-based way to interact with video content for users who prefer reading over watching. üõ†Ô∏è Resources for Getting Started Google Cloud Console** (for API setup): Visit Google Cloud's Get Started Guide to configure your API access. YouTube Data API Key Setup**: Follow this guide to create and manage your YouTube Data API key. Install n8n Locally**: Refer to this installation guide for setting up n8n on your local machine. ‚ú® Sample Prompts ""Tell me about this YouTube video with id: JWfNLF_g_V0"" ""Can you provide a list of key takeaways from this video with id: [youtube-video-id]?""",17635,2025-02-20 18:58:33.160000+00:00,True,6
2812,Scrape any web page into structured JSON data with ScrapeNinja and AI,"Disclaimer: This template only works on self-hosted for now, as it uses a community node. Use Case Web scrapers often break due to web page layout changes. This workflow attempts to mitigate this problem by auto-generating web scraping data extractor code via LLM. How It Works This workflow leverages ScrapeNinja n8n community node to: scrape webpage HTML, feed it into LLM (Google Gemini) and ask to write a JS extractor function code, then it executes the written JS extractor against scraped HTML to extract useful data from webpage (the code is safely executed in a sandbox) Installation To install ScrapeNinja n8n node, in your self-hosted instance, go to Settings -&gt; Community nodes, enter ""n8n-nodes-scrapeninja"", and install. Make sure you are using at least v0.3.0. See this in action: https://www.linkedin.com/feed/update/urn:li:activity:7289659870935490560/",17135,2025-01-28 07:07:34.371000+00:00,True,2
3053,"Technical stock analysis with Telegram, Airtable and a GPT-powered AI Agent","Video Guide I prepared a detailed guide that demonstrates the complete process of building a trading agent automation using n8n and Telegram, seamlessly integrating various functions for stock analysis. Youtube Link Who is this for? This workflow is perfect for traders, financial analysts, and developers looking to automate stock analysis interactions via Telegram. It‚Äôs especially valuable for those who want to leverage AI tools for technical analysis without needing to write complex code. What problem does this workflow solve? Many traders desire real-time analysis of stock data but lack the technical expertise or tools to perform in-depth analysis. This workflow allows users to easily interact with an AI trading agent through Telegram for seamless stock analysis, chart generation, and technical evaluation, all while eliminating the need for manual interventions. What this workflow does This workflow utilizes n8n to construct an end-to-end automation process for stock analysis through Telegram communication. The setup involves: Receiving messages via a Telegram bot. Processing audio or text messages for trading queries. Transcribing audio using OpenAI API for interpretation. Gathering and displaying charts based on user-specified parameters. Performing technical analysis on generated charts. Sending back the analyzed results through Telegram. Setup Prepare Airtable: Create simple table to store tickers. Prepare Telegram Bot: Ensure your Telegram bot is set up correctly and listening for new messages. Replace Credentials: Update all nodes with the correct credentials and API keys for services involved. Configure API Endpoints: Ensure chart service URLs are correctly set to interact with the corresponding APIs properly. Start Interaction: Message your bot to initiate analysis; specify ticker symbols and desired chart styles as required.",16634,2025-03-01 17:09:30.445000+00:00,True,8
3089,Reply to Outlook Emails with OpenAI,"Who is this template for? This template is for any Microsoft Outlook user who wants a trained AI agent to reason and reply on their behalf. Teach your agent tone and writing style to replicate your own, or develop a persona for a shared inbox. Requirements Outlook with authentication credentials OpenAI account with authentication credentials A few sample email replies of various lengths and topics How it works: Connect your Outlook account. Select (filter) which email sender(s) your trained AI agent will reply to. [Tip: pick a sender that has some repeatability either with a topic (ie. sales) or an individual (coworker@yourcompany.com)] Connect your OpenAI account. Choose your AI model (ie. gpt-4o-mini) Add Prompt (User Message) and select ""system message"" from the option below Update the instructions by filling in your name (or persona), response style, and add full email replies from the topic or individual you want the AI agent to emulate. [Tip: Add actual replies from your email sent folder, including your greeting and sign off. Paste each email sample between a set of &lt;example&gt; .... &lt;/example&gt; tags] Configure the reply (or reply all) to remain within the original email string Test it! Send an email from the address to which your agent wants to respond. Check your sent (or draft) folder for the result. Enjoy all the free time you now have!! If you have questions or need assistance, email us at: support@teambisonandbird.com ++This template does not include retrieving email addresses out of the message or body of the email.++",16178,2025-03-05 20:50:48.843000+00:00,True,2
2907,"A Very Simple ""Human in the Loop"" Email Response System Using AI and IMAP","Functionality This workflow automates the handling of incoming emails by summarizing their content, generating appropriate responses, and validating the responses through a ""Human-in-the-Loop"" system. It integrates with IMAP email services (e.g., Gmail, Outlook) and uses AI models to streamline the email response process. The workflow ensures that all AI-generated responses are reviewed by a human before being sent, maintaining a high level of professionalism and accuracy. This approach is particularly useful for businesses that receive a high volume of emails and need to respond quickly while ensuring quality control. How It Works Email Trigger: The workflow starts with the Email Trigger (IMAP) node, which monitors an email inbox for new messages. When a new email arrives, it triggers the workflow. Email Preprocessing: The Markdown node converts the email's HTML content into plain text for easier processing by the AI models. Email Summarization: The Email Summarization Chain node uses an AI model (OpenAI) to generate a concise summary of the email. The summary is limited to 100 words and is written in a professional tone. Email Response Generation: The Write email node uses an AI model (OpenAI) to draft a professional response to the email. The response is based on the email content and is limited to 100 words. Human-in-the-Loop Approval: The Set Email text node prepares the drafted response for approval. The Approve Email node sends the drafted response to a human approver (e.g., an internal email address) for review. The email includes: The original message. The AI-generated response. The Approved? node checks if the response has been approved by the human reviewer. If approved, the workflow proceeds to send the response; otherwise, it stops. Sending the Response: The Send Email node sends the approved response back to the original sender. Key Features Automated Email Summarization**: Summarizes incoming emails to provide a quick overview of the content. AI-Powered Response Generation**: Drafts professional responses to emails using AI. Human-in-the-Loop Approval**: Ensures all AI-generated responses are reviewed and approved by a human before being sent. IMAP Integration**: Works with IMAP email services like Gmail and Outlook. Efficient Email Management**: Reduces the time and effort required to handle incoming emails while maintaining high-quality responses. This workflow is ideal for businesses looking to automate their email response process while maintaining control over the quality of outgoing communications. It leverages AI to handle repetitive tasks and ensures that all responses are reviewed by a human, providing a balance between automation and human oversight. Need help customizing? Contact me for consulting and support or add me on Linkedin.",16049,2025-02-14 16:31:27.525000+00:00,True,4
3005,‚ú®üî™ Advanced AI Powered Document Parsing & Text Extraction with Llama Parse,"Description This workflow automates document processing using LlamaParse to extract and analyze text from various file formats. It intelligently processes documents, extracts structured data, and delivers actionable insights through multiple channels. How It Works Document Ingestion & Processing üìÑ Monitors Gmail for incoming attachments or accepts documents via webhook Validates file formats against supported LlamaParse extensions Uploads documents to LlamaParse for advanced text extraction Stores original documents in Google Drive for reference Intelligent Document Analysis üß† Automatically classifies document types (invoices, reports, etc.) Extracts structured data using customized AI prompts Generates comprehensive document summaries with key insights Converts unstructured text into organized JSON data Invoice Processing Automation üíº Extracts critical invoice details (dates, amounts, line items) Organizes financial data into structured formats Calculates tax breakdowns, subtotals, and payment information Maintains detailed records for accounting purposes Multi-Channel Delivery üì± Saves extracted data to Google Sheets for tracking and analysis Sends concise summaries via Telegram for immediate review Creates searchable document archives in Google Drive Updates spreadsheets with structured financial information Setup Steps Configure API Credentials üîë Set up LlamaParse API connection Configure Gmail OAuth for email monitoring Set up Google Drive and Sheets integrations Add Telegram bot credentials for notifications Customize AI Processing ‚öôÔ∏è Adjust document classification parameters Modify extraction templates for specific document types Fine-tune summary generation prompts Customize invoice data extraction schema Test and Deploy üöÄ Test with sample documents of various formats Verify data extraction accuracy Confirm notification delivery Monitor processing pipeline performance",15905,2025-02-25 00:38:08.693000+00:00,True,8
3054,Generate AI Videos from Text with HeyGen and Voice Cloning.,"üé• AI Video Generator with HeyGen üöÄ Create AI-Powered Videos in n8n with HeyGen This workflow enables you to generate realistic AI videos using HeyGen, an advanced AI platform for video automation. Simply input your text, choose an AI avatar and voice, and let HeyGen generate a high-quality video for you ‚Äì all within n8n! ‚úÖ Ideal for: Content creators & marketers üèÜ Automating personalized video messages üì© AI-powered video tutorials & training materials üéì üîß How It Works 1Ô∏è‚É£ Provide a text script ‚Äì This will be spoken in the AI-generated video. 2Ô∏è‚É£ Select an Avatar & Voice ‚Äì Choose from a variety of AI-generated avatars and voices. 3Ô∏è‚É£ Run the workflow ‚Äì HeyGen processes your request and generates a video. 4Ô∏è‚É£ Download your video ‚Äì Get the direct link to your AI-powered video! ‚ö° Setup Instructions 1Ô∏è‚É£ Get Your HeyGen API Key Sign up for a HeyGen account. Go to your account settings and retrieve your API Key. 2Ô∏è‚É£ Configure n8n Credentials In n8n, create new credentials and select ""Custom Auth"" as the authentication type. In the Name provide : X-Api-Key And in the value paste your API key from Heygen Update the 2 http node with the right credentials. 3Ô∏è‚É£ Select an AI Avatar & Voice Browse available avatars & voices in your HeyGen account. Copy the Avatar ID and Voice ID for your video. 4Ô∏è‚É£ Run the Workflow Enter your text, avatar ID, and voice ID. Execute the workflow ‚Äì your video will be generated automatically! üéØ Why Use This Workflow? ‚úîÔ∏è Fully Automated ‚Äì No manual editing required! ‚úîÔ∏è Realistic AI Avatars ‚Äì Choose from a variety of digital avatars. ‚úîÔ∏è Seamless Integration ‚Äì Works directly within your n8n workflow. ‚úîÔ∏è Scalable & Fast ‚Äì Generate multiple videos in minutes. üîó Start automating AI-powered video creation today with n8n & HeyGen!",15452,2025-03-01 18:04:31.929000+00:00,True,1
2957,üí°üåê Essential Multipage Website Scraper with Jina.ai,"üí°üåê Essential Multipage Website Scraper with Jina.ai Use responsibly and follow local rules and regulations This N8N workflow enables automated multi-page website scraping using Jina.ai's powerful web scraping capabilities, with seamless integration to Google Drive for content storage. Here's how it works: Main Features The workflow automatically scrapes multiple pages from a website's sitemap and saves each page's content as a separate Google Drive document. Key Components Input Configuration Starts with a sitemap URL (default: https://ai.pydantic.dev/sitemap.xml)** Processes the sitemap to extract individual page URLs Includes filtering options to target specific topics or pages Scraping Process Uses Jina.ai's web scraper to extract content from each URL Converts webpage content into clean markdown format Extracts page titles automatically for document naming Storage Integration Creates individual Google Drive documents for each scraped page Names documents using the format ""URL - Page Title"" Saves content in markdown format for better readability Usage Instructions Set your target website's sitemap URL in the ""Set Website URL"" node Configure the ""Filter By Topics or Pages"" node to select specific content Adjust the ""Limit"" node (default: 20 pages) to control batch size Connect your Google Drive account Run the workflow to begin automated scraping Additional Features Built-in rate limiting through the Wait node to prevent overloading servers Batch processing capability for handling large sitemaps The workflow requires no API key for Jina.ai, making it accessible for immediate use while maintaining responsible scraping practices.",15070,2025-02-20 20:24:51.925000+00:00,False,3
4635,Extract & Enrich LinkedIn Comments to Leads with Apify ‚Üí Google Sheets/CSV,"üöÄ LinkedIn Comments to Leads Extractor & Enricher (Apify) ‚Üí Google Sheets / CSV Overview Automate LinkedIn lead generation by scraping comments from targeted posts and enriching profiles with detailed data This n8n workflow automatically extracts leads from LinkedIn post comments using Apify's powerful scrapers (no LinkedIn login required), enriches the data with additional profile information, and exports everything to Google Sheets or CSV format. ‚ú® Key Features üîç No Login Required: Scrape LinkedIn data without sharing credentials üí∞ Cost-Effective: First 1,000 comments are free with Apify üìä Data Enrichment: Enhance basic comment data with full profile details üìà Export Options: Choose between Google Sheets or CSV output üéØ Targeted Scraping: Focus on specific posts for quality leads üõ†Ô∏è Apify Scrapers Used 1. LinkedIn Post Comments Scraper Tool**: LinkedIn Post Comments, Replies, Engagements Scraper | No Cookies Pricing**: $5.00 per 1,000 results Function**: Extracts all comments and engagement data from specified LinkedIn posts 2. LinkedIn Profile Batch Scraper Tool**: LinkedIn Profile Details Batch Scraper (No Cookies Required) Pricing**: $5.00 per 1,000 results Function**: Enriches scraped profiles with detailed information &gt; üí° Free Tier: Apify provides 1,000 free scraped comments to get you started! üìã Prerequisites Required API Credentials Apify Token Add your APIFY_TOKEN to the workflow credentials Get your token from Apify Console Google Sheets Credentials (if using Sheets export) Configure OAuth credentials for Google Sheets integration Follow n8n's Google Sheets setup guide üîÑ Workflow Process Default Mode: Form-Based Execution Manual Trigger ‚Üí Launches the workflow Form Submission ‚Üí User-friendly form for inputting LinkedIn post URLs Comment Scraping ‚Üí Apify extracts all comments from specified posts Profile Enrichment ‚Üí Additional profile data gathered for each commenter Data Processing ‚Üí Creates unique, enriched lead list Google Sheets Export ‚Üí Automatically populates your spreadsheet Result: You'll be redirected to a Google Sheets document containing all enriched leads Alternative Mode: CSV Export For users preferring CSV output: Disable: Form trigger nodes Enable: Manual trigger node Disable: Google Sheets export nodes Enable: CSV download nodes Configure: Add post IDs/URLs in ""Set manual fields"" node Execute: Run workflow and download CSV from the CSV node üìä Output Data Structure Your exported data will include: Basic Info**: Name, headline, location Profile Details**: Company, position, industry Engagement Data**: Comment content, engagement metrics Contact Info**: Available profile links and connections Enriched Data**: Additional profile insights from Apify üí° Pro Tips Quality over Quantity**: Target posts with high-quality, relevant engagement Monitor Costs**: Track your Apify usage to stay within budget Data Hygiene**: Regularly clean and deduplicate your lead lists Compliance**: Ensure your scraping activities comply with LinkedIn's terms of service üÜò Troubleshooting Common Issues: Authentication Errors**: Verify your Apify token is correctly configured Empty Results**: Check that your LinkedIn post URLs are valid and public Export Failures**: Ensure Google Sheets credentials are properly set up Need Help? Contact Saverflow.ai for support and custom workflow development.",14729,2025-06-04 02:39:54.846000+00:00,False,3
3131,Chatbot Appointment Scheduler With Google Calendar for Dental assistant,"This workflow template is designed for dental assistants and anyone looking to automate appointment scheduling. It integrates Google Calendar for booking appointments and Google Sheets as a database to store patient information. How It Works The user interacts with the chatbot to schedule an appointment. The chatbot collects necessary details and checks availability via Google Calendar. If the requested time is available, the AI books the appointment. If unavailable, the AI suggests alternative time slots. Once booked, the AI logs the appointment details into Google Sheets for record-keeping. Setup Instructions üìå Watch this üé• Setup Video for detailed instructions on running and customizing this workflow. Step 1: Set Up Credentials OpenAI API Key (for chatbot functionality). Google Account (for Google Sheets & Google Calendar integration). Step 2: Choose the Right Tools Select the correct Google Calendar in the Google Calendar tool. Choose the appropriate Google Sheets file in the Google Sheets tool. Step 3: Test Run a test to ensure everything works correctly. Once tested. Example Templates Below are sample Google Sheets template to help you get started.",14481,2025-03-11 06:54:18.228000+00:00,True,3
3004,üöÄ TikTok Video Automation Tool ‚ú® ‚Äì Highly Optimized with OpenAI & Replicate,"üöÄ TikTok Video Automation Tool ‚ú® (Frequent Updates) Create viral TikTok videos effortlessly ‚Äî no editing skills required! Who is this for? üéØ ‚úÖ Content Creators: Pump out engaging short videos in minutes. ‚úÖ Marketing Agencies: Deliver high-quality client content ‚Äî fast. ‚úÖ Business Owners: Promote your brand with attention-grabbing TikToks. What problem does this solve? üõ†Ô∏è Short-form video content is king, but creating it takes time, skill, and effort. This tool automates the entire process, from scriptwriting to video production, ensuring SEO-optimized, high-quality content without any manual editing. How it works üåü 1Ô∏è‚É£ Enter your video idea and choose where to receive the final video (TikTok upload or link via WhatsApp, Gmail, etc.). 2Ô∏è‚É£ AI crafts a high-engagement script with SEO optimization. 3Ô∏è‚É£ Voiceover is generated with ultra-realistic AI narration. 4Ô∏è‚É£ Relevant visuals are selected to perfectly match the script. 5Ô∏è‚É£ Your TikTok video is assembled and either directly uploaded to TikTok or delivered via a shareable link. Quick & Easy Setup ‚öôÔ∏è (5-10 min) üîπ Connect required APIs (most have free plans). üîπ Follow the step-by-step setup (video tutorial included). üîπ Start generating professional TikTok videos instantly! Required APIs üîó Content & Voiceover:** OpenAI (Paid), ElevenLabs (Free) Media Processing:** Cloudinary (Free), Replicate (Paid) Video Assembly:** 0codekit (Free), Creatomate (Free) Optional Integrations üîó Messaging & Sharing:** WhatsApp, Telegram, Gmail, Outlook Direct Upload:** TikTok Customization üé® Adjust script styles & voiceover preferences. Modify visuals to align with your brand. Optimize video length and format. üöÄ Start automating your TikTok content today and grow your audience on autopilot!",14311,2025-02-24 22:44:59.697000+00:00,True,6
2941,üé¨ YouTube Shorts Automation Tool üöÄ,"üé¨ YouTube Shorts Automation Tool üöÄ Automate the creation of high-performing YouTube Shorts in minutes! Who is this for? üéØ Content Creators**: Generate engaging short videos effortlessly. Marketing Agencies**: Produce client-ready content quickly. Business Owners**: Promote products/services through viral short-form content. What problem does this solve? üõ†Ô∏è Creating short-form video content is time-consuming, complex, and skill-intensive. This workflow automates video creation, eliminating the need for video editing expertise while ensuring SEO optimization, high-quality visuals, and professional voiceovers. How it works üåü Enter your video idea into the chat interface. AI generates a script optimized for engagement and SEO. Voiceover is created with realistic AI narration. Relevant visuals are selected to match the script. The video is assembled and delivered via a shareable link. Setup ‚öôÔ∏è (5-10 min) Connect required APIs (most have free tiers). Follow the guided setup (video tutorial included). Start generating professional YouTube Shorts instantly! Required APIs üîó Content Generation**: OpenAI, ElevenLabs (script & voiceover) Media Processing**: Cloudinary, Replicate (images & storage) Integration Tools**: 0codekit, Creatomate (video assembly) Customization üé® Adjust script styles & voiceover preferences. Modify visuals to match your brand. Optimize video length and format. üöÄ Start automating your YouTube Shorts today and grow your audience effortlessly!",14048,2025-02-19 01:50:47.837000+00:00,True,2
2931,AI Agent with Ollama for current weather and wiki,"This workflow template demonstrates how to create an AI-powered agent that provides users with current weather information and Wikipedia summaries. By integrating n8n with Ollama's local Large Language Models (LLMs), this template offers a seamless and privacy-conscious solution for real-time data retrieval and summarization. Who is this for? Developers and Enthusiasts: Individuals interested in building AI-driven workflows without relying on external APIs. Privacy-Conscious Users: Those who prefer processing data locally to maintain control over their information. Educators and Students: Learners seeking hands-on experience with AI integrations and workflow automation. What problem does this workflow solve? Accessing up-to-date weather information and concise Wikipedia summaries typically requires multiple API calls to external services, which can raise privacy concerns and incur costs. This workflow addresses these issues by utilizing Ollama's self-hosted LLMs within n8n, enabling users to retrieve and process information locally. What this workflow does: User Input Capture: Begins with a chat interface where users can input queries. AI Processing: The input is sent to an AI Agent node configured with Ollama's LLMs, which interprets the query and determines the required actions. Weather Retrieval: For weather-related queries, the workflow fetches current weather data from a specified source. Wikipedia Summarization: For queries seeking information, it retrieves relevant Wikipedia content and generates concise summaries. Setup: Install Required Tools: Ollama: Install and run Ollama to manage local LLMs. Configure n8n Workflow: Import the provided workflow template into your n8n instance. Set up the AI Agent node to connect with Ollama's API. Ensure nodes responsible for fetching weather data and Wikipedia content are correctly configured. Run the Workflow: Start the workflow and interact with the chat interface to test various queries. How to customize this workflow to your needs: Automate Triggers: Set up scheduled triggers to provide users with regular updates, such as daily weather forecasts or featured Wikipedia articles.",13401,2025-02-18 04:00:45.136000+00:00,True,5
3132,Extract Google Trends Keywords & Summarize Articles in Google Sheets,"Stay ahead of trends by automating your content research. This workflow fetches trending keywords from Google Trends RSS, extracts key insights from top articles, and saves structured summaries in Google Sheets‚Äîhelping you build a data-driven editorial plan effortlessly. How it works Fetch Google Trends RSS ‚Äì The workflow retrieves trending keywords along with three related article links. Extract & Process Content ‚Äì It fetches the content of these articles, cleans the HTML, and generates a concise summary using Jina AI. Store in Google Sheets ‚Äì The processed insights, including the trending keyword and summary, are saved in a pre-configured Google Sheet. Setup Steps Prepare a Google Sheet ‚Äì Ensure you have a Google Sheet ready to store the extracted data. Configure API Access ‚Äì Set up Google Sheets API and any required authentication. Get Jina.ai API key Adjust Workflow Settings ‚Äì A dedicated configuration node allows you to fine-tune how data is processed and stored. Customization Modify the RSS source to focus on specific Google Trends regions or categories. Adjust the content processing logic to refine how article summaries are created. Expand the workflow to integrate with CMS (e.g., WordPress) for automated content planning. This workflow is ideal for content strategists, SEO professionals, and news publishers who want to quickly identify and act on trending topics without manual research. üöÄ Google Sheets Fields Copy and paste these column headers into your Google Sheet: | Column Name | Description | |------------------------|-------------| | status | Initial status of the keyword (e.g., ""idea"") | | trending_keyword | Trending keyword extracted from Google Trends | | approx_traffic | Estimated traffic for the trending keyword | | pubDate | Date when the keyword was fetched | | news_item_url1 | URL of the first related news article | | news_item_title1 | Title of the first news article | | news_item_url2 | URL of the second related news article | | news_item_title2 | Title of the second news article | | news_item_url3 | URL of the third related news article | | news_item_title3 | Title of the third news article | | news_item_picture1 | Image URL from the first news article | | news_item_source1 | Source of the first news article | | news_item_picture2 | Image URL from the second news article | | news_item_source2 | Source of the second news article | | news_item_picture3 | Image URL from the third news article | | news_item_source3 | Source of the third news article | | abstract | AI-generated summary of the articles (limited to 49,999 characters) | Instructions Open Google Sheets and create a new spreadsheet. Copy the column names from the table above. Paste them into the first row of your Google Sheet.",13321,2025-03-11 08:17:16.863000+00:00,False,3
3146,Download TikTok Videos without Watermarks and Upload to Google Drive,"Who is this for? Content creators, social media managers, digital marketers, and researchers who need to download original TikTok videos without watermarks for analysis, repurposing, or archiving purposes. What problem does this workflow solve? Downloading TikTok videos without watermarks typically requires using questionable third-party websites that may have limitations, ads, or privacy concerns. This workflow provides a clean, automated solution that can be integrated into your own systems and processes. What this workflow does This workflow automates the process of downloading TikTok videos without watermarks in three simple steps: Fetch the TikTok video page by providing the video URL Extract the raw video URL from the page's HTML data Download the original video file without watermark (Optional) Upload to Google Drive with public sharing link generation The workflow uses web scraping techniques to extract the original video source directly from TikTok's own servers, maintaining the highest possible quality without any added watermarks or branding. Setup (Est. time: 5-10 minutes) Before getting started, you'll need: n8n installation The URL of a TikTok you want to download (Optional) Google Drive API enabled in Google Cloud Console with OAuth Client ID and Client Secret credentials if you want to use the upload feature How to customize this workflow to your needs Replace the example TikTok URL with your desired video links Modify the file naming convention for downloaded videos Integrate with other nodes to process videos after downloading Create a webhook to trigger the workflow from external applications Set up a schedule to regularly download videos from specific accounts This workflow can be extended to support various use cases like trending content analysis, competitor research, creating compilation videos, or building a content library for inspiration. It provides a foundation that can be customized to fit into larger automated workflows for content creation and social media management.",13087,2025-03-12 19:10:36.178000+00:00,False,3
2971,Automated Faceless YouTube Video Generator Using Leonardo AI and Creatomate,"Automate Your Video Content Creation: AI-Powered Video Generation This n8n template streamlines the creation of high-quality, faceless videos, automating the entire process from scriptwriting to final video production. Leveraging AI tools like Leonardo AI and Creatomate, this workflow empowers content creators to efficiently produce engaging videos without on-camera presence. Who is this for? This template is ideal for: Content creators looking to scale their presence on YouTube, Instagram, and TikTok. Marketers seeking to automate video marketing campaigns. Anyone wanting to produce professional-looking videos quickly and easily. Individuals wishing to create faceless video content. What problem is this workflow solving? Creating engaging videos can be time-consuming and resource-intensive. This workflow eliminates the manual effort involved in scripting, image sourcing, and video editing, allowing creators to focus on strategy and content ideation. It addresses the challenge of consistent video production by automating repetitive tasks. What this workflow does: This workflow automates the following steps: AI-Driven Scriptwriting:** Uses an LLM (default: DeepSeek V3) to generate a compelling video script based on your system prompt and desired number of scenarios. AI Image Generation:** Integrates with Leonardo AI to create visually appealing images for each scenario in the script. You define the image style and resolution. Automated Video Editing:** Connects to Creatomate to assemble the script and images into a polished video, ready for upload. You‚Äôll use a pre-created ‚ÄúAI generated story template‚Äù within Creatomate. Setup: Video Script Generation:** Provide a detailed system prompt describing your video‚Äôs topic, target audience, and key message. Specify the number of scenarios. Select your preferred LLM (DeepSeek V3 is the default). Image Generation:** Connect your Leonardo AI account. Choose your preferred image generation model and set the video resolution. Craft a detailed prompt for Leonardo AI, defining the image style (e.g., ‚Äúcinematic,‚Äù ‚Äúcartoon,‚Äù ‚Äúrealistic‚Äù). Video Editing:** Link your Creatomate account. Create an empty ‚ÄúAI generated story template‚Äù in Creatomate. Copy the cURL from the Creatomate template to the video generation node. Link your ElevenLabs account to Creatomate and choose the desired voice. How to customize this workflow: Script Customization:** Refine the system prompt to guide the AI. Experiment with different LLMs. Image Style:** Adjust the Leonardo AI prompt. Explore different image generation models. Video Editing:** Modify the Creatomate template to customize transitions, text overlays, and other elements. Add voiceover using the ElevenLabs integration in Creatomate. Category: Marketing, Social Media, Content Creation",12587,2025-02-22 07:20:26.447000+00:00,True,6
3151,"Build an AI-Powered Tech Radar Advisor with SQL DB, RAG, and Routing Agents","AI-Powered Tech Radar Advisor This project is built on top of the famous open source ThoughtWorks Tech Radar. You can use this template to build your own AI-Powered Tech Radar Advisor for your company or group of companies. Target Audience This template is perfect for: Tech Audit & Governance Leaders:** Those seeking to build a tech landscape AI platform portal. Tech Leaders & Architects:** Those aiming to provide modern AI platforms that help others understand the rationale behind strategic technology adoption. Product Managers:** Professionals looking to align product innovation with the company's current tech trends. IT & Engineering Teams:** Teams that need to aggregate, analyze, and visualize technology data from multiple sources efficiently. Digital Transformation Experts:** Innovators aiming to leverage AI for actionable insights and strategic recommendations. Data Analysts & Scientists:** Individuals who want to combine structured SQL analysis with advanced semantic search using vector databases. Developers:** Those interested in integrating RAG chatbot functionality with conversation storage. 1. Description Tech Constellation is an AI-powered Tech Radar solution designed to help organizations visualize and steer their technology adoption strategy. It seamlessly ingests data from a Tech Radar Google Sheet‚Äîconverting it into both a MySQL database and a vector index‚Äîto consolidate your tech landscape in one place. The platform integrates an interactive AI chat interface powered by four specialized agents: AI Agent Router:** Analyzes and routes user queries to the most suitable processing agent. SQL Agent:** Executes precise SQL queries on structured data. RAG Agent:** Leverages semantic, vector-based search for in-depth insights. Output Guardrail Agent:** Validates responses to ensure they remain on-topic and accurate. This powerful template is perfect for technology leaders, product managers, engineering teams, and digital transformation experts looking to make data-driven decisions aligned with strategic initiatives across groups of parent-child companies. 2. Features Data Ingestion A Google Sheet containing tech radar data is used as the primary source. The data is ingested and converted into a MySQL database. Simultaneously, the data is indexed into a vector database for semantic (vector-based) search. Interactive AI Chat Chat Integration:** An AI-powered chat interface allows users to ask questions about the tech radar. Customizable AI Agents:** AI Agent Router: Determines the query type and routes it to the appropriate agent. SQL Agent: Processes queries using SQL on structured data. RAG Agent: Performs vector-based searches on document-like data. Output Guardrail Agent: Validates queries and ensures that the responses remain on-topic and accurate. Usage Examples Tell me, is TechnologyABC adopted or on hold, and why? List all the tools that are considered part of the strategic direction for company3 but are not adopted. Project Links & Additional Details GitHub Repository (Frontend Interface Source Code):** github.com/dragonjump/techconstellation Try It:** https://scaler.my",12453,2025-03-13 13:37:14.192000+00:00,True,12
2917,Allow Users to Send a Sequence of Messages to an AI Agent in Telegram,"Use Case When creating chatbots that interface through applications such as Telegram and WhatsApp, users can often sends multiple shorter messages in quick succession, in place of a single, longer message. This workflow accounts for this behaviour. What it Does This workflow allows users to send several messages in quick succession, treating them as one coherent conversation instead of separate messages requiring individual responses. How it Works When messages arrive, they are stored in a Supabase PostgreSQL table The system waits briefly to see if additional messages arrive If no new messages arrive within the waiting period, all queued messages are: Combined and processed as a single conversation Responded to with one unified reply Deleted from the queue Setup Create a table in Supabase called message_queue. It needs to have the following columns: user_id (uint8), message (text), and message_id (uint8) Add your Telegram, Supabase, OpenAI, and PostgreSQL credentials Activate the workflow and test by sending multiple messages the Telegram bot in one go Wait ten seconds after which you will receive a single reply to all of your messages How to Modify it to Your Needs Change the value of Wait Amount in the Wait 10 Seconds node in order to to modify the buffering window Add a System Message to the AI Agent to tailor it to your specific use case Replace the OpenAI sub-node to use a different language model",12165,2025-02-16 12:03:51.655000+00:00,True,5
2981,‚úçÔ∏èüåÑ Your First Wordpress + AI Content Creator - Quick Start,"‚úçÔ∏èüåÑ WordPress + AI Content Creator This workflow automates the creation and publishing of multi-reading-level content for WordPress blogs. It leverages AI to generate optimized articles, automatically creates featured images, and provides versions of the content at different reading levels (Grade 2, 5, and 9). How It Works Content Generation & Processing üéØ Starts with a manual trigger and a user-defined blog topic Uses AI to create a structured blog post with proper HTML formatting Separates and validates the title and content components Saves a draft version to Google Drive for backup Multi-Reading Level Versions üìö Automatically rewrites the content for different reading levels: Grade 9: Sophisticated language with appropriate metaphors Grade 5: Simplified with light humor and age-appropriate examples Grade 2: Basic language with simple metaphors and child-friendly explanations WordPress Integration üåê Creates a draft post in WordPress with the Grade 9 version Generates a relevant featured image using Pollinations.ai Automatically uploads and sets the featured image Sends success/error notifications via Telegram Setup Steps Configure API Credentials üîë Set up WordPress API connection Configure OpenAI API access Set up Google Drive integration Add Telegram bot credentials for notifications Customize Content Parameters ‚öôÔ∏è Adjust reading level prompts as needed Modify image generation settings Set WordPress post parameters Test and Deploy üöÄ Run a test with a sample topic Verify all reading level versions Check WordPress draft creation Confirm notification system This workflow is perfect for content creators who need to maintain a consistent blog presence while catering to different audience reading levels. It's especially useful for educational content, news sites, or any platform that needs to communicate complex topics to diverse audiences.",12010,2025-02-23 01:20:13.550000+00:00,True,8
3277,Smart Email Auto-Responder Template using AI,"Smart Email Auto-Responder with AI Classification Automatically Categorize and Reply to Emails using LangChain + Google Gemini + Gmail + SMTP + Brevo This n8n workflow is designed to intelligently manage incoming emails and automatically send personalized responses based on the content. It classifies emails using LangChain's Text Classifier, sends HTML responses depending on the category, and updates Gmail and Brevo CRM accordingly. Key Features Triggers and Classifies Emails Listens for new Gmail messages every hour Uses AI-based classification to identify the type of inquiry For Example: Guest Post YouTube Review Udemy Course Inquiry Responds Automatically Sends professional HTML replies customized for each type Uses SMTP to deliver emails from your domain Enhances Workflow with Automation Marks processed emails as read Applies Gmail labels Adds sender to Brevo contact list Optional AI Chat Integration Uses Google Gemini (PaLM 2) to enhance classification or summarization Tools & Integrations Required Gmail account (OAuth2) LangChain (Text Classifier node) Google Gemini API account SMTP credentials (e.g., Gmail SMTP, Brevo, etc.) Brevo/Sendinblue account and API key Step-by-Step Node Guide 1. Gmail Trigger Polls Gmail every hour for new emails. Filters out internal addresses (e.g., @syncbricks.com). Avoids replying to already-responded emails (Re: subject filter). 2. LangChain Text Classifier Uses AI to categorize the content of the email based on pre-defined categories: Guest Post** Youtube** Udemy Courses** 3. Google Gemini (PaLM) Chat Model (Optional) Provides additional AI support to enhance classification accuracy. Can be used to summarize or enrich the context if needed. 4. Email Send Nodes Each response category has a separate SMTP node with a custom HTML email: Guest Post Inquiry** YouTube Video Inquiry** Udemy Course Inquiry** 5. Gmail: Mark as Read Marks the email so it isn‚Äôt processed again. 6. Gmail: Apply Label Adds a label (e.g., Handled by Bot) for organization. 7. Brevo: Create/Update Contact Saves the sender to your CRM for future communication or marketing. Email Templates Included Guest Post Template Includes pricing, website list, submission guidelines, and payment instructions. YouTube Review Template Includes package pricing, review samples, video thumbnails, and inquiry instructions. Step by Step Tutorial GET n8n Now N8N COURSE n8n Book More courses: http://lms.syncbricks.com YouTube Channel: https://youtube.com/@syncbricks How to Use Import the template into your n8n instance. Configure your Gmail OAuth2 and SMTP credentials. Set up your LangChain Text Classifier and Google Gemini API credentials. Update label ID in the Gmail node and ensure all custom fields like from.value[0].name match your use case. Run the workflow and watch it respond intelligently to new inquiries. Best Practices Always test with mock emails first. Keep the Google Gemini node optional if you want to reduce cost/API calls. Use Gmail filters to auto-label certain types of emails. Monitor your Brevo contacts to track new leads. Attribution & Support Developed by Amjid Ali This template took extensive time and effort to build. If you find it useful, please consider supporting my work. Buy My Book: Mastering n8n on Amazon Full Courses & Tutorials: http://lms.syncbricks.com Follow Me Online: LinkedIn: https://linkedin.com/in/amjidali Website: https://amjidali.com YouTube: https://youtube.com/@syncbricks",11950,2025-03-21 14:03:18.184000+00:00,True,5
3178,Get Real-time Crypto Token Insights via Telegram with DexScreener and GPT-4o,"Instantly access real-time decentralized exchange (DEX) insights directly in Telegram! This workflow integrates the DexScreener API with GPT-4o-powered AI and Telegram, allowing users to fetch the latest blockchain token analytics, liquidity pools, and trending tokens effortlessly. Ideal for crypto traders, DeFi analysts, and investors who need actionable market data at their fingertips. How It Works A Telegram bot listens for user queries about tokens or trading pairs. The workflow interacts with the DexScreener API (no API key required) to fetch real-time data, including: Token fundamentals (profiles, images, descriptions, and links) Trending and boosted tokens (hyped projects, potential market movers) Trading pair analytics (liquidity, price action, volumes, volatility) Order and payment activity (transaction insights, investor movements) Liquidity pool depth (market stability, capital flows) Multi-chain pair comparisons (performance tracking across networks) An AI-powered language model (GPT-4o-mini) enhances responses for better insights. The workflow logs session data to improve user interaction tracking. The requested DEX insights are sent back via Telegram in an easy-to-read format. What You Can Do with This Agent This AI-driven Telegram bot enables you to: ‚úÖ Track trending and boosted tokens before they gain mainstream traction. ‚úÖ Monitor real-time liquidity pools to assess token stability. ‚úÖ Analyze active trading pairs across different blockchains. ‚úÖ Identify transaction trends by checking paid orders for tokens. ‚úÖ Compare market activity with detailed trading pair analysis. ‚úÖ Receive instant insights with AI-enhanced responses for deeper understanding. Set Up Steps Create a Telegram Bot Use @BotFather on Telegram to create a bot and obtain an API token. Configure Telegram API Credentials in n8n Add your Telegram bot token under Telegram API credentials. Deploy and Test Send a query (e.g., ""SOL/USDC"") to your Telegram bot and receive real-time insights instantly! üöÄ Unlock powerful, real-time DEX insights directly in Telegram‚Äîno API key required! üì∫ Setup Video Tutorial Watch the full setup guide on YouTube:",11890,2025-03-16 06:08:06.781000+00:00,True,5
2993,Copy Viral Reels with Gemini AI,"Video Guide I prepared a detailed guide that shows the whole process of building an AI tool to analyze Instagram Reels using n8n. Youtube Link Who is this for? This workflow is ideal for social media analysts, digital marketers, and content creators who want to leverage data-driven insights from their Instagram Reels. It's particularly useful for those looking to automate the analysis of video performance to inform strategy and content creation. What problem does this workflow solve? Analyzing video performance on Instagram can be tedious and time-consuming, requiring multiple steps and data extraction. This workflow automates the process of fetching, analyzing, and recording insights from Instagram Reels, making it simpler for users to track engagement metrics without manual intervention. What this workflow does This workflow integrates several services to analyze Instagram Reels, allowing users to: Automatically fetch recent Reels from specified creators. Analyze the most-watched videos for insights. Store and manage data in Airtable for easy access and reporting. Initial Trigger: The process begins with a manual trigger that can later be modified for scheduled automation. Data Retrieval: It connects to Airtable to fetch a list of creators and their respective Instagram Reels. Video Analysis: It handles the fetching, downloading, and uploading of videos for analysis using an external service, simplifying performance tracking through a structured query process. Record Management: It saves relevant metrics and insights into Airtable, ensuring that users can access and organize their video analytics effectively. Setup Create accounts: Set up Airtable, Edify, n8n, and Gemini accounts. Prepare triggers and modules: Replace credentials in each node accordingly. Configure data flow: Ensure modules are set to fetch and analyze the correct data fields as outlined in the guide. Test the workflow: Run the scenario manually to confirm that data is fetched and analyzed correctly.",11867,2025-02-24 10:38:34.868000+00:00,True,2
3139,üîêü¶ôPrivate & Local Ollama Self-Hosted + Dynamic LLM Router,"Who is this for? This workflow template is designed for AI enthusiasts, developers, and privacy-conscious users who want to leverage the power of local large language models (LLMs) without sending data to external services. It's particularly valuable for those running Ollama locally who want intelligent routing between different specialized models. What problem is this workflow solving? When working with multiple local LLMs, each with different strengths and capabilities, it can be challenging to manually select the right model for each specific task. This workflow automatically analyzes user prompts and routes them to the most appropriate specialized Ollama model, ensuring optimal performance without requiring technical knowledge from the end user. What this workflow does This intelligent router: Analyzes incoming user prompts to determine the nature of the request Automatically selects the optimal Ollama model from your local collection based on task requirements Routes requests between specialized models for different tasks: Text-only models (qwq, llama3.2, phi4) for various reasoning and conversation tasks Code-specific models (qwen2.5-coder) for programming assistance Vision-capable models (granite3.2-vision, llama3.2-vision) for image analysis Maintains conversation memory for consistent interactions Processes everything locally for complete privacy and data security Setup Ensure you have Ollama installed and running locally Pull the required models mentioned in the workflow using Ollama CLI (e.g., ollama pull phi4) Configure the Ollama API credentials in n8n (default: http://127.0.0.1:11434) Activate the workflow and start interacting through the chat interface How to customize this workflow to your needs Add or remove models from the router's decision framework based on your specific Ollama collection Adjust the system prompts in the LLM Router to prioritize different model selection criteria Modify the decision tree logic to better suit your specific use cases Add additional preprocessing steps for specialized inputs This workflow demonstrates how n8n can be used to create sophisticated AI orchestration systems that respect user privacy by keeping everything local while still providing intelligent model selection capabilities.",11663,2025-03-12 03:04:44.720000+00:00,True,3
3057,Create Social Media Content from Telegram with AI,"Description: Create Social Media Content from Telegram with AI This n8n workflow empowers you to effortlessly generate social media content and captivating image prompts, all powered by AI. Simply send a topic request through Telegram (as a voice or text message), and watch as the workflow conducts research, crafts engaging social media posts, and creates detailed image prompts ready for use with your preferred AI art generation tool. What does this workflow do? This workflow streamlines the content creation process by automating research, social media content generation, and image prompt creation, triggered by a simple Telegram message. Who is this for? Social Media Managers:** Quickly generate engaging content and image ideas for various platforms. Content Creators:** Overcome writer's block and discover fresh content ideas with AI assistance. Marketing Teams:** Boost productivity by automating social media content research and drafting. Anyone** looking to leverage AI for efficient and creative social media content creation. Benefits Effortless Content and Image Prompt Generation:** Automate the creation of social media posts and detailed image prompts. AI-Powered Creativity:** Leverage the power of LLMs to generate original content ideas and captivating image prompts. Increased Efficiency:** Save time and resources by automating the research and content creation process. Voice-to-Content:** Use voice messages to request content, making content creation even more accessible. Enhanced Engagement:** Create high-quality, attention-grabbing content that resonates with your audience. How it Works Receive Request: The workflow listens for incoming voice or text messages on Telegram containing your content request. Process Voice (if necessary): If the message is a voice message, it's transcribed into text using OpenAI's Whisper API. AI Takes Over: The AI agent, powered by an OpenAI Chat Model and SerpAPI, conducts online research based on your request. Content and Image Prompt Generation: The AI agent generates engaging social media content and a detailed image prompt based on the research. Image Generation (Optional): You can use the generated image prompt with your preferred AI art generation tool (e.g., DALL-E, Stable Diffusion) to create a visual. Output: The workflow provides you with the social media content and the detailed image prompt, ready for you to use or refine. n8n Nodes Used Telegram Trigger Switch Telegram (for fetching voice messages) OpenAI (Whisper API for voice-to-text) Set (for preparing variables) AI Agent (with OpenAI Chat Model and SerpAPI tool) HTTP Request (for optional image generation) Extract from File (for optional image processing) Set (for final output) Prerequisites Active n8n instance Telegram account with a bot OpenAI API key SerpAPI account Hugging Face API key (if you want to generate images within the workflow) Setup Import the workflow JSON into your n8n instance. Configure the Telegram Trigger node with your Telegram bot token. Set up the OpenAI and SerpAPI credentials in the respective nodes. If you want to generate images directly within the workflow, configure the HTTP Request node with your Hugging Face API key. Test the workflow by sending a voice or text message to your Telegram bot with a topic request. This workflow combines the convenience of Telegram with the power of AI to provide a seamless content creation experience. Start generating engaging social media content today!",11474,2025-03-02 10:54:06.090000+00:00,True,7
4024,Generate & Publish SEO-Optimized WordPress Blog Posts with AI,"Generate and Publish SEO-Optimized Blog Posts to WordPress This n8n workflow, BlogBlitz, automates the creation and publishing of SEO-optimized blog posts for WordPress, saving you hours of content creation time. Triggered via Telegram or a scheduled interval, it generates 1,500‚Äì2,500-word articles on Technology, AI, Tech Facts, History, or Tips, complete with catchy titles, slugs, meta descriptions, and realistic featured images. Perfect for bloggers and marketers, BlogBlitz ensures your site stays fresh with high-quality content. Who is this for? Bloggers, content marketers, and WordPress site owners who want to automate high-quality, SEO-ready blog content creation without manual writing or formatting. What problem is this workflow solving? Manually creating engaging, SEO-optimized blog posts is time-consuming and requires expertise. BlogBlitz solves this by automating the entire process‚Äîfrom generating ideas and writing articles to publishing and notifying you‚Äîkeeping your site active and discoverable. What this workflow does Triggers**: Runs every 3 hours via a Schedule Trigger or on-demand with a Telegram command (‚Äúgenerate‚Äù). Generates Content**: Uses OpenRouter to pick a category (e.g., Technology, AI) and create a title, slug, focus keyphrase, and meta description. Writes Articles**: OpenAI crafts 1,500‚Äì2,500-word posts with SEO-friendly structure, headings, and a call-to-action. Adds Visuals**: Generates realistic featured images with OpenAI and uploads them to WordPress. SEO Features**: Generate optimized slug, focus keyphrase, meta description Publishes**: Posts articles to WordPress with proper categories and featured images. Notifies**: Sends publish alerts with links to Discord and Telegram. Setup n8n instance (Cloud or self-hosted): Ensure you have a cloud or self-hosted n8n instance. Credentials: WordPress: API access for wp-json/wp/v2 endpoint. OpenAI: For article and image generation. OpenRouter: For title and category generation. Telegram: Bot API for triggers and notifications. Discord: Webhook for publish alerts. WordPress Configuration: Set up categories (Technology [ID:3], AI [ID:4], Tech Fact [ID:7], Tech History [ID:8], Tech Tips [ID:9]). Ensure an admin user ID is available (default: 1). Node Setup: Use the Edit Fields node to centralize variables like category IDs. Test: Send ‚Äúgenerate‚Äù via Telegram to test the workflow. Check WordPress for the published post. How to customize this workflow to your needs Change Categories**: Update the WordPress Post Draft node to match your site‚Äôs category IDs. Adjust Schedule**: Modify the Schedule Trigger node for different intervals (e.g., daily). Tweak Tone**: Edit the prompt in the Basic LLM Chain node for a different writing style (e.g., formal or casual). Add Platforms**: Extend notifications to Slack or email by adding nodes. Image Style**: Adjust the OpenAI image node for different sizes or styles (e.g., ‚Äúnatural‚Äù instead of ‚Äúvivid‚Äù). Pre-Requirements n8n Instance**: Cloud or self-hosted. Credentials**: WordPress API (wp-json/wp/v2 endpoint). OpenAI API for text and images. OpenRouter API for AI content. Telegram bot API. Discord Webhook API. Dependencies**: @n8n/n8n-nodes-langchain package. WordPress Setup**: Categories and admin user ID configured. Network**: Stable internet for API calls. Made by: Khaisa Studio Tag: youtube, summarizer, telegram, openai Category: AI Automation, Video Tools Need a custom? Contact Me",11430,2025-05-13 14:29:01.632000+00:00,True,9
3123,Automatic Reminders For Follow-ups with AI and Human in the loop Gmail,"This n8n template extends the idea of follow-up reminders by having an AI agent suggest and book the next call or message to re-engage prospects which have been ignored. What makes this template particularly interesting and actually usable is that it uses the Human-in-the-loop approach to wait for a user's approval before actually making the booking or otherwise not if the user declined. A twist on a traditional idea where we can reduce the number of actionable tasks a human has to make by delegating them to AI. How it works A scheduled trigger checks your google calendar for sales meetings which happened a few days ago. For each event, gmail search is used to figure out if a follow-up message has been sent or received from the other party since the meeting. If none, it might mean the user needs a reminder to follow-up. For leads applicable for follow-up, we first get an AI Agent to find available meeting slots in the calendar. These slots and reminder are sent to the user via send-and-approval mode of the gmail node. The user replies in natural language either picking a slot, suggesting an entirely new slot or declines the request. When accepted, another AI Agent books the meeting in the calendar with the proposed dates and lead. When declined, no action is taken. How to use Update all calendar nodes (+subnodes) to point to the right calendar. If this is a shared-purpose calendar, you may need to either filter or create a new calendar. Update the gmail nodes to point to the right accounts. Requirements Google OAuth for Email and Calendar OpenAI for LLM Customising the template Not using Google? Swap out for Microsoft Outlook/Calendar or something else. Try swapping out or adding in additional send-for-approval methods such as telegram or whatsapp.",11378,2025-03-10 15:36:31.595000+00:00,True,5
3012,üåê Confluence Page AI Chatbot Workflow,"üåê Confluence Page AI Chatbot Workflow This n8n workflow template enables users to interact with an AI-powered chatbot designed to retrieve, process, and analyze content from Confluence pages. By leveraging Confluence's REST API and an AI agent, the workflow facilitates seamless communication and contextual insights based on Confluence page data. üåü How the Workflow Works üîó Input Chat Message The workflow begins when a user sends a chat message containing a query or request for information about a specific Confluence page. üìÑ Data Retrieval The workflow uses the Confluence REST API to fetch page details by ID, including its body in the desired format (e.g., storage, view). The retrieved HTML content is converted into Markdown for easier processing. ü§ñ AI Agent Interaction An AI-powered agent processes the Markdown content and provides dynamic responses to user queries. The agent is context-aware, ensuring accurate and relevant answers based on the Confluence page's content. üí¨ Dynamic Responses Users can interact with the chatbot to: Summarize the page's content. Extract specific details or sections. Clarify complex information. Analyze key points or insights. üöÄ Use Cases üìö Knowledge Management**: Quickly access and analyze information stored in Confluence without manually searching through pages. üìä Team Collaboration**: Facilitate discussions by summarizing or explaining page content during team chats. üîç Research and Documentation**: Extract critical insights from large documentation repositories for efficient decision-making. ‚ôø Accessibility**: Provide an alternative way to interact with Confluence content for users who prefer conversational interfaces. üõ†Ô∏è Resources for Getting Started Confluence API Setup: Generate an API token for authentication via Atlassian's account management portal. Refer to Confluence's REST API documentation for endpoint details and usage instructions. n8n Installation: Install n8n locally or on a server using the official installation guide. AI Agent Configuration: Set up OpenAI or other supported language models for natural language processing.",11370,2025-02-25 16:29:19.644000+00:00,True,5
3090,‚ú®üìäMulti-AI Agent Chatbot for Postgres/Supabase DB and QuickCharts + Tool Router,"Multi-AI Agent Chatbot for Postgres/Supabase Databases and QuickChart Generation Who is this for? This workflow is ideal for data analysts, developers, and business intelligence teams who need an AI-powered chatbot to query Postgres/Supabase databases and generate dynamic charts for data visualization. What problem does this solve? It simplifies data exploration by combining conversational AI with database querying and chart generation. Users can interact with their database using natural language, retrieve insights, and visualize data without manual SQL queries or chart configuration. What this workflow does AI-Powered Chat Interface: Accepts natural language prompts to query databases or generate charts. Routes user requests through a tool agent system to determine the appropriate action (query or chart). Database Querying: Executes SQL queries on Postgres/Supabase databases based on user input. Retrieves schema information, table definitions, and specific data records. Dynamic Chart Generation: Uses QuickChart to create bar charts, line charts, or other visualizations from database records. Outputs a shareable chart URL or JSON configuration for further customization. Memory Integration: Maintains chat history using Postgres memory nodes, enabling context-aware interactions. Workflow diagram showcasing AI agents, database querying, and chart generation paths. Setup Prerequisites: A Postgres-compatible database (e.g., Supabase). API credentials for OpenAI. Configuration Steps: Add your database connection credentials in the Postgres nodes. Set up OpenAI credentials for GPT-4o-mini in the language model nodes. Adjust the QuickChart schema in the ""QuickChart Object Schema"" node to fit your use case. Testing: Trigger the chat workflow via the ""When chat message received"" node. Test with prompts like ""Generate a bar chart of sales data"" or ""Show me all users in the database."" How to customize this workflow Modify AI Prompts** Add Chart Types** Integrate Other Tools**",11051,2025-03-05 21:40:28.647000+00:00,True,6
3068,AI-Powered Research with Jina AI Deep Search,"Unlock AI-Driven Research with Jina AI (No API Key Needed!) Following the success of Open Deep Research 1.0, we are excited to introduce an improved and fully free version: AI-Powered Research with Jina AI Deep Search. This workflow leverages Jina AI‚Äôs Deep Search API, a free and powerful AI research tool that requires no API key. It automates querying, analyzing, and formatting research reports, making AI-driven research accessible to everyone. Key Features No API Keys Required** - Start researching instantly without setup hassle. Automated Deep Search* - Uses Jina AI to fetch *relevant and high-quality information**. Structured AI Reports** - Generates clear, well-formatted research documents in markdown. Flexible and Customizable* - Modify the workflow to fit *your specific research needs**. Ideal for Researchers, Writers & Students** - Speed up your research workflow. Use Cases This workflow is particularly useful for: Researchers** - Quickly gather and summarize academic papers, online sources, and deep web content. Writers & Journalists** - Automate background research for articles, essays, and investigative reports. Students & Educators** - Generate structured reports for assignments, literature reviews, or presentations. Content Creators** - Find reliable sources for blog posts, videos, or social media content. Data Analysts** - Retrieve contextual insights from various online sources for reports and analysis. How It Works The user submits a research query via chat. The workflow sends the query to Jina AI‚Äôs Deep Search API. The AI processes and generates a well-structured research report. A code node formats the response into clean markdown. The final output is a structured, easy-to-read AI-generated report. Pre-Conditions & Requirements An n8n instance (self-hosted or cloud). No API keys needed** ‚Äì Jina AI Deep Search is completely free. Basic knowledge of n8n workflow automation is recommended for customization. Customization Options This workflow is fully modular, allowing users to: Modify the query prompt to refine the research focus. Adjust the report formatting to match personal or professional needs. Expand the workflow by adding additional AI tools or data sources. Integrate it with other workflows in n8n to enhance automation. Users are free to connect it with other workflows, add custom nodes, or tweak existing configurations. Getting Started Setup Time: Less than 5 minutes Import the workflow into n8n. Run the workflow and input a research topic. Receive a fully formatted AI-generated research report. Try It Now! Start your AI-powered research with Jina AI Deep Search today! Get the workflow on n8n.io",10848,2025-03-03 09:08:56.454000+00:00,True,2
3192,"IT Support Chatbot with Google Drive, Pinecone & Gemini | AI Doc Processing","This n8n template empowers IT support teams by automating document ingestion and instant query resolution through a conversational AI. It integrates Google Drive, Pinecone, and a Chat AI agent (using Google Gemini/OpenRouter) to transform static support documents into an interactive, searchable knowledge base. With two interlinked workflows‚Äîone for processing support documents and one for handling chat queries‚Äîemployees receive fast, context-aware answers directly from your support documentation. Overview Document Ingestion Workflow Google Drive Trigger:** Monitors a specified folder for new file uploads (e.g., updated support documents). File Download & Extraction:** Automatically downloads new files and extracts text content. Data Cleaning & Text Splitting:** Utilizes a Code node to remove line breaks, trim extra spaces, and strip special characters, while a text splitter segments the content into manageable chunks. Embedding & Storage:** Generates text embeddings using Google Gemini and stores them in a Pinecone vector store for rapid similarity search. Chat Query Workflow Chat Trigger:** Initiates when an employee sends a support query. Vector Search & Context Retrieval:** Retrieves the top relevant document segments from Pinecone based on similarity scores. Prompt Construction:** A Code node combines the retrieved document snippets with the user‚Äôs query into a detailed prompt. AI Agent Response:** The constructed prompt is sent to an AI agent (using OpenRouter Chat Model) to generate a clear, step-by-step solution. Key Benefits & Use Case Imagine a large organization where every IT support document‚Äîfrom troubleshooting guides to system configurations‚Äîis stored in a single Google Drive folder. When an employee encounters an issue (e.g., ‚ÄúHow do I reset my VPN credentials?‚Äù), they simply type the query into a chat interface. Instantly, the workflow retrieves the most relevant context from the ingested documents and provides a detailed, actionable answer. This process reduces resolution times, enhances support consistency, and significantly lightens the load on IT staff. Prerequisites A valid Google Drive account with access to the designated folder. A Pinecone account for storing and retrieving text embeddings. Google Gemini* (or *OpenRouter**) credentials to power the Chat AI agent. An operational n8n instance configured with the necessary nodes and credentials. Workflow Details 1 Document Ingestion Workflow Google Drive Trigger Node:** Listens for file creation events in the specified folder. Google Drive Download Node:** Downloads the newly added file. Extract from File Node:** Extracts text content from the downloaded file. Code Node (Data Cleaning):** Cleans the extracted text by removing line breaks, trimming spaces, and eliminating special characters. Recursive Text Splitter Node:** Segments the cleaned text into manageable chunks. Pinecone Vector Store Node:** Generates embeddings (via Google Gemini) and uploads the chunks to Pinecone. 2 Chat Query Workflow Chat Trigger Node:** Receives incoming user queries. Pinecone Vector Store Node (Query):** Searches for relevant document chunks based on the query. Code Node (Context Builder):** Sorts the retrieved documents by relevance and constructs a prompt merging the context with the query. AI Agent Node:** Sends the prompt to the Chat AI agent, which returns a detailed answer. How to Use Import the Template: Import the template into your n8n instance. Configure the Google Drive Trigger: Set the folder ID (e.g., 1RQvAHIw8cQbtwI9ZvdVV0k0x6TM6H12P) and connect your Google Drive credentials. Set Up Pinecone Nodes: Enter your Pinecone index details and credentials. Configure the Chat AI Agent: Provide your Google Gemini (or OpenRouter) API credentials. Test the Workflows: Validate the document ingestion workflow by uploading a sample support document. Validate the chat query workflow by sending a test query and verifying the returned support information. Additional Notes Ensure all credentials (Google Drive, Pinecone, and Chat AI) are correctly set up and tested before deploying the workflows in production. The template is fully customizable. Adjust the text cleaning, splitting parameters, or the number of document chunks retrieved based on your support documentation's size and structure. This template not only enhances IT support efficiency but also offers a scalable solution for managing and leveraging growing volumes of support content.",10822,2025-03-17 06:34:44.037000+00:00,True,8
3051,Effortless Job Hunting: Let this Automation Find Your Next Role,"Find Job Postings from LinkedIn, Indeed, and Glassdoor and Save Them to Google Sheets Using AI Overview Effortlessly discover and apply to jobs tailored to your profile‚ÄîAI handles the search, you handle the interviews. Say goodbye to endless job board scrolling. This automation leverages AI to analyze your resume, identify your skills, experience, and more, to match you with the most relevant job opportunities. It sources job postings from LinkedIn, Indeed, Glassdoor, ZipRecruiter, Monster, and other public job sites on the web. With seamless integration and automatic organization of results, you can focus on applying rather than searching. Key Features Intelligent Resume Parsing Extracts key information from your PDF resume using AI. Identifies skills, experience, education, and job preferences. Targeted Job Matching Uses the parsed resume data to search for jobs that align with your profile. Ensures relevance by analyzing job descriptions for matching criteria. Automated Data Organization Compiles job listings into a structured Google Spreadsheet. Eliminates the need for manual data entry, saving valuable time. Easy Access and Review Stores results in a familiar Google Sheets format for easy tracking. Allows for filtering and sorting to prioritize applications. Setup Instructions Prerequisites A free API key for the job search service. Google Drive and Google Sheets accounts. An updated resume in PDF format. Step 1: Connect Your Resume Parsing AI Upload your PDF resume to Google Drive. Configure the AI parser node in n8n to extract relevant information. Map the extracted fields (e.g., skills, job title, experience) for job searching. Step 2: Automate the Job Search Use the extracted data to perform a job search on LinkedIn, Indeed, Glassdoor, and other supported job sites. Retrieve job postings based on relevant keywords and location preferences. Step 3: Save Job Listings to Google Sheets Create a new Google Sheet to store job listings. Set up the automation to write job details (e.g., title, company, location, link) into the sheet. Format the sheet for better readability and tracking. Step 4: Review and Apply to Jobs Open your Google Sheet to view job matches. Click on the links to apply directly on the respective job sites. Update the status of each job application as you progress. Why Use This Automation? Saves Time**: Automates job searching and listing compilation. Enhances Efficiency**: Eliminates manual scrolling and data entry. Improves Organization**: Keeps all job opportunities in a structured format. Optimizes Your Job Hunt**: Increases chances of landing the perfect role. Designed specifically for job seekers aiming to optimize their search process, this automation integrates with Google Drive and Sheets, streamlining your job hunt and enhancing your chances of finding the right opportunity. Get started today and accelerate your career growth!",10635,2025-03-01 14:56:21.432000+00:00,True,4
2158,Send Daily Meetings in Google Calendar to Telegram,"This workflow automatically sends you a list of your daily meetings every morning via a Telegram bot. Use Cases: This workflow is useful for anyone who wants to be automatically informed of their daily meetings, especially for busy professionals, students, and anyone with a hectic schedule. Setup: Google Calendar connected to n8n A Telegram bot created and connected to n8n Your Telegram user ID specified Notes: You need to replace the placeholder in the Telegram node with your actual Telegram user ID. You can customize the formatting of the Telegram message in the JavaScript Code node.",10601,2024-03-01 11:10:21.906000+00:00,False,2
2216,Image Creation with OpenAI and Telegram,"Image Creation with OpenAI and Telegram Check this channel: AutoTechAi_bot Description: In the realm of automation and artificial intelligence, n8n offers a sophisticated platform for seamlessly integrating AI algorithms to enhance image creation and communication processes. This innovative workflow leverages the capabilities of OpenAI and Telegram to facilitate creative image generation and streamline communication channels, ultimately enhancing user engagement and interaction. How to Use: Set Up Credentials: Configure credentials for the Telegram account and OpenAI API to enable seamless integration. Configure Nodes: Telegram Trigger Node: Set up the node to initiate the workflow based on incoming messages from users on Telegram. OpenAI Node: Utilize advanced AI algorithms to analyze text content from messages and generate intelligent responses. Telegram Node: Send processed data, including images and responses, back to users on Telegram for seamless communication. Merge Node: Organize and combine processed data for efficient handling and integration within the workflow. Aggregate Node: Aggregate all item data, including binaries if specified, for comprehensive reporting and analysis purposes. Run Workflow: Initiate the workflow to leverage AI-enhanced image processing and communication capabilities for enhanced user interactions. Monitor Execution: Keep an eye on the workflow execution for any errors or issues that may occur during processing. Customize Workflow: Tailor the workflow nodes, parameters, or AI models to align with specific business objectives and user engagement strategies. Experience Benefits: Embrace the power of AI-driven image processing and interactive communication on Telegram to elevate user engagement and satisfaction levels. By following these steps, businesses can unlock the transformative potential of AI integration in image creation and communication workflows using n8n. Elevate your user engagement strategies and deliver exceptional experiences to your audience through innovative AI-driven solutions. Embark on a journey of innovation and efficiency with AI integration in image creation and communication workflows using n8n!",10338,2024-04-06 21:50:18.223000+00:00,True,2
3101,Monitor competitors' websites for changes with OpenAI and Firecrawl,"Who is this template for? This workflow template is designed for people seeking alerts when certain specific changes are made to any web page. Leveraging agentic AI, it analyzes the page every day and autonomously decides whether to send you an e-mail notification. Example use cases Track price changes on [competitor's website]. Notify me when the price drops below ‚Ç¨50. Monitor new blog posts on [industry leader's website] and summarize key insights. Check [competitor's job page] for new job postings related to software development. Watch for new product launches on [e-commerce site] and send me a summary. Detect any changes in the terms and conditions of [specific website]. Track customer reviews for [specific product] on [review site] and extract key themes. How it works When clicking 'test workflow' in the editor, a new browser tab will open where you can fill in the details of your espionage assignment Make sure you be as concise as possible when instructing AI. Instruct specific and to the point (see examples at the bottom). After submission, the flow will start off by extracting both the relevant website url and an optimized prompt. OpenAI's structured outputs is utilized, followed by a code node to parse the results for further use. From here on, the endless loop of daily checks will begin: Initial scrape 1 day delay Second scrape AI agent decides whether or not to notify you Back to step 1 You can cancel an espionage assignment at any time in the executions tab Set up steps Insert your OpenAI API key in the structured outputs node (second one) Create a Firecrawl account and connect your Firecrawl API key in both 'Scrape page'-nodes Connect your OpenAI account in the AI agents' model node Connect your Gmail account in the AI agents' Gmail tool node",10328,2025-03-06 16:14:34.088000+00:00,True,4
2314,Convert HTML to PDF using ConvertAPI,"Who is this for? For developers and organizations that need to convert HTML files to PDF. What problem is this workflow solving? The file format conversion problem. What this workflow does Converts HTML to file. Converts the HTML file to PDF. Stores the PDF file in the local file system. How to customize this workflow to your needs Open the HTTP Request node. Adjust the URL parameter (all endpoints can be found here). Use your API Token for authentication. Pass the token in the Authorization header as a Bearer token. You can manage your API Tokens in the User panel ‚Üí Authentication. Optionally, additional Body Parameters can be added for the converter.",9974,2024-07-05 09:19:03.760000+00:00,False,2
2173,Auto document your n8n workflows,"Who is this for? This workflow is designed for n8n users and developers who need to automate the documentation process of their n8n workflows. It's particularly useful for teams looking to streamline their documentation efforts and ensure consistency across their workflow documentation. What problem is this workflow solving? / Use case The primary problem this workflow addresses is the manual and time-consuming process of creating documentation for n8n workflows. It automates the generation of concise, clear, and comprehensive documentation directly from the workflow's JSON, making it easier for both technical and non-technical users to understand what the workflow does and how it operates. What this workflow does Upon receiving a form submission with the workflow title and JSON, this workflow automatically generates documentation that includes: A brief introduction to the workflow. The trigger mechanism (webhook URLs for test and production environments, or cron schedules). Setup requirements, including necessary credentials and external dependencies. Setup Credentials Setup: Ensure you have OpenAI API credentials configured in n8n to use the GPT model for generating documentation text. Form Submission: Users must submit the form with the workflow title and JSON. The form is accessible via: Test URL: domain/form-test/{webhookId} Production URL: domain/form/{webhookId} How to customize this workflow to your needs Modify Trigger URLs**: Adjust the webhook or form URLs based on your domain and specific n8n setup. Customize Documentation Template**: Edit the OpenAI node's prompt to change the structure or details of the generated documentation. Extend Functionality**: Add nodes to integrate with other systems (e.g., automatically publishing the documentation to a wiki or sending it via email). This workflow simplifies the documentation process, making it accessible and manageable for teams of all sizes and technical abilities. By automating documentation, it ensures that all workflows are properly documented, enhancing understanding and efficiency within teams.",9873,2024-03-14 09:43:03.527000+00:00,True,1
5589,Create a Multi-Modal Telegram Support Bot with GPT-4 and Supabase RAG,"üß† What It Does This n8n workflow turns your Telegram bot into a smart, multi-modal AI assistant that accepts text, documents, images, and audio messages, interprets them using OpenAI models, and responds instantly with context-aware answers. It integrates a Supabase vector database to store document embeddings and retrieve relevant information before sending a prompt to OpenAI ‚Äî enabling a full RAG experience üí° Why This Workflow? Most support bots can only handle basic text input. This workflow: Supports multiple input formats (voice, documents, images, text) Dynamically extracts and processes data from uploaded files Implements RAG by combining user input with relevant memory or vector-based context Delivers more accurate, relevant, and human-like AI responses. üë§ Who It's For Businesses looking to automate support using Telegram Freelancers or solopreneurs offering AI Chatbots for businesses. Creators building AI-powered bots for real use cases as it's great for Customer support knowledge, Legal or Policy document, long FAQs, Project documentation, and Product information retrieval. Devs or analysts exploring AI + multi-format input + vector memory. ‚öôÔ∏è How It Works üóÇÔ∏è Knowledge Base Setup Run the ‚ÄúAdd to Supabase Vector DB‚Äù workflow manually to upload a document from your google drive and embed it into your vector database. This powers the Telegram chatbot‚Äôs ability to answer questions using your content. üîÅ Telegram Message Routing Telegram Trigger captures the user message (Text, Image, Voice, Document) Message Router routes input by type using a Switch node Each type is handled separately: Voice ‚Üí Translate recording to text (.ogg, .mp3) Image ‚Üí Analyze image to text. Text ‚Üí Sent directly to AI Agent (.txt). Document ‚Üí Parsed (e.g. .docx to .txt) accordingly. üìé Document Type Routing Before routing documents by type, the Supported Document File Types node first checks if the file extension is allowed. If not supported, it exits early with an error message ‚Äî preventing unnecessary processing. Supported documents are then routed using the Document Router node, and converted to text for further processing. Supported Document File Types .jpg .jpeg .png .webp .pdf .doc .docx .xls .xlsx .json .xml. The text content is combined with stored memory and embedded knowledge using a RAG approach, enabling the AI to respond based on real uploaded data. üß† RAG via Supabase Uploaded documents are vectorized using OpenAI Embeddings. Embeddings are stored in Supabase with metadata. On new questions, the chatbot: Extracts question intent Queries Supabase for semantically similar chunks Ranks retrieved chunks to find the most relevant match. Injects them into the prompt for OpenAI. OpenAI generates a grounded response based on actual document content. Response is sent to the Telegram user with content awareness. üõ† How to Set It Up Open n8n or your local/self-hosted instance. Import the `.json ` workflow file. Set up these credentials: Google drive API Key Telegram API (Bot Token) Guide OpenAI API Supabase API Key + Environment ConvertAPI API Key Postgres API Key Cohere API Key Add a prompt suited to your business. Add a custom AI agent prompt that reflects your business domain, tone, and purpose. This is very important. Without it, your agent won't know how best to respond. Activate the workflow. Start testing by sending a message or document to your Telegram bot.",9819,2025-07-02 09:02:20.931000+00:00,True,12
2125,Summarize social media activity of a company before a call,"How it works It‚Äôs very important to come prepared to Sales calls. This often means a lot of manual research about the person you‚Äôre calling with. This workflow delivers a summary of the latest social media activity (LinkedIn + X) for businesses you are about to interact with each day. Scans Your Calendar**: Each morning, it reviews your Google Calendar for any scheduled meetings or calls with companies based on each attendee email address. Fetches Latest Posts**: For each identified company, it fetches recent LinkedIn and X posts and summerizes them using AI to deliver a qucik overview for a busy sales rep. Delivers Insights**: You receive personalized emails via Gmail, each dedicated to a company you‚Äôre meeting with that day, containing a reminder of the meeting and a summary of company's recent social media activity. Setup steps The workflow requires you to have the following accounts set up in their respective nodes: Google Calendar GMail Clearbit OpenAI Besides those, you will need an account on the RapidAPI platform and subscribe to the following APIs: Fresh LinkedIn Profile Data Twitter Email example",9644,2024-02-23 10:24:50.915000+00:00,True,6
3446,"Daily Newsletter Service using Excel, Outlook and AI","This n8n template builds a newsletter (""daily digest"") delivery service which pulls and summarises the latest n8n.io template in select categories defined by subscribers. It's scheduled to run once a day and sends the newsletter directly to subscriber via a nicely formatted email. If you've had trouble keeping up with the latest and greatest templates beign published daily, this workflow can save you a lot of time! How it works A scheduled trigger pulls a list of subscribers (email and category preferences) from an Excel workbook. We work out unique categories amongst all subscribers and only fetch the latest n8n website templates from these categories to save on resources and optimise the number of API calls we make. The fetched templates are summarised via AI to produce a short description which is more suitable for our email format. For each subscriber, we filter and collect only the templates relevant to their category preferences (as defined in the Excel) and ensure that duplicate templates or those which have been ""seen before"" are omitted. A HTML node is then used to generate the email newsletter. HTML emails are the perfect format since we can add links back to the template. Finally, we use the Outlook node to send the email digest to the subscriber. How to use Populate your Excel sheet with 3 columns: name, email and categories. Categories is a comma-delimited list of categories which match the n8n template website. The available categories are AI, SecOps, Sales, IT Ops, Marketing, Engineering, DevOps, Building Blocks, Design, Finance, HR, Other, Product and Support. To subscribe a new user, simply add their email to the Excel sheet with at least one category. To unsubscribe a user, remove them from the sheet. If you're not interested in paid templates, you may want to filter them out after fetching. Requirements Microsoft Excel for subscriber list Microsoft Outlook for delivering emails OpenAI for AI-generated descriptions Customising the workflow Use AI to summarise the week's trend of templates types and use-cases This template can be the basis for other similar newsletters - just pull in a list of things from anywhere!",9576,2025-04-06 14:53:44.596000+00:00,True,6
2157,ü§ñ Advanced Slackbot with n8n,"Use case Slackbots are super powerful. At n8n, we have been using them to get a lot done.. But it can become hard to manage and maintain many different operations that a workflow can do. This is the base workflow we use for our most powerful internal Slackbots. They handle a lot from running e2e tests for Github branch to deleting a user. By splitting the workflow into many subworkflows, we are able to handle each command seperately, making it easier to debug as well as support new usecases. In this template, you can find eveything to setup your own Slackbot (and I made it simple, there's only one node to configure üòâ). After that, you need to build your commands directly. This bot can create a new thread on an alerts channel and respond there. Or reply directly to the user. It responds for help request to return a help page. It automatically handles unknown commands. It also supports flags and environment variables. For example /cloudbot-test info mutasem --full-info -e env=prod would give you the following info, when calling subworkflow. How to setup Add Slack command and point it up to the webhook. For example. Add the following to the Set config node alerts_channel with alerts channel to start threads on instance_url with this instance url to make it easy to debug slack_token with slack bot token to validate request slack_secret_signature with slack secret signature to validate request help_docs_url with help url to help users understand the commands Build other workflows to call and add them to commands in Set Config. Each command must be mapped to a workflow id with an Execute Workflow Trigger node Activate workflow üöÄ How to adjust Add your own commands. Depending on your need, you might need to lock down who can call this.",9377,2024-03-01 07:31:03.428000+00:00,False,4
3363,Automated Interview Scheduling with GPT-4o and Google Calendar Chat Bot,"‚ú® Overview This workflow allows candidates to schedule interviews through a conversational AI assistant. It integrates with your Google Calendar to check for existing events and generates a list of available 30-minute weekday slots between 9 AM and 5 PM Eastern Time. Once the candidate selects a suitable time and provides their contact information, the AI bot automatically books the meeting on your calendar and confirms the appointment. ‚ö° Prerequisites To use this workflow, you need an OpenAI account with access to the GPT-4o model, a Google account with a calendar that can be accessed through the Google Calendar API, and an active instance of n8n‚Äîeither self-hosted or via n8n cloud. Within n8n, you must have two credential configurations ready: one for Google Calendar using OAuth2 authentication, and another for your OpenAI API key. üîê API Credentials Setup For Google Calendar, go to the Google Cloud Console and create a new project. Enable the Google Calendar API, then create OAuth2 credentials by selecting ‚ÄúWeb Application‚Äù as the application type. Add http://localhost:5678/rest/oauth2-credential/callback as the redirect URI if using local n8n. After that, go to n8n, navigate to the Credentials section, and create a new Google Calendar OAuth2 credential using your account. For OpenAI, visit platform.openai.com to retrieve your API key. Then go to the n8n Credentials page, create a new credential for OpenAI, paste your key, and name it for reference. üîß How to Make This Workflow Yours To customize the workflow for your use, start by replacing all instances of the calendar email rbreen.ynteractive@gmail.com with your own Google Calendar email. This email is referenced in multiple places, including Google Calendar nodes and the ToolWorkflow JSON for the node named ""Run Get Availability."" Also update any instances where the Google Calendar credential is labeled as Google Calendar account to match your own credential name within n8n. Do the same for the OpenAI credential label, replacing OpenAi account with the name of your own credential. Next, go to the node labeled Candidate Chat and copy the webhook URL. This is the public chat interface where candidates will engage with the bot‚Äîshare this URL with them through email, your website, or anywhere you want to allow access. Optionally, you can also tweak the system message in the Interview Scheduler node to modify the tone, language, or logic used during conversations. If you want to add branding, update the title, subtitle, and inputPlaceholder in the Candidate Chat node, and consider modifying the final confirmation message in Final Response to User to reflect your brand voice. You can also update the business rules such as time zone, working hours, or default duration by editing the logic in the Generate 30 Minute Timeslots code node. üß© Workflow Explanation This workflow begins with the Candidate Chat node, which triggers when a user visits the public chat URL. The Interview Scheduler node acts as an AI agent, guiding the user through providing their email, phone number, and preferred interview time. It checks availability using the Run Get Availability tool, which in turn reads your calendar and compares it with generated free time slots from the Generate 30 Minute Timeslots node. The check day names tool helps the AI interpret natural language date expressions like ‚Äúnext Tuesday.‚Äù The schedule is only populated with 30-minute weekday slots from 9 AM to 5 PM Eastern Time, and no events are scheduled if they overlap with existing ones. When a suitable time is confirmed, the AI formats the result into structured JSON, creates an event on your Google Calendar, and sends a confirmation back to the user with all relevant meeting details. üöÄ Deployment Steps To deploy the interview scheduler, import the provided workflow JSON into your n8n instance. Update the Google Calendar email, OpenAI and Google credential labels, system prompts, and branding as needed. Test the connections to ensure the API credentials are working correctly. Once everything is configured, copy and share the public chat URL from the Candidate Chat node. When candidates engage with the chat, the workflow will walk them through the interview booking process, check your availability, and finalize the booking automatically. üí° Additional Tips By default, the workflow avoids scheduling interviews on weekends and outside of 9‚Äì5 EST. Each interview lasts exactly 30 minutes, and overlapping with existing events is prevented. The assistant does not reveal details about other meetings. You can customize every part of this workflow to fit your use case, including subworkflows like Get Availability and check day names, or even white-label it for client use. This workflow is ready to become your AI-powered interview scheduling assistant. ü§ù Connect with Me Description I‚Äôm Robert Breen, founder of Ynteractive ‚Äî a consulting firm that helps businesses automate operations using n8n, AI agents, and custom workflows. I‚Äôve helped clients build everything from intelligent chatbots to complex sales automations, and I‚Äôm always excited to collaborate or support new projects. If you found this workflow helpful or want to talk through an idea, I‚Äôd love to hear from you. Links üåê Website: https://www.ynteractive.com üì∫ YouTube: @ynteractivetraining üíº LinkedIn: https://www.linkedin.com/in/robert-breen üì¨ Email: rbreen@ynteractive.com",9332,2025-03-30 03:22:28.201000+00:00,True,7
3328,Automatically Create and Upload YouTube Videos with Quotes in Thai Using FFmpeg,"Who is this for? This workflow is perfect for digital content creators, marketers, and social media managers who regularly create engaging short-form videos featuring inspirational or motivational quotes. While the workflow is universally applicable, it specifically highlights Thai as an example to demonstrate effective language and font integration. What problem is this workflow solving? Creating consistent and engaging multilingual video content manually, including attractive fonts and proper video formatting, is time-consuming and repetitive. Additionally, managing files, background music, and updating statuses manually can be tedious and prone to errors. What this workflow does Automatically fetches background video and music files stored on Google Drive. Randomly selects a quote (demonstrated with Thai language) and author information from Google Sheets. Dynamically combines the selected quote and author text using appealing fonts, such as the Thai font ""Kanit,"" directly onto the video using FFmpeg on your n8n local environment. Creates visually engaging videos with a 9:16 aspect ratio, optimized for YouTube Shorts and other vertical video platforms. Automatically uploads the finalized video to YouTube. Updates the status and YouTube URL back into your Google Sheet, ensuring you have up-to-date records. Setup Requirements: This workflow requires a self-hosted n8n instance, as the execution of FFmpeg commands is not supported on n8n Cloud. Ensure FFmpeg is installed on your self-hosted environment. Google Sheets Setup: Your Google Sheet must include at least these columns: Index: (Unique identifier for each quote) Quote: (Text of the quote) Author: (Author of the quote) CreateStatus: (Track video creation status; values like 'DONE' or blank for pending) YoutubeURL: (Automatically updated after upload) To help you get started quickly, you can use this template spreadsheet. Next steps: Organize your video and music files in separate folders in Google Drive. Authenticate your Google Sheets, Google Drive, and YouTube accounts in n8n. Ensure fonts compatible with your target languages (such as Kanit for Thai) are available in your FFmpeg installation. How to customize this workflow to your needs Fonts:** Adjust font styles and sizes within the workflow's code node. Ensure the fonts you choose fully support the language you wish to use. Quote Management:** Easily add or remove quotes and authors in your Google Sheets document. Media Files:** Change or update background videos and music by modifying the files in your Google Drive folders. Video Specifications:** Customize video dimensions, text positioning, opacity, and music volume directly in the provided FFmpeg commands. Benefits of Using Localized Fonts and Quotes Utilizing fonts specific to your target language, as demonstrated with Thai, significantly increases audience engagement by making your content more relatable, shareable, and visually appealing. Ensure you select fonts that properly support the language you're targeting.",9268,2025-03-26 03:39:20.929000+00:00,False,4
3342,Automate Sales for Digital Products & SaaS with AI (GPT-4o),"Skyrocket Your Sales Outreach with AI-Powered Automation! Tired of manually finding leads, collecting emails, and sending messages that get ignored? Let AI do the heavy lifting. Introducing AI-Powered Cold Outreach Engine, an n8n workflow that automates prospecting, email discovery, and personalized outreach, designed for digital products, SaaS, and online services. üöÄ How It Works 1Ô∏è‚É£ Smart Prospecting Enter your product name, description, and link. The AI searches Google Maps for businesses in your niche. It extracts website URLs, filtering out irrelevant results. 2Ô∏è‚É£ Email Discovery Scrapes professional emails from websites. Generates a clean, targeted list. 3Ô∏è‚É£ AI-Powered, SEO-Optimized Emails GPT-4o** analyzes website content. Crafts concise, personalized outreach emails (&lt;200 words). Uses SEO-friendly language with strategic keywords. Embeds your product link naturally in a compelling CTA. Sends via Gmail or SMTP with smart delays for better deliverability. üî• Why It Stands Out ‚úÖ Saves Time ‚Äì Automates lead generation & outreach. ‚úÖ Scales Effortlessly ‚Äì Finds and targets ideal prospects. ‚úÖ SEO-Optimized ‚Äì GPT-4o enhances discoverability. ‚úÖ Boosts Replies ‚Äì Personalized emails = higher engagement. ‚úÖ Drives Conversions ‚Äì Directs traffic to your product page. üîß What You Need n8n** (Cloud or self-hosted). OpenAI GPT-4o** (API costs apply). Gmail + Google Cloud OR SMTP node** for email sending. Optional:** Jina AI for advanced data extraction. ‚ö° Quick Setup Import the workflow into n8n. Connect GPT-4o & Gmail or SMTP. Add your product details. Test & launch üöÄ üí° Sell Smarter, Not Harder This isn‚Äôt just automation‚Äîit‚Äôs a growth engine. Let AI handle outreach while you focus on scaling. üîó Get started today! ‚ö†Ô∏è Disclaimer API Fees Apply** ‚Äì OpenAI, Google services may have costs. Email Compliance* ‚Äì Follow *Gmail/SMTP limits & anti-spam laws** (e.g., CAN-SPAM, GDPR). Scraping Updates** ‚Äì Website structures may change over time.",9197,2025-03-28 09:49:32.961000+00:00,True,7
2242,Post New YouTube Videos to X,"Automated YouTube Video Promotion Workflow Automate the promotion of new YouTube videos on X (formerly Twitter) with minimal effort. This workflow is perfect for content creators, marketers, and social media managers who want to keep their audience updated with fresh content consistently. How it works This workflow triggers every 30 minutes to check for new YouTube videos from a specified channel. If a new video is found, it utilizes OpenAI's ChatGPT to craft an engaging, promotional message for X. Finally, the workflow posts the generated message to Twitter, ensuring your latest content is shared with your audience promptly. Set up steps Schedule the workflow to run at your desired frequency. Connect to your YouTube account and set up the node to fetch new videos based on your Channel ID. Integrate with OpenAI to generate promotional messages using GPT-3.5 turbo. Link to your X account and set up the node to post the generated content. Please note, you'll need API keys and credentials for YouTube, OpenAI, and X. Check out this quick video tutorial to make the setup process a breeze. Additional Tips Customize the workflow to match your branding and messaging tone. Test each step to ensure your workflow runs smoothly before going live.",9170,2024-04-27 19:34:00.523000+00:00,True,3
2269,ü¶Ö Get a bird's-eye view of your n8n instance with the Workflow Dashboard!,"‚ö° UPDATE on May 2025 ‚Äì added section with all n8n instance webhooks Using n8n a lot? Soar above the limitations of the default n8n dashboard! This template gives you an overview of your workflows, nodes, and tags ‚Äì all in one place. üí™ Built using XML stylesheets and the Bootstrap 5 library, this workflow is self-contained and does not depend on any third-party software. üôå It generates a comprehensive overview JSON that can be easily integrated with other BI tools for further analysis and visualization. üìä Reach out to Eduard if you need help adapting this workflow to your specific use-case! üöÄ Benefits: Workflow Summary** üìà: Instant overview of your workflows, active counts, and triggers. Left-Side Panel** üìã: Quick access to all your workflows, nodes, and tags for seamless navigation. Workflow Details** üî¨: Deep dive into each workflow's nodes, timestamps, and tags. Node Analysis** üß©: Identify the most frequently used nodes across your workflows. Tag Organization** üóÇÔ∏è: Workflows are grouped according to their tags. Webhooks** ‚ö°: List of all webhook endpoints with the links to workflows. Visually Stunning** üé®: Clean, intuitive, and easy-to-navigate dashboard design. XML & Bootstrap 5** üõ†Ô∏è: Built using XML stylesheets and Bootstrap 5, ensuring a self-contained and responsive dashboard. No Dependencies** üîí: The workflow does not rely on any third-party software. Bootstrap 5 files are loaded via CDN but can be delivered directly from your server. ‚ö†Ô∏è Important note for cloud users Since the cloud version doesn't support environmental variables, please make the following changes: get-nodes-via-jmespath node. Update the instance_url variable: enter your n8n URL instead of {{$env[""N8N_PROTOCOL""]}}://{{$env[""N8N_HOST""]}} Create HTML node. Please provide the n8n instance URL instead of {{ $env.WEBHOOK_URL }} üåüExample: Follow me on LinkedIn for more tips on AI automation and n8n workflows!",8839,2024-05-17 11:16:22.248000+00:00,True,2
2134,Extract emails from website HTMLs,"How to scrap emails from websites This workflow shows how to quickly build an Email scraping API using n8n. Email marketing is at the core of most marketing strategies, be it content marketing, sales, etc. As such, being able to find contacts in bulk for your business on a large scale is key. There are available tools available in the market that can do this, but most are premium; why not build a custom one with n8n? Usage The workflow gets the data from a website and performs an extraction based on the date around on the website Copy the webhook URL to your browser Add a query parameter eg ?Website=https://mailsafi.com . This should give you a URL like this {{$n8nhostingurl/webhook/ea568868-5770-4b2a-8893-700b344c995e?Website=https://mailsafi.com Click on the URL and wait for the extracted email to be displayed. This will return the email address on the website, or if there is no email, the response will be ""workflow successfully executed."" Make sure to use HTTP:// for your domains Otherwise, you may get an error.",8806,2024-02-27 21:32:42.162000+00:00,False,1
2164,Summarize Google Sheets form feedback via OpenAI's GPT-4,"This n8n workflow was developed to collect and summarize feedback from an event that was collected via a Google Form and saved in a Google Sheets document. The workflow is triggered manually by clicking on the ""Test workflow"" button. The Google Sheets node retrieves the responses from the feedback form. The Aggregate node then combines all responses for each question into arrays and prepares the data for analysis. The OpenAI node processes the aggregated feedback data. System Prompt instructs the model to analyze the responses and generate a summary report that includes the overall sentiment regarding the event and constructive suggestions for improvement. The Markdown node converts the summary report, which is in Markdown format, into HTML. Finally, the Gmail node sends an HTML-formatted email to the specified email address.",8532,2024-03-05 09:30:29.981000+00:00,True,3
2233,AI-Powered Children's English Storytelling on Telegram with OpenAI,"Unleashing Creativity: Transforming Children's English Storytelling with Automation and AI Check this example: https://t.me/st0ries95 Summary In the realm of children's storytelling, automation is revolutionizing the way captivating tales are created and shared. This article highlights the transformative power of setting up a workflow for AI-powered children's English storytelling on Telegram. By delving into the use cases and steps involved, we uncover how this innovative approach is inspiring young minds and fostering a love for storytelling in children. Usecase The workflow for children's stories is a game-changer for content creators, educators, and parents seeking to engage children through imaginative and educational storytelling. Here's how this workflow is making a difference: Streamlined Content Creation: By providing a structured framework and automation for story generation, audio creation, and image production, the workflow simplifies the process of crafting captivating children's stories. Enhanced Educational Resources: Teachers can leverage this workflow to develop interactive educational materials that incorporate storytelling, making learning more engaging for students. Personalized Parental Engagement: Parents can share personalized stories with their children, nurturing a passion for reading and creativity while strengthening family bonds through shared storytelling experiences. Community Connection: Organizations and community groups can use the workflow to connect with their audience and promote literacy and creativity by creating and sharing children's stories. Inspiring Imagination: Through automated creation and sharing of enchanting stories, the workflow aims to spark imagination, inspire young minds, and instill a love for storytelling in children. Node Explanation OpenAI Chat Model: Utilizes the OpenAI Chat Model to generate text for the children's stories. Schedule Trigger: Triggers the workflow at set intervals (every 12 hours) to generate new stories. Recursive Character Text Splitter: Splits text into smaller chunks for processing. OpenAI Chat Model2: Another OpenAI Chat Model node for generating prompts for image creation. Send Story Text: Sends the generated story text to a specified Telegram chat. Send Audio for the Story: Sends audio files of the stories to the Telegram chat. Send Story Picture: Shares images related to the stories on Telegram. Create a Kids Stories: Generates captivating short tales for kids using prompts provided. Generate Audio for the Story: Converts generated text into audio files for storytelling. Create a Prompt for DALL-E: Creates prompts for generating images related to the stories. Generate a Picture for the Story: Generates pictures based on the prompts for visual storytelling. By embracing automation in children's storytelling, we unleash creativity, inspire young minds, and create magical experiences that resonate with both storytellers and listeners alike.",8418,2024-04-13 19:30:51.866000+00:00,True,5
3433,Smart Sales Support Chatbot with GPT-4o and Google Sheets,"Who is this tempate for? This workflow powers a simple yet effective customer and sales support chatbot for your webshop. It's perfect for solopreneurs who want to automate customer interactions without relying on expensive or complex support tools. How it works? The chatbot listens to user requests‚Äîsuch as checking product availability‚Äîand automatically handles the following Fetches product information from a Google Sheet Answers customer queries Places an order Updates the stock after a successful purchase Everything runs through a single Google Sheet used for both stock tracking and order management. Setup Instructions Before you begin, connect your Google Sheets credentials by following this guide: This will be used to connect all the tools to Google Sheets üëâ Setup Google sheets credentials Get Stock Open ""Get Stock"" tool node and select the Google sheet credentials you created. Choose the correct google sheet document and sheet name and you are done. Place order Go to your ""Place Order"" tool node and select the Google sheet credentials you have created. Choose the correct google sheet document and sheet name. Update Stock - Open your ""Update Stock"" tool node and select the Google sheet credentials you have created. Choose the correct google sheet document and sheet name. In ""Mapping Column Mode"" section select map each column manually. In ""Column to match on"" select the column with a unique identifier (e.g., Product ID) to match stock items. In values to update section, add only the column(s) that need to be updated‚Äîusually the stock count. AI Agent node Adjust the prompt according to your use case and customize what you need. Google Sheet Template Stock sheet |Case ID|Phone Model|Case Name|Case Type|Image URL|Quantity Avaialble|Initital Inventory|Sold| |-|-|-|-|-|-|-|-| |1023|Iphone 14 pro|Black Leather|Magsafe|https://example.com/url|90|100|10 Order sheet |Case ID|Phone Model|Case Name|Name|Phone Number|Address| |-|-|-|-|-|-| |1023|Black Leather |Iphone 14 pro|Fernando Torres|9998898888|Paris, France",7648,2025-04-04 09:13:57.734000+00:00,True,3
3414,"Slack AI Chatbot for business team with RAG, Claude 3.7 Sonnet and Google Drive","Imagine having an AI chatbot on Slack that seamlessly integrates with your company‚Äôs workflow, automating repetitive requests. No more digging through emails or documents to find answers about IT requests, company policies, or vacation days‚Äîjust ask the bot, and it will instantly provide the right information. With its 24/7 availability, the chatbot ensures that team members get immediate support without waiting for a colleague to be online, making assistance faster and more efficient. Moreover, this AI-powered bot serves as a central hub for internal communication, allowing everyone to quickly access procedures, documents, and company knowledge without searching manually. A simple Slack message is all it takes to get the information you need, enhancing productivity and collaboration across teams. How It Works Slack Trigger: The workflow starts when a user mentions the AI bot in a Slack channel. The trigger captures the message and forwards it to the AI Agent. AI Agent Processing: The AI Agent, powered by Anthropic's Claude 3.7 Sonnet model, processes the query. It uses Retrieval-Augmented Generation (RAG) to fetch relevant information from the company‚Äôs internal knowledge base stored in Qdrant (a vector database). A Simple Memory buffer retains recent conversation context (last 10 messages) for continuity. Knowledge Retrieval: The RAG tool searches Qdrant‚Äôs vector store using OpenAI embeddings to find the most relevant document chunks (top 10 matches). Response Generation: The AI synthesizes the retrieved data into a concise, structured response (1-2 sentences for the answer, 2-3 supporting details, and a source citation). The response is formatted in Slack-friendly markdown (bullet points, blockquotes) and sent back to the user. Set Up Steps Prepare Qdrant Vector Database: Create a Qdrant collection via HTTP request (Create collection node). Optionally, refresh/clear the collection (Refresh collection node) before adding new documents. Load Company Documents: Fetch files from a Google Drive folder (Get folder ‚Üí Download Files). Process documents: Split text into chunks (Token Splitter) and generate embeddings (Embeddings OpenAI2). Store embeddings in Qdrant (Qdrant Vector Store1). Configure Slack Bot: Create a Slack bot via Slack API with required permissions Add the bot to the desired Slack channel and note the channelId for the workflow. Deploy AI Components: Connect the AI Agent to Anthropic‚Äôs model, RAG tool, and memory buffer. Ensure OpenAI embeddings are configured for both RAG and document processing. Test & Activate: Use the manual trigger (When clicking ‚ÄòTest workflow‚Äô) to validate document ingestion. Activate the workflow to enable real-time Slack interactions. Need help customizing? Contact me for consulting and support or add me on Linkedin.",7366,2025-04-03 07:33:30.762000+00:00,True,11
3535,"AI Agent: Scrape, Summarize & Save Articles to Notion (Gemini, Browserless)","This n8n workflow automates the process of saving web articles or links shared in a chat conversation directly into a Notion database, using Google's Gemini AI and Browserless for web scraping. Who is this AI automation template for? It's useful for anyone wanting to reduce manual copy-pasting and organize web findings seamlessly within Notion. A smarter web clipping tool! What this AI automation workflow does Starts when a message is received Uses a Google Gemini AI Agent node to understand the context and manage the subsequent steps. It identifies if a message contains a request to save an article/link. If a URL is detected, it utilizes a tool configured with the Browserless API (via the HTTP Request node) to scrape the content of the web page. Creates a new page in a specified Notion database, populating it with thea summary scraped content, in a specific format, never leaving out any important details. It also saves the original URL, smart tags, publication date, and other metadata extracted by the AI. Posts a confirmation message (e.g., to a Discord channel) indicating whether the article was saved successfully or if an error occurred. Setup Import Workflow: Import this template into your n8n instance. Configure Credentials & Notion Database: Notion Database: Create or designate a Notion database (like the example ""Knowledge Database"") where articles will be saved. Ensure this database has the following properties (fields): Name (Type: Text) - This will store the article title. URL (Type: URL) - This will store the original article link. Description (Type: Text) - This can store the AI-generated summary. Tags (Type: Multi-select) - Optional, for categorization. Publication Date (Type: Date) - *Optional, store the date the article was published. Ensure the n8n integration has access to this specific database. If you require a different format to the Notion Database, not that you will have to update the Notion tool configuration in this n8n workflow accordingly. Notion Credential: Obtain your Notion API key and add it as a Notion credential in n8n. Select this credential in the save_to_notion tool node. Configure save_to_notion Tool: In the save_to_notion tool node within the workflow, set the 'Database ID' field to the ID of the Notion database you prepared above. Map the workflow data (URL, AI summary, etc.) to the corresponding database properties (URL, Description, etc.). In the blocks section of the notion tool, you can define a custom format for the research page, allowing the AI to fill in the exact details you want extracted from any web page! Google Gemini AI: Obtain your API key from Google AI Studio or Google Cloud Console (if using Vertex AI) and add it as a credential. Select this credential in the ""Tools Agent"" node. Discord (or other notification service): If using Discord notifications, create a Webhook URL (instructions) or set up a Bot Token. Add the credential in n8n and select it in the discord_notification tool node. Configure the target Channel ID. Browserless/HTTP Request: Cloud: Obtain your API key from Browserless and configure the website_scraper HTTP Request tool node with the correct API endpoint and authentication header. Self-Hosted: Ensure your Browserless Docker container is running and accessible by n8n. Configure the website_scraper HTTP Request tool node with your self-hosted Browserless instance URL. Activate Workflow: Save test and activate the workflow. How to customize this workflow to your needs Change AI Model:** Experiment with different AI models supported by n8n (like OpenAI GPT models or Anthropic Claude) in the Agent node if Gemini 2.5 Pro doesn't fit your needs or budget, keeping in mind potential differences in context window size and processing capabilities for large content. Modify Notion Saving:** Adjust the save_to_notion tool node to map different data fields (e.g., change the summary style by modifying the AI prompt, add specific tags, or alter the page content structure) to your Notion database properties. Adjust Scraping:** Modify the prompt/instructions for the website_scraper tool or change the parameters sent to the Browserless API if you need different data extracted from the web pages. You could also swap Browserless for another scraping service/API accessible via the HTTP Request node.",7175,2025-04-13 08:42:11.141000+00:00,True,3
2237,Automated Customer Service Ticket Creation & Notifications with Asana & WhatsApp,"How it works: This workflow automates your customer service with built in notifications for your users & ticket creation with Asana. If a user submits a form, he gets send a confirmation message via WhatsApp a task is opened in Asana with his request in it. Setup: You need to add your credentials to the WhatsApp Business Cloud node. You need to add your credentials to the Asana node. Replace the placeholders with the correct phone number, id, and so on. Change the confirmation message to your liking. Optional Changes: You could extend this workflow to update your user on the progress of the ticket in Asana. You can change the messaging from WhatsApp to E-Mail. You can change the form submission service from n8n-native to Typeform or similar. You can change the task management software from Asana to the one you use. Click here to find a blog post with additional information.",7097,2024-04-18 15:06:46.754000+00:00,False,2
3585,"ü§ñ AI Restaurant Assistant for WhatsApp, Instagram & Messenger","Hi, I‚Äôm Amanda! üíå This workflow was created with so much love, care, and attention‚Ä¶ especially for you, who runs a restaurant, a cozy little burger place, or a delivery business full of heart. ü•∞ I know how busy your days can be, so I made this sweet AI assistant to help you take care of your customers on WhatsApp, Instagram, Messenger (or Evolution API). It sends your beautiful menu, checks ZIP codes, creates payment links, and even notifies the kitchen when the order is ready. All gentle, all automatic, all with love. üíõ üí° What this workflow does Replies to customers via WhatsApp API, Instagram Direct, Messenger, and Evolution API Checks ZIP codes to see if delivery is available using Google Maps Sends your menu as images, because food should look as good as it tastes üçï Collects item selections and offers lovely upsells like drinks or extras Creates payment links with the Asaas API Confirms when the payment is complete and sends the order to the kitchen Stores all messages and session data safely in Supabase Uses OpenAI GPT-4o to talk naturally and kindly with your customers ‚öôÔ∏è How to set it up (I‚Äôll guide you with care üß∏) Connect your webhook from WhatsApp, Instagram, Messenger, or Evolution API Create a Supabase table called n8n_workflow_followup You can use this ready-made template here: üëâ Supabase Sheet Template Add your API keys (OpenAI, Supabase, Google Maps, and Asaas) securely in n8n Customize the AI prompt with your brand‚Äôs voice and sweet style üí´ Set your delivery radius (default is 10km, but you can change it!) Upload your menu images (from Google Drive, your website, or any link) That‚Äôs it! Your assistant is now ready to serve with kindness and automation üíï üçØ Works with: ‚úÖ n8n Cloud and Self-Hosted n8n üîê All API credentials are safely stored using n8n‚Äôs secure credential manager Want something customized just for you? Chat with me, I‚Äôd love to help üíªüíõ Chat via WhatsApp (+55 17 99155-7874) . . . Tradu√ß√£o em Portugu√™s: Oi, eu sou a Amanda! üíå Esse workflow foi feito com muito carinho, dedica√ß√£o e cuidado... pensando especialmente em voc√™, que tem um restaurante, lanchonete ou delivery cheio de amor pelo que faz. ü•∞ Eu sei como o dia a dia pode ser corrido, e foi por isso que eu criei esse atendente com IA: pra te ajudar a responder clientes no WhatsApp, Instagram, Messenger (ou Evolution API), enviar card√°pio com imagens lindas, calcular entregas, gerar links de pagamento e at√© avisar a cozinha. Tudo com jeitinho, sem complica√ß√£o, e com muito cora√ß√£o. üíõ üí° O que esse fluxo faz Atende clientes pelo WhatsApp API, Instagram Direct, Messenger e Evolution API Valida CEP e calcula se o cliente est√° dentro da √°rea de entrega (usando Google Maps) Envia card√°pio com imagens, porque comer come√ßa pelos olhos üçï Coleta os pedidos e tamb√©m oferece bebidas e adicionais Gera link de pagamento automaticamente com a API do Asaas Confirma o pagamento e avisa a cozinha quando estiver tudo certo Armazena mensagens, hor√°rios e hist√≥rico no Supabase Usa o GPT-4o da OpenAI pra conversar de forma educada e natural com seus clientes ‚öôÔ∏è Como configurar (com meu passo a passo cheio de cuidado üß∏) Conecte seu webhook do WhatsApp, Instagram, Messenger ou Evolution API Crie uma tabela no Supabase chamada n8n_workflow_followup Voc√™ pode usar esse modelo aqui: üëâ Planilha modelo Supabase Adicione suas chaves de API do OpenAI, Google Maps, Supabase e Asaas no gerenciador do n8n Personalize o prompt da IA com o nome do seu restaurante, estilo de fala e sua magia üí´ Defina a dist√¢ncia m√°xima de entrega (padr√£o: 10km) Coloque seus pr√≥prios links de imagens do card√°pio (pode ser do Drive, site ou CDN) Prontinho! Agora o seu restaurante tem um atendente inteligente, gentil e muito eficiente üíï üçØ Funciona com: ‚úÖ n8n Cloud e n8n auto-hospedado üîê E suas credenciais ficam guardadinhas com seguran√ßa no pr√≥prio n8n, t√° bom? Quer algo feito especialmente pra voc√™? Fala comigo com todo carinho üíªüíõ Falar no WhatsApp (+55 17 99155-7874)",6624,2025-04-17 11:46:33.900000+00:00,True,12
2323,Customer Support Channel and Ticketing System with Slack and Linear,"This n8n workflow demonstrates how to create a really simple yet effective customer support channel and pipeline by combining Slack, Linear and AI tools. Built on n8n's ability to integrate anything, this workflow is intended for small support teams who want to maximise re-use of the tools they already have with an interface which is doesn't require any onboarding. Read the blog post here: https://blog.n8n.io/automated-customer-support-tickets-with-n8n-slack-linear-and-ai/ How it works The workflow is connected to a slack channel setup with the customer to capture support issues. Only messages which are tagged with a ""‚úÖ"" reaction are captured by the workflow. Messages are tagged by the support team in the channel. Each captured support issue is sent to the AI model to classify, prioritise and rewrite into a support ticket. The generated support ticket is uploaded to Linear for the support team to investigate and track. Support team is able to report back to the user via the channel when issue is fixed. Requirements Slack channel to be monitored Linear account and project Customising this workflow Don't have Linear? This workflow can work just as well with traditional ticketing systems like JIRA.",6594,2024-07-09 13:29:02.369000+00:00,True,5
3592,Automate Web Interactions with Claude 3.5 Haiku and Airtop Browser Agent,"About this AI Agent This workflow is designed to automate web interactions by simulating a human user, using a combination of the Agent node and AI tools powered by Airtop. How does this workflow works? Form Submission Trigger: The workflow starts with a form submission trigger node named ""On form submission"". This node collects user instructions for the web AI agent, including a prompt and an optional Airtop profile name for sites requiring authentication. AI Agent: The core of the workflow is the ""AI Agent"" node, which uses a smart web agent to manage a remote web browser. It is designed to fulfill user requests by interacting with the browser through various tools. Browser Session Management Start Browser: The ""Start browser"" node initiates a new browser session and window. It is essential for obtaining the sessionId and windowId required for subsequent operations. Session and Window Management: The workflow includes nodes for creating and managing browser sessions and windows, such as ""Session"" and ""Window"". Web Interaction Tools: Load URL: This node loads a specified URL into the browser window. Query: The ""Query"" node allows the agent to ask questions and extract information from the current web page. Click: This node simulates clicking on elements within the web page. Type: The ""Type"" node types text into specified elements on the page. Session Termination: The ""End session"" node is used to terminate the browser session once the tasks are completed. Output Handling Structured Output Parser: This node processes the agent's results into a structured format. Output: The final results are set and prepared for output. Slack Integration: Although currently disabled, there is a ""Slack"" node intended to send messages to a Slack channel, potentially for notifications or live view URLs. Seting up your agent Airtop API Credentials: Users must have valid Airtop API credentials to interact with the web browser tools. This includes nodes like ""Click"", ""Query"", ""Load URL"", ""End session"", ""Type"", ""Session"", and ""Window"". Slack API Credentials (Optional): If users want to enable Slack notifications, they need to configure Slack OAuth2 credentials. The Slack node is currently disabled but can be used to send messages to a Slack channel. Anthropic API Credentials: The ""Claude 3.5 Haiku"" node requires Anthropic API credentials. Users need to have access to this API to utilize the language model features.",6534,2025-04-17 19:32:52.649000+00:00,True,6
2331,"Build Your Own Image Search Using AI Object Detection, CDN and ElasticSearch","This n8n workflow demonstrates how to automate indexing of images to build a object-based image search. By utilising a Detr-Resnet-50 Object Classification model, we can identify objects within an image and store these associations in Elasticsearch along with a reference to the image. How it works An image is imported into the workflow via HTTP request node. The image is then sent to Cloudflare's Worker AI API where the service runs the image through the Detr-Resnet-50 object classification model. The API returns the object associations with their positions in the image, labels and confidence score of the classification. Confidence scores of less the 0.9 are discarded for brevity. The image's URL and its associations are then index in an ElasticSearch server ready for searching. Requirements A Cloudflare account with Workers AI enabled to access the object classification model. An ElasticSearch instance to store the image url and related associations. Extending this workflow Further enrich your indexed data with additional attributes or metrics relevant to your users. Use a vectorstore to provide similarity search over the images.",6369,2024-07-10 12:35:33.563000+00:00,True,3
2290,Store Notion's Pages as Vector Documents into Supabase with OpenAI,"Workflow updated on 17/06/2024:** Added 'Summarize' node to avoid creating a row for each Notion content block in the Supabase table.* Store Notion's Pages as Vector Documents into Supabase This workflow assumes you have a Supabase project with a table that has a vector column. If you don't have it, follow the instructions here: Supabase Langchain Guide Workflow Description This workflow automates the process of storing Notion pages as vector documents in a Supabase database with a vector column. The steps are as follows: Notion Page Added Trigger: Monitors a specified Notion database for newly added pages. You can create a specific Notion database where you copy the pages you want to store in Supabase. Node: Page Added in Notion Database Retrieve Page Content: Fetches all block content from the newly added Notion page. Node: Get Blocks Content Filter Non-Text Content: Excludes blocks of type ""image"" and ""video"" to focus on textual content. Node: Filter - Exclude Media Content Summarize Content: Concatenates the Notion blocks content to create a single text for embedding. Node: Summarize - Concatenate Notion's blocks content Store in Supabase: Stores the processed documents and their embeddings into a Supabase table with a vector column. Node: Store Documents in Supabase Generate Embeddings: Utilizes OpenAI's API to generate embeddings for the textual content. Node: Generate Text Embeddings Create Metadata and Load Content: Loads the block content and creates associated metadata, such as page ID and block ID. Node: Load Block Content & Create Metadata Split Content into Chunks: Divides the text into smaller chunks for easier processing and embedding generation. Node: Token Splitter",6262,2024-06-12 10:33:37.912000+00:00,True,5
3666,Real Estate Lead Generation with BatchData Skip Tracing & CRM Integration,"How It Works This workflow automates the entire property lead generation process in a few simple steps: Property Search: Connects to BatchData's Property Search API with customizable parameters (location, property type, value range, equity percentage, etc.) Lead Filtering & Scoring: Processes results to identify the most promising leads based on criteria like absentee ownership, years owned, equity percentage, and tax status. Each property receives a lead score to prioritize follow-up. Skip Tracing: Automatically retrieves owner contact information (phone, email, mailing address) for each qualified property. Data Formatting: Structures all property and owner data into a clean, organized format ready for your systems. Multi-Channel Output: Generates an Excel spreadsheet with all lead details Pushes leads directly to your CRM (configurable for HubSpot, Salesforce, etc.) Sends a summary email with the spreadsheet attached The workflow can run on a daily schedule or be triggered manually as needed. All parameters are easily configurable through dedicated nodes, requiring no coding knowledge. Who's It For This workflow is perfect for: Real Estate Investors looking to find off-market properties with motivated sellers Real Estate Agents who want to generate listing leads from distressed or high-equity properties Investment Companies that need regular lead flow for acquisitions Real Estate Marketers who run targeted campaigns to property owners Wholesalers seeking to build a pipeline of potential deals Property Service Providers (roof repair, renovation contractors, etc.) who target specific property types Anyone who needs reliable, consistent lead generation for real estate without the manual work of searching, filtering, and organizing property data will benefit from this automation. About BatchData BatchData is a comprehensive property data provider that offers access to nationwide property information, owner details, and skip tracing services. Key features include: Extensive Database: Covers 150+ million properties across all 50 states Rich Property Data: Includes ownership information, tax records, sales history, valuation estimates, equity positions, and more Skip Tracing Services: Provides owner contact information including phone numbers, email addresses, and mailing addresses Distressed Property Indicators: Flags for pre-foreclosure, tax delinquency, vacancy, and other motivation factors RESTful API: Professional API for programmatic access to all property data services Regular Updates: Continuously refreshed data for accurate information BatchData's services are designed for real estate professionals who need reliable property and owner information to power their marketing and acquisition strategies. Their API-first approach makes it ideal for workflow automation tools like N8N.",6215,2025-04-22 21:27:31.650000+00:00,False,4
2333,Recipe Recommendations with Qdrant and Mistral,"This n8n workflow demonstrates creating a recipe recommendation chatbot using the Qdrant vector store recommendation API. Use this example to build recommendation features in your AI Agents for your users. How it works For our recipes, we'll use HelloFresh's weekly course and recipes for data. We'll scrape the website for this data. Each recipe is split, vectorised and inserted into a Qdrant Collection using Mistral Embeddings Additionally the whole recipe is stored in a SQLite database for later retrieval. Our AI Agent is setup to recommend recipes from our Qdrant vector store. However, instead of the default similarity search, we'll use the Recommendation API instead. Qdrant's Recommendation API allows you to provide a negative prompt; in our case, the user can specify recipes or ingredients to avoid. The AI Agent is now able to suggest a recipe recommendation better suited for the user and increase customer satisfaction. Requirements Qdrant vector store instance to save the recipes Mistral.ai account for embeddings and LLM agent Customising the workflow This workflow can work for a variety of different audiences. Try different sets of data such as clothes, sports shoes, vehicles or even holidays.",6207,2024-07-10 13:30:56.749000+00:00,True,10
2152,API that extracts engaging moments from YouTube videos,"How it works This template takes a YouTube video ID and identifies potentially engaging moments based on the intensity of specific timestamps üëá Ideal for vloggers and YouTube content creators, it serves as a foundation for various automations to streamline content calendars or highlight popular moments in your videos. You can leverage it for: Automatic processes to analyze YouTube videos and create sizzle reels or clips for social media, particularly effective for microcontent strategies like those endorsed by Gary Vee. Instant alerts via Slack, Telegram, Email, or WhatsApp when significant moments occur in your videos. Utilizing transcripts of these moments with AI to refine content ideas or brainstorm chatbots in your editorial workflow. Example response from the Workflow-as-an-API A GET request to {your instance URL}/webhook/youtube-engaging-moments-extractor?ytID=IZsQqarWXtYy returns üëá The workflow generates multiple moments; the screenshot above shows a truncated version. Not all videos contain timestamp intensity data, the workflow handles this case as well üëá How to use Import the template into your n8n workspace or self-hosted instance, then activate the workflow. Open the Webhook trigger node and copy the Production URL. In a web browser or any tool capable of consuming HTTP Requests (e.g., native code, Bubble app, n8n workflow, another automation tool, Postman, etc.), pass along the URL parameter ?ytID={youtube video ID} when invoking the API endpoint. Your URL should resemble something like https://acme.app.n8n.cloud/webhook/youtube-engaging-moments-extractor?ytID=IZsQqarWXtYy. Keep in mind This workflow relies on an unofficial YouTube API graciously hosted for free by the folks at lemnoslife.com. It's not recommended for high-volume production usecases.",6147,2024-02-29 08:36:52.732000+00:00,True,1
3546,Screen and Score Resumes from Gmail to Sheets with AI,"Description This intelligent n8n automation streamlines the process of collecting, extracting, and scoring resumes sent to a Gmail inbox‚Äîmaking it an ideal solution for recruiters who regularly receive hundreds of applications. The workflow scans incoming emails with attachments, extracts relevant candidate information from resumes using AI, evaluates each candidate based on customizable criteria, and logs their scores alongside contact details in a connected Google Sheet. Who Is This For? Recruiters & Hiring Managers**: Automate the resume screening process and save hours of manual work. HR Teams at Startups & SMBs**: Quickly evaluate talent without needing large HR ops infrastructure. Agencies & Talent Acquisition Firms**: Screen large volumes of resumes efficiently and with consistent criteria. Solo Founders Hiring for Roles**: Use AI to help score and shortlist top candidates from email applications. What Problem Does This Workflow Solve? Manually reviewing resumes is time-consuming, error-prone, and inconsistent. This workflow solves these challenges by: Automatically detecting and extracting resumes from Gmail attachments. Using OpenAI to intelligently extract candidate info from unstructured PDFs. Scoring resumes using customizable evaluation criteria (e.g., relevant experience, skills, education). Logging all candidate data (Name, Email, LinkedIn, Score) in a centralized, filterable Google Sheet. Enabling faster, fairer, and more efficient candidate screening. How It Works 1. Gmail Trigger Runs on a scheduled interval (e.g., every 6 or 24 hours). Scans a connected Gmail inbox (using OAuth credentials) for unread emails that contain PDF attachments. 2. Extract Attachments Downloads the attached resumes from matching emails. 3. Parse Resume Text Sends the PDF file to OpenAI's API (via GPT-4 or GPT-3.5 with file support or via base64 + PDF-to-text tool). Prompts GPT with a structured format to extract fields like Name, Email, LinkedIn, Skills, and Education. 4. Score Resume Evaluates the resume on predefined scoring logic using AI or logic inside the workflow (e.g., ""Has X skill = +10 points""). 5. Log to Google Sheets Appends a new row in a connected Google Sheet, including: Candidate Name Email Address LinkedIn URL Resume Score Setup Accounts & API Keys You‚Äôll need accounts and credentials for: n8n** (hosted or self-hosted) Google Cloud Platform** (for Gmail, Drive, and Sheets APIs) OpenAI** (for GPT model access) Google Sheet Make a Google Sheet and connect it via Google Sheets node in n8n. Columns should include: Name Email LinkedIn Score Configuration Google Cloud: Enable Gmail API and Google Sheets API. Set up OAuth 2.0 Credentials in Google Console. Connect n8n Gmail, Drive, and Sheets nodes to these credentials. OpenAI: Generate an API Key. Use the HTTP Request node or official OpenAI node to send prompt requests. n8n Workflow: Add Gmail Trigger. Add extraction logic (e.g., filter PDFs). Add OpenAI prompt for resume parsing and scoring. Connect structured output to a Google Sheets node. Requirements Accounts: n8n** Google** (Gmail, Sheets, Drive, Cloud Console) OpenAI** API Keys & Credentials: OpenAI API Key Google Cloud OAuth Credentials Gmail Access Scopes (for reading attachments) Configured Google Sheet OpenAI usage (after free tier) Google Cloud API usage (if exceeding free quota)",5976,2025-04-14 11:05:17.689000+00:00,True,4
2144,Turn any RSS feed into email,"Use case This workflow automatically turns any RSS feed into a newsletter. It send an email every time a new post is published to an RSS feed in the last hour. I have been using this personally to follow Derek Sivers and Anil Dash, who don't have newsletters. Check them out üòâ How to setup Add your email and email creds Add the RSS feed URLs you want to follow How to adjust this template Add the feeds you want to follow. It does not have to be just email. Why not get a telegram message? Or Slack message? Ah the power of n8n üöÄ",5878,2024-02-28 17:28:48.484000+00:00,False,1
3605,Gmail MCP Server ‚Äì Your All‚Äëin‚ÄëOne AI Email Toolkit,"Gmail MCP Server Expose Gmail‚Äôs full API as a single SSE ‚Äútool server‚Äù endpoint for your AI agents. What it does Spins up an MCP Trigger that streams Server‚ÄëSent Events to LangChain/N8N AI Agent nodes. ‚Äã Maps 20+ common Gmail operations (search, send, reply, draft, label & thread management, mark read/unread, delete, etc.) to ai_tool connections, so agents can invoke them with a simple JSON payload. Why you‚Äôll love it Agent‚Äëready: Plug the SSE URL into any N8N Agent or any other AI tool that uses MCP and start reasoning over email immediately. Extensible: Add more GmailTool operations or swap credentials without touching your agent logic. How to use Import the workflow (n8n ‚â• v1.88). Set up a gmailOAuth2 credential and select it on the GmailTool nodes. Open the Gmail MCP Server node, copy the SSE URL, and paste it into your AI agent‚Äôs ‚ÄúTool Server‚Äù field.",5811,2025-04-18 20:12:47.018000+00:00,True,0
3631,Build your own PostgreSQL MCP server,"This n8n demonstrates how to build a simple PostgreSQL MCP server to manage your PostgreSQL database such as HR, Payroll, Sale, Inventory and More! This MCP example is based off an official MCP reference implementation which can be found here -https://github.com/modelcontextprotocol/servers/tree/main/src/postgres How it works A MCP server trigger is used and connected to 5 tools: 2 postgreSQL and 3 custom workflow. The 2 postgreSQL tools are simple read-only queries and as such, the postgreSQL tool can be simply used. The 3 custom workflow tools are used for select, insert and update queries as these are operations which require a bit more discretion. Whilst it may be easier to allow the agent to use raw SQL queries, we may find it a little safer to just allow for the parameters instead. The custom workflow tool allows us to define this restricted schema for tool input which we'll use to construct the SQL statement ourselves. All 3 custom workflow tools trigger the same ""Execute workflow"" trigger in this very template which has a switch to route the operation to the correct handler. Finally, we use our standard PostgreSQL node to handle select, insert and update operations. The responses are then sent back to the the MCP client. How to use This PostgreSQL MCP server allows any compatible MCP client to manage a PostgreSQL database by supporting select, create and update operations. You will need to have a database available before you can use this server. Connect your MCP client by following the n8n guidelines here - https://docs.n8n.io/integrations/builtin/core-nodes/n8n-nodes-langchain.mcptrigger/#integrating-with-claude-desktop Try the following queries in your MCP client: ""Please help me check if Alex has an entry in the users table. If not, please help me create a record for her."" ""What was the top selling product in the last week?"" ""How many high priority support tickets are still open this morning?"" Requirements PostgreSQL for database. This can be an external database such as Supabase or one you can host internally. MCP Client or Agent for usage such as Claude Desktop - https://claude.ai/download Customising this workflow If the scope of schemas or tables is too open, try restrict it so the MCP serves a specific purpose for business operations. eg. Confine the querying and editing to HR only tables before providing access to people in that department. Remember to set the MCP server to require credentials before going to production and sharing this MCP server with others!",5672,2025-04-21 10:09:09.689000+00:00,False,2
2217,Configure your own Image Creation API Using OpenAI DALLE-3,"How it works: Webhook URL that responds to Requests with an AI generated Image based on the prompt provided in the URL. Setup Steps: Ideate your prompt URL Encode The Prompt (as shown in the Template) Authenticate with your OpenAI Credentials Put together the Webhook URL with your prompt and enter into a webbrowser In this way you can expose a public url to users, employee's etc. without exposing your OpenAI API Key to them. Click here to find a blog post with additional information.",5653,2024-04-08 09:48:55.111000+00:00,True,1
3553,AI-Powered YouTube Shorts Automation: Create & Publish with OpenAI & ElevenLabs,"üë• Who is this for? Digital marketers, content creators, social media managers, and businesses who want to use AI marketing automation for YouTube Shorts without spending hours on production. This AI workflow helps anyone looking to create more short-form video marketing content without needing fancy editing skills or tons of time. It's perfect for marketing teams who want to automate content creation while keeping their brand's voice and boosting audience engagement through artificial intelligence technology. üîç What problem does this workflow solve? Let's be honest - creating high-performing YouTube Shorts consistently takes way too much work. You need AI script generation, voiceover production, video editing, and a solid content marketing strategy to keep your channel growing. Our intelligent automation workflow turns this whole headache into a simple two-click process, while still giving you videos that look and sound professional. This AI solution tackles the real marketing challenge of scaling short-form video production without sacrificing quality or burning through your team's resources. ‚öôÔ∏è What this workflow does This all-in-one AI marketing automation solution handles your entire YouTube Shorts creation process through five easy steps: üß† Smart Idea Generation**: Advanced AI creates engaging video concepts, SEO-optimized titles, and descriptions that work with YouTube's algorithm and improve content marketing performance üé§ Natural Voice Creation**: Makes professional-sounding voiceovers using ElevenLabs' artificial intelligence technology, no recording equipment needed üñºÔ∏è Automatic Visual Creation**: Uses cutting-edge AI models to make custom images and animated clips that match your video marketing style and brand identity üé¨ Smart Video Editing**: AI-powered editing automatically combines all elements with smooth transitions for maximum viewer retention and marketing impact üì± Easy Content Distribution**: Automates the YouTube publishing process with proper metadata to maximize your marketing reach and content discovery The whole marketing automation system runs through Telegram with just two human checkpoints - approving the initial AI idea and giving the final video a thumbs-up. This makes YouTube marketing automation so much easier while letting you keep control over the AI-generated content that gets published. üõ†Ô∏è Setup (About 10-15 minutes) Before using this AI marketing automation tool, you'll need: n8n installation (cloud or self-hosted) Telegram account (to interact with your workflow) OpenAI API Key (for AI content generation) ElevenLabs API Key (for AI voice creation) Replicate API Key (for AI video generation) Cloudinary account (for media asset storage) Creatomate API Key (for automated video assembly) 0CodeKit API Key (for script processing automation) YouTube channel with API access (for content publishing) Just add your API keys to the ""Set API Keys"" node, connect your Telegram bot, and you're ready to start your automated marketing content creation. We've included a step-by-step video walkthrough in the workflow to make setup super easy for your AI marketing system. üîß How to customize this workflow to your needs This AI marketing workflow offers tons of flexibility to fit your specific content strategy: Voice Options**: Pick from different AI voices to match your brand's tone and what your audience prefers in marketing content Visual Styles**: Choose from over 40 different AI image models to get the perfect visual marketing approach Video Effects**: Try various artificial intelligence video models for different animation styles that make your content marketing more engaging Content Tweaking**: Adjust the AI prompts to perfectly align with your brand voice and content marketing strategy YouTube Settings**: Easily optimize category selection and metadata for better audience targeting with your automated marketing content Every part of the AI automation workflow comes with easy-to-follow instructions in color-coded notes, so even if you're new to marketing automation tools, you can still customize everything to fit your needs. Transform your YouTube Shorts marketing strategy with this complete AI automation solution that brings together multiple artificial intelligence technologies to help you create consistent, high-quality short-form video content that drives engagement and scales your digital marketing efforts without the usual production headaches.",5428,2025-04-15 00:18:11.214000+00:00,True,8
2199,Auto Tweet: spreadsheet-to-Tweet Automation,"üéâ Do you want to master AI automation, so you can save time and build cool stuff? I‚Äôve created a welcoming Skool community for non-technical yet resourceful learners. üëâüèª Join the AI Atelier üëàüèª This is probably the most straightforward way to schedule your tweets. This n8n workflow addresses a simple problem: how to maintain consistency in your publications on X. üìã Blog post üì∫ Youtube Video The workflow is direct and to the point. In a Google spreadsheet, write all your tweets, one tweet per line. Then, at a defined pace (for example, every 6 hours), the workflow will publish the first tweet from the list on your X account and remove it from the sheet. This method is really simple but very efficient if you are looking for a direct and super fast solution to share regularly on X! Created by the n8ninja.",5284,2024-03-27 16:34:02.366000+00:00,True,2
3596,Daily AI News Translation & Summary with GPT-4 and Telegram Delivery,"üìù What this workflow does Every morning at 8 a.m., this workflow fetches the latest AI-related articles from both GNews and NewsAPI. It merges up to 40 new articles daily, selects the 15 most relevant ones on AI technology and applications, and uses GPT-4.1 to generate concise summaries in accurate Traditional Chinese (while preserving essential English technical terms). Each summary also includes the article link for easy referral. The compiled digest is then posted to your designated Telegram account or group. üë• Who is this for? AI enthusiasts, professionals, and anyone interested in artificial intelligence news Individuals and teams wanting a concise daily digest of AI developments in Traditional Chinese Telegram users who prefer automated information delivery üéØ What problem does this workflow solve? With the rapid evolution of AI technology, it can be overwhelming to keep up with new developments. This workflow addresses information overload by automatically collecting, summarizing, and translating the most important AI news each morning ‚Äî all delivered conveniently to your chosen Telegram channel or group. ‚öôÔ∏è Setup üîë Add NewsAPI and GNews API Keys Register for accounts on NewsAPI.org and GNews to obtain your API keys. Input your NewsAPI key directly into the Fetch NewsAPI articles node. Input your GNews API key into the Fetch GNews articles node. ü§ñ Set up your Telegram Bot Create a Telegram Bot via BotFather and copy the generated Bot Token. In n8n, create Telegram Bot credentials using this token. In the Send summary to Telegram node, enter the chat ID of your target user, group, or channel to receive the messages. üß† Configure OpenAI Credentials In n8n, create a new credential using your OpenAI API key. Assign this credential to the GPT-4.1 Model node (or equivalent OpenAI/AI nodes). After completing these steps, your workflow is fully configured to fetch, summarize, and deliver daily AI news to your selected Telegram chat automatically. üõ†Ô∏è How to customize this workflow üîç Change the topic:** Update the keywords in the NewsAPI and GNews nodes for other subjects (e.g., ""blockchain"", ""quantum computing""). ‚è∞ Adjust delivery time:** Modify the scheduled trigger to your preferred hour. ‚úçÔ∏è Tweak summary style or language:** Refine the prompt in the AI summarizer node for different tones or translate into other languages as needed. üì¶ Dependencies NewsAPI account GNews account Telegram Bot OpenAI API access (for GPT-4.1) or compatible AI model for Langchain agent",5241,2025-04-18 07:05:41.418000+00:00,True,4
2123,Simple Google indexing Workflow in N8N,"What it does The workflow is a simple yet efficient way to automate the process of indexing your website on Google using the Google Indexing API. How it works It works by extracting information from your sitemap, converting it into a JSON file, and looping through each URL to submit it for indexing. Here's a brief rundown of the workflow: The workflow can be triggered manually via the ""Execute Workflow"" button or scheduled to run at a specific time using the ""Schedule Trigger"" node. The sitemap of your website is fetched using the ""sitemap_set"" node with a HTTP Request to the sitemap URL. This XML sitemap is then converted into a JSON file using the ""sitemap_convert"" node. The ""sitemap_parse"" node splits the JSON file into individual URLs. The ""url_set"" node then prepares each URL to be sent to the Google Indexing API. A loop is created using the ""loop"" node to process each URL individually and make a POST request to Google Indexing API indicating that the URL has been updated. If the POST request is successful and the URL has been updated, the workflow waits for 2 seconds before moving to the next URL. In case the daily limit for the Google Indexing API is reached (200/day by default), an error message is triggered using the ""Stop and Error"" node. Before you use the workflow Activate the indexing API Create an account with Google Cloud Platform &gt; Console and then create a new project Search for the Indexing API in the Library Activate the API Create a Service Account and get credentials Open the Service accounts page. If prompted, select a project. Click add Create Service Account, enter a name and description for the service account. You can use the default service account ID, or choose a different, unique one. When done click Create. On the Grant users access to this service account screen, scroll down to the Create key section. Click add Create key. In the side panel that appears, select the JSON format Click Create. Your new public/private key pair is generated and downloaded to your machine. Open the file and copy the private key. Add the credentials in the url_index node Add the user as owner of the site Beware, for each site you need to add the user as a owner like this: Set your sitemap Open the sitemap_set node and add the url to your sitemap. Now you should be able to ensure that Google is always up-to-date with the latest content on your website, improving your website's visibility and SEO rankings, have fun!",5211,2024-02-23 08:10:35.720000+00:00,False,1
2195,Simplest way to create a Stripe Payment Link,"üéâ Do you want to master AI automation, so you can save time and build cool stuff? I‚Äôve created a welcoming Skool community for non-technical yet resourceful learners. üëâüèª Join the AI Atelier üëàüèª Accepting payments via credit card online is a crucial component for the majority of businesses. Stripe provides a robust suite of tools for processing payments, yet many people still find it challenging to create a simple payment page and distribute it to their customers. üìã Blog post üì∫ Youtube Video This n8n workflow aims to offer the simplest and most direct method for generating a Stripe payment link. Features Quick Stripe Payment Link Creation:** Simply enter a title and select a price to create a Stripe payment link in seconds. Set Up Steps Connect your Stripe credentials. Fill the config node (currency). This n8n workflow template is crafted to significantly reduce the creation time of a Stripe Payment link. Created by the n8ninja.",5137,2024-03-26 14:22:15.711000+00:00,True,1
2219,Transforming Emails into Podcasts,"Transforming Emails into Podcasts üéôÔ∏è Check out this channel for example. The n8n workflow described here aims to revolutionize the way users engage with promotional emails by converting them into entertaining audio podcasts. This innovative project leverages automation through n8n to streamline tasks and enhance user experience. Project Benefit üéßüåü The primary goal of this project is to transform ""CATEGORY_PROMOTIONS"" emails into engaging audio content. By converting text into speech, users can enjoy promotional material hands-free, making it easier to consume information while on the go or relaxing. The workflow consists of several key steps orchestrated seamlessly to deliver a delightful experience to users. How to Use the Workflow: Gmail trigger Node: Initiates the workflow by fetching ""CATEGORY_PROMOTIONS"" emails at regular intervals. The Gmail Trigger node in your N8N workflow is set to poll for new emails every minute and is configured to filter emails with the label ""CATEGORY_PROMOTIONS"" before triggering the workflow. Steps to Use Filters Inside the Gmail Trigger Node: Configure Gmail Trigger Node: Set ""Poll Times"" to ""Every Minute"" to check for new emails at regular intervals. Enable the ""Simple"" toggle if you want to simplify the node interface. Under ""Filters"", specify the label IDs you want to filter by. In this case, it's set to ""CATEGORY_PROMOTIONS"". Adjust any additional options as needed. // Configure Gmail Trigger node pollTimes: { item: [ { mode: ""everyMinute"" } ] }, simple: false, filters: { labelIds: [ ""CATEGORY_PROMOTIONS"" ] }, options: {} Save and Execute: Save your workflow and execute it to start monitoring your Gmail account for new emails with the specified label filter. By following these steps, your workflow will effectively trigger based on new emails that match the ""CATEGORY_PROMOTIONS"" label in your Gmail account. Get message content Node: Extracts the email content for processing. Summarization Chain Node: Generates concise summaries using advanced methods for better readability. Delete the unnecessary items Node: Removes irrelevant details from the email content. Text to Free TTS Node: Converts the summary text into speech using Free TTS technology. Convert from base64 to File Node: Transforms the audio data into a compatible file format. Merge Text with Audio Node: Combines the text and audio components seamlessly. Aggregate in same cell Node: Gathers all processed data for finalization. Send Message to Telegram Node: Dispatches the audio message along with a caption to a designated Telegram chat ID. By automating these tasks, the workflow ensures efficient communication and delivers content in a more engaging format, fostering a positive user experience. Configuration Instructions: The configuration of this workflow involves setting up the necessary nodes and establishing connections between them. Each node performs a specific function crucial to the overall operation of the workflow. Additionally, credentials need to be provided for accessing Gmail and OpenAI services to enable seamless data processing and summarization. Utilizing Text-to-Speech API üéß In addition to n8n automation, an external Text-to-Speech API plays a pivotal role in generating audio content from text data. By sending a POST request with JSON data containing the text and voice preferences, users can quickly receive audio files of the converted content. The API offers a straightforward interface for text-to-speech conversion, making it ideal for creating audio clips efficiently. To access this API, simply submit the desired text and voice selection to receive the generated speech audio file. The API endpoint can be accessed at https://tiktok-tts.weilnet.workers.dev/api/generation or through https://tiktokvoicegenerator.com/. In conclusion, this n8n workflow coupled with a Text-to-Speech API presents a powerful solution for transforming emails into captivating podcasts, enhancing user engagement and communication effectiveness. By embracing automation and innovative technologies, this project aims to improve user experience and streamline content delivery processes. üåà‚ú®üöÄ",5126,2024-04-08 21:09:20.159000+00:00,True,6
3567,Automate PDF Image Extraction & Analysis with GPT-4o and Google Drive,"Use Case Manually extracting images from PDF files for analysis is often slow and inefficient. Many users resort to taking screenshots of each page, uploading them to an AI tool like OpenAI for image analysis, and then manually copying the insights into a document. This manual process is time-consuming and prone to errors. This workflow streamlines the entire process by automatically extracting images from a PDF, analyzing them using the GPT-4o model, and saving the results in seconds‚Äîeliminating the need for manual effort. What This Workflow Does Extracts all images from the uploaded PDF file automatically The workflow scans each page of the PDF and identifies embedded images without manual intervention. Uses the GPT-4o model to analyze each extracted image Each image is processed through GPT-4o to generate descriptive insights, summaries, or context-specific analysis depending on the use case. Saves the analysis results to a .txt file, including image URLs The final output is a plain text file containing both the image URLs (e.g., hosted on cloud storage) and the corresponding GPT-4o analysis, ready for further use or sharing. Setup 1.Set up your credentials when you first open the workflow. You‚Äôll need accounts for OpenAI, Convert API, and Google Drive. 2.Convert API does not rate-limit your API, sometimes you may receive 503 service unavailable error. Nevertheless, it doesn‚Äôt mean that you cannot convert your file. It simply means that you should retry the conversion in a few seconds. 3.Upload a PDF with images to Google Drive. 4.Remove unnecessary parts and retrieve image-related information. 5.Integrate image and image analysis information together. 6.Analyze each image using the OPENAI GPT-4o model. 7.Retrieve all image analysis content and image URL 8.Integrate multiple image URLs and analysis content 9.Output content to a .txt file. Template was created in n8n v1.83.2 How to Customize Replace the manual trigger with a Google Drive trigger or other automation triggers Change the image analysis model (e.g., switch or fine-tune GPT-4o) Send the results to other platforms (e.g., Slack, Telegram, LINE, etc.) instead of saving to a .txt file",5049,2025-04-16 02:02:34.526000+00:00,True,4
2289,Restore backed up workflows from GitHub to n8n,"Restore backed up workflows from GitHub to your n8n workspace. This workflow was inspired by this one that lets you back up your n8n workflows to GitHub. It will let you restore your backed up workflows in your workspace, without creating duplicates. In case of issue with your instance, it will save you a lot of time to restore them. How it works It retrieves the workflows saved in a GitHub repository. Then compares these saved workflows with the ones in your n8n workspace based on the name. It will only create them if they don't already exist. Set up steps Open the ""Global"" node and set your own information (see Configuration below) Click on ""Test workflow"" It will run through all the workflows in the GitHub repository, check if the name doesn't already exist in your workspace and, in this case, create it. Configuration repo.owner: your GitHub owner name repo.name: your GitHub repository name repo.path: the path within the GitHub repository",4987,2024-06-11 09:05:22.408000+00:00,False,1
2347,Query n8n Credentials with AI SQL Agent,"This n8n workflow is a fun way to query and search over your credentials on your n8n instance. Good to know Your credentials should remain safe as this workflow does not decrypt or use any decrypted data. Example Usage ""Which workflows are using Slack and Google Calendar?"" ""Which workflows have AI in their name but are not using openAI?"" How it works Using the n8n API, it fetches all workflow data on the instance. Workflow data contains references to credentials used so this will be extracted. With some necessary reformatting, the workflows and their credentials metadata are stored to a SQLite database. Next, an AI agent is used with a custom SQL tool that reads the SQLite database created in the previous step. The AI agent is instructed to perform SQL queries against our workflow credential table when asked about credentials by the user. Requirements You'll need an n8n API key. Please note that only workflows will be scoped to your API key. Customising the workflow Add extra table fields to the SQLite database to answer even more complex queries such as: workflow status to differentiate between active and inactive workflows.",4893,2024-07-15 20:42:12.214000+00:00,True,5
2330,Enrich Property Inventory Survey with Image Recognition and AI Agent,"This n8n workflow assists property managers and surveyors by reducing the time and effort it takes to complete property inventory surveys. In such surveys, articles and goods within a property may need to be captured and reported as a matter of record. This can take a sizable amount of time if the property or number of items is big enough. Our solution is to delegate this task to a capable AI Agent who can identify and fill out the details of each item automatically. How it works An AirTable Base is used to capture just the image of an item within the property Our workflow monitoring this AirTable Base sends the photo to an AI image recognition model to describe the item for purpose of identification. Our AI agent uses this description and the help of Google's reverse image search in an attempt to find an online product page for the item. If found, the product page is scraped for the item's specifications which are then used to fill out the rest of the details of the item in our Airtable. Requirements Airtable for capturing photos and product information OpenAI account to for image recognition service and AI for agent SerpAPI account for google reverse image search. Firecrawl.dev account for webspacing. Customising this workflow Try building an internal inventory database to query and integrate into the workflow. This could save on costs by avoiding fetching new each time for common items.",4857,2024-07-10 12:23:15.128000+00:00,True,7
2322,Speed Up Social Media Banners With BannerBear.com,"This n8n workflow shows an easy way to automate the creation of social media assets using AI and a service like BannerBear. Designed for the busy marketer, leveraging AI image generation capabilities can help cut down production times and allow reinvesting into higher quality content. How it works This workflow generates social media banners for online events. Using a form trigger, a user can define the banner text and suggest an image to be generated. This request is passed to OpenAI's Dalle-3 image generation service to produce a relevant graphic for the event banner. This generated image is uploaded and sent to BannerBear where a template will use it and the rest of the form data to produce the banner. BannerBear returns the final banner which can now be used in an assortment of posts and publications. Requirements A BannerBear.com account and template is required An OpenAI account to use the Dalle-3 service. Customising the workflow We've only shown a small section of what BannerBear has to offer. With experimentation and other asset generating services such as AI audio and video, you should be able to generate more than just static banners!",4798,2024-07-09 13:13:45.473000+00:00,True,4
3634,Build your own Google Drive MCP server,"This n8n demonstrates how to build a simple Google Drive MCP server to search and get contents of files from Google Drive. This MCP example is based off an official MCP reference implementation which can be found here -https://github.com/modelcontextprotocol/servers/tree/main/src/gdrive How it works A MCP server trigger is used and connected to 1x Google Drive tool and 1x Custom Workflow tool. The Google Drive tool is set to perform a search on files within our Google Drive folder. The Custom Workflow tool downloads target files found in our drive and converts the binaries to their text representation. Eg. PDFs have only their text contents extracted and returned to the MCP client. How to use This Google Drive MCP server allows any compatible MCP client to manage a person or shared Google Drive. Simple select a drive or for better control, specify a folder within the drive to scope the operations to. Connect your MCP client by following the n8n guidelines here - https://docs.n8n.io/integrations/builtin/core-nodes/n8n-nodes-langchain.mcptrigger/#integrating-with-claude-desktop Try the following queries in your MCP client: ""Please help me search for last month's expense reports."" ""What does the company policy document say about cancellations and refunds?"" Requirements Google Drive for documents. OpenAI for image and audio understanding. MCP Client or Agent for usage such as Claude Desktop - https://claude.ai/download Customising this workflow Add additional capabilities such as renaming, moving and/or deleting files. Remember to set the MCP server to require credentials before going to production and sharing this MCP server with others!",4768,2025-04-21 10:11:41.815000+00:00,True,3
2385,Save your workflows into a Gitlab repository,"This template is inspired by Save your workflows into a GitHub repository by hikerspath and Back Up Your n8n Workflows To Github by jon-n8n. Basic Retrieve all workflows from an n8n instance and save it on a gitlab project. If the workflow exist, il will only save the changes. Flow What the workflow does : Sets custom parameters Gets workflows Iterates through each workflow one by one Get the file from Gitlab if exists Compare the files as objects (not as strings) Return a status on the workflow Create, Edit or ignore the file depending on the status Return a list of status for each workflow Configuration Select a credential in each Gitlab nodes. Edit the data in node ""Globals"" : repo.owner : slug of the user or team owning the repo repo.name : slug of the repository repo.branch : branch to commit on repo.path : from root of the repository. Should end with / Comments Error on gitlab nodes will not stop the run but will list the current workflow as error in the results Some fields are ignored to determined if there is changes : updatedAt : should be ignored if only ignores fields are changed globals : it's running information, no need to follow the changes",4702,2024-08-22 10:14:19.166000+00:00,False,2
3478,"Automate Instagram Posts with Google Drive, AI Captions & Facebook API","This template streamlines your Instagram content posting workflow by connecting Google Drive for image storage, using OpenAI for AI-generated captions, and leveraging Facebook Graph API for automated publishing. Pre-requisites Before setting up this workflow, ensure you have: A Google account with access to Google Drive An OpenAI API key for AI caption generation A Facebook Developer account with Instagram Graph API access An Instagram Business or Creator account connected to a Facebook Page n8n.io account with workflow access Setup Instructions Configure Data Source Create a Google Sheet with the following columns: Name: Filename of your image in Google Drive Caption: Optional custom caption (leave empty for AI-generated captions) URL: your Video Reel or Image in Google Drive Connect Google Drive Add your Google Drive credentials in the ""Google Drive"" node Specify the folder path where your Instagram image/Video are stored Configure the node to retrieve image files based on filenames from your Google Sheet Set Up OpenAI Integration Add your OpenAI API key to the credentials Configure the OpenAI node to generate engaging captions based on image content Adjust temperature and model parameters for desired creativity level Configure Facebook Graph API Connect your Facebook account with Instagram access Set up the Facebook Graph API node to post to your Instagram Business/Creator account Ensure proper image formatting (1:1, 4:5, or 16:9 aspect ratios supported by Instagram) Workflow Automation Setup Configure the scheduler node to run at your preferred frequency Set up error handling to notify you of any posting failures Add conditional nodes to use either custom or AI-generated captions Execution Instructions After completing all connections, test the workflow with a single image Monitor the execution in the n8n dashboard to ensure proper functioning View the ""Executions"" tab to track successful posts and troubleshoot any errors Adjust posting frequency and scheduling as needed This template saves hours of manual Instagram posting work while maintaining an authentic presence. Perfect for social media managers, content creators, and businesses looking to maintain consistent Instagram activity without the daily manual effort. The workflow handles image retrieval, caption generation or customization, proper Instagram API formatting, scheduled posting, and execution tracking - all in one automated solution.",4658,2025-04-08 17:43:28.144000+00:00,True,4
3579,"Automated Research Report Generation with AI, Wiki, Search & Gmail/Telegram","Automated Research Report Generation with OpenAI, Wikipedia, Google Search, Gmail/Telegram and PDF Output Description What Problem Does This Solve? üõ†Ô∏è This workflow automates the process of generating professional research reports for researchers, students, and professionals. It eliminates manual research and report formatting by aggregating data, generating content with AI, and delivering the report as a PDF via Gmail or Telegram. Target audience: Researchers, students, educators, and professionals needing quick, formatted research reports. What Does It Do? üåü Aggregates research data from Wikipedia, Google Search, and SerpApi. Refines user queries and generates structured content using OpenAI. Converts the content into a professional HTML report, then to PDF. Sends the PDF report via Gmail or Telegram. Key Features üìã Real-time data aggregation from multiple sources. AI-driven content generation with OpenAI. Automated HTML-to-PDF conversion for professional reports. Flexible delivery via Gmail or Telegram. Error handling for robust execution. Setup Instructions Prerequisites ‚öôÔ∏è n8n Instance**: Self-hosted or cloud n8n instance. API Credentials**: OpenAI API: API key with GPT model access, stored in n8n credentials. SerpApi (Google Search): API key from SerpApi, stored in n8n credentials (do not hardcode in nodes). Gmail API: Credentials from Google Cloud Console with Gmail scope. Telegram API: Bot token from BotFather on Telegram. Installation Steps üì¶ Import the Workflow: Copy the workflow JSON from the ""Template Code"" section below. Import it into n8n via ""Import from File"" or ""Import from URL"". Configure Credentials: Add API credentials in n8n‚Äôs Credentials section for OpenAI, SerpApi, Gmail, and Telegram. Assign credentials to respective nodes. For example: In the SerpApi Google Search node, use n8n credentials for SerpApi: api_key={{ $credentials.SerpApiKey }}. In the Send Research PDF on Gmail node, use Gmail credentials. In the Send PDF to Telegram node, use Telegram bot credentials. Set Up Nodes: OpenAI Nodes (Research AI Agent, OpenAI Chat Model, OpenAI Chat Middle Memory): Update the model (e.g., gpt-4o) and prompt as needed. Input Validation (Input Validation node): Ensure your input query format matches the expected structure (e.g., topic: ""AI ethics""). Delivery Options (Send Research PDF on Gmail, Send PDF to Telegram): Configure recipient email or Telegram chat ID. Test the Workflow: Run the workflow by clicking the ""Test Workflow"" node. Verify that the research report PDF is generated and sent via Gmail or Telegram. How It Works High-Level Steps üîç Query Refinement**: Refines the input query for better research. Aggregate Data**: Fetches data from Wikipedia, Google Search, and SerpApi. Generate Report**: Uses OpenAI to create a structured report. Convert to PDF**: Converts the report to HTML, then PDF. Deliver Report**: Sends the PDF via Gmail or Telegram. Detailed descriptions are available in the sticky notes within the workflow screenshot above. Node Names and Actions Research and Report Generation Test Workflow: Triggers the workflow for testing. Input Validation: Validates the input query. Query Refiner: Refines the query for better results. Research AI Agent: Coordinates research using OpenAI. OpenAI Chat Model: Generates content for the report. Structured Output Parser: Parses OpenAI output into structured data. OpenAI Chat Middle Memory: Retains context during research. Wikipedia Google Search: Fetches data from Wikipedia. SerpApi Google Search: Fetches data via SerpApi. Merge Split Items: Merges data from multiple sources. Aggregate: Aggregates all research data. Generate PDF HTML: Creates an HTML report. Convert HTML to PDF: Converts HTML to PDF. Download PDF: Downloads the PDF file. Send PDF to Telegram: Sends the PDF via Telegram. Send Research PDF on Gmail: Sends the PDF via Gmail. Customization Tips Expand Data Sources** üì°: Add more sources (e.g., academic databases) by adding nodes to Merge Split Items. Change Report Style** ‚úçÔ∏è: Update the Generate PDF HTML node to modify the HTML template (e.g., adjust styling or sections). Alternative Delivery** üìß: Add nodes to send the PDF via other platforms (e.g., Slack). Adjust AI Model** üß†: Modify the OpenAI Chat Model node to use a different model (e.g., gpt-3.5-turbo).",4632,2025-04-16 16:59:02.174000+00:00,True,11
3896,"ü§ñ Instagram MCP AI Agent ‚Äì Read, Reply & Manage Comments with GPT-4o","ü§ñ Instagram AI Agent with MCP Server ‚Äì Built for Smart Engagement and Automation Hi! I‚Äôm Amanda ü•∞ I build intelligent automations with n8n and Make. This powerful workflow was designed to help teams automatically handle Instagram interactions with AI. Using Meta Graph API, LangChain, MCP Server, and GPT-4o, it allows your AI agent to search for posts, read captions, fetch comments, and even reply or message followers, all through structured tools. üîß What the workflow does Searches for recent media using Instagram ID and access token Reads and extracts captions or media URLs Fetches comments and specific replies from each post Replies to comments automatically with GPT-generated responses Sends direct messages to followers who commented Maps user input and session to keep memory context via LangChain Communicates via Server-Sent Events (SSE) using your MCP Server URL üß∞ Nodes & Tech Used LangChain Agent + Chat Model with GPT-4o Memory Buffer for session memory toolHttpRequest to search media, comments, and send replies MCP Trigger and MCP Tool (custom SSE connection) Set node for input and variable assignment Webhook and JSON for Instagram API structure ‚öôÔ∏è Setup Instructions Create your Instagram App in Meta Developer Portal Add your Instagram ID and Access Token in the Set node Update the MCP Server Tool URL in the MCP Instagram node Use your n8n server URL (e.g. https://yourdomain.com/mcp/server/instagram/sse) Trigger the workflow using the included LangChain Chat Trigger Interact via text to ask the agent to: ‚ÄúGet latest posts‚Äù ‚ÄúReply to comment X with this message‚Äù ‚ÄúSend DM to this user about...‚Äù üë• Who this is for Social media teams managing multiple comments Brands automating engagement with followers Agencies creating smart, autonomous digital assistants Developers building conversational Instagram bots ‚úÖ Requirements Meta Graph API access Instagram Business account n8n instance (Cloud or Self-hosted) MCP Server configured (SSE Endpoint enabled) OpenAI API Key (for GPT-4o + LangChain) üåê Want to use this workflow? ‚ù§Ô∏è Buy workflows: https://iloveflows.com ‚òÅÔ∏è Try n8n Cloud: https://n8n.partnerlinks.io/amanda",4562,2025-05-06 12:33:19.751000+00:00,True,5
3804,"Automated PR Code Reviews with GitHub, GPT-4, and Google Sheets Best Practices","AI-Agent Code Review for GitHub Pull Requests Description: This n8n workflow automates the process of reviewing code changes in GitHub pull requests using an OpenAI-powered agent. It connects your GitHub repo, extracts modified files, analyzes diffs, and uses an AI agent to generate a code review based on your internal code best practices (fed from a Google Sheet). It ends by posting the review as a comment on the PR and tagging it with a visual label like ‚úÖ Reviewed by AI. üîß What It Does Triggered on PR creation Extracts code diffs from the PR Formats and feeds them into an OpenAI prompt Enriches the prompt using a Google Sheet of Swift best practices Posts an AI-generated review as a comment on the PR Applies a PR label to visually mark reviewed PRs ‚úÖ Prerequisites Before deploying this workflow, ensure you have the following: n8n Instance (Self-hosted or Cloud) GitHub Repository with PR activity OpenAI API Key** for GPT-4o, GPT-4-turbo, or GPT-3.5 GitHub OAuth App** (or PAT) connected to n8n to post comments and access PR diffs (Optional) Google Sheets API credentials if using the code best practices lookup node. ‚öôÔ∏è Setup Instructions 1. Import the Workflow in n8n, click on Workflows ‚Üí Import from file or JSON Paste or upload the JSON code of this template 2. Configure Triggers and Connections üîÅ GitHub Trigger Node**: PR Trigger Repository**: Select the GitHub repo(s) to monitor Events**: Set to pull_request Auth**: Use GitHub OAuth2 credentials üì• HTTP Request Node: Get file's Diffs from PR No authentication needed; it uses dynamic path from trigger üß† OpenAI Model Node**: OpenAI Chat Model Model**: Select gpt-4o, gpt-4-turbo, or gpt-3.5-turbo Credential**: Provide your OpenAI API Key üßë‚Äçüíª Code Review Agent Node : Code Review Agent Connected to OpenAI and optionally to tools like Google Sheets üí¨ GitHub Comment Poster Uses GitHub API to post review comments back on PR Node: GitHub Robot Credential: Use the agent Github account (OAuth or PAT) Repo : Pick your owen Github Repository üè∑Ô∏è PR Labeler (optional) Adds label ReviewedByAI after successful comment Node: Add Label to PR Label : you ca customize the label text of your owen tag. üìä Google Sheet Best Practices config (optional) Connects to a Google Sheet for coding guideline lookups, we can replace Google sheet by another tool or data base First prepare your best practices list with the clear description and the code bad/good examples Add al the best practices in your Google Sheet Configure* the Code *Best Practices node** in the template : Credential : Use your Google Sheet account by OAuth2 URL : Add your Google Sheet document URL Sheet : Add the name of the best practices sheet",4545,2025-04-30 14:07:04.489000+00:00,True,5
3769,Perform SEO Keyword Research & Insights with Ahrefs API and Gemini 1.5 Flash,"This n8n workflow automates SEO keyword research by querying the Ahrefs API for keyword data and related keyword insights. The enriched data is then processed by an AI agent to format a response and provide valuable SEO recommendations. Perfect for SEO specialists, content marketers, digital agencies, and anyone looking to gain valuable insights into keyword opportunities to boost their rankings. ‚öôÔ∏è How This Workflow Works This workflow guides you through the entire SEO keyword research process, from entering the initial keyword to receiving detailed insights and related keyword suggestions. 1. üó£Ô∏è User Input (Keyword Query) The user enters a keyword they want to research. This input is captured by the Chat Input Node, ready for analysis. 2. ü§ñ AI Agent (Input Verification) The AI Agent reviews the keyword input for any grammatical errors or extra commentary. If necessary, it cleans the input to ensure a seamless query to the API. 3. üîë Ahrefs API (Keyword Data Retrieval) The cleaned keyword is sent to the Ahrefs Keyword Tool API. This retrieves a detailed report including metrics like search volume, keyword difficulty, and CPC. 4. üí° Related Keywords Extraction (Using JavaScript Function) The workflow uses a JavaScript function to extract main keyword data and 10 related keywords data from the Ahrefs response. You can tweak the script to adjust the number of related keywords or the level of detail you want. 5. üß† AI Agent (Text Formatting) The aggregated data, including both the main keyword and related keywords, is sent to an AI agent. The AI agent formats the data into a concise, readable format that can be shared with the user. 6. üì® Final Response The formatted text is delivered to the user with keyword insights, recommendations, and related keyword suggestions. ‚úÖ Smart Retry & Error Handling Each subworkflow includes a fail-safe mechanism to ensure: ‚úÖ Proper error handling for any issues with the API request. üïí Failed API requests are retried after a customizable period (e.g., 2 hours or 1 day). üí¨ User input validation prevents any incorrect or malformed queries from being processed. üìã Ahrefs API Setup To use this workflow, you‚Äôll need to set up your Ahrefs API credentials: üîë Ahrefs API Sign up for an Ahrefs account and get your key here: Ahrefs Keyword Tool API Once signed up, you'll receive an API key, which you‚Äôll use in the x-rapidapi-key header in n8n. Ensure you check the Ahrefs Keyword Tool API documentation for more details on available parameters. üì• How to Import This Workflow Copy the json code. Open your n8n instance. Open a new workflow. Paste anywhere inside the workflow. Voila. üõ†Ô∏è Customization Options Adjust the number of related keywords extracted (default is 10). Customize the AI agent response formatting or add specific recommendations for users. Modify the JavaScript function to extract different metrics from the Ahrefs API. üß™ Use Case Example Trying to optimize your blog post around a specific keyword? Query a broad keyword, like ‚ÄúSEO tips‚Äù. Get related keyword data and search volume insights. Use the AI agent to provide keyword recommendations and additional topics to target. üí• Boost your content strategy with fresh keywords and relevant search data!",4510,2025-04-28 08:09:05.096000+00:00,True,5
3772,Automatically Classify and Label Gmail Emails with Google Gemini AI,"Description Quickly organize your inbox with AI! This simple workflow automatically classifies incoming emails into different categories ‚Äî like High Priority, Work Related, or Promotions ‚Äî and applies Gmail labels accordingly. Setup takes less than 2 minutes, and it runs 24/7, helping you stay focused on what matters most without manual sorting. Tools/Services Needed Gmail: To trigger the workflow and label emails. Google Gemini (or any LLM Model): To intelligently classify email content. How It Works Gmail Trigger: Detects every new incoming email. Text Classifier Node: Classifies the email content into predefined categories. Google Gemini Chat Model: Provides the AI-powered understanding behind the classification. Conditional Labeling: If the email is High Priority, label it accordingly. If it‚Äôs Work Related (e.g., internal emails), apply the work label. If it‚Äôs a Promotion, sort it into the promotions label. Gmail Labeling: Automatically adds the correct label to the email. Setup Instructions Connect your Gmail account to n8n. Connect your Google Gemini (or other LLM) credentials. Customize the categories and labels if needed. Activate the workflow ‚Äî and that's it! Notes You can easily add more categories (like ""Finance,"" ""Newsletters,"" etc.) by adjusting the classification prompt. Works best with a clean and minimal set of categories to avoid overlap. Can be adapted to work with any other large language model (OpenAI, Claude, etc.) if preferred.",4476,2025-04-28 12:57:37.035000+00:00,True,5
2151,Automatically document n8n workflows directly in Notion database,"Use case n8n workflows can go out of hand when you're automating as much as we do at n8n. We needed a place to document them and keep track of who owns and maintains them. To facilitate this we use this n8n workflow to automatically sync workflows directly to a Notion database if it has the tag sync-to-notion. How to setup Add your n8n api creds Add your Notion creds Create notion database with fields env id as text, isActive (dev) as boolean, URL (dev) as url, Workflow created at as date, Workflow updated at as date, Error workflow setup as boolean (Make sure page is connected) Add tag sync-to-notion to some workflows",4381,2024-02-28 20:05:08.622000+00:00,False,2
2294,Convert DOCX to PDF using ConvertAPI,"Who is this for? For developers and organizations that need to convert DOCX files to PDF. What problem is this workflow solving? The file format conversion problem. What this workflow does Downloads the DOCX file from the web. Converts the DOCX file to PDF. Stores the PDF file in the local file system. How to customize this workflow to your needs Open the HTTP Request node. Adjust the URL parameter (all endpoints can be found here). Use your API Token for authentication. Pass the token in the Authorization header as a Bearer token. You can manage your API Tokens in the User panel ‚Üí Authentication. Optionally, additional Body Parameters can be added for the converter.",4375,2024-06-20 12:09:52.671000+00:00,False,1
2179,Translate PDF documents from Google drive folder with DeepL,"This workflow will translate all your PDF documents from specified Google Drive folder to the desired language. Translated files will be automatically uploaded to the original folder. Required accounts 1Ô∏è‚É£ Google Drive account 2Ô∏è‚É£ DeepL developer account and API key How to setup? 1Ô∏è‚É£ Setup your google drive folder url, target and source language in the configuration node 2Ô∏è‚É£ Connect your Google Drive account with all Google Drive nodes 3Ô∏è‚É£ Setup HTTP header credentials that should be used for HTTP nodes in the template (replace yourAuthKey with your DeepL API key) 4Ô∏è‚É£ Set your DeepL header credentials in all HTTP nodes",4365,2024-03-16 13:41:24.568000+00:00,False,2
3799,Interactive Knowledge Base Chat with Supabase RAG using AI üìöüí¨,"Google Drive File Ingestion to Supabase for Knowledge Base üìÇüíæ Overview üåü This n8n workflow automates the process of ingesting files from Google Drive into a Supabase database, preparing them for a knowledge base system. It supports text-based files (PDF, DOCX, TXT, etc.) and tabular data (XLSX, CSV, Google Sheets), extracting content, generating embeddings, and storing data in structured tables. This is a foundational workflow for building a company knowledge base that can be queried via a chat interface (e.g., using a RAG workflow). üöÄ Problem Solved üéØ Manually managing a knowledge base with files from Google Drive is time-consuming and error-prone. This workflow solves that by: Automatically ingesting files from Google Drive as they are created or updated. Extracting content** from various file types (text and tabular). Generating embeddings for text-based files to enable vector search. Storing data in Supabase for efficient retrieval. Handling duplicates and errors to ensure data consistency. Target Audience: Knowledge Managers**: Build a centralized knowledge base from company files. Data Teams**: Automate the ingestion of spreadsheets and documents. Developers**: Integrate with other workflows (e.g., RAG for querying the knowledge base). Workflow Description üîç This workflow listens for new or updated files in Google Drive, processes them based on their type, and stores the extracted data in Supabase tables for later retrieval. Here‚Äôs how it works: File Detection: Triggers when a file is created or updated in Google Drive. File Processing: Loops through each file, extracts metadata, and validates the file type. Duplicate Check: Ensures the file hasn‚Äôt been processed before. Content Extraction: Text-based Files: Downloads the file, extracts text, splits it into chunks, generates embeddings, and stores the chunks in Supabase. Tabular Files: Extracts data from spreadsheets and stores it as rows in Supabase. Metadata Storage: Stores file metadata and basic info in Supabase tables. Error Handling: Logs errors for unsupported formats or duplicates. Nodes Breakdown üõ†Ô∏è 1. Detect New File üîî Type**: Google Drive Trigger Purpose**: Triggers the workflow when a new file is created in Google Drive. Configuration**: Credential: Google Drive OAuth2 Event: File Created Customization**: Specify a folder to monitor specific directories. 2. Detect Updated File üîî Type**: Google Drive Trigger Purpose**: Triggers the workflow when a file is updated in Google Drive. Configuration**: Credential: Google Drive OAuth2 Event: File Updated Customization**: Currently disconnected; reconnect if updates need to be processed. 3. Process Each File üîÑ Type**: Loop Over Items Purpose**: Processes each file individually from the Google Drive trigger. Configuration**: Input: {{ $json.files }} Customization**: Adjust the batch size if processing multiple files at once. 4. Extract File Metadata üÜî Type**: Set Purpose**: Extracts metadata like file_id, file_name, mime_type, and web_view_link. Configuration**: Fields: file_id: {{ $json.id }} file_name: {{ $json.name }} mime_type: {{ $json.mimeType }} web_view_link: {{ $json.webViewLink }} Customization**: Add more metadata fields if needed (e.g., size, createdTime). 5. Check File Type ‚úÖ Type**: IF Purpose**: Validates the file type by checking the MIME type. Configuration**: Condition: mime_type contains supported types (e.g., application/pdf, application/vnd.openxmlformats-officedocument.spreadsheetml.sheet). Customization**: Add more supported MIME types as needed. 6. Find Duplicates üîç Type**: Supabase Purpose**: Checks if the file has already been processed by querying knowledge_base. Configuration**: Operation: Select Table: knowledge_base Filter: file_id = {{ $node['Extract File Metadata'].json.file_id }} Customization**: Add additional duplicate checks (e.g., by file name). 7. Handle Duplicates üîÑ Type**: IF Purpose**: Routes the workflow based on whether a duplicate is found. Configuration**: Condition: {{ $node['Find Duplicates'].json.length &gt; 0 }} Customization**: Add notifications for duplicates if desired. 8. Remove Old Text Data üóëÔ∏è Type**: Supabase Purpose**: Deletes old text data from documents if the file is a duplicate. Configuration**: Operation: Delete Table: documents Filter: metadata-&gt;&gt;'file_id' = {{ $node['Extract File Metadata'].json.file_id }} Customization**: Add logging before deletion. 9. Remove Old Data üóëÔ∏è Type**: Supabase Purpose**: Deletes old tabular data from document_rows if the file is a duplicate. Configuration**: Operation: Delete Table: document_rows Filter: dataset_id = {{ $node['Extract File Metadata'].json.file_id }} Customization**: Add logging before deletion. 10. Route by File Type üîÄ Type**: Switch Purpose**: Routes the workflow based on the file‚Äôs MIME type (text-based or tabular). Configuration**: Rules: Based on mime_type (e.g., application/pdf for text, application/vnd.openxmlformats-officedocument.spreadsheetml.sheet for tabular). Customization**: Add more routes for additional file types. 11. Download File Content üì• Type**: Google Drive Purpose**: Downloads the file content for text-based files. Configuration**: Credential: Google Drive OAuth2 File ID: {{ $node['Extract File Metadata'].json.file_id }} Customization**: Add error handling for download failures. 12. Extract PDF Text üìú Type**: Extract from File (PDF) Purpose**: Extracts text from PDF files. Configuration**: File Content: {{ $node['Download File Content'].binary.data }} Customization**: Adjust extraction settings for better accuracy. 13. Extract DOCX Text üìú Type**: Extract from File (DOCX) Purpose**: Extracts text from DOCX files. Configuration**: File Content: {{ $node['Download File Content'].binary.data }} Customization**: Add support for other text formats (e.g., TXT, RTF). 14. Extract XLSX Data üìä Type**: Extract from File (XLSX) Purpose**: Extracts tabular data from XLSX files. Configuration**: File ID: {{ $node['Extract File Metadata'].json.file_id }} Customization**: Add support for CSV or Google Sheets. 15. Split Text into Chunks ‚úÇÔ∏è Type**: Text Splitter Purpose**: Splits extracted text into manageable chunks for embedding. Configuration**: Chunk Size: 1000 Chunk Overlap: 200 Customization**: Adjust chunk size and overlap based on document length. 16. Generate Text Embeddings üåê Type**: OpenAI Purpose**: Generates embeddings for text chunks using OpenAI. Configuration**: Credential: OpenAI API key Operation: Embedding Model: text-embedding-ada-002 Customization**: Switch to a different embedding model if needed. 17. Store Text in Supabase üíæ Type**: Supabase Vector Store Purpose**: Stores text chunks and embeddings in the documents table. Configuration**: Credential: Supabase credentials Operation: Insert Documents Table Name: documents Customization**: Add metadata fields to store additional context. 18. Store Tabular Data üíæ Type**: Supabase Purpose**: Stores tabular data in the document_rows table. Configuration**: Operation: Insert Table: document_rows Columns: dataset_id, row_data Customization**: Add validation for tabular data structure. 19. Store File Metadata üìã Type**: Supabase Purpose**: Stores file metadata in the document_metadata table. Configuration**: Operation: Insert Table: document_metadata Columns: file_id, file_name, file_type, file_url Customization**: Add more metadata fields as needed. 20. Record in Knowledge Base üìö Type**: Supabase Purpose**: Stores basic file info in the knowledge_base table. Configuration**: Operation: Insert Table: knowledge_base Columns: file_id, file_name, file_type, file_url, upload_date Customization**: Add indexes for faster lookups. 21. Log File Errors ‚ö†Ô∏è Type**: Supabase Purpose**: Logs errors for unsupported file types. Configuration**: Operation: Insert Table: error_log Columns: error_type, error_message Customization**: Add notifications for errors. 22. Log Duplicate Errors ‚ö†Ô∏è Type**: Supabase Purpose**: Logs errors for duplicate files. Configuration**: Operation: Insert Table: error_log Columns: error_type, error_message Customization**: Add notifications for duplicates. Interactive Knowledge Base Chat with Supabase RAG using GPT-4o-mini üìöüí¨ Introduction üåü This n8n workflow creates an interactive chat interface that allows users to query a company knowledge base using Retrieval-Augmented Generation (RAG). It retrieves relevant information from text documents and tabular data stored in Supabase, then generates natural language responses using OpenAI‚Äôs GPT-4o-mini model. Designed for teams managing internal knowledge, this workflow enables users to ask questions like ‚ÄúWhat‚Äôs the remote work policy?‚Äù or ‚ÄúShow me the latest budget data‚Äù and receive accurate, context-aware responses in a conversational format. üöÄ Problem Statement üéØ Managing a company knowledge base can be a daunting task‚Äîemployees often struggle to find specific information buried in documents or spreadsheets, leading to wasted time and inefficiencies. Traditional search methods may not understand natural language queries or provide contextually relevant results. This workflow solves these issues by: Offering a chat-based interface for natural language queries, making it easy for users to ask questions in their own words. Leveraging RAG to retrieve relevant text and tabular data from Supabase, ensuring responses are accurate and context-aware. Supporting diverse file types, including text-based files (e.g., PDFs, DOCX) and tabular data (e.g., XLSX, CSV), for comprehensive knowledge access. Maintaining conversation history to provide context during interactions, improving the user experience. Target Audience üë• This workflow is ideal for: HR Teams**: Quickly access company policies, employee handbooks, or benefits documents. Finance Teams**: Retrieve budget data, expense reports, or financial summaries from spreadsheets. Knowledge Managers**: Build a centralized assistant for internal documentation, streamlining information access. Developers**: Extend the workflow with additional tools or integrations for custom use cases. Workflow Description üîç This workflow consists of a chat interface powered by n8n‚Äôs Chat Trigger node, an AI Agent node for RAG, and several tools to retrieve data from Supabase. Here‚Äôs how it works step-by-step: User Initiates a Chat: The user interacts with a chat interface, sending queries like ‚ÄúSummarize our remote work policy‚Äù or ‚ÄúShow budget data for Q1 2025.‚Äù Query Processing with RAG: The AI Agent processes the query using RAG, retrieving relevant data from Supabase tables and generating a response with OpenAI‚Äôs GPT-4o-mini model. Data Retrieval and Response Generation: The workflow uses multiple tools to fetch data: Retrieves text chunks from the documents table using vector search. Fetches tabular data from the document_rows table based on file IDs. Extracts full document text or lists available files as needed. Generates a natural language response combining the retrieved data. Conversation History Management: Stores the conversation history in Supabase to maintain context for follow-up questions. Response Delivery: Formats and sends the response back to the chat interface for the user to view. Nodes Breakdown üõ†Ô∏è 1. Start Chat Interface üí¨ Type**: Chat Trigger Purpose**: Provides the interactive chat interface for users to input queries and receive responses. Configuration**: Chat Title: Company Knowledge Base Assistant Chat Subtitle: Ask me anything about company documents! Welcome Message: Hello! I‚Äôm your Company Knowledge Base Assistant. How can I help you today? Suggestions: What is the company policy on remote work?, Show me the latest budget data., List all policy documents. Output Chat Session ID: true Output User Message: true Customization**: Update the title and welcome message to align with your company branding (e.g., HR Knowledge Assistant). Add more suggestions relevant to your use case (e.g., What are the company benefits?). 2. Process Query with RAG üß† Type**: AI Agent Purpose**: Orchestrates the RAG process by retrieving relevant data using tools and generating responses with OpenAI‚Äôs GPT-4o-mini. Configuration**: Credential: OpenAI API key Model: gpt-4o-mini System Prompt: You are a helpful assistant for a company knowledge base. Use the provided tools to retrieve relevant information from documents and tabular data. If the query involves tabular data, format it clearly in your response. If no relevant data is found, respond with ""I couldn‚Äôt find any relevant information. Can you provide more details?"" Input Field: {{ $node['Start Chat Interface'].json.message }} Customization**: Switch to a different model (e.g., gpt-3.5-turbo) to adjust cost or performance. Modify the system prompt to change the tone (e.g., more formal for HR use cases). 3. Retrieve Text Chunks üìÑ Type**: Supabase Vector Store (Tool) Purpose**: Retrieves relevant text chunks from the documents table using vector search. Configuration**: Credential: Supabase credentials Operation Mode: Retrieve Documents (As Tool for AI Agent) Table Name: documents Embedding Field: embedding Content Field: content_text Metadata Field: metadata Embedding Model: OpenAI text-embedding-ada-002 Top K: 10 Customization**: Adjust Top K to retrieve more or fewer results (e.g., 5 for faster responses). Ensure the match_documents function (see prerequisites) is defined in Supabase. 4. Fetch Tabular Data üìä Type**: Supabase (Tool, Execute Query) Purpose**: Retrieves tabular data from the document_rows table based on a file ID. Configuration**: Credential: Supabase credentials Operation: Execute Query Query: SELECT row_data FROM document_rows WHERE dataset_id = $1 LIMIT 10 Tool Description: Run a SQL query - use this to query from the document_rows table once you know the file ID you are querying. dataset_id is the file_id and you are always using the row_data for filtering, which is a jsonb field that has all the keys from the file schema given in the document_metadata table. Customization**: Modify the query to filter specific columns or add conditions (e.g., WHERE dataset_id = $1 AND row_data-&gt;&gt;'year' = '2025'). Increase the LIMIT for larger datasets. 5. Extract Full Document Text üìú Type**: Supabase (Tool, Execute Query) Purpose**: Fetches the full text of a document by concatenating all text chunks for a given file_id. Configuration**: Credential: Supabase credentials Operation: Execute Query Query: SELECT string_agg(content_text, ' ') as document_text FROM documents WHERE metadata-&gt;&gt;'file_id' = $1 GROUP BY metadata-&gt;&gt;'file_id' Tool Description: Given file id fetch the text from the documents Customization**: Add filters to the query if needed (e.g., limit to specific metadata fields). 6. List Available Files üìã Type**: Supabase (Tool, Select) Purpose**: Lists all files in the knowledge base from the document_metadata table. Configuration**: Credential: Supabase credentials Operation: Select Schema: public Table: document_metadata Tool Description: Use this tool to fetch all documents including the table schema if the file is csv, excel or xlsx Customization**: Add filters to list specific file types (e.g., WHERE file_type = 'application/pdf'). Modify the columns selected to include additional metadata (e.g., file_size). 7. Manage Chat History üíæ Type**: Postgres Chat Memory (Tool) Purpose**: Stores and retrieves conversation history to maintain context. Configuration**: Credential: Supabase credentials (Postgres-compatible) Table Name: n8n_chat_history Session ID Field: session_id Session ID Value: {{ $node['Start Chat Interface'].json.sessionId }} Message Field: message Sender Field: sender Timestamp Field: timestamp Context Window Length: 5 Customization**: Increase the context window length for longer conversations (e.g., 10 messages). Add indexes on session_id and timestamp in Supabase for better performance. 8. Format and Send Response üì§ Type**: Set Purpose**: Formats the AI Agent‚Äôs response and sends it back to the chat interface. Configuration**: Fields: response: {{ $node['Process Query with RAG'].json.output }} Customization**: Add additional formatting to the response if needed (e.g., prepend with a timestamp or apply markdown formatting). Setup Instructions üõ†Ô∏è Prerequisites üìã n8n Setup: Ensure you‚Äôre using n8n version 1.0 or higher. Enable the AI features in n8n settings. Supabase: Create a Supabase project and set up the following tables: documents: id (uuid), content_text (text), embedding (vector(1536)), metadata (jsonb) document_rows: id (uuid), dataset_id (varchar), row_data (jsonb) document_metadata: file_id (varchar), file_name (varchar), file_type (varchar), file_url (text) knowledge_base: id (serial), file_id (varchar), file_name (varchar), file_type (varchar), file_url (text), upload_date (timestamp) n8n_chat_history: id (serial), session_id (varchar), message (text), sender (varchar), timestamp (timestamp) Add the match_documents function to Supabase to enable vector search: CREATE OR REPLACE FUNCTION match_documents ( query_embedding vector(1536), match_count int DEFAULT 5, filter jsonb DEFAULT '{}' ) RETURNS TABLE ( id uuid, content_text text, metadata jsonb, similarity float ) LANGUAGE plpgsql AS $$ BEGIN RETURN QUERY SELECT documents.id, documents.content_text, documents.metadata, 1 - (documents.embedding &lt;=&gt; query_embedding) as similarity FROM documents WHERE documents.metadata @&gt; filter ORDER BY similarity DESC LIMIT match_count; END; $$;",4362,2025-04-30 09:12:57.114000+00:00,True,11
3647,üì• Transform Google Drive Documents into Vector Embeddings,"Automatically convert documents from Google Drive into vector embeddings using OpenAI, LangChain, and PGVector ‚Äî fully automated through n8n. ‚öôÔ∏è What It Does This workflow monitors a Google Drive folder for new files, supports multiple file types (PDF, TXT, JSON), and processes them into vector embeddings using OpenAI‚Äôs text-embedding-3-small model. These embeddings are stored in a Postgres database using the PGVector extension, making them query-ready for semantic search or RAG-based AI agents. After successful processing, files are moved to a separate ‚Äúvectorized‚Äù folder to avoid duplication. üí° Use Cases Powering Retrieval-Augmented Generation (RAG) AI agents Semantic search across private documents AI assistant knowledge ingestion Automated document pipelines for indexing or classification üß† Workflow Highlights Trigger Options:** Manual or Scheduled (3 AM daily by default) Supported File Types:** PDF, TXT, JSON Embedding Stack:** LangChain Text Splitter, OpenAI Embeddings, PGVector Deduplication:** Files are moved after processing License:** CC BY-SA 4.0 Author:** AlexK1919 üõ† What You‚Äôll Need Google Drive OAuth2** credentials (connected to Search Folder, Download File, and Move File nodes) OpenAI API Key** (used in the Embeddings OpenAI node) Postgres + PGVector** database (connected in the Postgres PGVector Store node) üîß Step-by-Step Setup Instructions Create Google OAuth2 credentials in n8n and connect them to all Google Drive nodes. Set your source folder ID in the Search Folder node ‚Äî this is where incoming files are placed. Set your processed folder ID in the Move File node ‚Äî files will be moved here after vectorization. Ensure you have a PGVector-enabled Postgres instance and input the table name and collection in the Postgres PGVector Store node. Add your OpenAI credentials to the Embeddings OpenAI node and select text-embedding-3-small. Optional: Activate the Schedule Trigger node to run daily or configure your own schedule. Run manually by triggering When clicking ‚ÄòTest workflow‚Äô for on-demand ingestion. üß© Customization Tips Want to support more file types or enhance the pipeline? Add new extractors**: Use Extract from File with other formats like DOCX, Markdown, or HTML. Refine logic by file type**: The Switch node routes files to the correct extraction method based on MIME type (application/pdf, text/plain, application/json). Pre-process with OCR**: Add an OCR step before extraction to handle scanned PDFs or images. Add filters**: Enhance the Search Folder or Switch node logic to skip specific files or folders. üìÑ License This workflow is available under Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International (CC BY-NC-SA 4.0) license. You are free to use, adapt, and share this workflow for non-commercial purposes under the terms of this license. Full license details: https://creativecommons.org/licenses/by-nc-sa/4.0/",4324,2025-04-21 18:34:36.809000+00:00,True,5
3908,Comprehensive SEO Keyword Research with OpenAI & DataForSEO Analytics to NocoDB,"AI-Powered SEO Keyword Research Workflow with n8n &gt; automates comprehensive keyword research for content creation Table of Contents Introduction Workflow Architecture NocoDB Integration Data Flow Core Components Setup Requirements Possible Improvements Introduction This n8n workflow automates SEO keyword research using AI and data-driven analytics. It combines OpenAI's language models with DataForSEO's analytics to generate comprehensive keyword strategies for content creation. The workflow is triggered by a webhook from NocoDB, processes the input data through multiple stages, and returns a detailed content brief with optimized keywords. Workflow Architecture The workflow follows a structured process: Input Collection: Receives data via webhook from NocoDB Topic Expansion: Generates keywords using AI Keyword Metrics Analysis: Gathers search volume, CPC, and difficulty metrics Competitor Analysis: Analyzes competitor content for ranking keywords Final Strategy Creation: Combines all data to generate a comprehensive keyword strategy Output Storage: Saves results back to NocoDB and sends notifications NocoDB Integration Database Structure The workflow integrates with two tables in NocoDB: Input Table Schema This table collects the input parameters for the keyword research: | Field Name | Type | Description | | --------------- | ------------- | --------------------------------------------------------------------------- | | ID | Auto Number | Unique identifier | | Primary Topic | Text | The main keyword/topic to research | | Competitor URLs | Text | Comma-separated list of competitor websites | | Target Audience | Single Select | Description of the target audience (Solopreneurs, Marketing Managers, etc.) | | Content Type | Single Select | Type of content (Blog, Product page, etc.) | | Location | Single Select | Target geographic location | | Language | Single Select | Target language for keywords | | Status | Single Select | Workflow status (Pending, Started, Done) | | Start Research | Checkbox | Active Workflow when you set this to true | Output Table Schema This table stores the generated keyword strategy: | Field Name | Type | Description | | ------------------ | ----------- | ------------------------------------------------ | | ID | Auto Number | Unique identifier | | primary_topic_used | Text | The topic that was researched | | report_content | Long Text | The complete keyword strategy in Markdown format | | generatedAt | Datetime | Automatically generated by NocoDb | Webhook Settings NocoDB Webhook Settings Data Flow The workflow handles data in the following sequence: Webhook Trigger: Receives input from NocoDB when a new keyword research request is created Field Extraction: Extracts primary topic, competitor URLs, audience, and other parameters AI Topic Expansion: Uses OpenAI to generate related keywords, categorized by type and intent Keyword Analysis: Sends primary keywords to DataForSEO to get search volume, CPC, and difficulty Competitor Research: Analyzes competitor pages to identify their keyword rankings Strategy Generation: Combines all data to create a comprehensive keyword strategy Storage & Notification: Saves the strategy to NocoDB and sends a notification to Slack Core Components 1. Topic Expansion This component uses OpenAI and a structured output parser to generate: 20 primary keywords 30 long-tail keywords with search intent 15 question-based keywords 10 related topics 2. DataForSEO Integration Two API endpoints are used: Search Volume & CPC**: Gets monthly search volume and cost-per-click data Keyword Difficulty**: Evaluates how difficult it would be to rank for each keyword 3. Competitor Analysis This component: Analyzes competitor URLs to identify which keywords they rank for Identifies content gaps or opportunities Determines the search intent their content targets 4. Final Keyword Strategy The AI-generated strategy includes: Top 10 primary keywords with metrics 15 long-tail opportunities with low competition 5 question-based keywords to address in content Content structure recommendations 3 potential content titles optimized for SEO Setup Requirements To use this workflow, you'll need: n8n Instance: Either cloud or self-hosted NocoDB Account: For data input and storage API Keys: OpenAI API key DataForSEO API credentials Slack API token (for notifications) Database Setup: Create the required tables in NocoDB as described above Possible Improvements The workflow could be enhanced with the following improvements: Enhanced Keyword Strategy Add topic clustering to group related keywords Enhance the final output with more specific content structure suggestions Include word count recommendations for each content section Additional Data Sources Integrate Google Search Console data for existing content optimization Add Google Trends data to identify rising topics Include sentiment analysis for different keyword groups Improved Competitor Analysis Analyze content length and structure from top-ranking pages Identify common backlink sources for competitor content Extract content headings to better understand content organization Automation Enhancements Add scheduling capabilities to run updates on existing content Implement content performance tracking over time Create alert thresholds for changes in keyword difficulty or search volume Example Output Here is an example Output the Workflow generated based on the following inputs. Inputs: Primary Topic: AI Automation Competitor URLs: n8n.io, zapier.com, make.com Target Audience: Small Business Owners Content Type: Landing Page Location: United States Language: English Output: Final Keyword Strategy The workflow provides a powerful automation for content marketers and SEO specialists to develop data-driven keyword strategies with minimal manual effort. &gt; Original Workflow: AI-Powered SEO Keyword Research Automation - The vibe Marketer",4313,2025-05-06 18:12:18.353000+00:00,True,6
3672,"SEO Blog Generator with GPT-4o, Perplexity, and Telegram Integration","SEO Blog Generator with GPT-4o, Perplexity, and Telegram Integration This workflow helps you automatically generate SEO-optimized blog posts using Perplexity.ai, OpenAI GPT-4o, and optionally Telegram for interaction. üöÄ Features üß† Topic research via Perplexity sub-workflow ‚úçÔ∏è AI-written blog post generated with GPT-4o üìä Structured output with metadata: title, slug, meta description üì© Integration with Telegram to trigger workflows or receive outputs (optional) ‚öôÔ∏è Requirements ‚úÖ OpenAI API Key (GPT-4o or GPT-3.5) ‚úÖ Perplexity API Key (with access to /chat/completions) ‚úÖ (Optional) Telegram Bot Token and webhook setup üõ† Setup Instructions Credentials: Add your OpenAI credentials (openAiApi) Add your Perplexity credentials under httpHeaderAuth Optional: Setup Telegram credentials under telegramApi Inputs: Use the Form Trigger or Telegram input node to send a Research Query Subworkflow: Make sure to import and activate the subworkflow Perplexity_Searcher to fetch recent search results Customization: Edit prompt texts inside the Blog Content Generator and Metadata Generator to change writing style or target industry Add or remove output nodes like Google Sheets, Notion, etc. üì¶ Output Format The final blog post includes: ‚úÖ Blog content (1500-2000 words) ‚úÖ Metadata: title, slug, and meta description ‚úÖ Extracted summary in JSON ‚úÖ Delivered to Telegram (if connected) Need help? Reach out on the n8n community forum",4271,2025-04-23 13:11:34.466000+00:00,True,6
2390,Automate company research using ProspectLens and Google Sheets,"This n8n workflow automates the process of researching companies by gathering relevant data such as traffic volume, foundation details, funding information, founders, and more. The workflow leverages the ProspectLens API, which is particularly useful for researching companies commonly found on Crunchbase and LinkedIn. ProspectLens is an API that provides very detailed company data. All you need to do is supply the company's domain name. You can obtain your ProspectLens API key here: https://apiroad.net/marketplace/apis/prospectlens In n8n, create a new ""HTTP Header"" credential. Set x-apiroad-key as the ""Name"" and enter your APIRoad API key as the ""Value"". Use this credential in the HTTP Request node of the workflow.",4267,2024-08-23 14:36:00.805000+00:00,False,2
2394,Use AI to organize your Todoist Inbox,"How it works This workflow adds a priority to each Todoist item in your inbox, based on a list of projects that you add in the workflow. Setup Add your Todoist credentials Add your OpenAI credentials Set your project names and add priority",4227,2024-08-30 08:07:44.526000+00:00,True,2
2352,Create Teams Notifications for new Tickets in ConnectWise with Redis,"This Workflow does a HTTPs request to ConnectWise Manage through their REST API. It will pull all tickets in the ""New"" status or whichever status you like, and notify your dispatch team/personnel whenever a new ticket comes in using Microsoft Teams. Video Explanation https://youtu.be/yaSVCybSWbM",4083,2024-07-22 10:13:42.326000+00:00,False,4
3895,"üì¢ Multi-Platform Video Publisher ‚Äì YouTube, Instagram & TikTok","Hi! I'm Amanda ‚ù§Ô∏è I build intelligent automation flows with n8n and Make. This one is for all content creators, marketing teams, and agencies who want to publish once and post everywhere. With this workflow, you can upload a single video to YouTube, Instagram Reels, and TikTok ‚Äî simultaneously and automatically. ‚úÖ What the workflow does Downloads a video from a provided URL Uploads the video to your YouTube channel with title and description Publishes it as a Reel on Instagram via the Meta Graph API Sends the same video to TikTok using their official API Supports credential input via Set node (tokens, titles, descriptions) ‚öôÔ∏è Nodes & Tech Used HTTP Request ‚Äì Download video and handle uploads to Instagram & TikTok YouTube node ‚Äì Official n8n integration for video upload Set node ‚Äì For handling user inputs (tokens, titles, video URLs) Switch, Wait, Merge ‚Äì Logic to control publishing status Manual or webhook start available üõ†Ô∏è Setup Instructions Open the workflow in your n8n (Cloud or self-hosted) instance Edit the Set node called Credentials and fill in: Token Instagram Token Tiktok YouTube title, description, and video URL Instagram account ID Connect your YouTube OAuth credentials in the YouTube node Optionally, trigger via webhook to automate from other apps (Typebot, CRM, Drive) Hit ""Execute Workflow"" or schedule via cron üë• Who this is for Content creators who want to post everywhere at once Agencies managing video distribution across platforms Social media managers and freelancers Anyone wanting a one-click multi-platform publishing workflow üåê Explore more workflows ‚ù§Ô∏è Buy workflows: https://iloveflows.com ‚òÅÔ∏è Try n8n Cloud: https://n8n.partnerlinks.io/amanda",4061,2025-05-06 11:21:23.115000+00:00,False,2
2159,üö® Report n8n workflow errors to Telegram,Use case Error workflows are an important part of running workflows in production. Make sure to set them up for all your important workflows. The message links directly to the execution. How to setup Add Telegram creds Set chat id in Telegram node Add this error workflow to other workflows https://docs.n8n.io/flow-logic/error-handling/#create-and-set-an-error-workflow,4043,2024-03-01 14:24:50.991000+00:00,False,1
2222,Convert an XML file to JSON via webhook call,"Who this template is for This template is for everyone who needs to work with XML data a lot and wants to convert it to JSON instead. Use case Many products still work with XML files as their main language. Unfortunately, not every software still supports XML, as many switched to more modern storing languages such as JSON. This workflow is designed to handle the conversion of XML data to JSON format via a webhook call, with error handling and Slack notifications integrated into the process. How this workflow works Triggering the workflow: This workflow initiates upon receiving an HTTP POST request at the webhook endpoint specified in the ""POST"" node. The endpoint, designated as , can be accessed externally by sending a POST request to that URL. Data routing and processing: Upon receiving the POST request, the Switch node routes the workflow's path based on conditions determined by the content type of the incoming data or any encountered errors. The Extract From File and Edit Fields (Set) nodes manage XML input processing, adapting their actions according to the data's content type. XML to JSON conversion: The XML data extracted from the input is passed through the ""XML"" node, which performs the conversion process, transforming it into JSON format. Response handling: If the XML-to-JSON conversion is successful, a success response is sent back with a status of ""OK"" and the converted JSON data. If there are any errors during the XML-to-JSON conversion process, an error response is sent back with a status of ""error"" and an error message. Error handling: in case of an error during processing, the workflow sends a notification to a Slack channel designated for error reporting. Set up steps Set up your own in the Webhook node. While building or testing a workflow, use a test webhook URL. When your workflow is ready, switch to using the production webhook URL. Set credentials for Slack.",4018,2024-04-09 08:52:33.336000+00:00,False,1
3717,Search LinkedIn companies and add them to Airtable CRM,"Search LinkedIn companies and add them to Airtable CRM Who is this for? This template is ideal for sales teams, business development professionals, and marketers looking to build a robust prospect database without manual LinkedIn research. Perfect for agencies, consultants, and B2B companies targeting specific business profiles. What problem does this workflow solve? Manually researching companies on LinkedIn and adding them to your CRM is time-consuming and error-prone. This automation eliminates the tedious process of finding, qualifying, and importing prospects into your database. What this workflow does This workflow automatically searches for companies on LinkedIn based on your criteria (keywords, size, location), retrieves detailed information about each company, filters them based on quality indicators (follower count and website availability), and adds new companies to your Airtable CRM while preventing duplicates. Setup Create a Ghost Genius API account and get your API key Configure HTTP Request nodes with Header Auth credentials (Name: ""Authorization"", Value: ""Bearer your_api_key"") Create an Airtable base named ""CRM"" with columns: name, website, LinkedIn, id, etc. Set up your Airtable credentials following n8n documentation Add your company search selection criteria to the ‚ÄúSet Variables‚Äù node. How to customize this workflow Modify search parameters in the ""Set Variables"" node to target different industries, locations, or company sizes Adjust the follower count threshold in the ""Filter Valid Companies"" node based on your qualification criteria Customize the Airtable fields mapping in the ""Add Company to CRM"" node to match your database structure Add notification nodes (Slack, Email) to alert you when new companies are added",3893,2025-04-26 13:58:44.656000+00:00,False,2
2207,Automate Screenshots with URLbox & Analyze them with AI,In this automation we first make a screenshot with a screenshot API called URLbox and then send this screenshot into the OpenAI API and analyze it. You can extend this automation by the way you want to ingest the website url's & names into this workflow. Options as data source: Postgres Google Sheets Your CRM ... Setup: Replace Website & URL in Setup Node Put in your URLbox API Key Put in your OpenAI credentials Click here for a blog article with more information on the automation.,3812,2024-03-30 23:04:08.589000+00:00,True,2
2211,Batch update all your Youtube video descriptions in one click,"üéâ Do you want to master AI automation, so you can save time and build cool stuff? I‚Äôve created a welcoming Skool community for non-technical yet resourceful learners. üëâüèª Join the AI Atelier üëàüèª Keeping your YouTube video descriptions updated and consistent across your channel can be a daunting task. Manually editing each video is not only time-consuming but also prone to errors. üìã Blog post üì∫ Youtube Video This workflow streamlines this process, allowing you to maintain a shared section in all your video descriptions and effortlessly update them all at once. By incorporating a unique identifier, you can automate updates across your entire channel, keeping your content fresh and relevant with minimal effort. How it Works Define Your Unique Delimiter:** Choose your unique delimiter (e.g., ""---n8ninja---""). It will be visible, so select something appropriate for your audience. Automate Updates:** Anything below the delimiter can be automatically updated by this workflow. Configure Text Updates:** Set the text you wish to add to every video description in the configuration node. Getting Started Integrate Google (YouTube) Credentials:** Securely add your credentials to enable API access. Set Up the Configuration Node:** Define your delimiter and the text for the shared section you wish to append to your video descriptions. Prepare Your Videos:** Add the chosen delimiter to all videos you want to update automatically. Execute the Workflow:** Run the workflow whenever you wish to batch update the descriptions of your videos. Created by the n8ninja ‚ú® follow on X üì∫ follow on YT",3809,2024-04-01 17:00:13.935000+00:00,True,1
3820,Dynamically switch between LLMs for AI Agents using LangChain Code,"Dynamically switch between LLMs for AI Agents using LangChain Code Purpose This example workflow demonstrates a way to connect multiple LLMs to a single AI Agent/LangChain Node and programmatically use one ‚Äì or in this case loop through them. What it does This AI workflow takes in customer complaints and generates a response that is being validated before returned. If the answer was not satisfactory, the response will be generated again with a more capable model. How it works A LangChain Code Node allows multiple LLMs to be connected to a single Basic LLM Chain. On every call only one LLM is actually being connected to the Basic LLM Chain, which is determined by the index defined in a previous Node. The AI output is later validated by a Sentiment Analysis Node If the result was not satisfactory, it loops back to the beginning and executes the same query with the next available LLM The loop ends either when the result passed the requirements or when all LLMs have been used before. Setup Clone the workflow and select the belonging credentials. You'll need an OpenAI Account, alternatively you can swap the LLM nodes with ones from a different provider like Anthropic after the import. How to use Beware that the order of the used LLMs is determined by the order they have been added to the workflow, not by the position on the canvas. After cloning this workflow into your environment, open the chat and send this example message: &gt; I really love waiting two weeks just to get a keyboard that doesn‚Äôt even work. Great job. Any chance I could actually use the thing I paid for sometime this month? Most likely you will see that the first validation fails, causing it to loop back to the generation node and try again with the next available LLM. Since AI responses are unpredictable, the results and number of tries will differ for each run. Disclaimer Please note, that this workflow can only run on self-hosted n8n instances, since it requires the LangChain Code Node.",3805,2025-05-01 12:28:53.112000+00:00,True,4
2424,Manipulate PDF with Adobe developer API,"Adobe developer API Did you know that Adobe provides an API to perform all sort of manipulation on PDF files : Split PDF, Combine PDF OCR Insert page, delete page, replace page, reorder page Content extraction (text content, tables, pictures) ... The free tier allows up to 500 PDF operation / month. As it comes directly from Adobe, it works often better than other alternatives. Adobe documentation: https://developer.adobe.com/document-services/docs/overview/pdf-services-api/howtos/ https://developer.adobe.com/document-services/docs/overview/pdf-extract-api/gettingstarted/ What does this workflow do The API is a bit painful to use. To perform a transformation on a PDF it requires to Authenticate and get a temporal token Register a new asset (file) Upload you PDF to the registered asset Perform a query according to the transformation requested Wait for the query to be proccessed by Adobe backend Download the result This workflow is a generic wrapper to perform all these steps for any transformation endpoint. I usually use it from other workflow with an Execute Workflow node. Examples are given in the workflow. Example use case This service is useful for example to clean PDF data for an AI / RAG system. My favorite use-case is to extract table as images and forward images to an AI for image recognition / description which is often more accuarate than feedind raw tabular data to a LLM.",3787,2024-09-23 12:32:15.091000+00:00,True,2
2212,Zalando Price Patrol: Monitor price evolution with email notification,"üéâ Do you want to master AI automation, so you can save time and build cool stuff? I‚Äôve created a welcoming Skool community for non-technical yet resourceful learners. üëâüèª Join the AI Atelier üëàüèª Monitor Zalando product pricing and get notified if a Zalando product price falls under a limit you have defined. This n8n workflow lets you follow the evolution of the price of products you select. For each product, you define a minimal price. The workflow automatically scrapes the price for you on a daily basis. If the price falls under your minimal price settings, you receive a notification. This workflow is very easy to use. From a simple form, just paste the URL of the Zalando product you want to monitor and fill in the minimal price. Features Monitor Zalando Product price: follow the price evolution of your favorite Zalando products. Email notification: set a minimal price, if the product price falls below this limit, you get notified by email. Visual price evolution: get a graphical overview of the product pricing evolutions. Automated Daily check-up: this workflow automatically checks the price of your selected Zalando products on a daily basis. Set up Copy this workflow to your n8n interface. Create a new Google Spreadsheet, copy this template Setup your workflow with your Google credential, your email, and your copy of the Spreadsheet. Activate the Workflow and start pasting Zalando product URLs. I hope you will enjoy this workflow that is probably one of the simplest ways to monitor the pricing evolution of your favorite Zalando products. Feel free to contact me should you have any questions or suggestions. Created by the n8n.inja ‚ú® follow on X üì∫ follow on YT",3785,2024-04-04 13:31:22.550000+00:00,True,3
4031,Cold Outreach Automation: Scrape Local Leads with Dumpling AI & Call via Vapi,"Who is this for? This template is for sales teams, agencies, or local service providers who want to quickly generate cold outreach lists and automatically call local businesses with a Vapi AI assistant. It‚Äôs perfect for automating cold calls from scraped local listings with no manual dialing or research. What problem is this workflow solving? Finding leads and initiating outreach calls can be time-consuming. This workflow automates the process: it scrapes business listings from Google Maps using Dumpling AI, extracts phone numbers, filters out incomplete data, formats the numbers, and uses Vapi to make outbound AI-powered calls. Every call is logged in Google Sheets for follow-up and tracking. What this workflow does Starts manually and pulls search queries (e.g., ""plumbers in Austin"") from Google Sheets. Sends each query to Dumpling AI‚Äôs Google Maps scraping endpoint. Splits the returned business data into individual leads. Extracts key info like business name, website, and phone number. Filters to only keep leads with valid phone numbers. Formats phone numbers for Vapi dialing (adds +1). Calls each business using Vapi AI. Logs each successful call in a Google Sheet. Setup Google Sheets Setup Create a sheet with business search queries in the first column (e.g., best+restaurants+in+Chicago) Make sure the tab name is set and authorized in your credentials. Connect your Google Sheets account in the Get Search Keywords from Google Sheets node. Dumpling AI Setup Go to dumplingai.com Generate an API Key and connect it as a header token in the Scrape Google Map Businesses using Dumpling AI node Vapi Setup Sign into Vapi and create an assistant Get your assistantId and phoneNumberId Insert these into the JSON payload of the Initiate Vapi AI Call to Business node Add your Vapi API key to the credentials section Call Logging Create another tab in your sheet (e.g., ‚Äúleads‚Äù) with these headers: company name phone number website This will be used in the Log Called Business Info to Sheet node How to customize this workflow to your needs Modify the business search terms in your Google Sheet to target specific industries or locations. Add filters to exclude certain businesses based on ratings, keywords, or location. Update your Vapi assistant script to match the type of outreach or pitch you‚Äôre using. Add additional integrations (e.g., CRM logging, Slack notifications, follow-up emails). Change the trigger to run on a schedule or webhook instead of manually. Nodes and Functions Breakdown Start Workflow Manually: Initiates the automation manually for testing or controlled runs. Get Search Keywords from Google Sheets: Reads search phrases from the spreadsheet. Scrape Google Map Businesses using Dumpling AI: Sends each search query to Dumpling AI and receives matching local business data. Split Each Business Result: Breaks the returned array of businesses into individual records for processing. Extract Business Name, Phone and website: Extracts title, phone, and website from each business record. Filter Valid Phone Numbers Only: Ensures only entries with a phone number move forward. Format Phone Number for Calling: Adds a +1 country code and strips non-numeric characters. Initiate Vapi AI Call to Business: Uses the business name and number to initiate a Vapi AI outbound call. Log Called Business Info to Sheet: Appends business details into a Google Sheet for tracking. Notes You must have valid API keys and authorized connections for Dumpling AI, Google Sheets, and Vapi. Make sure to handle API rate limits if you're running the workflow on large datasets. This workflow is optimized for US-based leads (+1 country code); adjust the formatting node if calling internationally.",3759,2025-05-14 01:34:16.860000+00:00,True,2
3868,Automate Support Ticket Triage and Resolution with JIRA and AI,"This n8n template automates triaging of newly opened support tickets and issue resolution via JIRA. If your organisation deals with a large number of support requests daily, automating triaging is a great use-case for introducing AI to your support teams. Extending the idea, we can also get AI to give a first attempt at resolving the issue intelligently. How it works A scheduled trigger picks up newly opened JIRA support tickets from the queue and discards any seen before. An AI agent analyses the open ticket to add labels, priority on the seriousness of the issue and simplifies the description for better readability and understanding for human support. Next, the agent attempts to address and resolve the issue by finding similar issues (by tags) which have been resolved. Each similar issue has its comments analysed and summarised to identify the actual resolution and facts. These summarises are then used as context for the AI agent to suggest a fix to the open ticket. How to use Simply connect your JIRA instance to the workflow and activate to start watching for open tickets. Depending on frequency, you may need to increase for decrease the intervals. Define labels to use in the agent's system prompt. Restrict to certain projects or issue types to suit your organisation. Requirements JIRA for issue management and support portal OpenAI for LLM Customising this workflow Not using JIRA? Try swapping out the nodes for Linear or your issue management system of choice. Try a different approach for issue resolution. You might want to try RAG approach where a knowledge base is used.",3736,2025-05-05 10:29:45.614000+00:00,True,4
2274,Low-code API for Flutterflow apps,Flow Start: The flow starts upon receiving an HTTP GET call. Webhook: Receives the HTTP GET call and triggers the flow. Database: Connects to the database (Customer Datastore) to retrieve all necessary information (getAllPeople). Data Processing: Variable Insertion: The retrieved data is inserted into a variable. Variable Aggregation: The variables are aggregated and prepared for use in FlutterFlow. Webhook Response: Sends the response back through the Webhook with the processed data ready for use in FlutterFlow.,3724,2024-05-28 20:09:31.792000+00:00,False,1
2312,Attach a default error handler to all active workflows,"How it works: Did you ever miss any errors in your workflow executions? I did! And I usually only realised a few days or weeks later. üôà This template attaches a default error workflow to all your active workflows. From now on, you'll receive a notification whenever a workflow errors and you'll have peace of mind again. It runs every night at midnight so you never have to think of this again. Of course, you can also run it manually. Steps to set up: Update the Gmail note with your own email address, or replace it with any other notification mechanism. You can also use Slack, Discord, Telegram or text messages.. Activate the workflow. Relax. Caveats: I did not add any rate limiting, so if you have a workflow that runs very frequently and it errors... well let's say your mailbox will not be a nice place anymore. Ideas for improvement? If you have any suggestions for improvement, feel free to reach out to me at bart@n8n.io. Enjoy!",3658,2024-07-04 15:38:49.281000+00:00,False,2
2223,Set credentials dynamically using expressions,"How it works This workflow shows how to set credentials dynamically using expressions. It accepts an API key via a form, then uses it in the NASA node to authenticate a request. Setup steps First, set up your NASA credential: Create a new NASA credential. Hover over API Key. Toggle Expression on. In the API Key field, enter {{ $json[""Enter your NASA API key""] }}. Then, test the workflow: Get an API key from NASA Select Test workflow Enter your key using the form. The workflow runs and sends you to the NASA picture of the day. For more information on expressions, refer to n8n documentation | Expressions.",3642,2024-04-10 14:40:44.805000+00:00,False,1
2442,üöÄ Local Multi-LLM Testing & Performance Tracker,"üöÄ Local Multi-LLM Testing & Performance Tracker This workflow is perfect for developers, researchers, and data scientists benchmarking multiple LLMs with LM Studio. It dynamically fetches active models, tests prompts, and tracks metrics like word count, readability, and response time, logging results into Google Sheets. Easily adjust temperature üî• and top P üéØ for flexible model testing. Level of Effort: üü¢ Easy ‚Äì Minimal setup with customizable options. Setup Steps: Install LM Studio and configure models. Update IP to connect to LM Studio. Create a Google Sheet for result tracking. Key Outcomes: Benchmark LLM performance. Automate results in Google Sheets for easy comparison. Version 1.0",3614,2024-09-28 19:28:58.189000+00:00,True,5
3960,Automated Financial Tracker: Telegram Invoices to Notion with Gemini AI Reports,"Automated Financial Tracker: Telegram Invoices to Notion with AI Summaries & Reports Tired of manually logging every expense? Streamline your financial tracking with this powerful n8n workflow! Snap a photo of your invoice in Telegram, and let AI (powered by Google Gemini) automatically extract the details, record them in your Notion database, and even send you a quick summary. Plus, get scheduled weekly reports with charts to visualize your spending. Automate your finances, save time, and gain better insights with this easy-to-use template! Transform your expense tracking from a chore into an automated breeze. Try it out! Overview: This workflow revolutionizes how you track your finances by automating the entire process from invoice capture to reporting. Simply send a photo of an invoice or receipt to a designated Telegram chat, and this workflow will: Extract Data with AI: Utilize Google Gemini's capabilities to perform OCR on the image, understand the content, and extract key details like item name, quantity, price, total, date, and even attempt to categorize the expense. Store in Notion: Automatically log each extracted transaction into a structured Notion database. Instant Feedback: Send a summary of the processed transaction back to your Telegram chat. Scheduled Reporting: Generate and send a visual summary of your expenses (e.g., weekly spending by category) as a chart to your preferred Telegram chat or group. This workflow is perfect for individuals, freelancers, or small teams looking to effortlessly manage their expenses without manual data entry. Key Features & Benefits: Effortless Expense Logging:** Just send a picture ‚Äì no more typing! AI-Powered Data Extraction:** Leverages Google Gemini for intelligent invoice processing. Centralized Data in Notion:** Keep all your financial records neatly organized in a Notion database. Automated Categorization:** AI helps in categorizing your expenses (e.g., Food & Beverage, Transportation). Instant Summaries:** Get immediate confirmation and a summary of what was recorded. Visual Reporting:** Receive scheduled charts (e.g., bar charts of spending by category) directly in Telegram. Customizable:** Easily adapt the workflow to your specific needs, categories, and reporting preferences. Time-Saving:** Drastically reduces the time spent on manual financial administration. How It Works (Workflow Breakdown): The workflow is divided into two main parts: Part 1: Real-time Invoice Processing & Logging (## Auto Notes Transaction with Telegram and Notion database) Telegram Trigger (Telegram Trigger | When recive photo): Activates when a new photo is sent to the configured Telegram chat. Get Photo Info (Get Info Photo from telegram chat): Retrieves the details of the received photo. Get Image Info (Get Image Info): Prepares the image data. AI Data Extraction (Google Gemini Chat Model & Basic LLM Chain): The image data is sent to the Google Gemini Chat Model. A specific prompt instructs the AI to extract details (date, ID, name, quantity, price, total, category, tax) in a JSON array format and provide a summary message. The categories include Food & Beverage, Transportation, Utilities, Shopping, Healthcare, Entertainment, Housing, and Education. Parse AI Output (Parse To your object | Table): Structures the AI's JSON output for easier handling. Split Transactions (Split Out | data transaction): If an invoice contains multiple items, this node splits them into individual records. Record to Notion (Record To Notion Database): Each transaction item is added as a new page/entry in your specified Notion database, mapping fields like Name, Quantity, Price, Total, Category, Date, and Tax. Send Telegram Summary (Sendback to chat and give summarize text): The summary message generated by the AI is sent back to the original Telegram chat. Part 2: Scheduled Financial Reporting (## Schedule report to send on chanel or private message) Schedule Trigger (Schedule Trigger | for send chart report): Runs at a predefined interval (e.g., every week) to generate reports. Get Recent Data from Notion (Get Recent Data from Notions): Fetches transaction data from the Notion database for a specific period (e.g., the past week). Summarize Data (Summarize Transaction Data): Aggregates the data, for example, by summing up the 'total' amount for each 'category'. Prepare Chart Data (Convert Data to JSON chart payload): Transforms the summarized data into a JSON format suitable for generating a chart (e.g., labels for categories, data for spending amounts). Generate Chart (Generate Chart): Uses the QuickChart node to create a visual chart (e.g., a bar chart) from the prepared data. Send Chart to Telegram (Send Chart Image to Group or Private Chat): Sends the generated chart image to a specified Telegram chat ID or group. Nodes Used (Key Nodes): Telegram Trigger & Telegram Node:** For receiving images and sending messages/images. Google Gemini Chat Model (Langchain):** For AI-powered OCR and data extraction from invoices. Basic LLM Chain (Langchain):** To interact with the language model using specific prompts. Output Parser Structured (Langchain):** To structure the output from the language model. Notion Node:** For reading from and writing to your Notion databases. Schedule Trigger:** To automate the reporting process. Summarize Node:** To aggregate data for reports. Code Node:** Used here to format data for the chart. QuickChart Node:** For generating charts. SplitOut Node:** To process multiple items from a single invoice. Setup Instructions: Credentials: Telegram: Create a Telegram bot and get its API token. You'll also need the Chat ID where you'll send invoices and where reports should be sent. Google Gemini (PaLM) API: You'll need an API key for Google Gemini. Notion: Create a Notion integration and get the API key. Create a Notion database with properties corresponding to the data you want to save (e.g., Name (Title), Quantity (Number), Price (Number), Total (Number), Category (Select), Date (Text or Date), Tax (Number)). Share this database with your Notion integration. Configure Telegram Trigger: Add your Telegram Bot API token. When you first activate the workflow or test the trigger, send /start to your bot in the chat you want to use for sending invoices. n8n will then capture the Chat ID. Configure Google Gemini Node (Google Gemini Chat Model): Select or add your Google Gemini API credentials. Review the prompt in the Basic LLM Chain node and adjust if necessary (e.g., date format, categories). Configure Notion Nodes: Record To Notion Database: Select or add your Notion API credentials. Select your target Notion Database ID. Map the properties from the workflow (e.g., ={{ $json.name }}) to your Notion database columns. Get Recent Data from Notions: Select or add your Notion API credentials. Select your target Notion Database ID. Adjust the filter if needed (default is ""past_week""). Configure Telegram Node for Reports (Send Chart Image to Group or Private Chat): Select or add your Telegram Bot API token. Enter the Chat ID for the group or private chat where you want to receive the reports. Configure Schedule Trigger (Schedule Trigger | for send chart report): Set your desired schedule (e.g., every Monday at 9 AM). Test: Send an image of an invoice to your Telegram bot and check if the data appears in Notion and if you receive a summary message. Wait for the scheduled report or manually trigger it to test the reporting functionality. Sticky Note Text for Your n8n Template: (These are suggestions. You would place these directly into the sticky notes within your n8n workflow editor.) Existing High-Level Sticky Notes: ## Auto Notes Transaction with Telegram and Notion database ## Schedule report to send on chanel or private message Specific Sticky Notes to Add: On Telegram Trigger | When recive photo:** üì∏ INVOICE INPUT üì∏ Bot listens here for photos of your receipts/invoices. Ensure your Telegram Bot API token is set in credentials. Near Google Gemini Chat Model & Basic LLM Chain:** ü§ñ AI MAGIC HAPPENS HERE üß† Image is sent to Google Gemini for data extraction. Check 'Basic LLM Chain' to customize the AI prompt (e.g., categories, output format). Requires Google Gemini API credentials. On Parse To your object | Table:** ‚ú® STRUCTURING AI DATA ‚ú® Converts the AI's text output into a usable JSON object. Check the schema if you modify the AI prompt significantly. On Record To Notion Database:** üìù SAVING TO NOTION üìù Extracted transaction data is saved here. Configure with your Notion API key & Database ID. Map fields correctly to your database columns! On Sendback to chat and give summarize text:** üí¨ TRANSACTION SUMMARY üí¨ Sends a confirmation message back to the user in Telegram with a summary of the recorded expense. On Schedule Trigger | for send chart report:** üóìÔ∏è REPORTING SCHEDULE üóìÔ∏è Set how often you want to receive your spending report (e.g., weekly, monthly). On Get Recent Data from Notions:** üìä FETCHING DATA FOR REPORT üìä Retrieves transactions from Notion for the report period. Default: ""Past Week"". Adjust filter as needed. Requires Notion API credentials & Database ID. On Summarize Transaction Data:** ‚ûï SUMMARIZING SPENDING ‚ûï Aggregates your expenses, usually by category, to prepare for the chart. On Convert Data to JSON chart payload (Code Node):** üé® PREPARING CHART DATA üé® This Code node formats the summarized data into the JSON structure needed by QuickChart. On Generate Chart (QuickChart Node):** üìà GENERATING VISUAL REPORT üìà Creates the actual chart image based on your spending data. You can customize chart type (bar, pie, etc.) here. On Send Chart Image to Group or Private Chat:** üì§ SENDING REPORT TO TELEGRAM üì§ Delivers the generated chart to your chosen Telegram chat/group. Set the correct Chat ID and Bot API token. General Sticky Note (Place where relevant):** üîë CREDENTIALS NEEDED üîë Remember to set up API keys/tokens for: Telegram Google Gemini Notion General Sticky Note (Place where relevant):** üí° CUSTOMIZE ME! üí° Adjust AI prompts for better accuracy. Change Notion database structure. Modify report frequency and content. `",3597,2025-05-09 11:57:48.753000+00:00,True,8
2156,Generate instant google meet links with a slack command,"How it works This template uses a slack app to connect with your google calendar, generate an instant google meet link and post it as a message in a slack channel Setup steps Firstly, you'll need to create a slack app Authenticate and connect your slack account Connect and choose the Google calendar you want to generate Google meet links for Customize your slack message Then using a /meet command in slack, you can instantly generate and post your Google meet links",3462,2024-02-29 22:39:33.103000+00:00,False,2
2122,Automatically email great leads when they submit a form and record in HubSpot,"Use case This workflow automatically qualifies great leads from a form and sends them an email üòÆ.. It also adds the user to Hubspot if not already added and records the outreach. How to setup Add you MadKudu, Hunter, and Gmail credentials Setup your HubSpot Oauth2 creds using n8n docs Set the email content and subject Click the Test Workflow button, enter your email and check the Slack channel Activate the workflow and use the form trigger production URL to collect your leads in a smart way How to adjust this template You may want to raise or lower the threshold for your leads, as you see fit. You also need to update the content (the email and the subject), obviously üòÖ.",3416,2024-02-22 16:30:20.688000+00:00,False,4
3971,Summarise MS Teams Channel Activity for Weekly Reports with AI,"This n8n template lets you summarize individual team member activity on MS Teams for the past week and generates a report. For remote teams, chat is a crucial communication tool to ensure work gets done but with so many conversations happening at once and in multiple threads, ideas, information and decisions usually live in the moment and get lost just as quickly - and all together forgotten by the weekend! Using this template, this doesn't have to be the case. Have AI crawl through last week's activity, summarize all messages and replies and generate a casual and snappy report to bring the team back into focus for the current week. A project manager's dream! How it works A scheduled trigger is set to run every Monday at 6am to gather all team channel messages within the last week. Messages are grouped by user. AI analyses the raw messages and replies to pull out interesting observations and highlights. This is referred to as the individual reports. All individual reports are then combined and summarized together into what becomes the team weekly report. This allows understanding of group and similar activities. Finally, the team weekly report is posted back to the channel. The timing is important as it should be the first message of the week and ready for the team to glance over coffee. How to use Ideally works best per project and where most of the comms happens on a single channel. Avoid combining channels and instead duplicate this workflow for more channels. You may need to filter for specific team members if you want specific team updates. Customise the report to suit your organisation, team or the channel. You may prefer to be more formal if clients or external stakeholders are also present. Requirements MS Teams for chat platform OpenAI for LLM Customising this workflow If the teams channel is busy enough already, consider posting the final report to email. Pull in project metrics to include in your report. As extra context, it may be interesting to tie the messages to production performance. Use an AI Agent to query for knowledgebase or tickets relevant to the messages. This may be useful for attaching links or references to add context.",3412,2025-05-10 15:05:32.396000+00:00,True,4
3934,Generate Lessons Learned Reports from Jira Epics with AI and Google Docs,"Who is this for? Jira users who want to automate the generation of a Lessons Learned or Retrospective report after an Epic is Done. What problem is this workflow solving? / use case Lessons Learned / Retrospective reports are often omitted in Agile teams because they take time to write. With the use of n8n and AI this process can be automated. What is this workflow doing Triggers automatically upon an Epic reaching the ""Done"" status in Jira. Collects all related tasks and comments associated with the completed Epic. Intelligently filters the gathered data to provide the LLM with the most relevant information. Utilizes an LLM with a structured System Message to generate insightful reports. Delivers the finalized report directly to your specified Google Docs document. Setup Create a Jira API key and follow the Credentials Setup in the Jira trigger node. Create credentials for Google Docs and paste your document ID into the Node. How to customize this workflow to your needs Change the System Message in the AI Agent to fit your needs.",3406,2025-05-08 07:35:46.686000+00:00,True,5
2272,Enrich up to 1500 emails per hour with Dropcontact batch requests,"The template allows to make Dropcontact batch requests up to 250 requests every 10 minutes (1500/hour). Valuable if high volume email enrichment is expected. Dropcontact will look for email & basic email qualification if first_name, last_name, company_name is provided. +++++++++++++++++++++++++++++++++++++++++ Step 1: Node ""Profiles Query"" Connect your own source (Airtable, Google Sheets, Supabase,...) the template is using Postgres by default. Note I: Be careful your source is only returning a maximum of 250 items. Note II: The next node uses the next variables, make sure you can map these from your source file: first_name last_name website (company_name would work too) full_name (see note) Note III: This template is using the Dropcontact Batch API, which works in a POST & GET setup. Not a GET request only to retrieve data, as Dropcontact needs to process the batch data load properly. +++++++++++++++++++++++++++++++++++++++++ Step 2: Node ""Data Transformation"" Will transform the input variables in the proper json format. This json format is expected from the Dropcontact API to make a batch request. ""full_name"" is being used as a custom identifier to update the returned email to the proper contact in your source database. To make things easy, use a unique identiefer in the full_name variable. +++++++++++++++++++++++++++++++++++++++++ Step3: Node: ""Bulk Dropcontact Requests"". Enter your Dropcontact credentials in the node: Bulk Dropcontact Requests. +++++++++++++++++++++++++++++++++++++++++ Step4: Connect your output source by mapping the data you like to use. +++++++++++++++++++++++++++++++++++++++++ Step5: Node: ""Slack"" (OPTIONAL) Connect your slack account, if an error occur, you will be notified. TIP: Try to run the workflow with a batch of 10 (not 250) as it might need to run initially before you will be able to map the data to your final destination. Once the data fields are properly mapped, adjust back to 250.",3370,2024-05-21 14:12:20.932000+00:00,False,4
2133,Provide latest euro exchange rates from European Central Bank via Webhook,"What is this workflow doing? This simple workflow is pulling the latest Euro foreign exchange reference rates from the European Central Bank and responding expected values to an incoming HTTP request (GET) via a Webhook trigger node. Setup no authentication** needed the workflow is ready to use test** the workflow template by hitting the test workflow button and calling the URL in the webhook node optional: choose your own Webhook listening path in the Webhook trigger node Usage There are two possible usage scenarios: get all Euro exchange rates as an array of objects get only a specific currency exchange rate as a single object All available rates Using the HTTP query ?foreign=USD (where USD is one of the available currency symbols) will provide only that specificly asked rate. Response example: {""currency"":""USD"",""rate"":""1.0852""} Single exchange rate If no query is provided, all available rates are returned. Response example: [{""currency"":""USD"",""rate"":""1.0852""},{""currency"":""JPY"",""rate"":""163.38""},{""currency"":""BGN"",""rate"":""1.9558""},{""currency"":""CZK"",""rate"":""25.367""},{""currency"":""DKK"",""rate"":""7.4542""},{""currency"":""GBP"",""rate"":""0.85495""},{""currency"":""HUF"",""rate"":""389.53""},{""currency"":""PLN"",""rate"":""4.3053""},{""currency"":""RON"",""rate"":""4.9722""},{""currency"":""SEK"",""rate"":""11.1675""},{""currency"":""CHF"",""rate"":""0.9546""},{""currency"":""ISK"",""rate"":""149.30""},{""currency"":""NOK"",""rate"":""11.4285""},{""currency"":""TRY"",""rate"":""33.7742""},{""currency"":""AUD"",""rate"":""1.6560""},{""currency"":""BRL"",""rate"":""5.4111""},{""currency"":""CAD"",""rate"":""1.4674""},{""currency"":""CNY"",""rate"":""7.8100""},{""currency"":""HKD"",""rate"":""8.4898""},{""currency"":""IDR"",""rate"":""16962.54""},{""currency"":""ILS"",""rate"":""3.9603""},{""currency"":""INR"",""rate"":""89.9375""},{""currency"":""KRW"",""rate"":""1444.46""},{""currency"":""MXN"",""rate"":""18.5473""},{""currency"":""MYR"",""rate"":""5.1840""},{""currency"":""NZD"",""rate"":""1.7560""},{""currency"":""PHP"",""rate"":""60.874""},{""currency"":""SGD"",""rate"":""1.4582""},{""currency"":""THB"",""rate"":""38.915""},{""currency"":""ZAR"",""rate"":""20.9499""}] Further info Read more about Euro foreign exchange reference rates here.",3368,2024-02-27 13:29:30.205000+00:00,False,1
2383,IOT Button Remote / Spotify Control Integration with MQTT,"Overview This template integrates an IOT multi-button switch (meant for controlling a dimmable light) with Spotify playback functions, via MQTT messages. This isn't likely to work without some tinkering, but should be a good head start on receiving/routing IOT/MQTT messages and hooking up to a Spotify-like API. Requirements An IOT device capable of generating events that can be delivered as MQTT messages through an MQTT Broker e.g. Ikea Strybar remote An MQTT Broker to which n8n can connect and consume messages e.g. Zigbee2MQTT in HomeAssistant A Spotify developer-account (which provides access to API functions via OAuth2 authorization) A Spotify user-account (which provides access to Spotify streamed content, user settings, etc.) Setup Create an MQTT Credential item in n8n and assign it to the MQTT Trigger node Modify the MQTT trigger node to match the topic for your IOT device messages Modify the switch/router nodes to map to the message text from your IOT button (e.g. arrow_left_click, brightness_up_click, etc.) Create a Spotify developer-account (or use the login for a user-account) Create an ""App"" in the developer-account to represent the n8n workflow Chicken/Egg ALERT: The n8n Spotify Credentials dialog box will display the ""OAuth Redirect URL"" required to create the App in Spotify, but the n8n Credential item itself cannot be created until AFTER the App has been created. Create a Spotify Credentials item in n8n Open the Settings on the Spotify App to find the required Client ID and Client Secret information. ALERT: Save this before proceeding to the Connect step. Connect the n8n Spotify Credential item to the Spotify user-account ALERT: Expect n8n to open a separate OAuth2 window on authorization.spotify.com here, which may require a login to the Spotify user-account Open each of the HTTP and Spotify nodes, one by one, and re-assign to your Spotify Credential (try not to miss any). (Then, probably, upvote this feature request: https://community.n8n.io/t/select-credentials-via-expression/5150 Modify the variable values in the Globals node to match your own environment. target_spotify_playback_device_name - The name of a playback device available to the Spotify user-account favorite_playlist_name - The name of a playlist to start when one of the button actions is indicated in the MQTT message. Used in example ""Custom Function 2"" sequence. Notes You're on your own for getting the multi-button remote switch talking to MQTT, figuring out what the exact MQTT topic name is, and mapping the message parts to the workflow (actions, etc.). The next / previous actions are wired up to not transfer control to the target device. This alternative routing just illustrates a different behavior than the remaining actions/functions, which include activation of the target device when required. Some of the Spotify API interactions use the Spotify node in n8n, but many of the available Spotify API functions are limited or not implemented at all in the Spotify node. So, in other cases, a regular HTTP node is used with the Spotify OAuth2 API credential instead. By modifying one of the examples included in the template, it should be possible to call nearly anything the Spotify API has to offer. Spotify+n8n OAuth Mini-Tutorial Definitions The developer-account is the Spotify login for creating a spotify-app which will be associated with a client id and client secret. The user-account is the Spotify login that has permission to stream songs, set up playback devices, etc. ++A spotify-login allows access to a Spotify user-account, or a Spotify developer-account, OR BOTH++ The spotify-app, which has a client id and client secret, is an object created in the developer-account. The app-implementation (in this case, an ++n8n workflow++) uses the spotify-app's credentials (client id / client secret) to call Spotify API endpoints on behalf of a user-account. Using One Spotify Login as Both User and Developer When an n8n Spotify-node or HTTP-node (i.e. an app-implementation) calls a Spotify API endpoint, the Credentials item may be using the client id and client secret from a spotify-app, which was created in a developer-account that is ++one and the same spotify-login as the user-account++. However, it helps to remind yourself that from the Spotify API server's perspective, the developer-account + spotify-app, and the user-account, are ++two independent entities++. n8n Spotify-OAuth2-API Credential Authorization Process The 2 layers/steps, in the process of authorizing an n8n Spotify-OAuth2-API credential to make API calls, are: n8n must identify itself to Spotify as the app-implementation associated with the developer-account/spotify-app by sending the app's credentials (client id and client secret) to Spotify. The Client ID and Client Secret are supplied in the n8n Spotify OAuth2 Credentials UI/dialog-box Separately, n8n must obtain an authorization token from Spotify to represent the permissions granted by the user to execute actions (call API endpoints) on behalf of the user (i.e. access things that belong to the user-account). This authorization for the user-account access is obtained when the ""Connect"" or ""Reconnect"" button is clicked in the n8n Spotify Credentials UI/dialog-box (which pops up a separate authorization UI/browser-window managed by Spotify). The Authorization for a given spotify-app stays ""registered"" in the user-account until revoked. See: https://support.spotify.com/us/article/spotify-on-other-apps/ Direct Link: https://www.spotify.com/account/apps/ More than one user-account can be authorized for a given spotify-app. A particular n8n Spotify-OAuth2-API credential item appears to cache an authorization token for the user-account that was most recently authorized. Up to 25 users can be allowed access to a spotify-app in Developer-Mode, but any user-account other than the one associated with the developer-account must be added by email address at https://developer.spotify.com/dashboard/{{app-credential-id}}/users ALERT: IF the browser running the n8n UI is ALSO logged into a Spotify account, and the spotify-app is already authorized for that Spotify account, the ""reconnect"" button in the Spotify-OAuth2-API credential dialog may automatically grab a token for that logged in user-account, offering no opportunity to select a different user-account. This can be managed somewhat by using ""incognito"" browser windows for n8n, Spotify, or both. References n8n Spotify Credentials Docs Spotify Authorization Docs",3364,2024-08-18 07:01:52.004000+00:00,False,2
2267,Extract Title tag and Meta description from url for SEO analysis with Airtable,"Extract Title tag and meta description from url for SEO analysis. How it works The workflows takes records from Airtable, get the url in the records and extract from the related webpage the title tag (&lt;title&gt;) and meta description (&lt;meta name=""description"" content=""Some content""&gt;). If title tag and/or meta description tag isn't available on the webpage, the result will be empty. Setup Set a Base in Airtable with a table with the following structure: url (field type url), title tag (field type text string), meta desc (field type text field) Minimum suggested table structure is: url (https://example.com), title (Title example), meta desc* (This is the meta description of the example page) Connect Airtable to both Airtable nodes in the template and, with the following formula, get all the records that miss title tag and meta desc. Formula: AND(url != """", {title tag} = """", {meta desc} = """") Insert the url to be analyzed in the table in the field url and let the workflow do the rest. Extra You can also calculate the length for title tag and meta desc using formula field inside Airtable. This is the formula: LEN({title tag}) or LEN({meta desc}) You can automate the process calling a Webhook from Airtable. For this, you need an Airtable paid plan.",3351,2024-05-14 10:28:21.097000+00:00,False,3
2380,Create Atlassian Confluence page from template,"How it works creates a new page in Confluence based on a page template also defined in Confluence replaces any number of placeholders with data from your workflow generic implementation for maximum flexibility Set up steps All parameters you need to change are defined in the Set node Set your Atlassian-domain Set the template id you want to use as the basis for new pages Set the target space and parent page for new pages added based on that template. üé• Explainer video has all the details. =) Feedback Any feedback is welcome. If you have ideas for improvements, let me know.",3244,2024-08-14 19:28:15.626000+00:00,False,2
2112,Send outreach/cold email using Gmail to new Hubspot contacts,"Use case This workflow uses Gmail to send outreach emails to new Hubspot contacts that have yet to be contacted (usually unknown contacts), and records the outreach in Hubspot. Setup Setup HubSpot Oauth2 creds (Be careful with scopes. They have to be exact, not less or more. Yes, it's not simple, but it's well documented in the n8n docs. Be smarter than me, read the docs) Setup Gmail creds. Change the from email and from name in the Record outreach in HubSpot node How to adjust this template to your needs Change the email message in the Set keys node Think about your criteria to reach out to new contacts. Here we simply filter for only contacts with unknown dates.",3241,2024-02-21 17:25:36.403000+00:00,False,2
4043,Adaptive RAG with Google Gemini & Qdrant: Context-Aware Query Answering,"Description This workflow automatically classifies user queries and retrieves the most relevant information based on the query type. üåü It uses adaptive strategies like; Factual, Analytical, Opinion, and Contextual to deliver more precise and meaningful responses by leveraging n8n's flexibility. Integrated with Qdrant vector store and Google Gemini, it processes each query faster and more effectively. üöÄ How It Works? Query Reception: A user query is triggered (e.g., through a chatbot interface). üí¨ Classification: The query is classified into one of four categories: Factual: Queries seeking verifiable information. Analytical: Queries that require in-depth analysis or explanation. Opinion: Queries looking for different perspectives or subjective viewpoints. Contextual: Queries specific to the user or certain contextual conditions. Adaptive Strategy Application: Based on classification, the query is restructured using the relevant strategy for better results. Response Generation**: The most relevant documents and context are used to generate a tailored response. üéØ Set Up Steps Estimated Time: ‚è≥ 10-15 minutes Prerequisites: You need an n8n account and a Qdrant vector store connection. Steps: Import the n8n workflow: Load the workflow into your n8n instance. Connect Google Gemini and Qdrant: Link these tools for query processing and data retrieval. Connect the Trigger Interface: Integrate with a chatbot or API to trigger the workflow. Customize: Adjust settings based on the query types you want to handle and the output format. üîß For more detailed instructions, please check the sticky notes inside the workflow. üìå",3237,2025-05-14 15:10:47.350000+00:00,False,5
2372,"Survey Insights with Qdrant, Python and Information Extractor","This n8n template is one of a 3-part series exploring use-cases for clustering vector embeddings: Survey Insights Customer Insights Community Insights This template demonstrates the Survey Insights scenario where survey participant responses can be quickly grouped by similarity and an AI agent can generate insights on those groupings. With this workflow, researchers can save days and even weeks of work breaking down cohorts of participants and identify frequently mentioned positives and negatives. Sample Output: https://docs.google.com/spreadsheets/d/e/2PACX-1vT6m8XH8JWJTUAfwojc68NAUGC7q0lO7iV738J7aO5fuVjiVzdTRRPkMmT1C4N8TwejaiT0XrmF1Q48/pubhtml# How it works All survey questions and responses are imported from a Google Sheet. Responses are then inserted into a Qdrant collection carefully tagged with the question and survey metadata. For each question, all relevant response are put through a clustering algorithm using the Python Code node. The Qdrant points are returned in clustered groups. Each group is looped to fetch the payloads of the points and feed them to the AI agent to summarise and generate insights for. The resulting insights and raw responses are then saved to the Google Spreadsheet for further analysis by the researcher. Requirements Survey data and format as shown in the attached google sheet. Qdrant Vectorstore for storing embeddings. OpenAI account for embeddings and LLM. Customising the Template Adjust clustering parameters which make sense for your data. Add more clusters for open-ended questions and less clusters when responses are multiple choice.",3224,2024-08-05 09:54:15.339000+00:00,True,8
2116,Verify emails & enrich new form leads and save them to HubSpot,"Use case When collecting leads via a form you're typically facing a few problems: Often end up with a bunch of leads who don't have a valid email address You want to know as much about the new lead as possible but also want to keep the form short After forms are submitted you have to walk over the submissions and see which you want to add to your CRM This workflow helps you to fix all those problems. What this workflow does The workflow checks every new form submission and verifies the email using Hunter.io. If the email is valid, it then tries to enrich the person using Clearbit and saves the new lead into your Hubspot CRM. Setup Add you Hunter, Clearbit and Hubspot credentials Click the Test Workflow button, enter your email and check your Hubspot Activate the workflow and use the form trigger production URL to collect your leads in a smart way How to adjust it to your needs Change the form to the form you need in your use case (e.g. Typeform, Google Forms, SurveyMonkey etc.) Add criteria before an account is added to your CRM. This could for example be the size of company, industry etc. You can find some inspiration in our other template Reach out via Email to new form submissions that meet a certain criteria Add more data sources to save the new lead in",3204,2024-02-22 09:59:02.137000+00:00,False,3
2393,Save n8n Cloud invoices received in Gmail in Google Drive,"Who this is for This template is for everyone that wants to download their n8n Cloud invoices automatically as a PDF instead of downloading them manually. How it works This workflow checks your Gmail inbox for new n8n invoice emails from n8n's payment provider Paddle. Once it finds something, it converts the URL into a PDF using pdflayer and saves it in Google Drive. Setup Setup your Gmail and Google Drive credentials Create a free account at https://pdflayer.com/ Insert your pdflayer API key into the Setup node Insert the URL to the wanted drive folder into the setup node (make sure to remove everything after the ?) How to adjust it to your need Instead of saving the PDF in Google drive, you could also save it in your local system, any other storage provider or send the PDF automatically to the right person in your company.",3181,2024-08-28 15:06:47.448000+00:00,False,3
3947,"AI Personal Assistant with GPT-4o, RAG & Voice for WhatsApp using Supabase","üß† Intelligent AI Assistant with RAG & Voice for WhatsApp ‚Äì Built with GPT-4o & Supabase üìå About this workflow and its creator Hi! I‚Äôm Amanda, a creator of intelligent automations using n8n and Make. I‚Äôve been building AI-powered workflows for over 2 years, always focused on usability and innovation. This one here is very special to me ‚Äì a truly advanced AI assistant that reads, listens, interprets and responds like a real human ü§ñ‚ú® This ready-to-use workflow acts as a powerful AI personal assistant capable of understanding messages via voice, text, documents, or even images. It supports full multi-channel operation (WhatsApp via Evolution API, Instagram, Facebook, and more), and includes advanced RAG capabilities using Supabase + GPT-4o. It‚Äôs designed to be highly extensible, with memory, prompt update tools, and knowledge base management. ‚öôÔ∏è What this workflow does üí¨ Understands user input via text, document, audio or image (voice, OCR, PDF) üé§ Transcribes and interprets voice messages using OpenAI Whisper üß† Understands prompts and user commands using GPT-4o via LangChain agent üóÇÔ∏è Searches knowledge base using RAG + Supabase vector DB üìÑ Accepts documents and automatically indexes them for future questions üßæ Summarizes documents and stores metadata in Supabase üóÉÔ∏è Offers memory support (PostgreSQL chat memory per user session) üìß Sends replies through WhatsApp (Evolution API), Instagram, Facebook, etc. üìÖ Manages schedules (via tool integration with Google Calendar) üì¨ Sends and searches emails (with support tools) üõ† Modular and expandable structure (tools for saving knowledge, deleting, updating prompt) üîß Setup Instructions n8n Hosting This workflow requires n8n self-hosted (or n8n Cloud with custom credentials + community nodes enabled). Create required databases Use the provided SQL queries inside the setar_supabase_tabelas_vectoriais, criar_cerebro, and criar_rag_controle nodes to initialize: documents table for RAG cerebro table for prompt memoria_chat for session memory rag_controle for summaries and indexing Credentials needed OpenAI API (for chat, embeddings and Whisper transcription) Redis (for managing message buffer) Supabase (for vector store + metadata) Postgres (for memory and prompts) Evolution API (or other messaging platforms) Webhook Set the webhook path to receive messages from your Evolution or WhatsApp API provider. Configure ‚ÄòSet‚Äô node In the config node, adjust: adminNumero: your personal WhatsApp or admin number evolutionApiKey: your private API key utilizacaoApenasViaAdmin: toggle if this should only respond to admin numbers Tool connections Ensure the supporting workflows are also imported and connected for: Emails Knowledge management Calendar events üìé Notes This workflow uses LangChain agents, OpenAI GPT-4o, Supabase, Redis, and PostgreSQL. It includes multiple ‚Äústicky notes‚Äù inside the workflow with explanations. Ideal for businesses, consultants, and developers looking to offer an intelligent and extendable AI chatbot experience. üõç Want to use this on your system? ‚ù§Ô∏è Buy workflows: https://iloveflows.com ‚òÅÔ∏è Use n8n Cloud with my partner link: https://n8n.partnerlinks.io/amanda",3100,2025-05-08 20:10:41.254000+00:00,True,12
2244,Dynamically replace images in Google Slides via API,"üéâ Do you want to master AI automation, so you can save time and build cool stuff? I‚Äôve created a welcoming Skool community for non-technical yet resourceful learners. üëâüèª Join the AI Atelier üëàüèª This workflow exposes an API endpoint that lets you dynamically replace an image in Google Slides, perfect for automating deck presentations like updating backgrounds or client logos. *üì∫ Youtube Overview üì∫ * Here's how to get started: Step 1: Set Up a Key Identifier in Google Slides Add a unique key identifier to the images you want to replace. Click on the image. Go to Format Options and then Alt Text. Enter your unique identifier, like client_logo or background. Step 2: Use a POST Request to Update the Image Send a POST request to the workflow endpoint with the following parameters in the body: presentation_id: The ID of your Google Slides presentation. You can find it in the URL of your Google presentation: https://docs.google.com/presentation/d/&lt;this-part&gt;/edit) image_key: The unique identifier you created. image_url: The URL of the new image. That's it! The specified image in your Google Slides presentation will be replaced with the new one from the provided URL. This workflow is designed to be flexible, allowing you to use the same identifier across multiple slides and presentations. I hope it streamlines your slide automation process! Example Curl Request to execute: curl --location 'https://workflow.url' \ --form 'presentation_id=""google-presentation-id""' \ --form 'image_key=""background""' \ --form 'image_url=""https://picsum.photos/536/354""' Happy automating! The n8Ninja ü•∑",3047,2024-04-28 09:14:19.066000+00:00,True,2
2441,create e-mail responses with fastmail and OpenAI,"Workflow Description: This n8n workflow automates the drafting of email replies for Fastmail using OpenAI's GPT-4 model. Here‚Äôs the overall process: Email Monitoring: The workflow continuously monitors a specified IMAP inbox for new, unread emails. Email Data Extraction: When a new email is detected, it extracts relevant details such as the sender, subject, email body, and metadata. AI Response Generation: The extracted email content is sent to OpenAI's GPT-4, which generates a personalized draft response. Get Fastmail Session and Mailbox IDs: Connects to the Fastmail API to retrieve necessary session details and mailbox IDs. Draft Identification: Identifies the ""Drafts"" folder in the mailbox. Draft Preparation: Compiles all the necessary information to create the draft, including the generated response, original email details, and specified recipient. Draft Uploading: Uploads the prepared draft email to the ""Drafts"" folder in the Fastmail mailbox. Prerequisites: IMAP Email Account: You need to configure an IMAP email account in n8n to monitor incoming emails. Fastmail API Credentials: A Fastmail account with JMAP API enabled. You should set up HTTP Header authentication in n8n with your Fastmail API credentials. OpenAI API Key: An API key from OpenAI to access GPT-4. Make sure to configure the OpenAI credentials in n8n. Configuration Steps: Email Trigger (IMAP) Node: Provide your email server settings and credentials to monitor emails. HTTP Request Nodes for Fastmail: Set up HTTP Header authentication in n8n using your Fastmail API credentials. Replace the httpHeaderAuth credential IDs with your configured credential IDs. OpenAI Node: Configure the OpenAI API key in n8n. Replace the openAiApi credential ID with your configured credential ID. By following these steps and setting up the necessary credentials, you can seamlessly automate the creation of email drafts in response to new emails using AI-generated content. This workflow helps improve productivity and ensures timely, personalized communication.",3040,2024-09-28 18:34:51.274000+00:00,True,2
2316,Convert image to PDF using ConvertAPI,"Who is this for? For developers and organizations that need to convert image files to PDF. What problem is this workflow solving? The file format conversion problem. What this workflow does Downloads the JPG file from the web. Converts the JPG file to PDF. Stores the PDF file in the local file system. How to customize this workflow to your needs Open the HTTP Request node. Adjust the URL parameter (all endpoints can be found here). Use your API Token for authentication. Pass the token in the Authorization header as a Bearer token. You can manage your API Tokens in the User panel ‚Üí Authentication. Optionally, additional Body Parameters can be added for the converter.",2967,2024-07-05 09:44:14.161000+00:00,False,1
2295,Export n8n Cloud execution data to CSV,"Overview This template helps n8n cloud plan users execute all executions to a CSV for easy data analysis. Identify what workflows are generating the most executions or could be optimized. How this workflow works Click ""Test Workflow"" to manually execute the workflow Open the ""Convert to CSV"" node to access the binary data of the CSV file Download the CSV file Nodes included: n8n node Convert to File No Operation, do nothing - replace with another Set up steps Import the workflow to your workspace Add your n8n API credential Benefits of Exporting n8n Cloud Executions to CSV Exporting n8n Cloud executions to CSV offers significant advantages for enhancing workflow management and data analysis capabilities. Here are three key benefits: Enhanced Data Analysis: Comprehensive Insights: Exporting execution data allows for in-depth analysis of workflow performance, helping identify bottlenecks and optimize processes. Custom Reporting: CSV files can be easily imported into various data analysis tools (e.g., Excel, Google Sheets, or BI software) to create custom reports and visualizations tailored to specific business needs. Improved Workflow Monitoring: Historical Data Review: Accessing historical execution data enables users to track workflow changes and their impacts over time, facilitating better decision-making. Error Tracking and Debugging: By reviewing execution logs, users can quickly identify and address errors or failures, ensuring smoother and more reliable workflow operations. Regulatory Compliance and Auditing: Audit Trails: Keeping a record of all executions provides a clear audit trail, essential for regulatory compliance and internal audits. Data Retention: Exported data ensures that execution records are preserved according to organizational data retention policies, safeguarding against data loss. By leveraging the capabilities of CSV exports, users can gain valuable insights, streamline workflow management, and ensure robust data handling practices, ultimately driving better performance and efficiency in their n8n Cloud operations.",2942,2024-06-20 12:14:04.215000+00:00,False,0
3930,"Voice-to-Email Response System with Telegram, OpenAI Whisper & Gmail","This workflow gives you the ability to reply to a long email with a voice note, rather than having to type everything out. ChatGPT will format your audio response and create an email draft for you. How it works When a new email arrives in your inbox, the workflow checks if it needs a response, and it it does, it sends a message to you on Telegram via a VoiceEmailer bot. When you reply to that message with an audio message, the second part of this workflow is triggered. It checks if the message is in the right format, transcribes the audio, and creates a draft response that shows up in the same email thread. Set up steps Add your credentials for Gmail and OpenAI Create an Telegram bot following the instructions here. Connect your telegram credentials so the workflow will use your bot. Turn on the workflow, and message the bot from your telegram. Find the Chat ID from the Executions tab of your workflow, and enter it in as a variable.",2927,2025-05-08 04:01:50.996000+00:00,True,6
2146,Automatically archive Gmail emails from Inbox,"Use Case Automatically archive emails in your Gmail inbox from the last day, unless they have been starred. Been using this with my personal and work emails to stick to an Inbox Zero strategy, without having to click or swipe a lot. Setup Add your Gmail creds How to adjust this template Set your own schedule for when to run this. Otherwise, should be good to go. ü§ûüèΩ",2887,2024-02-28 18:15:13.466000+00:00,False,1
4285,Automatically Delete Spam Emails in Gmail on Schedule,"Who is this for? If you hate SPAM emails or don't want to clean them up frequently. What problem is this workflow solving? It automatically deletes SPAM emails for you, so you don't have to. Save a bit of your time. What this workflow does You can specify when to execute the workflow: daily, weekly, or monthly. Setup Select the preferred execution time Configure credentials for Gmail OAuth2 How to customize this workflow to your needs There's no need. It just works!",2864,2025-05-21 14:19:52.816000+00:00,False,1
2213,Traveler Co-Pilot: AI-Powered Telegram for Easy Language and Image Translation,"Introduction The Traveler Co-Pilot empowers you to confidently traverse the world, connecting with ease and breaking language barriers: Engage in conversations with locals Navigate menus at foreign eateries Comprehend road signs effortlessly. Features Seamless Speech-to-Speech Translation** Communicate in any of the 55 supported languages, and witness the bot translate your words into another language in real-time, all through speech. Visual Translation Magic** Capture images containing text, and the bot will work its magic by recognizing and translating the text into the desired language, right before your eyes. Setup Steps Open the Settings node and specify the languages you would like to work with",2861,2024-04-05 00:38:59.832000+00:00,False,5
5664,Bulk Create Shopify Products with Inventory Management from Google Sheets,"Create Products in Shopify from a Google Sheet This workflow creates products in your Shopify store from a google sheet. It also enables inventory tracking and sets the quantity of an inventory item at your store's default location. This is a great way to get test data into test or staging stores to try out apps, update templates or try out new designs. This Automation will only import new products. It will skip existing products if the slug matches an existing product's handle (Shopify's term for a slug). Setup Notes The Google Sheet has the following columns : title - free text description - free text company - free text category - free text status - ACTIVE, DRAFT or ARCHIVE slug - used in the product url, text with no spaces, can also use hyphen. price - sale price of the products compare_at_price - compare at price for products sku - unique code for each product stock_on_hand - quantity of this item available for purchase. Use those labels in the first row of your sheet and N8N will create one object per row with the column names as object fields. Update GraphQL nodes with your Shopify store URL 1) Replace the URL in all GraphQL nodes with the URL for your Shopify store. 2) These GraphQL requests all use the Shopify 2025-04 GraphQL Admin API. This flow is updated to use Airtable at Sync Products between Airtable and Shopify with Inventory Management",2853,2025-07-04 00:44:26.296000+00:00,False,2
2310,Convert web page to PDF using ConvertAPI,"Who is this for? For developers and organizations that need to convert web page to PDF. What problem is this workflow solving? The web page conversion to PDF problem. What this workflow does Converts web page to PDF. Stores the PDF file in the local file system. How to customize this workflow to your needs Open the HTTP Request node. Adjust the URL parameter (all endpoints can be found here). Use your API Token for authentication. Pass the token in the Authorization header as a Bearer token. You can manage your API Tokens in the User panel ‚Üí Authentication. Change the parameter url to the webpage you want to convert to pdf Optionally, additional Body Parameters can be added for the converter.",2842,2024-07-04 09:00:41.915000+00:00,False,1
2293,Sync blog posts from Notion to Webflow,"Who is this for? This template is for everyone who manages their blog entries in Notion and want to have an easy way to transform them to Webflow. What this workflow does This workflow syncs your blog posts saved in a Notion Database once a day to Webflow. Sync Notion properties, rich text and cover image with your collection. Works with most elements: H1, H2, H3, normal text, bold text, italic text, links, quotes, bulleted lists, numbered lists, and images (under 4MB). Set up steps Connect your accounts. Add a ""slug"" field in Notion. Add a ""Sync to Webflow?"" checkbox in Notion. Run a test and map your collection data. Whenever the workflow runs, all the checked posts will be updated in the Webflow collection, whether it's a new post or an existing one.",2827,2024-06-15 14:19:34.317000+00:00,False,4
2445,Send events to Facebook Events Manager using the Meta Conversions API,"This workflows sends events to your Facebook Events Manager using the Meta Conversions API. Why should I use this? Enhance your Meta Ads tracking by taking control of the events you send. This n8n template allows you to: Improve Tracking Quality**: Even if your Facebook Pixel fails, events are still captured through the Meta Conversions API. Send Offline Events**: Record events that happen offline, like personal sales or in-store purchases. Integrate with Your CRM**: Automatically send events to Facebook when a lead converts after a sales call, updating customer details so Meta knows about the conversion. Use Advanced Tracking**: Tracking has become harder in recent years, making campaigns more expensive. By using Advanced Tracking, you help Facebook track events more easily, reducing campaign costs and improving the results. What events can I send? All of them! Purchase InitiateCheckout Lead Subscribe ViewContent ‚Ä¶ And many others. You can even send your own custom events. Who is this template for? Marketing professionals looking to improve the quality of the Meta Ads tracking. How it works You can send any event you want, as long as you have the necessary data (customer email, name, value of the purchase etc.) How to set up All information you need to set up this workflow is already inside the template. Check out my other templates üëâ https://n8n.io/creators/solomon/",2767,2024-09-30 18:50:26.429000+00:00,False,2
2285,Download recently liked songs automatically with Spotify,"Purpose This workflow enables you to listen to your recent favorites in very hight quality offline without sacrificing all of your storage. How it works This workflow automatically creates a playlist in Spotify named ""Downloads"" which periodically gets updated so it always contains only a defined amount of the latest liked songs. This enables only the Downloads playlist to set for automatic downloading and thus free up space on the device. Setup The workflow is ready to go. Just select your Spotify credentials and activate the workflow. In Spotify just enable automatic downloads on the automatically created Downloads folder after the first workflow run. Current limitations This setup currently supports a maximum of 50 songs in the Downloads Playlist. This is due to the paylod limits defined by Spotify encountered in the Get liked songs node. Implementing batching would solve the issue.",2731,2024-06-09 20:54:19.950000+00:00,False,2
2254,Streamlining Document Automation with n8n and JSReport,"Who is this template for? This workflow template is designed for accounting, human resources, and IT project management teams looking to automate the generation of PDF and Word documents. It can be particularly useful for: The accounting department: for generating invoices in PDF format, thus streamlining the invoicing process and payment tracking. The human resources department: for creating employment contracts in PDF, simplifying the administrative management of employees. IT project management teams: for producing Word documents, such as project specifications, to clearly define project requirements and objectives. Example result in mail This PDF and Word document generation workflow offers a practical and efficient solution for automating administrative and document-related tasks, allowing teams to focus on higher-value activities. How it works This workflow currently operates with an n8n form, but you can easily replace this form with a webhook triggered by an external application such as AirTable, SharePoint, DocuWare, etc. Once the configuration information is retrieved, we fill the API request body of JSReport. The body is defined at the time of template creation in JSReport (Example of JSReport usage). Then, in a straightforward manner, we fetch the PDF and send it via email. Here's a brief overview of this n8n workflow template: Link to n8n workflow template presentation To summarize This workflow integrates with an n8n form, but it's flexible to work with various triggering methods like webhooks from other applications such as AirTable, SharePoint, or DocuWare. After configuring the necessary information, it populates the API request body of JSReport, which defines the template in JSReport. Once the template is populated, it retrieves the PDF and sends it via email. In essence, it streamlines the process of generating PDF documents based on user input and distributing them via email. Instructions: Create a JSReport Account: Sign up for a JSReport account to create your PDF template model. Define PDF Template in JSReport: Use JSON data from your system to set up the content of your PDF template in JSReport. Configure HTTP Request in n8n: Use the HTTP Request node in n8n to send a request to JSReport. Set the node's body to the JSON data defining your PDF template. Watch the Video: For detailed setup guidance, watch the setup video. Remember, this template was created in n8n v1.38.2.",2684,2024-05-03 13:52:43.985000+00:00,False,2
2524,Monitor if a page is alive and notify via Twilio SMS if not,"Workflow Purpose This workflow periodically checks a service's availability and sends an SMS notification if the service is down. High-Level Steps Schedule Trigger: The workflow is triggered at a specified interval, such as every minute. HTTP Request: An HTTP request is sent to the URL of the service being monitored. If: The HTTP status code of the response is checked. If the status code is 200 (OK), the workflow ends. If the status code is not 200, indicating a potential issue, an SMS notification is sent using Twilio. Setup Setting up this workflow is relatively straightforward and should only take a few minutes: Create a new n8n workflow. Add the nodes: Schedule Trigger, HTTP Request, If, and Twilio. Configure the nodes: Schedule Trigger: Specify the desired interval. HTTP Request: Enter the URL of the service to be monitored. If: Set the condition to check for a status code other than 200. Twilio: Enter the Twilio account credentials and the phone numbers for sending and receiving the SMS notification. Connect the nodes: Connect the nodes as shown in the workflow diagram. Activate the workflow: Save the workflow and activate it. Additional Notes The workflow can be customized by changing the interval, the URL, the Twilio credentials, and the SMS message. This workflow is a simple example, and more complex workflows can be created to meet specific needs.",2663,2024-11-04 14:33:26.382000+00:00,False,2
2351,2-way sync Notion and Google Calendar,"This workflow syncs multiple Notion databases to your Google Calendar. And it works both ways. What events are supported? Everything except recurring events. All day events, multiple day events, start and end date‚Ä¶ these are all supported. You set them in Notion and they stay in sync with Google. And vice versa. Why doesn‚Äôt it support recurring events? Notion doesn‚Äôt support recurring events yet. So when you create a recurring event in Google, it will only consider the first date, ignoring future occurrences of the event. Can I connect more than one Notion database? Yes. You can have many Notion databases synced to one Google Calendar account. You can see how to do it in the workflow instructions. It is recommended that you create more calendars in your account, so that you can link each calendar to a different database in Notion. But that‚Äôs a choice. What happens if I delete an event or page? Notion page deleted ‚Üí Deletes event in Google Notion date property cleared ‚Üí Deletes event in Google Google event deleted ‚Üí Clears the date property in Notion, but keeps the page, so you don‚Äôt lose your work. Does it update the events? Yes. When you update the event in Google or in Notion it syncs both ways. How can I know what Notion item was linked to an event? Either by the name or by clicking the hyperlink in the event description that says: üëâ View in Notion. When I create a new event in Google, does it add an item to Notion? Yes. When you create an event inside one of your calendars, the item is synced to the corresponding Notion database. Does it sync event descriptions? No. The event description will always be ‚ÄúView in Notion‚Äù. Even if you change it in Google Calendar it will be overwritten when you make a change to the Notion page. üéâ When you buy this template you receive step-by-step instructions on how to set it up. Check out my other templates üëâ https://n8n.io/creators/solomon/",2639,2024-07-19 18:38:58.692000+00:00,False,4
2478,Send a message via a Lark Bot,"What this workflow does This workflow in n8n demonstrates how to send a message in Lark using a Lark bot. It begins with a manual trigger and then retrieves the necessary Lark token via a POST request. The token is used to authenticate and send a message to a specific chat using the Lark API. The input node provides the required app_id, app_secret, chat_id, and message content. After obtaining the token, the message is sent with the Lark API's message/v4/send/ endpoint. Who This Is For This n8n workflow is ideal for organizations, teams, and developers who need to automate message sending within Lark, especially those managing notifications, alerts, or team reminders. It can help users reduce manual messaging tasks by leveraging a Lark bot to deliver messages at specific intervals or based on particular conditions, enhancing team communication and responsiveness. Setup Fill the Input node with your values Exchange the bearer token in the Send Message node with your token Author: Hiroshi",2617,2024-10-22 04:15:35.750000+00:00,False,1
2280,Send a message with an inline embedded image with Gmail,"The built-in Gmail node doesn't yet support embedding images within the body of the email, but you can pull this off using the HTTP node, and this template shows you how. Requirements A Gmail account How it works The workflow downloads an image, converts it into the format that the Gmail API expects (base64), packages it into a multipart MIME email and uses the HTTP node to send it.",2583,2024-06-05 19:46:52.226000+00:00,False,1
2562,Simple Bluesky multi-image post using native Bluesky API,"Who is this workflow for? This workflow is the baseline workflow for anyone who needs to automate the process of posting 1-4 images to Bluesky using the Bluesky API. It is ideal for anyone looking to streamline their social media posting process, saving time and ensuring consistent content delivery. Use Case / Problem Solved Manually posting images and captions on Instagram can be time-consuming, especially for businesses and content creators managing multiple accounts. This workflow automates the process from image preparation to publishing, reducing manual effort and increasing efficiency. What this workflow does Trigger Initialization: The workflow starts with a manual trigger that can be adapted to other triggers (e.g., HTTP webhook or schedule). Set Parameters: The workflow includes a node that sets essential parameters, such as the Bluesky account ID, image URLs, and caption. Prepare Bluesky Session: A node creates the authenticated session data used by the upload and post operations later in the workflow. Publish Media: Nodes retrieve image files from the specified URLS and uploads them as blobs to Bluesky, a necessary pre-requisite for a Bluesky post to have images attached. Post text caption + images: A node does the final call to the Bluesky API, including the text caption and relevant image references. Setup Sign-in to Bluesky and create an App Password Add your username and App Password to the Define Credentials node. Set the caption (text content) of your post in the Set Caption node. Set 1-4 image URLs in the Set Images node Adapt the initial trigger as needed to fit your workflow's requirements (e.g., schedule, webhook). Adapt the caption and images nodes to accept dynamic parameters. Limitations This workflow assumes a minimum of 1 image URL to function. If you want a text-only post, remove the whole embed section from the JSON in last Post to Bluesky node, as well as relevant image attachment nodes. The 300-character limit in Bluesky includes the caption + hashtags + image alt text. Going over 300 will return a Record/description must not be longer than 300 graphemes error.",2577,2024-11-21 13:17:26.961000+00:00,False,2
2221,YouTube Advanced RSS Generator with Telegram Formation,"Overview The [n8n] YouTube Channel Advanced RSS Feeds Generator workflow facilitates the generation of various RSS feed formats for YouTube channels without requiring API access or administrative permissions. It utilizes third-party services to extract data, making it extremely user-friendly and accessible. Key Use Cases and Benefits Content Aggregation**: Easily gather and syndicate content from any public YouTube channel. No API Key Required**: Avoid the complexities and limitations of Google's API. Multiple Formats**: Supports ATOM, JSON, MRSS, Plaintext, Sfeed, and direct YouTube XML feeds. Flexibility**: Input can be a YouTube channel or video URL, ID, or username. Services/APIs Utilized This workflow integrates with: commentpicker.com**: For retrieving YouTube channel IDs. rss-bridge.org**: To generate various RSS formats. Configuration Instructions Start the Workflow: Activate the workflow in your n8n instance. Input Details: Enter the YouTube channel or video URL, ID, or username via the provided form trigger. Run the Workflow: Execute the workflow to receive links to 13 different RSS feeds, including community and video content feeds. Screenshots Additional Notes Customization**: You can modify the RSS feed formats or integrate additional services as needed. Support and Contributions For support, questions, or contributions, please visit the n8n community forum or the GitHub repository. We welcome contributions from the community!",2552,2024-04-08 23:17:15.590000+00:00,False,2
2171,Verify mailing address deliverability of new contacts in HighLevel Using Lob,"This n8n workflow verifies the deliverability of mailing addresses stored in HighLevel by integrating with Lob's address verification service. Who is this for? This template is designed for HighLevel users who need to ensure the accuracy of mailing addresses stored in their CRM systems. What problem is this workflow solving? / Use Case This workflow addresses the challenge of maintaining accurate mailing addresses in CRM databases by verifying the deliverability of addresses. What this workflow does A new contact is created in HighLevel Webhook sent to n8n Verify if the address is deliverable via LOB Report back to HighLevel Set Up Steps Watch this setup video: https://www.youtube.com/watch?v=T7Baopubc-0 Takes 10-30 minutes to set up Accounts Needed: HighLevel LOB Account (https://www.lob.com $0.00/mo 300 US addresses Verifications) n8n Before using this template, ensure you have API keys for your HighLevel app and Lob. Set up authentication for both services within n8n. How to customize this workflow to your needs You can customize this workflow by adjusting the trigger settings to match HighLevel's workflow configuration. Additionally, you can modify the actions taken based on the deliverability outcome, such as updating custom fields or sending notifications.",2525,2024-03-13 20:55:44.951000+00:00,False,2
2276,Send HTTP Requests to a list of URLs,"How it works This workflow reads a list of URLs every 15 minutes, and sends an HTTP request to every URL on the list. Set up steps Schedule the workflow to run at your desired frequency (default is every 15 minutes). Add your desired URLs to the list. The list should be in the same format as the image below (Don't forget to have single quotes around every URL in the list, and separate each one with a comma!): Turn the workflow ON. Ideas to customize the workflow for your own use cases: Change the HTTP method Add headers Add a request body",2523,2024-06-02 03:24:01.880000+00:00,False,1
2583,Optimize & Update Printify Title and Description Workflow,"Printify Automation - Update Title and Description Workflow This n8n workflow automates the process of retrieving products from Printify, generating optimized product titles and descriptions, and updating them back to the platform. It leverages OpenAI for content generation and integrates with Google Sheets for tracking and managing updates. Features Integration with Printify**: Fetch shops and products through Printify's API. AI-Powered Optimization**: Generate engaging product titles and descriptions using OpenAI's GPT model. Google Sheets Tracking**: Log and manage updates in Google Sheets. Custom Brand Guidelines**: Ensure consistent tone by incorporating brand-specific instructions. Loop Processing**: Iteratively process each product in batches. Workflow Structure Nodes Overview Manual Trigger: Manually start the workflow for testing purposes. Printify - Get Shops: Retrieves the list of shops from Printify. Printify - Get Products: Fetches product details for each shop. Split Out: Breaks down the product list into individual items for processing. Loop Over Items: Iteratively processes products in manageable batches. Generate Title and Desc: Uses OpenAI GPT to create optimized product titles and descriptions. Google Sheets Integration: Trigger: Monitors Google Sheets for changes. Log Updates: Records product updates, including old and new titles/descriptions. Conditional Logic: If Nodes: Ensure products are ready for updates and stop processing once completed. Printify - Update Product: Sends updated titles and descriptions back to Printify. Brand Guidelines + Custom Instructions: Sets brand tone and seasonal instructions. Setup Instructions Prerequisites n8n Instance: Ensure n8n is installed and configured. Printify API Key: Obtain an API key from your Printify account. Add it to n8n under HTTP Header Auth. OpenAI API Key: Obtain an API key from OpenAI. Add it to n8n under OpenAI API. Google Sheets Integration: Share your Google Sheets with the Google API service account. Configure Google Sheets credentials in n8n. Workflow Configuration Set Brand Guidelines: Update the Brand Guidelines + Custom Instructions node with your brand name, tone, and seasonal instructions. Batch Size: Configure the Loop Over Items node for optimal batch sizes. Google Sheets Configuration: Set the correct Google Sheets document and sheet names in the integration nodes. Run the Workflow: Start manually or configure the workflow to trigger automatically. Key Notes Customization**: Modify API calls to support other platforms like Printful or Vistaprint. Scalability**: Use batch processing for efficient handling of large product catalogs. Error Handling**: Configure retries or logging for any failed nodes. Output Examples Optimized Content Example Input Title**: ""Classic White T-Shirt"" Generated Title**: ""Stylish Classic White Tee for Everyday Wear"" Input Description**: ""Plain white T-shirt made of cotton."" Generated Description**: ""Discover comfort and style with our classic white tee, crafted from premium cotton for all-day wear. Perfect for casual outings or layering."" Next Steps Monitor Updates: Use Google Sheets to review logs of updated products. Expand Integration: Add support for more Printify shops or integrate with other platforms. Enhance AI Prompts: Customize prompts for different product categories or seasonal needs. Feel free to reach out for additional guidance or troubleshooting!",2503,2024-11-28 00:40:31.475000+00:00,True,6
2118,Send follow-ups using Gmail to Hubspot contacts,"Use Case Following up at the right time is one of the most important parts of sales. This workflow uses Gmail to send outreach emails to Hubspot contacts that have already been contacted only once more than a month ago, and records the engagement in Hubspot. Setup Setup HubSpot Oauth2 creds (Be careful with scopes. They have to be exact, not less or more. Yes, it‚Äôs not simple, but it‚Äôs well documented in the n8n docs. Be smarter than me, read the docs) Setup Gmail creds. Change the email variables in the Set keys node How to adjust this template There's plenty to do here because the approach here is really just a starting point. Most important here is to figure out what your rules are to follow up. After a month? More than once? Also, remember to update the follow-up email! Unless you want to sell n8n üòâ",2455,2024-02-22 12:29:34.962000+00:00,False,3
2488,Automated Notion Task Reminders via Slack,"This workflow is designed to automate task reminders by retrieving tasks from a Notion database and sending reminders to Slack users. It checks for incomplete tasks from a Notion database and sends a Slack message to the relevant users with the task details and due dates. The automation is scheduled to run every weekday at 9:00 AM, ensuring that users are always reminded of pending tasks. Who is this for? This workflow is ideal for teams or individuals who manage their tasks using Notion but rely on Slack for communication. It provides an automated solution for ensuring that tasks in Notion are followed up on, reducing the risk of missing deadlines. What problem is this workflow solving? Often, team members need to be reminded of tasks from various platforms. This workflow bridges the gap between task management in Notion and communication in Slack by automatically sending task reminders. It ensures that team members are informed of their pending tasks each morning, helping them stay organized and on top of their work. What this workflow does Triggers every weekday at 9:00 AM: The workflow runs at 9:00 AM, Monday through Friday. Fetches tasks from Notion: It retrieves tasks from a Notion database. Filters incomplete tasks: The workflow filters tasks that are not marked as ""Done."" Fetches Slack users: It retrieves all Slack users to ensure that the reminders are sent to the correct user. Matches tasks to the correct user: It checks the Notion task assignee and matches it with the appropriate Slack user. Sends Slack reminders: Sends a Slack direct message to each user with their incomplete tasks and due dates. Setup Connect Notion: You will need to connect your Notion account and specify the database containing tasks. Connect Slack: Authenticate with Slack using OAuth to allow the workflow to send messages on your behalf. Notion user email mapping: Ensure that the Notion users‚Äô email addresses are correctly mapped to their corresponding Notion user profiles. Slack user full name mapping: Ensure that the Slack users‚Äô full names are correctly mapped to their corresponding Slack user profiles. Adjust schedule: If needed, modify the schedule node to run at a different time or frequency. How to customize this workflow Change the database**: You can adjust the workflow to pull tasks from a different Notion database by modifying the ""Get To Dos from Tasks Database"" node. Add more users**: The workflow currently supports two users, but you can expand it to support more by adding additional logic in the ""Switch for Notion Users Emails"" node. Modify the message format**: The Slack message content can be customized further to include more task details or change the message format. Workflow Summary This workflow automates sending task reminders from a Notion database to Slack users. By running every weekday morning, it ensures that users receive timely reminders of their incomplete tasks, helping them stay organized and efficient.",2453,2024-10-23 15:12:58.135000+00:00,False,2
2506,Import workflows and map their credentials using a Multi-Form,"Purpose This workflow allows you to import any workflow from a file or another n8n instance and map the credentials easily. How it works A multi-form setup guides you through the entire process At the beginning you have two options: Upload a workflow file (JSON) Copy workflow from a remote n8n instance If you choose the second option, you get to choose one of your predefined (in the Settings node) remote instances first, then it retrieves a list of all the workflows using the n8n API which you then can choose a workflow from. Now both initial options come together - the workflow file is being processed In parallel all credentials of the current instance are being retrieved using the Execute Command node The next form page enables a mapping of all the credentials used in the workflow. The matching happens between the names (because one workflow can contain different credentials of the same type) of the original credentials and the ones available on the current instance. Every option then shows all available credentials of the same type. In addition the user has always the choice to create a new credential on the fly. For every option which was set to create a new credential, an empty credential is being created on the current instance using the n8n API. An emoji is being appended to the name, which indicates that it needs to be populated. Finally the workflow gets updated with the new credential ID‚Äôs and created on the current instance using the n8n API. Then the user gets a message, if the process has succeeded or not. Setup Select your credentials in the nodes which require those Configure your remote instance(s) in the Settings node. (You can skip this step, if you only want to use the File Upload feature) Every instance is defined as object with the keys name, apiKey and baseUrl. Those instances are then wrapped inside an array. You can find an example described within a note on the workflow canvas. How to use Grab the (production) URL of the Form from the first node Open the URL and follow the instructions given in the multi-form Disclaimer Security: Beware, that all credentials are being decrypted and processed within the workflow. Also the API keys to other n8n instances are stored within the workflow. This solution is primarily meant for transferring data between testing environments. For production use consider the n8n enterprise edition which provides a reliable way to deploy workflows between different environments without the need of manual credential mapping.",2449,2024-10-29 19:05:29.604000+00:00,False,2
2511,Qualys Vulnerability Trigger Scan SubWorkflow,"This workflow is triggered by a parent workflow initiated via a Slack shortcut. Upon activation, it collects input from a modal window in Slack and initiates a vulnerability scan using the Qualys API. Key Features Trigger:** Launched by a parent workflow through a Slack shortcut with modal input. API Integration:** Utilizes the Qualys API for vulnerability scanning. Data Conversion:** Converts XML scan results to JSON for further processing. Loop Mechanism:** Continuously checks the scan status until completion. Slack Notifications:** Posts scan summary and detailed results to a specified Slack channel. Workflow Nodes Start VM Scan in Qualys: Initiates the scan with specified parameters. Convert XML to JSON: Converts the scan results from XML format to JSON. Fetch Scan Results: Retrieves scan results from Qualys. Check if Scan Finished: Verifies whether the scan is complete. Loop Mechanism: Handles the repetitive checking of the scan status. Slack Notifications: Posts updates and results to Slack. Relevant Links Qualys API Documentation Qualys Platform Documentation Parent workflow link Link to Report Generator Subworkflow",2444,2024-10-31 00:18:38.836000+00:00,False,2
2554,MongoDB AI Agent - Intelligent Movie Recommendations,"Who is this for? This workflow is designed for: Database administrators and developers working with MongoDB Content managers handling movie databases Organizations looking to implement AI-powered search and recommendation systems Developers interested in combining LangChain, OpenAI, and MongoDB capabilities What problem does this workflow solve? Traditional database queries can be complex and require specific MongoDB syntax knowledge. This workflow addresses: The complexity of writing MongoDB aggregation pipelines The need for natural language interaction with movie databases The challenge of maintaining user preferences and favorites The gap between AI language models and database operations What this workflow does This workflow creates an intelligent agent that: Accepts natural language queries about movies Translates user requests into MongoDB aggregation pipelines Queries a movie database containing detailed information including: Plot summaries Genre classifications Cast and director information Runtime and release dates Ratings and awards Provides contextual responses using OpenAI's language model Allows users to save favorite movies to the database Maintains conversation context using a window buffer memory Setup Required Credentials: OpenAI API credentials MongoDB connection details Node Configuration: Configure the MongoDB connection in the MongoDBAggregate node Set up the OpenAI Chat Model with your API key Ensure the webhook trigger is properly configured for receiving chat messages Database Requirements: A MongoDB collection named ""movies"" with the specified document structure Proper indexes for efficient querying Appropriate user permissions for read/write operations How to customize this workflow Modify the Document Structure: Update the tool description in the MongoDBAggregate node to match your collection schema Adjust the aggregation pipeline templates for your specific use case Enhance the AI Agent: Customize the prompt in the ""AI Agent - Movie Recommendation"" node Modify the window buffer memory size based on your context needs Add additional tools for more functionality Extend Functionality: Add more MongoDB operations beyond aggregation Implement additional workflows for different types of queries Create custom error handling and validation Add user authentication and rate limiting Integration Options: Connect to external APIs for additional movie data Add webhook endpoints for different platforms Implement caching mechanisms for frequent queries Add data transformation nodes for specific output formats This workflow serves as a foundation that can be adapted to various use cases beyond movie recommendations, such as e-commerce product search, content management systems, or any scenario requiring intelligent database interaction.",2428,2024-11-17 12:54:00.225000+00:00,True,4
2369,"Parse DMARC reports, save them in database and notify on DKIM or SPF error","Who is it for If you are a postmaster or you manage email server, you can set up DKIM and SPF records to ensure that spoofing your email address is hard. On your domain you can also set up DMARC record to receive XML reports from email providers (rua tag). Those reports contain data if email they received passed DKIM and SPF verifications. Since DMARC email is public, you will receive a lot of emails from email providers, not only if DKIM/SPF fail. There is no need for it - you probably only need to know if SPF/DKIM failed. So this script is intended to automatically parse all DMARC reports that come from email providers, but ONLY send you notification if SPF or DKIM failed - meaning that either someone tries to spoof your email or your DKIM/SPF is improperly set up. How it works script monitors postmaster email for DMARC reprots (rua) unpacks report and parses XML into JSON maps JSON and formats fields for MySQL/MariaDB input inputs into database sends notification on DKIM or SPF failure Remember to set up email input mailbox notification channels for slack for email",2424,2024-07-29 08:57:53.096000+00:00,False,4
4370,"Auto Meeting Summarizer with Google Drive, OpenAI Whisper & GPT-4 to Sheets","üé§ Audio-to-Insights: Auto Meeting Summarizer Transform your meeting recordings into actionable insights automatically. This powerful n8n workflow monitors your Google Drive for new audio files, transcribes them using OpenAI's Whisper, generates intelligent summaries with ChatGPT, and logs everything in Google Sheets - all without lifting a finger. üîÑ How It Works This workflow operates as a seamless 6-step automation pipeline: Step 1: Smart Detection The workflow continuously monitors a designated Google Drive folder (polls every minute) for newly uploaded audio files. Step 2: Secure Download When a new audio file is detected, the system automatically downloads it from Google Drive for processing. Step 3: AI Transcription OpenAI's Whisper technology converts your audio recording into accurate text transcription, supporting multiple audio formats. Step 4: Intelligent Summarization ChatGPT processes the transcript using a specialized prompt that extracts: Key discussion points and decisions Action items with assigned persons and deadlines Priority levels and follow-up tasks Clean, professional formatting Step 5: Timestamp Generation The system automatically adds the current date and formats it consistently for tracking purposes. Step 6: Automated Logging The final summary is appended to your Google Sheets document with the date, creating a searchable archive of all meeting insights. ‚öôÔ∏è Setup Steps Prerequisites Before setting up the workflow, ensure you have: Active Google Drive account OpenAI API key with credits Google Sheets access n8n instance (cloud or self-hosted) Configuration Steps 1. Credential Setup Google Drive OAuth2**: Required for folder monitoring and file downloads OpenAI API Key**: Needed for both transcription (Whisper) and summarization (ChatGPT) Google Sheets OAuth2**: Essential for writing summaries to your spreadsheet 2. Google Drive Configuration Create a dedicated folder in Google Drive for meeting recordings Copy the folder ID from the URL (the long string after /folders/) Update the folderToWatch parameter in the workflow 3. Google Sheets Preparation Create a new Google Sheet or use an existing one Ensure it has columns: Date and Meeting Summary Copy the spreadsheet ID from the URL Update the documentId parameter in the workflow 4. Audio Requirements Supported Formats**: MP3, WAV, M4A, MP4 Recommended Size**: Under 100MB for optimal processing Language**: Optimized for English (customizable for other languages) Quality**: Clear audio produces better transcriptions 5. Workflow Activation Import the workflow JSON into your n8n instance Configure all credential connections Test with a sample audio file Activate the workflow trigger üöÄ Use Cases Project Management Team Standup Summaries**: Convert daily standups into actionable task lists Sprint Retrospectives**: Extract improvement points and action items Stakeholder Updates**: Generate concise reports for leadership Sales & Customer Success Discovery Call Notes**: Capture prospect pain points and requirements Demo Follow-ups**: Track questions, objections, and next steps Customer Check-ins**: Monitor satisfaction and expansion opportunities Consulting & Professional Services Client Strategy Sessions**: Document recommendations and implementation plans Requirements Gathering**: Organize complex project specifications Progress Reviews**: Track deliverables and milestone achievements HR & Training Interview Debriefs**: Standardize candidate evaluation notes Training Sessions**: Create searchable knowledge bases Performance Reviews**: Document development plans and goals Research & Development Brainstorming Sessions**: Capture innovative ideas and concepts Technical Reviews**: Log decisions and architectural choices User Research**: Organize feedback and insights systematically üí° Advanced Customization Options Enhanced Summarization Modify the ChatGPT prompt to focus on specific elements: Add speaker identification for multi-person meetings Include sentiment analysis for customer calls Generate department-specific summaries (technical, sales, legal) Extract financial figures and metrics automatically Integration Expansions Slack Integration**: Auto-post summaries to relevant channels Email Notifications**: Send summaries to meeting participants CRM Updates**: Push action items directly to Salesforce/HubSpot Calendar Integration**: Schedule follow-up meetings based on action items Quality Improvements Audio Preprocessing**: Add noise reduction before transcription Multi-language Support**: Configure for international teams Custom Templates**: Create industry-specific summary formats Approval Workflows**: Add human review before final storage üõ†Ô∏è Troubleshooting & Best Practices Common Issues Large File Processing**: Split recordings over 100MB into smaller segments Poor Audio Quality**: Use noise reduction tools before uploading API Rate Limits**: Implement delay nodes for high-volume usage Formatting Issues**: Adjust ChatGPT prompts for consistent output Optimization Tips Upload files in supported formats only Ensure stable internet connection for cloud processing Monitor OpenAI API usage and costs Regularly backup your Google Sheets data Test workflow changes with sample files first üìä Expected Outputs Sample Summary Format: Meeting Summary - March 15, 2024 Key Discussion Points: Q1 budget review and allocation decisions New product launch timeline and milestones Team restructuring and role assignments Action Items: John: Finalize budget proposal by March 20th (High Priority) Sarah: Schedule product demo sessions for March 25th Team: Submit org chart feedback by March 18th Decisions Made: Approved additional marketing budget of $50K Delayed product launch to April 15th for quality assurance Promoted Lisa to Senior Developer role üìû Questions & Support For any questions, customizations, or technical support regarding this workflow: üìß Email Support Primary Contact**: Yaron@nofluff.online Response Time**: Within 24 hours on business days Best For**: Setup questions, customization requests, troubleshooting üé• Learning Resources YouTube Channel**: https://www.youtube.com/@YaronBeen/videos Step-by-step setup tutorials Advanced customization guides Workflow optimization tips üîó Professional Network LinkedIn**: https://www.linkedin.com/in/yaronbeen/ Connect for ongoing support Share your workflow success stories Get updates on new automation ideas üí° What to Include in Your Support Request Describe your specific use case Share any error messages or logs Mention your n8n version and setup type Include sample audio file characteristics (if relevant) Ready to transform your meeting chaos into organized insights? Download the workflow and start automating your meeting summaries today!",2408,2025-05-25 05:37:57.429000+00:00,True,3
2585,Upload images to an S3 Bucket via a Slack Bot,"Upload Public-Facing Images to an S3 Cloudflare Bucket via Slack Modal üõ† Who is this for? This workflow is for teams that use Slack for internal communication and need a streamlined way to upload public-facing images to an S3 Cloudflare bucket. It's especially beneficial for DevOps, marketing, or content management teams who frequently share assets and require efficient cloud storage integration. üí° What problem does this workflow solve? Manually uploading images to cloud storage can be time-consuming and disruptive, especially if you're already working in Slack. This workflow automates the process, allowing you to upload images directly from Slack via a modal popup. It reduces friction and keeps your workflow within a single platform. üîç What does this workflow do? This workflow connects Slack with an S3 Cloudflare bucket to simplify the image-uploading process: Slack Modal Interaction**: Users trigger a Slack modal to select images for upload. Dynamic Folder Management**: Choose to create a new folder or use an existing one for uploads. S3 Integration**: Automatically uploads the images to a specified S3 Cloudflare bucket. Slack Confirmation**: After upload, Slack sends a confirmation with the uploaded file URLs. üöÄ Setup Instructions Prerequisites Slack Bot with the following permissions: commands files:write files:read chat:write Cloudflare S3 Credentials: Create an API token with write access to your S3 bucket. n8n Instance: Ensure n8n is properly set up with webhook capabilities. Steps Configure Slack Bot: Set up a Slack app and enable the Events API. Add your n8n webhook URL to the Events Subscription section. Add Credentials: Add your Slack API and S3 Cloudflare credentials to n8n. Customize the Workflow: Open the Idea Selector Modal node and update folder options to suit your needs. Update the Post Image to Channel node with your Slack channel ID. Deploy the Workflow: Activate the workflow and test by triggering the Slack modal. üõ† How to Customize This Workflow Adjust the Slack Modal You can modify the modal layout in the Idea Selector Modal node to add additional fields or adjust the styling. Change the Bucket Structure Update the Upload to S3 Bucket node to customize the folder paths or change naming conventions. üîó References and Helpful Links Slack API Documentation Cloudflare S3 Setup n8n Documentation üìì Workflow Notes Key Features: Slack Integration**: Uses Slack modal interactions to streamline the upload process. Cloud Storage**: Automatically uploads to a Cloudflare S3 bucket. User Feedback**: Sends a Slack message with file URLs upon successful upload. Setup Dependencies: Slack API token Cloudflare S3 credentials n8n webhook configuration Sticky Notes Included Sticky notes are embedded within the workflow to guide you through configuration and explain node functionality. üåü Why Use This Workflow? This workflow keeps your image-uploading process intuitive, efficient, and fully integrated with tools you already use. By leveraging n8n's flexibility, you can ensure smooth collaboration and quick sharing of public-facing assets without switching contexts.",2407,2024-11-29 06:14:01.540000+00:00,False,3
2192,"Streamline Your Zoom Meetings with Secure, Automated Stripe Payments","üéâ Do you want to master AI automation, so you can save time and build cool stuff? I‚Äôve created a welcoming Skool community for non-technical yet resourceful learners. üëâüèª Join the AI Atelier üëàüèª Unlock streamlined Zoom Meeting organization and exclusive access management with this n8n workflow. Designed for educators, event organizers, and businesses, this tool automates your event logistics, so you can focus on delivering valuable content. Features Zoom Meetings Creation:** Instantly generate new Zoom meetings with the n8n built-in form. Collect Payments Using Stripe:** Effortlessly monetize your events with secure, automatically created Stripe payment pages for each meeting. Exclusive Gated Access:** Ensure your content remains exclusive by sending Zoom meeting passwords only to verified subscribers who have completed their payment through Stripe. Participants Email Notifications:** Automate the distribution of Zoom meeting details post-payment, eliminating the need for manual email management and ensuring participants are promptly informed. Instant and Easy Participants Overview:** Manage and track your event registrations with ease. All related data is stored in a Google Sheets document that you own. You're notified via email with each new subscription, simplifying participant management. Set Up Steps Connect your Zoom, Stripe, Gmail and Google Sheet credentials. Create an empty Google Sheet in your Google Drive. Fill the config node (Sheet URL, email and currency). Edit email text. This n8n workflow template is designed to minimize setup time and maximize efficiency, allowing you to focus on delivering value to your subscribers. With just a few clicks, you can automate the entire process of organizing and monetizing your Zoom meetings. Created by the n8ninja.",2376,2024-03-25 10:09:05.315000+00:00,True,4
2414,Extract spending history from gmail to google sheet,"How it works Fetch transaction notification emails (including attachments) Clean up data Let AI (Basic LLM Chain node) generate bookkeeping item Send to Google sheet Details The example fetch email from Gmail lables, suggested using filters to automatically orgianize email into the labels Data will send to ""raw data"" sheet Example google sheet: https://docs.google.com/spreadsheets/d/1_IhdHj8bxtsfH2MRqKuU2LzJuzm4DaeKSw46eFcyYts/edit?gid=1617968863#gid=1617968863",2375,2024-09-15 07:38:34.216000+00:00,True,6
4414,Automate News Publishing to LinkedIn with Gemini AI and RSS Feeds,"üì∞ LinkedIn News Auto-Publisher Overview üìã This project is an automated news publisher for LinkedIn. It uses RSS feeds to fetch news, processes the content with the Gemini API to generate precise summaries, and automatically publishes to LinkedIn via its API. How It Works Architecture and Workflow ‚öôÔ∏è n8n**: Efficient orchestration of workflow with automation. RSS**: News sources such as TechCrunch and MIT Technology Review. Gemini API**: Dynamic generation of content and precise summaries. LinkedIn API**: Automatic publication on profiles and corporate pages. Content Processing üß† Fetching news through RSS feeds. Processing and generating summaries with the Gemini API. Automatic publication on LinkedIn. Key Benefits ‚úÖ Complete automation of the news publishing process. Dynamic generation of precise and relevant content. Integration with reliable news sources and publication on a professional platform. Use Cases üíº Automation of news publishing for businesses and professionals. Keeping corporate profiles and pages updated with relevant content. Saving time in managing content on social networks. Requirements üë®‚Äçüíª n8n instance (self-hosted or cloud). Gemini API credentials. LinkedIn bot setup and API credentials. Configured RSS feeds to fetch news. Authors üë• Joel Choez Alan Baja√±a Jaren Pazmi√±o David Sandoval Members of CIAP",2373,2025-05-27 04:13:35.056000+00:00,True,6
4029,"Create & Approve POV Videos with AI, ElevenLabs & Multi-Posting (TikTok/IG/YT)","POV Video Creator: Automating TikTok-Style Instagram Video Automation, Approval, and Multi-Platform Posting Using AI, ElevenLabs, Google Sheets, and Social Media APIs Description What Problem Does This Solve? üé• This workflow automates the creation, rendering, approval, and posting of TikTok-style POV (Point of View) videos to Instagram, with cross-posting to Facebook and YouTube. It eliminates manual video production, approval delays, and inconsistent posting schedules. It ensures high-quality content creation and distribution for social media managers and content creators Target audience: Social media managers, content creators, small to medium-sized businesses, and n8n users familiar with AI tools, Google Sheets, and social media APIs What Does It Do? üåü Generates daily POV video ideas using OpenAI Creates images, videos, and audio with PIAPI.ai and ElevenLabs Renders final videos with Creatomate Manages approvals via email and Google Sheets Posts approved videos to Instagram, Facebook, and YouTube Tracks progress in a Google Sheet for transparency Key Features AI-driven idea generation and script creation Automated media production with image, video, and audio synthesis Email-based approval system for quality control Cross-platform posting to Instagram, Facebook, and YouTube Real-time tracking in Google Sheets and Google Drive Error handling for rendering and posting failures Setup Instructions Prerequisites n8n Instance**: Self-hosted or cloud n8n instance API Credentials**: OpenAI API: API key for idea generation, stored in n8n credentials PIAPI.ai API: API key for image and video generation, stored in n8n credentials ElevenLabs API: API key for audio generation, stored in n8n credentials Creatomate API: API key for video rendering, stored in n8n credentials Google Sheets/Drive API: OAuth2 credentials from Google Cloud Console with Sheets and Drive scopes Gmail API: OAuth2 credentials from Google Cloud Console with Gmail scope Instagram Graph API: User Access Token with instagram_content_publish permission from a Facebook App Facebook Graph API: Access Token from the same Facebook App YouTube API: OAuth2 credentials for YouTube uploads Google Sheet**: A sheet named ""POV Videos"" with a tab ""Instagram"" and columns: Timestamp, ID, Subject, Topic, Caption, POV_Status, Prompt, Publish_Status, Link, Final Video, Approval, row_number Creatomate Template**: A pre-configured template with video, audio, and text elements Installation Steps Import the Workflow: Copy the workflow JSON from the ‚ÄúTemplate Code‚Äù section (to be provided) Import it into n8n via ‚ÄúImport from File‚Äù or ‚ÄúImport from URL‚Äù Configure Credentials: Add API credentials in n8n‚Äôs Credentials section for OpenAI, PIAPI.ai, ElevenLabs, Creatomate, Google Sheets/Drive, Gmail, Instagram Graph, Facebook Graph, and YouTube Assign credentials to respective nodes. For example: In ""Text-to-Image"", use PIAPI.ai credentials: {{ $credentials.PIAPI }} In ""Render with Creatomate"", use Creatomate credentials: {{ $credentials.Creatomate }} In ""Send Approval Request"", use Gmail credentials Set Up Nodes: Schedule Trigger: Configure to run daily Approval Email (Send Approval Request): Customize the HTML email template with approval/rejection links Post to Social Media Nodes (Instagram Container, Facebook Posts, Post YouTube): Configure with your Instagram Business Account ID, Facebook Page ID, and YouTube channel details Configure Google Sheet and Drive: Create ""POV Videos"" Google Sheet with ""Instagram"" tab and specified columns Share the sheet with your Google Sheets credential email Create ""Audio"" and ""Video"" folders in Google Drive, noting their IDs Test the Workflow: Run manually to verify idea generation, media creation, and posting Check email notifications, Google Sheet updates, and social media posts Schedule the Workflow: Enable ""Schedule Trigger"" and ""Schedule Trigger1"" for daily runs Enable ""Get Latest Approved Video"" to poll at 7 PM daily How It Works High-Level Steps Generate Video Ideas: Creates daily POV video concepts with OpenAI Create Media: Produces images, videos, and audio using AI tools Render Video: Combines media into a final video with Creatomate Manage Approvals: Sends approval emails and processes decisions Post to Platforms: Publishes approved videos to Instagram, Facebook, and YouTube Detailed Descriptions Detailed node descriptions are available in the sticky notes within the workflow (to be provided). Below is a summary of key actions Node Names and Actions Video Idea Generation and Script Creation Schedule Trigger**: Initiates daily workflow Get Title**: Fetches pending video ideas from Google Sheet Generate Topics**: Uses OpenAI to create a new video idea Format Row**: Structures the idea into a Google Sheet row Insert new Prompt, Caption and Title/Topic**: Adds the idea to Google Sheet Generate Ideas**: Produces 3 POV sequences Generate Script**: Expands a sequence into a detailed script Set Topics**: Stores the script for media creation Media Creation Text-to-Image**: Generates an image with PIAPI.ai Get Image**: Retrieves the generated image Generate Video Prompt**: Creates a video prompt from the image Generate Video**: Produces a 5-second video with PIAPI.ai Access Videos**: Retrieves the video URL Store Video**: Updates Google Sheet with video URL Generate Sound Prompt**: Creates an audio prompt Text-to-Sound**: Generates a 20-second audio clip with ElevenLabs Store Sound**: Uploads audio to Google Drive Allow Access**: Sets audio file permissions Video Rendering Merge**: Combines script, video, and audio data List Elements**: Formats data for Creatomate Render with Creatomate**: Renders the final video Check Video Status**: Routes based on render success/failure Storage and Notification Google Drive**: Uploads the rendered video New Render Video Alert**: Sends success email Failed Render**: Sends failure email Render Video Link**: Updates Google Sheet with final video URL Approval Process Approval Email**: Sends approval request email Handle Approval/Rejection1**: Processes approval/rejection via webhook Video Update1**: Updates Google Sheet with approval status Social Media Posting Get Latest Approved Video**: Polls for approved videos Check Approval**: Routes based on approval status Instagram Container**: Creates Instagram media container Post to Instagram**: Publishes to Instagram Facebook Posts**: Posts to Facebook Download Video**: Downloads video for YouTube Post YouTube**: Uploads to YouTube Mark Rejected**: Updates status for rejected videos Update Google Sheet**: Updates publish status Customization Tips Expand Platforms**: Add nodes to post to other platforms Modify Approval Email**: Update the Send Approval Request node to customize the HTML template Alternative Notifications**: Add nodes for Slack or Telegram alerts Adjust Video Duration**: Modify Generate Video node to change duration (default: 5 seconds)",2371,2025-05-14 00:06:42.535000+00:00,True,12
4288,Convert RSS News to AI Avatar Videos with Heygen & GPT-4o,"üé¨ Automated News-to-Video Workflow (n8n + Heygen + GPT-4o) üìÑ Overview: This n8n workflow turns news from an RSS feed (e.g., CNN) into short, AI-generated avatar videos using Heygen. It: Fetches news from an RSS feed. Logs headlines to Google Sheets. Uses GPT-4o or Google Gemini to generate a 30‚Äì60 sec script. Sends the script to Heygen to create an avatar video. Monitors and retrieves the final video. Logs video metadata (title, link, etc.) to Google Sheets. üéØ Ideal for content creators, marketers, or media pages repurposing written news into video content at scale. ‚öôÔ∏è Setup Guide (No Sensitive Info) üîë 1. Heygen API Paid Heygen plan required. Add your API key in the Setup Heygen node: ""heygen_api_key"": ""your_key_here"" Optional: Set ""avatar_id"" and ""voice_id"" as desired. üí° 2. AI Model: GPT-4o or Gemini GPT-4o**: Use OpenAI‚Äôs node or HTTP request with your API key. Gemini**: Link your Google Cloud project and connect the Gemini node using OAuth2 credentials. üì• 3. RSS Feed Add an RSS node (e.g., CNN). Extract title, link, and content. üìä 4. Google Sheets + Drive Connect via OAuth2: ""Google Sheets account 2"" ""Google Drive account 2"" Replace sheet IDs in: Log news to sheets Log video URL and title to sheets üìπ 5. Create Video (Heygen) Send a POST request to Heygen's API using the generated script, avatar, and voice ID. ‚è≥ 6. Monitor Status Poll the status endpoint until video is ready. Capture the download link. üßæ 7. Log Final Output Save video metadata to a Google Sheet for publishing or archiving. Set up video: Link in Workflow",2359,2025-05-21 17:52:54.270000+00:00,True,6
4088,Extract and Merge Twitter (X) Threads using TwitterAPI.io,"Twitter (X) Thread Fetcher: Extract and Merge Tweets from Threads What it does Thread Detection:** Automatically detects whether the provided Twitter link is a single tweet or a thread. Tweet Extraction:** Fetches and returns the content of a single tweet, or gathers all tweets in a thread by retrieving the first tweet and all connected replies. Thread Merging:** Merges all tweets in the correct order to reconstruct the complete thread, filtering out any empty results. Seamless Integration:** Easily trigger this workflow from other n8n workflows to automate Twitter thread extraction from various sources. How it works Accepts a Twitter link as input-either a single tweet or a thread. If the link is for a single tweet, fetches and returns the tweet content. If the link is for a thread, fetches the first tweet, then iteratively retrieves all connected replies that form the thread, ensuring only relevant tweets are included. Merges the first tweet and all subsequent thread tweets in order, filters out any empty results, and returns the complete thread. Uses twitterapi.io for all Twitter API requests. Set up steps Setup typically takes just a few minutes. You‚Äôll need to configure your Twitter API credentials for twitterapi.io. You can trigger this workflow manually for testing or call it from another workflow to automate thread fetching from sources like Notion, spreadsheets, or other platforms. For best results, create a separate workflow to gather Twitter links from your preferred source, then trigger this workflow to fetch and return the full thread. &gt; Detailed configuration instructions and node explanations are included as sticky notes within the workflow canvas. Benefits Light speed:* Fetches a 15-tweet thread in just *3 seconds** for rapid results. Cost effective:* Processes a *15-tweet thread* for only *$0.0027*, making it highly affordable. *(Cost may vary depending on the density of replies in the thread.)",2346,2025-05-15 22:37:55.997000+00:00,False,2
4030,"AI Newsletter Builder: Crawl Sites with Dumpling AI, Summarize with GPT-4o","Who is this for? This workflow is built for newsletter writers, marketers, content creators, or anyone who curates and summarizes web articles. It‚Äôs especially helpful for virtual assistants and founders who need to quickly turn web content into digestible, branded newsletters using AI. What problem is this workflow solving? Manually reading, summarizing, and formatting multiple articles into a newsletter takes time and focus. This workflow automates the process using Dumpling AI for crawling, GPT-4o for summarization, and Gmail for delivery‚Äîso you can go from raw URLs to a polished email in minutes. What this workflow does Starts manually (can also be scheduled) Reads a list of article URLs from Google Sheets Sends URLs to Dumpling AI to crawl and extract content Splits each article into a single item for processing Uses a Code node to clean and structure article data Uses an Edit Fields node to merge articles into one JSON block GPT-4o summarizes and generates HTML content for the newsletter Sends the formatted newsletter via Gmail Setup Google Sheets Create a sheet with a column (A) for article URLs Update the Read URLs from Google Sheet node to use your Sheet ID and tab name Connect your Google account in the credentials Dumpling AI Sign up at https://app.dumplingai.com Create an agent for web crawling under /crawl Add your Dumpling API key in the HTTP headers of the Crawl Content with Dumpling AI node Split Node Breaks apart the array of articles from Dumpling AI so each article is processed individually Code Node Structures each article as JSON with title, url, and cleaned text content Edit Fields Node Gathers all structured articles back into a single JSON array to prepare for AI summarization OpenAI (GPT-4o) Processes the article list and returns a formatted subject line and HTML newsletter content Gmail Connect your Gmail account to send the AI-generated newsletter to your inbox or team Update the recipient field in the Send HTML Email via Gmail node How to customize this workflow to your needs Replace the manual trigger with a Schedule node to send newsletters weekly Modify the GPT-4o prompt to change tone (e.g., more professional, funny, casual) Add filtering logic to skip low-value articles Connect Slack, Airtable, or Notion for internal team usage Change Gmail to SendGrid or Outlook if preferred Final Notes This workflow uses: Dumpling AI** /crawl endpoint to extract article content Split, **Code, and Edit Fields nodes to format multi-article input GPT-4o** for summarization and HTML formatting Gmail** for delivery This setup eliminates manual steps and delivers fast, consistent newsletters powered by AI.",2341,2025-05-14 01:17:44.194000+00:00,True,5
4235,Monitor and Track brand Sentiment on Facebook Groups with Bright data,"Workflow documentation updated on 21 May 2025 This workflow keeps track of your brand mentions across different Facebook groups and provides an analysis of the posts as positive, negative or neutral and updates this to Googe sheets for further analysis This is useful and relevants for brands looking to keep track of what people are saying about their brands and guage the customer satisfaction or disatisfaction based on what they are talking about Who is this template for? This workflow is for you if You Need to keep track of your brand sentiments across different niche facebook groups Own a saas and want to monitor it across different local facebook Groups Are looking to do some competitor research to understand what others dont like about their products Are testing the market on different market offerings and products to get best results Are looking for sources other that review sites for product, software or service reviews Need to keep track of your brand sentiments across different niche facebook groups Are starting on market research and would like to get insights from differnt facebook groups on app usage, strngths weaknesses, features etc How it works You will set the desired schedule by which to monitor the groups This gets the brand names and facebook Groups to monitor. Setup Steps Before you begin You will need access to a Bright Data API to run this workflows Make a copy of the sheet below and add the urls for the facebook groups to scrap and the brand names you wish to monitor. Import the workflow json to your canvas Make a copy of this Google sheet to get started easily Set your APi key in the Map out the Google sheet to your tables You can use/update the current AI models to differnt models eg Gemini or anthropic Run the workflow Setup B Bright Data provides an option to receive the results on an external webhook via a POST call. This can be collected via the",2326,2025-05-20 05:59:11.051000+00:00,True,5
2124,List social media activity of a company before a call,"How it works It‚Äôs very important to come prepared to Sales calls. This often means a lot of manual research about the person you‚Äôre calling with. This workflow delivers the latest social media activity (LinkedIn + X) for businesses you are about to interact with each day. Scans Your Calendar**: Each morning, it reviews your Google Calendar for any scheduled meetings or calls with companies based on each attendee email address. Fetches Latest Posts**: For each identified company, it fetches recent LinkedIn and X posts Delivers Insight**s: You receive personalized emails via Gmail, each dedicated to a company you‚Äôre meeting with that day, containing a reminder of the meeting, list of posts categorized by the social media platform, and direct links to posts. Setup steps The workflow requires you to have the following accounts set up in their respective nodes: Google Calendar GMail Clearbit Besides those, you will need an account on the RapidAPI platform and subscribe to the following APIs: Fresh LinkedIn Profile Data Twitter Email example",2320,2024-02-23 08:13:28.377000+00:00,False,6
2145,"When specific event created in Google Calendar, duplicate & rename Google File","Who is this template for? This template is for everyone who has to take notes during a call: Talent Acquisition Managers / Talent Acquisition Specialists / Recruiters HR professionals Sales teams, customer success teams Product teams / User Experience Designers / anyone conducting user research interviews Use case This workflow allows specific events created on Google Calendar (or any other meeting scheduling tool like Calendly) to trigger the duplication and renaming of a specific template document. Example: For each new screening call that is scheduled in your calendar, you want to create a draft of your screening interview template for the role, titled ""{Name of the candidate} | {Date of the interview}"", and located in your Google Drive in a specific folder This workflow could then be extended to copy the link to the file on a Notion database that is shared with the team (check ""To go further"" section). This workflow ensures that if you're jumping from calls to calls, you're already set up to take notes, and every document is tidied up and sorted in a structured way! How it works The workflow starts when a new event is created in Google Calendar The Filter node then selects a specific type of events, depending on a chosen pattern (title includes a specific term, organizer is X, attendees include Y, etc.) The workflow then searches for a specific folder in your Google Drive, where the file you want to duplicate is located The workflow then searches for the specific file you want to duplicate The last step allows to duplicate and rename the file with variables from your Google Calendar event Set up Set up credentials for Google Calendar, Google Drive, and Google File. You'll need a Google Workspace account. Set up the Filter node with the pattern you want to look for to retreive specific events in your calendar Set up the Google Drive you want to search in Set up the Google File you want to duplicate Set up variable at the last step to rename your duplicated file however you want it, or add a description To go further Here's a few idea to enhance this workflow depending on your specific needs: Instead of a filter, separate your flow depending on your use case (ex: you have want to fetch different templates depending on the type of call it'll be). Switch Google Calendar for another trigger (Calendly, Hubspot..) 10 minutes before the event, send the duplicate Google File to the meeting organizer through Slack The day after the event, if the event hasn't been cancelled, add the link to the Google File to your ATS, Hubspot, etc.",2289,2024-02-28 18:06:42.472000+00:00,False,1
4106,Resume Data Extraction and Storage in Supabase from Email Attachments,"Description What Problem Does This Solve? üõ†Ô∏è This workflow automates the process of extracting key information from resumes received as email attachments and storing that data in a structured format within a Supabase database. It eliminates the manual effort of reviewing each resume, identifying relevant details, and entering them into a database. This streamlines the hiring process, making it faster and more efficient for recruiters and HR professionals. Target audience: Recruiters, HR departments, and talent acquisition teams. What Does It Do? üåü Monitors a designated email inbox for new messages with resume attachments. Extracts key information such as name, contact details, education, work experience, and skills from the attached resumes. Cleans and formats the extracted data. Stores the processed data securely in a Supabase database. Key Features üìã Automatic email monitoring for resume attachments. Intelligent data extraction from various resume formats (e.g., PDF, DOC, DOCX). Customizable data fields to capture specific information. Seamless integration with Supabase for data storage. Uses OpenRouter to streamline API key management for services such as AI-powered parsing. Setup Instructions Prerequisites ‚öôÔ∏è n8n Instance**: Self-hosted or cloud instance of n8n. Email Account**: Gmail account with Gmail API access for receiving resumes. Supabase Account**: A Supabase project with a database/table ready to store extracted resume data. You'll need the Supabase URL and API key. OpenRouter Account**: For managing AI model API keys centrally when using LLM-based resume parsing. Installation Steps üì¶ 1. Import the Workflow: Copy the exported workflow JSON. Import it into your n8n instance via ‚ÄúImport from File‚Äù or ‚ÄúImport from URL‚Äù. 2. Configure Credentials: In n8n &gt; Credentials, add credentials for: Email account (Gmail API): Provide Client ID and Client Secret from the Google Cloud Platform. Supabase: Provide the Supabase URL and the anon public API key. OpenRouter (Optional): Add your OpenRouter API key for use with any AI-powered resume parsing nodes. Assign these credentials to their respective nodes: Gmail Trigger ‚Üí Email credentials. Supabase Insert ‚Üí Supabase credentials. AI Parsing Node ‚Üí OpenRouter credentials. 3. Set Up Supabase Table: Create a table in Supabase with columns such as: name, email, phone, education, experience, skills, received_date, etc. Make sure the field names align with the structure used in your workflow. 4. Customize Nodes: Parsing Node(s):* Modify the workflow to use an *OpenAI model* directly for field extraction, replacing the *Basic LLM Chain** node that utilizes OpenRouter. 5. Test the Workflow: Send a test email with a resume attachment. Check n8n's execution log to confirm the workflow triggered, parsed the data, and inserted it into Supabase. Verify data integrity in your Supabase table. How It Works High-Level Workflow üîç Email Monitoring: Triggered when a new email with an attachment is received (via Gmail API). Attachment Check: Verifies the email contains at least one attachment. Prepare Data: Extracts the attachment and prepares it for analysis. Data Extraction: Uses OpenRouter-powered LLM (if configured) to extract structured information from the resume. Data Storage: The structured information is saved into the Supabase database. Node Names and Actions (Example) Gmail Trigger:** Triggers when a new email is received. IF**: Checks whether the received email includes any attachments. Get Attachments:** Retrieves attachments from the triggering email. Prepare Data:** Prepares the attachment content for processing. Basic LLM Chain:** Uses an AI model via OpenRouter to extract relevant resume data and returns it as structured fields. Supabase-Insert:** Inserts the structured resume data into your Supabase database.",2264,2025-05-16 13:42:57.033000+00:00,True,5
4474,"Automate Multi-Channel Customer Support with Gmail, Telegram, and GPT AI","Smart Customer Support AI Agent with Gmail and Telegram Who is this for? This workflow is perfect for: Small to medium businesses** looking to automate customer support E-commerce stores** handling order inquiries and customer questions SaaS companies** providing technical support to users Service providers** managing appointment bookings and general inquiries Startups** wanting to provide 24/7 customer service without hiring full-time staff Agencies** managing client communications across multiple channels What problem is this workflow solving? Customer support is essential but resource-intensive. Common challenges include: Slow response times** leading to frustrated customers Repetitive questions** consuming valuable staff time Inconsistent responses** across different support agents Limited availability** outside business hours Scaling support costs** as business grows Context loss** when customers switch between channels This workflow eliminates these pain points by providing instant, consistent, and intelligent responses 24/7. What this workflow does Core Functionality Multi-Channel Monitoring: Simultaneously watches Gmail and Telegram for customer inquiries Intelligent Processing: Uses AI to understand customer intent and context Knowledge Base Integration: Accesses your company's FAQ and support information Contextual Responses: Provides personalized, helpful replies maintaining conversation history Smart Escalation: Automatically escalates complex issues to human agents Comprehensive Logging: Tracks all interactions for analytics and improvement AI Agent Capabilities Natural Language Understanding**: Comprehends customer questions in plain English Context Awareness**: Remembers previous conversations with each customer Knowledge Retrieval**: Searches your knowledge base for accurate information Response Generation**: Creates professional, brand-appropriate responses Escalation Decision**: Identifies when human intervention is needed Multi-Channel Support**: Handles Gmail and Telegram with channel-specific formatting Automation Features Auto-Response**: Replies to customers within seconds Email Management**: Marks processed emails as read Conversation Threading**: Maintains context in email threads and Telegram chats Error Handling**: Gracefully handles failures with admin notifications Analytics Tracking**: Logs interactions for performance monitoring Setup Prerequisites Active Google Workspace or Gmail account Telegram account for bot creation OpenAI API access Google Sheets access n8n instance (cloud or self-hosted) Step 1: Credential Setup Gmail OAuth2 Configuration Go to Google Cloud Console Create new project or select existing one Enable Gmail API Create OAuth 2.0 credentials Add authorized redirect URIs for n8n In n8n: Settings ‚Üí Credentials ‚Üí Add Gmail OAuth2 Enter Client ID and Client Secret Complete OAuth flow Telegram Bot Setup Message @BotFather on Telegram Create new bot with /newbot command Choose bot name and username Copy the bot token In n8n: Settings ‚Üí Credentials ‚Üí Add Telegram Enter bot token Set webhook URL in bot settings OpenAI API Configuration Sign up at OpenAI Platform Generate API key in API Keys section In n8n: Settings ‚Üí Credentials ‚Üí Add OpenAI Enter API key Choose appropriate model (gpt-4o-mini recommended) Google Sheets Setup Use existing Google account from Gmail setup In n8n: Settings ‚Üí Credentials ‚Üí Add Google Sheets OAuth2 Complete authorization flow Step 2: Google Sheets Preparation Create three Google Sheets in your Google Drive: Knowledge Base Sheet Sheet Name**: ""Knowledge Base"" Columns**: ID, Category, Question/Topic, Answer/Response, Keywords, Last_Updated Import sample data from the Knowledge Base example Customize with your company's FAQs and policies Escalation Tracker Sheet Sheet Name**: ""Escalations"" Columns**: Timestamp, Customer_Name, Customer_Contact, Inquiry_Summary, Escalation_Reason, Priority, Status, Assigned_To This will be auto-populated by the AI agent Interaction Log Sheet Sheet Name**: ""Interaction Log"" Columns**: Timestamp, Channel, Customer_Name, Customer_Contact, Inquiry_Subject, Customer_Message, AI_Response, Response_Time, Status This tracks all customer interactions for analytics Step 3: Workflow Configuration Import Template Copy the workflow JSON from the template In n8n: Import workflow from JSON Replace placeholder Sheet IDs with your actual Google Sheet IDs Update Sheet References Open each Google Sheets node Select your created sheets from the dropdown Verify column mappings match your sheet structure Customize AI Prompts Edit the ""Customer Support AI Agent"" node Update system message with: Your company name and description Brand voice and tone guidelines Specific policies and procedures Escalation criteria Configure Error Notifications (Optional) Set up Slack webhook or email notifications Update error notification node with your webhook URL Customize error message format Step 4: Testing Test Gmail Integration Send test email to your support Gmail account Check workflow execution in n8n Verify response is sent and email marked as read Check interaction logging in Google Sheets Test Telegram Integration Send message to your Telegram bot Verify bot responds appropriately Test conversation memory with follow-up messages Check escalation functionality with complex request Test Knowledge Base Ask questions covered in your knowledge base Verify AI retrieves and uses correct information Test with variations of the same question Ensure responses are consistent and helpful How to customize this workflow to your needs Brand Voice Customization Update the AI system prompt to include: Your company's tone (formal, casual, friendly) Key phrases and terminology you use Brand personality traits Communication style preferences Knowledge Base Expansion Add industry-specific FAQs Include product documentation Add troubleshooting guides Create category-specific responses Escalation Rules Customize when to escalate by modifying the AI agent instructions: Billing disputes over $X amount Technical issues requiring developer help Angry or dissatisfied customers Requests outside standard services Legal or compliance questions Additional Channels Extend the workflow to support: Slack**: Add Slack triggers and response nodes WhatsApp**: Integrate WhatsApp Business API Web Chat**: Add webhook triggers for website chat Discord**: Connect Discord bot integration Analytics Enhancement Add sentiment analysis to customer messages Implement customer satisfaction scoring Create automated reporting dashboards Set up alert thresholds for escalation rates Integration Opportunities CRM Integration**: Connect to HubSpot, Salesforce, or Pipedrive Ticketing System**: Link to Zendesk, Freshdesk, or Jira Service Desk E-commerce Platform**: Integrate with Shopify, WooCommerce, or Magento Calendar Booking**: Connect to Calendly or Acuity for appointment scheduling Advanced Features Multi-language Support**: Add translation capabilities Voice Messages**: Integrate speech-to-text for Telegram voice notes Image Recognition**: Process customer screenshots for technical support Proactive Outreach**: Send follow-up messages based on customer behavior Workflow Maintenance Daily Tasks Review escalation queue Monitor error notifications Check response quality in interaction log Weekly Reviews Analyze customer interaction patterns Update knowledge base with new common questions Review escalation reasons and optimize AI prompts Monthly Optimization Export interaction data for detailed analysis Calculate key metrics (response time, resolution rate, escalation rate) Update AI model parameters based on performance Expand knowledge base with seasonal or trending topics Key Metrics to Track Response Time**: Average time from customer message to AI response Resolution Rate**: Percentage of inquiries resolved without escalation Customer Satisfaction**: Based on follow-up surveys or sentiment analysis Escalation Rate**: Percentage of conversations requiring human intervention Channel Performance**: Effectiveness of Gmail vs Telegram vs other channels Knowledge Base Usage**: Which topics are accessed most frequently Peak Hours**: When customers contact support most often Troubleshooting Common Issues Gmail not triggering**: Check OAuth permissions and API quotas Telegram bot not responding**: Verify bot token and webhook configuration AI responses seem off**: Review and update system prompts Escalations not logging**: Check Google Sheets permissions and column mapping High escalation rate**: Expand knowledge base and refine AI instructions Performance Optimization Monitor OpenAI API usage and costs Adjust AI model temperature for response consistency Optimize knowledge base for faster searches Set appropriate conversation memory limits This workflow provides a solid foundation for automated customer support that can be extensively customized to match your specific business needs and grow with your company.",2259,2025-05-29 05:09:06.053000+00:00,True,7
2526,Telegram to Spotify with OpenAI,"Search music and play to Spotify from Telegram This workflow is a simple demonstration on accessing a message model from Telegram and it makes searching for songs an easy task even if you can't remember the artist or song name. An OpenAI message model tries to figure out the song and sends it to an active Spotify device**. Use case Imagine an office where you play music in the background and the employees can control the music without having to login to the playing account. How it works You describe the song in Telegram. Telegram bot sends the text to n8n. An OpenAI message model tries to find the song. Spotify gets the search query string. First match is then added to queue. -- If there is no match a message is sent to Telegram and the process ends. We change to the next track in the list. We make sure the song starts playing by trying to resume. We fetch the currently playing track. We return ""now playing"" information to Telegram: Song Name - Artist Name - Album Name. Error handling Every Spotify step has it's on error handler under settings where we output the error. Message parser receives the error and sends it to Telegram. Requirements Active workflow* OpenAI API key Telegram bot Spotify account and Oauth2 API Spotify active on a device** .* The Telegram trigger is activated only if this workflow is active. You can however TEST the workflow in the editor by clicking ""Test step"" and then it waits for the Telegram event. When event is received, just step through all steps or just clicking ""Test step"" on the ""Fetch Now Playing"" node. .** You must have a Spotify device active when trying to communicate with a device. Open Spotify and play something - not it is active.",2239,2024-11-05 09:33:03.164000+00:00,True,3
2129,Enrich lead that booked a call on Calendly and save it on Hubspot,"Use Case When having a call with a new potential customer, one of the keys to getting the most out of the call is to find out as much information as you can about them before the call. Normally this involves a lot of manual research before every call. This workflow automates this tedious work for you. What this workflow does The workflow runs every time a new call is booked via your Calendly. It then filters out personal emails, before enriching the email. If the email is attached to a company it enriches the company and upserts it in your Hubspot CRM. Setup Add Clearbit, Hubspot, and Calendly credentials. Click on Test workflow. Book a meeting on Calendly so the event starts the workflow. Be aware that you can adapt this workflow to work with your enrichment tool, CRM, and booking tool of choice.",2220,2024-02-23 14:18:58.663000+00:00,False,2
2535,Get Long Lived Facebook User or Page Access Token,"Facebook access tokens expire quickly, requiring regular updates for continued API access. This workflow simplifies the process of exchanging short-lived tokens for long-lived ones, saving time and reducing manual effort. What this workflow does Exchanges a short-lived Facebook User Access Token for a long-lived token using the Facebook Graph API. Optionally retrieves a long-lived Page Access Token associated with the user. Outputs both the user and page tokens for further use in automation or integrations. Setup Prerequisites: A valid Facebook App ID and App Secret. A short-lived User Access Token from the Facebook platform. (Optional) The App-Scoped User ID for fetching associated page tokens. Workflow Configuration: Replace placeholder values in the ""Set Parameter"" node with your Facebook credentials and token. Run the workflow manually to generate long-lived tokens. Documentation Reference: Follow the official Facebook guide for more details: https://developers.facebook.com/docs/facebook-login/guides/access-tokens/get-long-lived/",2218,2024-11-09 00:42:28.140000+00:00,False,1
2521,Transfer credentials to other n8n instances using a Multi-Form,"Purpose This workflow allows you to transfer credentials from one n8n instance to another. How it works A multi-form setup guides you through the entire process You get to choose one of your predefined (in the Settings node) remote instances first Then all credentials of the current instance are being retrieved using the Execute Command node On the next form page you can select one of the credentials by their name and initiate the transfer Finally the credential is being created on the remote instance using the n8n API. A final form ending indicates if that action succeeded or not. Setup Select your credentials in the nodes which require those Configure your remote instance(s) in the Settings node Every instance is defined as object with the keys name, apiKey and baseUrl. Those instances are then wrapped inside an array. You can find an example described within a note on the workflow canvas. How to use Grab the (production) URL of the Form from the first node Open the URL and follow the instructions given in the multi-form Disclaimer Please note, that this workflow can only run on self-hosted n8n instances, since it requires the Execute Command Node. Security: Beware, that all credentials are being decrypted and processed within the workflow. Also the API keys to other n8n instances are stored within the workflow. This solution is primarily meant for transferring data between testing environments. For production use consider the n8n enterprise edition which provides a reliable way to manage credentials across different environments.",2190,2024-11-03 20:13:25.521000+00:00,False,2
2615,Get Airtable data via AI and Obsidian Notes,"I am submitting this workflow for the Obsidian community to showcase the potential of integrating Obsidian with n8n. While straightforward, it serves as a compelling demonstration of the potential unlocked by integrating Obsidian with n8n. How it works This workflow lets you retrieve specific Airtable data you need in seconds, directly within your Obsidian note, using n8n. By highlighting a question in Obsidian and sending it to a webhook via the Post Webhook Plugin, you can fetch specific data from your Airtable base and instantly insert the response back into your note. The workflow leverages OpenAI‚Äôs GPT model to interpret your query, extract relevant data from Airtable, and format the result for seamless integration into your note. Set up steps Install the Post Webhook Plugin: Add this plugin to your Obsidian vault from the plugin store or GitHub. Set up the n8n Webhook: Copy the webhook URL generated in this workflow and insert it into the Post Webhook Plugin's settings in Obsidian. Configure Airtable Access: Link your Airtable account and specify the desired base and table to pull data from. Test the Workflow: Highlight a question in your Obsidian note, use the ‚ÄúSend Selection to Webhook‚Äù command, and verify that data is returned as expected.",2173,2024-12-08 19:21:21.748000+00:00,True,2
2301,Check if workflows contain build-in nodes that are not of the latest version,"How it works it will return workflows that have buil-in nodes not of latest version with information of node name, type, current version and latest version for that type Set up steps: You need to have n8n credentials set, you can get n8n API key under settings set your instance base URL in ""instance base url"" node Disclaimar: Only check build-in nodes, community nodes are not supported",2110,2024-06-27 12:22:52.669000+00:00,False,2
4217,Create OpenAI-Compatible API Using GitHub Models for Free AI Access,"This n8n template shows you how to connect Github's Free Models to your existing n8n AI workflows. Whilst it is possible to use HTTP nodes to access Github Models, The aim of this template is to use it with existing n8n LLM nodes - saves the trouble of refactoring! Please note, Github states their model APIs are not intended for production usage! If you need higher rate limits, you'll need to use a paid service. How it works The approach builds a custom OpenAI compatible API around the Github Models API - all done in n8n! First, we attach an OpenAI subnode to our LLM node and configure a new OpenAI credential. Within this new OpenAI credential, we change the ""Base URL"" to point at a n8n webhook we've prepared as part of this template. Next, we create 2 webhooks which the LLM node will now attempt to connect with: ""models"" and ""chat completion"". The ""models"" webhook simply calls the Github Model's ""list all models"" endpoint and remaps the response to be compatible with our LLM node. The ""Chat Completion"" webhook does a similar task with Github's Chat Completion endpoint. How to use Once connected, just open chat and ask away! Any LLM or AI agent node connected with this custom LLM subnode will send requests to the Github Models API. Allowing your to try out a range of SOTA models for free. Requirements Github account and credentials for access to Models. If you've used the Github node previously, you can reuse this credential for this template. Customising this workflow This template is just an example. Use the custom OpenAI credential for your other workflows to test Github models. References https://docs.github.com/en/github-models/prototyping-with-ai-models https://docs.github.com/en/github-models",2084,2025-05-19 07:46:28.781000+00:00,True,3
2403,Create Snapshot of Contabo VPS instances on a daily basis,"Workflow: Snapshot Contabo How it Works This workflow automates daily backups (snapshots) of VPS instances hosted on Contabo. Each day at midnight, it checks for existing snapshots and ensures that only the latest backups are retained by removing older ones. It provides a seamless, hands-off backup process to keep your data secure. Setup Steps Setting up this workflow is quick, typically taking about 10-15 minutes. The essential part of the setup is providing the necessary credentials, which you can easily retrieve from your Contabo control panel. Import the Workflow: Download and upload the workflow JSON into n8n. Configure Credentials: Add CLIENT_ID, CLIENT_SECRET, API_USER, and API_PASSWORD in the credential node. Activate the Workflow: Enable it to run automatically at midnight every day. Flow Overview Schedule Trigger (00:00 daily):** Automatically initiates the workflow. Formatted Date:** Prepares a timestamp for naming the snapshot. List Snapshots:** Verifies if an existing snapshot is available for each VPS. Conditional Logic:** No Snapshot? Proceeds to create a new one. Snapshot Found? Deletes the old snapshot before creating a new one. Key Points Snapshot Retention:** Old snapshots are deleted to ensure only the latest backups are stored. Unique Identifiers:** UUIDs are used to track and guarantee unique operations.",2080,2024-09-05 19:31:46.542000+00:00,False,1
2461,"Telegram Payment, Invoicing and Refund Workflow for Stars","This workflow provides a complete solution for handling Telegram Stars payments, invoicing and refunds using n8n. It automates the process of sending invoices, managing pre-checkout approvals, recording transactions, and processing refunds for stars, making it ideal for businesses using Telegram Stars for digital payments. What are Telegram Stars? Learn more here. Key Features Payment Handling**: Automate invoice creation and sending via Telegram, with pre-checkout approval for smooth payment processing. Refund Management: Simplify the refund process using user IDs and payment charge IDs from successful **Telegram Stars transactions. Transaction Recording**: Record all payment details, such as user information and payment charge IDs, in Google Sheets for transparent financial tracking. Who Can Use This Workflow? Developers and Businesses: Looking to implement **Telegram Stars as a payment system within Telegram. Service Providers**: Managing subscriptions, donations, or digital sales through Telegram automation. Use Cases Subscription Sales Automation**: Use the workflow to issue invoices and automatically process payments for recurring subscriptions. Infopreneurs and Marketers: Use the workflow for delivering lead magnets, tripwires, and further automating sales via **Telegram Stars. Course Sales Automation**: Automate invoicing and refunds for educational platforms selling online courses. Developers and Businesses: Looking to implement **Telegram Stars as a payment system within Telegram. Service Providers**: Managing subscriptions, donations, or digital sales through Telegram automation. Online Educational Platforms**: Automate billing for courses and handle refunds easily. Setup Instructions Replace the Telegram API credentials with your bot API token from BotFather. Customize invoice details, including product name, description, and payment amount. Connect your Google Sheets for storing transaction logs. Configure refund steps for easy processing of star refunds when needed. Note: The setup is very simple‚Äîjust follow the instructions provided on the yellow sticky notes within the workflow and insert your data. All other nodes are pre-configured and require no additional customization. The entire setup process takes just 1 minute. I provided a short Loom record with an explanation. Extensibility This workflow can be further customized to include user profile management, payment analytics, or integration with external services like CRMs or accounting tools. Additional modules can be easily connected to manage advanced features like Telegram User Registration. Available Templates Telegram Bot Starter Template: A foundational setup for creating custom Telegram bots. Telegram User Registration Workflow: Automate user registration and data collection via Telegram. Telegram Payment and Refund Workflow for Stars**: Streamline your Telegram payment processing with stars, invoices, and refunds. Support and Updates This workflow is supported and regularly updated to stay compatible with the latest Telegram features and n8n improvements. If you encounter any issues, technical support is available to ensure smooth integration and setup. Key terms: Telegram Stars payment workflow, Telegram refund automation, n8n payment template, Google Sheets transaction logging, Telegram bot for payments, automated refunds on Telegram, Telegram Stars invoice workflow. Please reach out to Victor if you need further assistance with your n8n workflows and automation!",2056,2024-10-12 20:46:43.895000+00:00,False,3
4373,Automate Reddit Trend Analysis with GPT-4 and Slack/Gmail Distribution,"ü§ñ AI Reddit Scout Agent: Auto Post Analysis & Insights Stay ahead of trends and conversations with this intelligent n8n workflow that automatically monitors Reddit, analyzes discussions using AI, and delivers actionable insights to your team via Slack and Gmail. Perfect for market research, content inspiration, and competitive intelligence. üîÑ How It Works This automated 4-step intelligence gathering system runs daily to keep you informed: Step 1: Scheduled Monitoring The workflow automatically triggers daily (default: 9 AM) to scan Reddit for fresh discussions and trending topics. Step 2: Smart Content Retrieval The Reddit API integration searches specified subreddits using customizable filters: Target subreddits (e.g., r/ChatGPT, r/technology, r/startups) Keyword matching for specific topics Post ranking criteria (hot, top, new) Engagement thresholds (upvotes, comments) Step 3: AI-Powered Analysis Each retrieved post is processed by GPT-4 through a specialized agent that: Extracts key discussion points and themes Classifies topics in 3-5 word categories Generates concise one-sentence summaries Identifies sentiment and engagement patterns Highlights actionable insights and trends Step 4: Multi-Channel Distribution Analyzed insights are automatically delivered to: Slack channels for immediate team visibility Gmail inboxes for stakeholders and decision-makers Formatted with links back to original Reddit discussions ‚öôÔ∏è Setup Steps Prerequisites Reddit account (for API access) OpenAI API key with GPT-4 access Slack workspace with posting permissions Gmail account for email distribution n8n instance (cloud or self-hosted) Configuration Steps Credential Setup Reddit API: Configure Reddit node with API credentials OpenAI API Key: Required for AI analysis and summarization Slack OAuth2: Connect workspace for channel posting Gmail OAuth2: Enable email distribution functionality Reddit Configuration Subreddit Selection: Define target communities to monitor Professional: r/entrepreneur, r/marketing, r/sales Technology: r/artificial, r/MachineLearning, r/programming Industry-specific: r/fintech, r/biotech, r/startups Search Parameters: Set keywords and filters Keywords: ""AI trends"", ""market analysis"", ""customer feedback"" Time ranges: past day, week, or month Minimum engagement thresholds AI Analysis Customization Default analysis prompt can be tailored for: Market Research Focus: Extract business insights and opportunities Competitive Intelligence: Identify competitor mentions and sentiment Content Ideas: Find trending topics for content creation Customer Research: Analyze user pain points and feedback Distribution Setup Slack Integration: Choose channels for different content types #market-intelligence for business insights #content-ideas for creative inspiration #competitive-intel for competitor discussions Email Configuration: Set recipient lists and formatting preferences Schedule Optimization Frequency: Daily, weekly, or custom intervals Timing: Optimize for team availability and time zones Volume Control: Limit posts per execution to avoid overwhelm üöÄ Use Cases Market Research & Intelligence Trend Identification: Spot emerging technologies and methodologies Customer Sentiment: Monitor brand mentions and user feedback Competitive Analysis: Track competitor discussions and reputation Industry Insights: Stay updated on regulatory changes and news Content Marketing Topic Discovery: Find trending subjects for blog posts and videos Audience Research: Understand community questions and pain points Content Validation: Test ideas before creating full content pieces Engagement Strategies: Learn what resonates with target audiences Product Development Feature Requests: Identify commonly requested functionality User Experience Feedback: Discover usability issues and improvements Market Validation: Gauge interest in new product concepts Beta Testing Insights: Monitor feedback on product releases Sales & Business Development Lead Generation: Find potential customers discussing relevant problems Objection Handling: Understand common concerns and hesitations Partnership Opportunities: Identify potential collaboration prospects Market Education: Understand knowledge gaps in your target market Investment & Strategy Startup Monitoring: Track emerging companies and funding news Technology Assessment: Evaluate new tools and platforms Risk Assessment: Monitor potential threats and challenges Opportunity Identification: Spot underserved markets and niches üîß Advanced Customization Options Multi-Subreddit Orchestration Configure different analysis approaches: Technical Communities: Focus on innovation and implementation details Business Communities: Extract market opportunities and strategies User Communities: Emphasize pain points and solution requests News Communities: Highlight breaking developments and reactions Sentiment & Engagement Scoring Enhance analysis with quantitative metrics: Sentiment Analysis: Positive, negative, neutral classification Engagement Prediction: Likelihood of viral content Influence Scoring: Identify high-authority contributors Trend Momentum: Track discussion growth over time Content Filtering & Prioritization Implement smart content curation: Quality Thresholds: Minimum upvotes, comments, or awards Relevance Scoring: AI-based topic matching and ranking Duplicate Detection: Avoid repetitive content in summaries Language Processing: Handle multiple languages and slang Extended Integration Options Connect additional platforms and tools: CRM Integration: Push leads to Salesforce, HubSpot, or Pipedrive Documentation Tools: Save insights to Notion, Obsidian, or Confluence Analytics Platforms: Feed data to Google Analytics or Mixpanel Social Media: Cross-post insights to Twitter, LinkedIn, or Facebook üìä Output Examples Sample Slack Message: üîç Reddit Intelligence Report - r/ChatGPT üìä Top Discussion: ""Why I stopped using ChatGPT for coding"" üìù Topic: AI Tool Limitations & Alternatives üìã Summary: Users discussing ChatGPT's declining code quality and exploring alternatives like Claude and GitHub Copilot for development work. üìà Engagement: 847 upvotes, 234 comments üí≠ Sentiment: Mixed (concerns about accuracy, praise for alternatives) üîó View Discussion: [Reddit Link] Key Insights: Growing demand for specialized coding AI tools Quality consistency remains a major user concern Opportunity for developer-focused AI solutions Generated at 9:15 AM ‚Ä¢ Next scan: Tomorrow 9:00 AM Gmail Report Format: Subject: Daily Reddit Intelligence - AI & Technology Trends üì° REDDIT SCOUT REPORT Date: March 15, 2024 üéØ TOP INSIGHTS TODAY: AI Development Tools (r/programming) Discussion: ""Best AI coding assistants in 2024"" Insight: Developers increasingly comparing specialized tools vs general AI Engagement: High (500+ upvotes) Startup Funding (r/entrepreneur) Discussion: ""Raised $2M seed round - lessons learned"" Insight: Current funding landscape and investor priorities Engagement: Moderate (150+ upvotes) Customer Feedback (r/SaaS) Discussion: ""Why our churn rate dropped 40%"" Insight: Onboarding and support strategy impacts Engagement: High (300+ upvotes) üîó View all discussions: [Links included] Powered by AI Reddit Scout Agent Next report: Tomorrow at 9:00 AM üõ†Ô∏è Troubleshooting & Best Practices Common Issues & Solutions API Rate Limiting Implement exponential backoff delays Distribute requests across multiple time periods Cache results to minimize redundant calls Monitor Reddit API status and limits Content Quality Control Set minimum engagement thresholds Filter out low-quality or spam posts Implement keyword blacklists for irrelevant content Regular prompt tuning for better AI analysis Information Overwhelm Limit posts per subreddit per day Prioritize by engagement and relevance scores Create digest formats for high-volume periods Allow team members to customize their alerts Optimization Strategies Performance Enhancement Use parallel processing for multiple subreddits Implement caching for repeated analysis Optimize OpenAI prompt length and complexity Monitor and optimize workflow execution time Cost Management Balance AI analysis depth with API costs Use different models for different content types Implement smart filtering before AI analysis Track ROI through engagement metrics Team Adoption Start with pilot programs in specific departments Provide training on interpreting AI insights Create feedback loops for continuous improvement Establish clear action protocols for different insight types üìà Success Metrics Intelligence Quality Indicators Relevance Score: Percentage of insights that lead to action Timeliness: How quickly trends are identified vs competitors Accuracy: Validation of AI predictions against actual outcomes Engagement: Team interaction with distributed insights Business Impact Measurements Content Performance: Improvement in content engagement rates Market Timing: Success rate of trend-based decisions Competitive Advantage: Early identification of opportunities/threats Time Savings: Reduction in manual research and monitoring time üìû Questions & Support Need assistance with your AI Reddit Scout Agent setup or customization? üìß Technical Support Email: Yaron@nofluff.online Response Time: Within 24 hours on business days Expertise: Reddit API integration, AI prompt optimization, workflow scaling üé• Learning Resources YouTube Channel: https://www.youtube.com/@YaronBeen/videos Complete setup and configuration guides Advanced filtering and analysis techniques Integration tutorials for popular business tools Troubleshooting common issues ü§ù Professional Networking LinkedIn: https://www.linkedin.com/in/yaronbeen/ Connect for ongoing automation support Share your intelligence gathering success stories Access to exclusive workflow templates and updates üí¨ Support Request Guidelines Include in your message: Target subreddits and use cases Current team size and distribution needs Specific industries or topics of interest Integration requirements with existing tools Any error messages or unexpected behaviors",2054,2025-05-25 05:45:03.463000+00:00,True,5
2603,Generate n8n Forms from Airtable and BaseRow Tables,"This n8n template showcases a cool feature of n8n Forms where the form itself can be defined dynamically using the form fields schema. It may be debateable how useful this template actually is since both Airtable and Baserow provide form interfaces already but still a great exercise and demonstration if ever the use-case comes around. How it works A form trigger is used to dynamically select a database/table from which to build the n8n form from. the table's schema is imported into the workflow and using the code node, is converted into the n8n form fields schema. This let's us dynamically build the fields in our n8n form when we choose to define the form using the JSON option. Once the n8n form submits, we convert the values back into our table's API schema so that we can create a new row. Note any files/attachments fields are removed as they need to be handled separately. Files are processed separately as they may first need to be stored. Once complete, the reference is saved into the newly created row. Check out the example Airtable here - https://airtable.com/appfP15Xd0aVZR9xV/shrGFgXLyQ4Jg58SU How to use The n8n form is autogenerated which means you only need provide access to the table. Using this approach, this template can be reused for any number of Airtable and/or Baserow tables. Requirements You'll need either an Airtable account or a Baserow account to use this template. Accessible n8n instance to your users Customising this workflow Not using either Airtable or Baserow? Theoretically any datastore which provides a fields schema can be used with this template. If you're feeling creative, split the table into multiple forms for a better user experience.",2047,2024-12-03 14:34:43.509000+00:00,False,3
2277,Training Feedback Automation with Usertask and Airtable,"Who is this template for? This workflow template is designed for teams involved in training management and feedback analysis. It is particularly useful for: HR Departments**: Automating the collection and response to training feedback. Training Managers**: Streamlining the process of handling feedback and ensuring timely follow-up. Corporate Trainers**: Receiving direct feedback and taking actions to improve training sessions. This workflow offers a comprehensive solution for automating feedback management, ensuring timely responses, and improving the quality of training programs. How it works This workflow operates with an Airtable trigger but can be easily adapted to work with other triggers like webhooks from external applications. Once feedback data is captured, the workflow evaluates the feedback and directs it to the appropriate channel for action. Tasks are created in Usertask based on the feedback rating, and notifications are sent to relevant parties. Here‚Äôs a brief overview of this n8n workflow template: Airtable Trigger**: Captures new or updated feedback entries from Airtable. Switch Node**: Evaluates the feedback rating and directs the workflow based on the rating. Webhook**: Retrieves the result of a Usertask task. Task Creation**: Creates tasks in Usertask for poor feedback. Creates follow-up tasks for fair to good feedback. Documents positive feedback and posts recognition on LinkedIn for very good to excellent ratings. Notifications**: Sends email notifications to responsible parties for urgent actions. Sends congratulatory emails and posts on LinkedIn for positive feedback. To summarize Flexible Integration**: This workflow can be triggered by various methods like Airtable updates or webhooks from other applications. Automated Task Management**: It creates tasks in Usertask based on feedback ratings to ensure timely follow-up. Multichannel Notifications**: Sends notifications via email and LinkedIn to keep stakeholders informed and recognize successes. Comprehensive Feedback Handling**: Automates the evaluation and response to training feedback, improving efficiency and response time. Instructions: Set Up Airtable: Create a table in Airtable to capture training feedback. Configure n8n: Set up the Airtable trigger in n8n to capture new or updated feedback entries. Set Up Usertask: Configure the Usertask nodes in n8n to create and manage tasks based on feedback ratings. Configure Email and LinkedIn Nodes: Set up the email and LinkedIn nodes to send notifications and post updates. Test the Workflow: Run tests to ensure the workflow captures feedback, creates tasks, and sends notifications correctly. Video : https://youtu.be/U14MhTcpqeY Remember, this template was created in n8n v1.38.2.",2044,2024-06-02 14:21:20.222000+00:00,False,3
2149,Automatically create daily list of todos in Todoist,"Use case Automatically create todo items in Todoist every morning. This workflow has two flows At 5am, delete any uncompleted tasks every morning At 5:10 am, copy all template tasks into Inbox In each template task, set the due dates and days to add the task. You can do that like this days:mon,tues; due:8pm which will add the task every Monday and Tuesday and make it due at 8pm. How to setup Add Todoist creds Create a template list to copy from in Todoist. Add days and due times on each task as necessary. Set the projects to copy from and to write to in each Todoist node",2035,2024-02-28 19:31:42.228000+00:00,False,2
4273,Evaluation metric example: RAG document relevance,"AI evaluation in n8n This is a template for n8n's evaluation feature. Evaluation is a technique for getting confidence that your AI workflow performs reliably, by running a test dataset containing different inputs through the workflow. By calculating a metric (score) for each input, you can see where the workflow is performing well and where it isn't. How it works This template shows how to calculate a workflow evaluation metric: retrieved document relevance (i.e. whether the information retrieved from a vector store is relevant to the question). The workflow takes a question and checks whether the information retrieved to answer it is relevant. To run this workflow, you need to insert documents into a vector data store, so that they can be retrieved by the agent to answer questions. You can do this by running the top part of the workflow once. The main workflow works as follows: We use an evaluation trigger to read in our dataset It is wired up in parallel with the regular trigger so that the workflow can be started from either one. More info We make sure that the agent outputs the list data from the tools that it used If we‚Äôre evaluating (i.e. the execution started from the evaluation trigger), we calculate the relevance metric using AI to compare the retrieved documents with the question We pass this information back to n8n as a metric If we‚Äôre not evaluating we avoid calculating the metric, to reduce cost",2022,2025-05-21 08:19:43.930000+00:00,True,9
2131,Send a notification to Slack when a new high-quality lead is added to Hubspot,"Use Case When tracking your contacts and leads in Hubspot CRM, every new contact might be a potential customer. To guarantee that you're keeping the overview you'd normally need to look at every new lead that is coming in manually to identify high-quality leads to prioritize their engagement and optimize the sales process. This workflow saves the work and does it for you. What this workflow does The workflow runs every 5 minutes. On every run, it checks the Hubspot CRM for contacts that were added since the last check. It then checks if they meet certain criteria (in this case if they are making +5m annual revenue) and alerts you in Slack for every match. Setup Add Hubspot, and Slack credentials. Click on Test workflow. How to adjust this workflow to your needs Change the schedule interval Adjust the criteria to send alerts",2012,2024-02-23 18:32:12.981000+00:00,False,2
2477,Mark outdated workflow nodes on canvas and send a summary with Gmail (add-on),"This is an add-on for the template Check if workflows contain build-in nodes that are not of the latest version Purpose This workflow highlights outdated nodes within all workflows of a single n8n instance and places an updated preconfigured node right next to it, so it can be swapped easily. How it works The parent workflow checks the entire n8n instance for outdated nodes within all workflows and passes a list of those alongside some metadata to this workflow This workflow then processes that data and updates the affected workflows Outdated nodes are renamed by prepending an emoji (default: ‚ö†Ô∏è) - this is also used for future checks to prevent from double-processing The latest version of each outdated node is added to the workflow canvas (not wired up) behind the old one, slightly shifted in position An Email is sent with a list of modified workflows In the settings it is possible to define: which symbol/emoji should be prepended to outdated notes whether to include only major node updates or all of them whether to add the new nodes to the canvas or not Setup Clone this template to your n8n instance Update the Settings node by setting at least the base URL of your n8n instance Set a recipient in the Gmail node Clone the parent template to your n8n instance and configure it as described in it‚Äôs description Add an ‚ÄúExecute Workflow‚Äù node to the end of the parent workflow and configure it, so it calls this workflow How to use Execute the parent workflow and check your Email Inbox. All linked workflows should contain one or more updated nodes with an emoji prepended to their names. Disclaimer Beware, that major updates can cause migrations of nodes to fail, since their structure can differ. So always compare the old nodes with the newly created, if all parameters still meet the requirements. Be careful when executing this workflow on a production environment, since it directly modifies your workflows. It is advisable to run this on your testing environment and migrate successfully tested workflows to your production environment using git or manually.",1983,2024-10-21 13:24:50.985000+00:00,False,2
2927,Generate multispeaker podcast üéôÔ∏è with AI natural-sounding ü§ñüß† & Google Sheets,"This workflow automates the generation of multi-speaker podcasts using AI-powered text-to-speech technology. It starts by retrieving a podcast script from a Google Sheets document, where each speaker‚Äôs lines are clearly defined. The workflow then processes the script, generates a natural-sounding audio file with different voices for each speaker, and stores the final audio file in Google Drive. The workflow is designed to save time and resources by automating the podcast production process, making it ideal for content creators, marketers, and businesses that need to produce high-quality audio content regularly. How It Works Triggering the Workflow: The workflow starts with the When clicking ‚ÄòTest workflow‚Äô node, which can be triggered manually to begin the process. Data Retrieval: The Get Podcast text node retrieves data from a Google Sheets document containing the podcast script. The document includes columns for the speaker's name and the corresponding text. Data Aggregation: The Get all rows node aggregates the data from the Google Sheets document, combining the speaker names and their respective text into a single dataset. Text Formatting: The Full Podcast Text node processes the aggregated data, formatting it into a single string where each speaker's text is prefixed with their name. Audio Generation: The Create Audio node sends a request to the API to generate a multi-speaker podcast audio file. The request includes the formatted text and specifies the voices for each speaker. When you register for the API service you will get 1$ for free. For continuous work add API credits to your account. Status Check: The Get status node checks the status of the audio generation request. If the status is ""COMPLETED"", the workflow proceeds; otherwise, it waits again. Audio Retrieval: The Get Url Audio node retrieves the URL of the generated audio file. The Get File Audio node downloads the audio file from the provided URL. Audio Storage: The Upload Audio node uploads the generated audio file to a specified Google Drive folder for storage. Key Features Multi-Speaker Support**: Generates podcasts with different voices for each speaker, creating a more dynamic and engaging listening experience. Google Sheets Integration**: Retrieves podcast scripts from a Google Sheets document, making it easy to manage and update content. AI-Powered Text-to-Speech**: Uses advanced AI models to generate natural-sounding audio from text. Automated Audio Generation**: Streamlines the process of creating podcast audio files, reducing the need for manual recording and editing. Google Drive Storage**: Automatically uploads the generated audio files to Google Drive for easy access and sharing. This workflow is ideal for businesses and content creators looking to automate the production of multi-speaker podcasts. It leverages AI to handle the complex task of generating natural-sounding audio, allowing users to focus on creating compelling content. Need help customizing? Contact me for consulting and support or add me on Linkedin.",1963,2025-02-17 15:49:59.027000+00:00,True,4
2550,Waitlist Form Stored in GoogleSheet with Email Verification Step,"Instructions This automation streamlines the process of collecting user information using a Form Node, enabling individuals to join a waitlist managed via Google Sheets. It also generates a verification code, prompting users to input this code in the Second Form Step. If the code is _nvalid, the workflow keeps the user in a verification loop until a valid code is entered. Once a valid code is provided, the workflow proceeds to the final step, where additional information can be collected. All entered data and intermediate states are securely stored in the Google Sheet. Structure of the GoogleSheet Firstname | Lastname | Email | Company | Verification-Code | Verified | Intended Use Marcel | Claus-Ahrens | foo[at]bar| foobar | abc123 | TRUE | Just testing Setup Set Up a Google Sheet: Create a Google Sheet with the specified columns, or customize them to suit your needs. Verify the ""Send Mail"" Node: Ensure your ""Send Mail"" node is functional, or replace it with another email-sending node. Customize Texts and Fields: Update email content, form texts, and form fields to align with your specific use case. Done! Enjoy the workflow! ‚ù§Ô∏è let the workf low ‚Äî Workflow Automation & Development",1959,2024-11-15 20:38:32.392000+00:00,False,3
2150,üö® Report n8n workflow errors to Slack,Use case Error workflows are an important part of running workflows in production. How to setup Add Slack creds Add this error workflow to other workflows (docs),1957,2024-02-28 19:39:07.425000+00:00,False,1
4687,AI Email Organizer for GMail - Advanced Email Management & Sorting,"Video Introduction Want to automate your inbox or need a custom workflow? üìû Book a Call | üí¨ DM me on Linkedin Transform your messy inbox into a calm, organized command center - in minutes - using this ready-to-use n8n automation! Tired of your Gmail looking like this? With this template, you can have this instead: What does this automation do? AI-powered categorization:** Every new email is analyzed with OpenRouter AI and sorted into categories you define (like Orders, Support, Invoices, Urgent, etc.). Instant color-coded labels:** The workflow creates and applies Gmail labels with custom colors, so you can spot important messages at a glance. Supports Gmail‚Äôs Multiple Inboxes:** Display different categories in their own sections‚Äîsee what matters most right away. Flexible and customizable:** You control the categories and definitions using a simple Google Sheet. How it works ‚Äì Step by Step See the full setup & demo: Copy the Template Open the n8n workflow template and click Use for free. Log in (or sign up) for n8n Cloud for the quickest start. Customize Your Categories in Google Sheets Use the provided Google Sheets template linked in the workflow notes. Go to File ‚Üí Make a copy to your own Drive. Edit the categories and their definitions for your business. Example: Add categories like ‚ÄúExisting Order Questions,‚Äù define each one to guide the AI, and copy your Google Sheet‚Äôs URL into the workflow config node. Connect AI with OpenRouter Go to OpenRouter.ai, log in, and generate a new API key. Paste your API key into the workflow where prompted. Test and Activate the Workflow Connect your Gmail account to n8n. Hit ‚ÄúTest Workflow‚Äù‚Äîwatch as the AI processes your latest emails and applies labels automatically. Labels will appear instantly in Gmail, and any missing ones are created by the automation. Schedule Automatic Runs Switch workflow status to Active in n8n. Set the scheduler trigger‚Äîmost people use hourly, but you can use crontab.guru for custom times (like only business hours). Tips for Best Results Color Code Your Labels:** In Gmail, you can assign colors to labels‚Äîset high-priority categories (like ‚ÄúCustomer Complaints‚Äù) to a bright color to stand out. Upgrade Your Gmail View:** Enable Multiple Inboxes in Gmail‚Äôs settings and set up sections for your key categories. Example search queries: in:inbox label:customer-complaints OR label:urgent-emails in:inbox label:existing-order-questions in:inbox label:support-requests Why Use This? Get rid of inbox chaos for good - no more lost emails or missed deadlines Fully customize the system to your business with just a Google Sheet Works with zero coding - set up in 10-15 minutes Flexible: add auto-replies, draft suggestions, and more as you grow",1954,2025-06-05 11:49:49.978000+00:00,True,4
2349,Access execution data from an error workflow,"Sometimes you want to take a different action in your error workflow based on the data that was flowing through it. This template illustrates how you can do that (more specifically, how you can retrieve the data of a webhook node). How it works Use the 'n8n' node to fetch the data of the failed execution Parse that data to find webhook nodes and extract the data of the one that was executed",1939,2024-07-17 07:24:34.009000+00:00,False,1
2298,Merge PDF files using ConvertAPI,"Who is this for? For developers and organizations that need to combine PDF files. What problem is this workflow solving? PDF file merging problem. What this workflow does Downloads two PDF files from the web. Merges two PDF files into one. Stores the PDF file in the local file system. How to customize this workflow to your needs Open the PDF merge HTTP Request node. Adjust the URL parameter (all endpoints can be found here). Use your API Token for authentication. Pass the token in the Authorization header as a Bearer token. You can manage your API Tokens in the User panel ‚Üí Authentication. Optionally, additional Body Parameters can be added for the converter.",1931,2024-06-25 12:31:47.472000+00:00,False,1
2135,Enrich new leads in Pipedrive and send an alert to Slack for high-quality ones,"Use Case This workflow is beneficial when you're automatically adding new leads to your Pipedrive CRM. Usually, you'd have to manually review each lead to determine if they're a good fit. This process is time-consuming and increases the chances of missing important leads. This workflow ensures every new lead is promptly evaluated upon addition. What this workflow does The workflow runs every 5 minutes. On every run, it checks your new Pipedrive leads and enriches them with Clearbit. It then marks items as enriched and checks if the company of the new lead matches certain criteria (in this case if they are B2B and have more than 100 employees) and sends a Slack alert to a channel for every match. Pre Conditions You must have Pipedrive, Clearbit, and Slack accounts. You also need to set up the custom fields Domain and Enriched at in Pipedrive. Setup Go to Company Settings -&gt; Data fields -&gt; Organization and add Domain as a custom field Go to Company Settings -&gt; Data fields -&gt; Leads and add Enriched at as a custom date field Add your Pipedrive, Clearbit and Slack credentials. Fill the setup node below. To get the ID of your custom domain fields, simply run the Show only custom organization fields and Show only custom lead fields nodes below and copy the keys of your domain, and enriched at fields. How to adjust this workflow to your needs Modify the criteria to suit your definition of an interesting lead. If you only want to focus on interesting leads in Pipedrive, add a node that archives all others. This workflow was built using n8n version 1.29.1",1926,2024-02-28 10:26:13.483000+00:00,False,4
2518,üéµ Sync YouTube and Spotify Music Playlists,"Workflow Overview This workflow automates the process of updating a Spotify playlist with tracks from a YouTube playlist, ensuring no duplicates are added. Key Components Manual Trigger: Starts the workflow when you click ‚ÄòTest workflow‚Äô. Spotify Integration: Retrieves tracks from a specified Spotify playlist. YouTube Integration: Fetches tracks from a designated YouTube playlist. Batch Processing: Processes tracks in batches to handle multiple items efficiently. Track Search: Searches for YouTube tracks on Spotify to find corresponding IDs. Comparison: Compares existing Spotify tracks with YouTube tracks to identify which ones to add. Track Addition: Adds new Spotify tracks to the playlist that are not already included. If you have any questions or need clarification, feel free to ask!",1913,2024-11-01 16:50:08.247000+00:00,False,2
2287,Classify lemlist replies using OpenAI and automate reply handling,"Who this is for This workflow is for sales people who want to quickly and efficiently follow up with their leads What this workflow does This workflow starts every time a new reply is received in lemlist. It then classifies the response using openAI and creates the correct follow up task. The follow-up tasks currently include: Slack alerts when a lead for each new replies Tag interested leads in lemlist Unsubscription of leads when they request it The Slack alerts include: Lead email address Sender email address Reply type (positive, not interested...etc) A preview of the reply Setup To set this template up, simply follow the stickies steps in it How to customize this workflow to your needs Adjust the follow up tasks to your needs Change the Slack notification to your needs ...",1910,2024-06-10 16:47:40.469000+00:00,True,6
4420,Automated Daily Outlook Calendar Meeting Digest,"Automated Daily Outlook Calendar Meeting Digest Overall Goal This workflow automatically runs at a scheduled time (daily at 8 AM by default), calculates the current day's date range, fetches all calendar events from a specified Microsoft Outlook account for that day, formats these events into a user-friendly HTML email, and then sends this digest to a designated email address. How it Works (Step-by-Step Breakdown): Node: Schedule Trigger (Schedule Trigger Node) Type:** n8n-nodes-base.scheduleTrigger Purpose:** Automatically starts the workflow at a predefined time. Configuration:** Rule &gt; Interval &gt; Trigger At Hour: 8 (Triggers every day at 8:00 AM according to the n8n server's timezone) Output:** Triggers the workflow execution at the scheduled time. Node: Code (Code Node) Type:** n8n-nodes-base.code Purpose:** Dynamically calculates the start and end timestamps for ""today,"" based on when the workflow is triggered. Configuration (JS Code):** Gets the current date and time (workflow runtime). Sets today to beginning of current day (00:00:00). Sets tomorrow to beginning of next day (00:00:00). Converts these to ISO string format (e.g., 2023-10-27T00:00:00Z). Output:** JSON object with today and tomorrow ISO date strings. Node: Microsoft Outlook (Microsoft Outlook Node) Type:** n8n-nodes-base.microsoftOutlook Purpose:** Fetch calendar events from Outlook within the calculated date range. Configuration:** Resource: Event Filters (Custom): start/dateTime ge '{{$json.today}}' and start/dateTime lt '{{$json.tomorrow}}' (OData filter to fetch events starting on or after today and before tomorrow, i.e., all today's events.) Output:** List of event objects from Outlook. Node: Edit Fields (Set Node) Type:** n8n-nodes-base.set Purpose:** Transform and simplify the event data structure from Outlook. Configuration:** Maps fields from Outlook event to new field names: id subject description (from bodyPreview) meeting_start meeting_end attendees meeting_organizer meeting_organizer_email meeting_link Output:** List of JSON objects with simplified meeting details. Node: Generate HTML (Code Node) Type:** n8n-nodes-base.code Purpose:** Generate a single HTML email body summarizing all meetings and create the email subject line. Configuration (JS Code):** Processes all meeting items from ""Edit Fields"" node. Defines generateMeetingReminderEmail function to format each meeting into an HTML ""card."" Escapes HTML special characters, formats times, attendees, etc. Concatenates all cards into a full HTML document. Generates subject line (e.g., ""üóìÔ∏è Your Meetings Today ‚Äì Friday, Oct 27""). Output:** JSON object with: { ""subject"": ""email subject string"", ""html"": ""generated HTML content string"" } Node: Send Email (Email Send Node) Type:** n8n-nodes-base.emailSend Purpose:** Send the generated HTML digest email to the designated recipient. Configuration:** From Email: test@gmail.com To Email: akhilgadiraju@gmail.com Subject: {{ $json.subject }} (dynamic from Generate HTML node) HTML: {{ $json.html }} (dynamic from Generate HTML node) Output:** Email sending status. Sticky Notes Update Time:** Near ""Schedule Trigger"" node; configure trigger time as needed. Update Email Details:** Near ""Send Email"" node; change sender and receiver email addresses. How to Customize It Schedule (Schedule Trigger node):** Modify the trigger hour, minutes, or days of week to change when the workflow runs. Date Range (Code node):** Adjust JS to change date range (e.g., next business day, upcoming week). Outlook Calendar (Microsoft Outlook node):** Specify Calendar ID or refine OData filters for event selection. Event Details (Edit Fields node):** Add/remove/modify event fields extracted. Email Appearance and Content (Generate HTML node):** Change CSS styling, meeting details, or subject line logic. No Meetings Scenario:** Use an ""If"" node after ""Edit Fields"" to handle no-meeting days (e.g., send ""No meetings today!"" email or skip email). Email Recipients (Send Email node):** Update ""From"" and ""To"" emails; multiple recipients separated by commas. Error Handling Use ""Error Trigger"" nodes to catch and handle failures (Outlook API, SMTP errors). Send alerts or log errors accordingly. Use Cases Automated Daily Personal Meeting Briefing:** Get daily email summaries of your meetings. Automated Team Meeting Digest:** Send daily team calendar digest emails. Proactive Daily Planning:** Automatically stay informed of your day‚Äôs schedule. Required Credentials Add these credentials in your n8n instance under Credentials: Microsoft Outlook (OAuth2 API):** Used by: ""Microsoft Outlook"" node Credential Name in Workflow: Outlook (ID: JcYqVJwcwZIhB8oy) Requires OAuth2 with Calendars.Read permission. SMTP:** Used by: ""Send Email"" node Credential Name in Workflow: SMTP account (ID: vCexcphurglwGBfk) Requires SMTP server details (host, port, username, password). Ensure these credentials are configured correctly with required permissions. Activate the workflow for scheduled execution. Made with ‚ù§Ô∏è using n8n by Akhil.",1905,2025-05-27 06:47:50.602000+00:00,False,3
2234,AI-Powered Children's Arabic Storytelling on Telegram,"Template for Kids' Story in Arabic The n8n template for creating kids' stories in Arabic offers a versatile platform for storytellers to captivate young audiences with educational and interactive tales. It allows for customization to suit various use cases and can be set up effortlessly. Check this example: https://t.me/st0ries95 Use Cases Educational Platforms: Educational platforms can automate the creation and distribution of educational stories in Arabic for children using this template. By incorporating visual and auditory elements into the storytelling process, educational platforms can enhance learning experiences and engage young learners effectively. Children's Libraries: Children's libraries can utilize this template to curate and share a diverse collection of Arabic stories with young readers. The automated generation of visual content and audio files enhances the storytelling experience, encouraging children to immerse themselves in new worlds and characters through captivating narratives. Language Learning Apps: Language learning apps focused on Arabic can integrate this template to offer culturally rich storytelling experiences for children learning the language. By translating stories into Arabic and supplementing them with visual and auditory components, these apps can facilitate language acquisition in an enjoyable and interactive manner. Configuration Guide for Nodes OpenAI Chat Model Nodes: Functionality**: Allows interaction with the OpenAI GPT-4 Turbo model. Purpose**: Enables communication with advanced chat capabilities. Create a Prompt for DALL-E Node: Customization**: Tailor prompts for generating relevant visual content. Summarization**: Define prompts for visual content generation without text. Generate an Image for the Story Node: Resource Type**: Specifies image as the resource. Prompt Setup**: Configures prompt for textless image creation within the visual content. Generate Audio for the Story Node: Resource Type**: Chooses audio as the resource. Input Definition**: Sets input text for audio file generation. Translate the Story to Arabic Node: Chunking Mode Selection**: Allows advanced chunking mode choice. Summarization Configuration**: Sets method and prompts for story translation into Arabic. Send the Story To Channel Node: Channel ID**: Specifies the channel ID for sending the story text. Text Configuration**: Sets up the text to be sent to the channel. By following these node descriptions, users can effectively configure the n8n template for kids' stories in Arabic, tailoring it to specific use cases for a seamless and engaging storytelling experience for young audiences.",1891,2024-04-13 20:07:06.571000+00:00,True,5
2675,Generate Stripe invoice and send it by email,"Generating Stripe invoices through the API can be tricky since it requires four steps to generate and send it via email to the customer. With this workflow you can create Stripe invoices automatically and make Stripe send the invoices to the customer email. How it works To generate a Stripe invoice, you need to create a customer, specify the invoice items, create the invoice, and finalize it. What should be a simple task involves multiple steps. This workflow simplifies the process by providing everything pre-built for you. Who is this for? Anyone who wants to generate invoices automatically and send them to the customer‚Äôs email. Stripe will only send invoices to customers if you generate the invoice correctly through the API. . Check out my other templates üëâ https://n8n.io/creators/solomon/",1881,2024-12-26 23:01:02.981000+00:00,False,2
2318,Enrich Pipedrive's Organization Data with OpenAI GPT-4o & Notify it in Slack,"This workflow enriches new Pipedrive organization's data by adding a note to the organization object in Pipedrive. It assumes there is a custom ""website"" field in your Pipedrive setup, as data will be scraped from this website to generate a note using OpenAI. Then, a notification is sent in Slack. ‚ö†Ô∏è Disclaimer This workflow uses a scraping API. Before using it, ensure you comply with the regulations regarding web scraping in your country or state. Important Notes The OpenAI model used is GPT-4o, chosen for its large input token capacity. However, it is not the cheapest model if cost is very important to you. The system prompt in the OpenAI Node generates output with relevant information, but feel free to improve or modify it according to your needs. How It Works Node 1: Pipedrive Trigger - An Organization is Created This is the trigger of the workflow. When an organization object is created in Pipedrive, this node is triggered and retrieves the data. Make sure you have a ""website"" custom field in Pipedrive (the name of the field in the n8n node will appear as a random ID and not with the Pipedrive custom field name). Node 2: ScrapingBee - Get Organization's Website's Homepage Content This node scrapes the content from the URL of the website associated with the Pipedrive Organization created in Node 1. The workflow uses the ScrapingBee API, but you can use any preferred API or simply the HTTP request node in n8n. Node 3: OpenAI - Message GPT-4o with Scraped Data This node sends HTML-scraped data from the previous node to the OpenAI GPT-4o model. The system prompt instructs the model to extract company data, such as products or services offered and competitors (if known by the model), and format it as HTML for optimal use in a Pipedrive Note. Node 4: Pipedrive - Create a Note with OpenAI Output This node adds a Note to the Organization created in Pipedrive using the OpenAI node output. The Note will include the company description, target market, selling products, and competitors (if GPT-4o was able to determine them). Node 5 & 6: HTML To Markdown & Code - Markdown to Slack Markdown These two nodes format the HTML output to Slack Markdown. The Note created in Pipedrive is in HTML format, as specified by the System Prompt of the OpenAI Node. To send it to Slack, it needs to be converted to Markdown and then to Slack Markdown. Node 7: Slack - Notify This node sends a message in Slack containing the Pipedrive Organization Note created with this workflow.",1864,2024-07-06 16:24:36.463000+00:00,True,5
2409,Weekly reminder on your notion tasks with a deadline,"Who might benfit from this workflow? Everyone organizing him/herself by using a notion database for tasks but losing track on some important tasks having a deadline. The weekly reminder helps you to not forget about your notion tasks. How it works: The workflow fetches all your notion tasks from a desired database but the closed ones It generates a html template for each tasks containing a headline and a short list of key data (prio, status deadline, tags) It creates two groups based on the deadline date if a task is already overdue or not It generates a complete html email containing both groups and some sugar around them It sends the email to your desired email It uses Pushover to send you a push notification to your phone It is scheduled by the beginning of each week How to set it up: Fill out the ""Set Workflow vars"" node with your data Connect your notion account and select the database your tasks are stored at define the status filters to the ones you are using for your tasks Setup your email server to enable the email node to deliver your html email Create a Pushover account and setup the authentication for the Pushover node Adjust the last html node to change email style for your desire How to customize this workflow for your needs? You might adjust the filtering of the notion fetch node to filter for other statuses than provided in the example. You apply your custom design to the html email You could remove the filter which is filtering for tasks having a deadline and just send yourself a reminder for all tasks You need help implementing this or any other n8n workflow? Feel free to contact me via LinkedIn or my business website. You want to start using n8n? Use this link to register for n8n (This is an affiliate link)",1859,2024-09-12 07:58:50.225000+00:00,False,4
2584,Telegram AI Bot: NeurochainAI Text & Image - NeurochainAI Basic API Integration,"This template provides a workflow to integrate a Telegram bot with NeurochainAI's inference capabilities, supporting both text processing and image generation. Follow these steps to get started: &gt; Purpose: Enables seamless integration between your Telegram bot and NeurochainAI for advanced AI-driven text and image tasks. Requirements Telegram Bot Token. NeurochainAI API Key. Sufficient credits to utilize NeurochainAI services. Features Text processing through NeurochainAI's inference engine. AI-powered image generation (Flux). Easy customization and scalability for your use case. Setup Import the template into N8N. Add your Telegram Bot Token and NeurochainAI API Key where prompted. Follow the step-by-step instructions embedded in the template for configuration. [NeurochainAI Website](https://www.neurochain.ai/ ) NeurochainAI Guides",1851,2024-11-28 14:46:40.622000+00:00,True,3
2586,Weekly dinner meal plan using recipes from Mealie,"This workflow randomly select recipes from a Mealie instance (can use a specific category) and then creates a meal plan in Mealie with those recipes. How it works: Workflow has a scheduled trigger (set to run weekly on a Friday) Config node sets a few properties to configure the workflow A call to the Mealie API to get the list of recipes The code node holds most of the logic, this will loop through the number of recipes defined in the config node and randomly select a recipe from the list (making sure not to double up any recipes) Once all the recipes are selected it will call the Mealie API to set up the meal plan on the days Setup Add your Mealie API token as a credential and set it on the Http Request nodes Set the relevant schedule trigger to run when you like Update the Config node with the config you want numberOfRecipes - Number of recipes to populate for the meal plan offsetPlanDays - Number of days in the future to start the plan (0 will start it today, 1 tomorrow, etc.) mealieCategoryId - A category id of the category you want to pull in recipes from (default to select from all recipes) mealieBaseUrl - The base url of your Mealie instance",1834,2024-11-29 06:35:38.804000+00:00,False,2
4556,Auto-Publish Latest News on X with AI Content Generation using Keywords and Bright Data,"üì∞ Publish Latest News on X and Other Social Media Platforms Using Keyword A comprehensive n8n automation that fetches the latest news based on keywords, generates AI-powered social media content, and automatically publishes to X (Twitter) with complete tracking and notification systems. üìã Overview This workflow provides a professional news publishing solution that automatically discovers breaking news, creates engaging social media content using AI, and publishes to X (Twitter) with comprehensive tracking. Perfect for news organizations, content creators, social media managers, and businesses wanting to stay current with automated news sharing. The system uses BrightData's Google News dataset, OpenAI's GPT-4o for content generation, and multi-platform integration for complete automation. ‚≠ê Key Features üìù Form-Based Input**: Clean web form for keyword and country submission üì∞ Real-Time News Fetching**: BrightData Google News integration for latest articles ü§ñ AI Content Generation**: GPT-4o powered tweet creation with hashtags üì± Auto X Publishing**: Direct posting to X (Twitter) with URL tracking üìä Complete Tracking**: Google Sheets logging of all published content üîî Email Notifications**: Success alerts with tweet links üåç Multi-Country Support**: Localized news for US, India, UK, Australia ‚ö° Status Monitoring**: Real-time progress tracking with retry logic üõ° Error Handling**: Robust error management and validation üîÑ Loop Management**: Intelligent waiting for news processing completion üéØ What This Workflow Does Input: News Name**: Keyword or topic for news search (required) Country**: Target country for localized news (dropdown: US/IN/GB/AU) Processing: Form Submission: Captures news keyword and target country News Triggering: Initiates BrightData Google News scraping job Status Monitoring: Checks scraping progress with intelligent retry loop Data Retrieval: Fetches latest news articles when ready AI Content Creation: Generates engaging tweet content using GPT-4o Social Publishing: Posts content to X (Twitter) automatically URL Generation: Creates direct tweet links for tracking Data Logging: Saves content and URLs to Google Sheets Email Notification: Sends success confirmation with tweet link Completion: Workflow ends with full audit trail üìã Output Data Points | Field | Description | Example | | :------------ | :---------------------------------- | :----------------------------------------------------------------------------------------------------- | | TweetMessage | AI-generated social media content | ""Breaking: AI revolution transforming healthcare with 40% efficiency gains. New study shows promising results in patient care automation. #AI #Healthcare #Innovation #TechNews #US"" | | TweetURL | Direct link to published tweet | https://twitter.com/i/web/status/1234567890123456789 | üõ†Ô∏è Setup Instructions Prerequisites: n8n instance (self-hosted or cloud) X (Twitter) account with API v2 access OpenAI account with GPT-4o access Gmail account for notifications Google account with Sheets access BrightData account with Google News dataset access Basic understanding of social media automation Step 1: Import the Workflow Copy the JSON workflow code from the provided file. In n8n, click ""+ Add workflow"". Select ""Import from JSON"". Paste the workflow code and click ""Import"". The workflow will appear with all nodes properly connected. Step 2: Configure API Credentials X (Twitter) API Setup: Create X Developer Account at developer.twitter.com. Create new app and generate API keys. In n8n: Credentials ‚Üí + Add credential ‚Üí Twitter OAuth2 API. Add your Twitter API credentials: API Key API Secret Key Bearer Token Access Token Access Token Secret Test the connection with a sample tweet. OpenAI API Configuration: Get API key from platform.openai.com. Ensure GPT-4o model access is available. In n8n: Credentials ‚Üí + Add credential ‚Üí OpenAI API. Add your OpenAI API key. Verify model access in the ""OpenAI Chat Model"" node. Gmail Integration: Create ""Gmail OAuth2"" credential. Follow OAuth setup process. Grant email sending permissions. Test with sample email. BrightData News API: The workflow uses pre-configured token: 5662edde-6735-4c5d-a6c6-693043a5a9a5. Dataset ID: gd_lnsxoxzi1omrwnka5r (Google News). Verify access to Google News dataset. Test API connection. Google Sheets Integration: Create ""Google Sheets OAuth2 API"" credential. Complete OAuth authentication. Grant read/write permissions. Test connection. Step 3: Configure Google Sheets Integration Create Google Sheets Structure: Sheet Name: ""Publish Latest News on Social Media Platforms Using Keyword"" Tab: ""Data"" (default) Columns: Tweet Message: AI-generated content posted to X Tweet URL: Direct link to published tweet Sheet Configuration: Create new Google Sheet or use existing one. Add the required column headers. Copy Sheet ID from URL: https://docs.google.com/spreadsheets/d/SHEET_ID_HERE/edit. Current configured Sheet ID: 1koxNrwdeuaSBdREuKc7JQh3d9blEk0sQDJ8VgVLjPOo. Update Workflow Settings: Open ""Google Sheets"" node. Replace Document ID with your Sheet ID. Select your Google Sheets credential. Choose ""Data"" sheet/tab. Verify column mapping is correct. Step 4: Configure Form Interface Form Settings: Open ""On form submission"" node. Form configuration: Title: ""News Publisher"" Description: ""publish latest news to direct social media"" Fields: News Name (text, required) Country (dropdown: US, IN, GB, AU, required) Webhook URL: Copy webhook URL from form trigger node. Current webhook ID: 8d320705-688c-4150-a393-cf899d2bbb52. Test form accessibility and submission. Step 5: Configure Email Notifications Gmail Setup: Open ""Gmail"" node. Update recipient email: raushan@iwantonlinemarketing.com. Email template includes: Success confirmation Direct tweet link Professional formatting Test email delivery. Step 6: Test the Workflow Sample Test Data: Use these examples for testing: | News Name | Country | Expected Results | | :-------------------- | :------ | :------------------------------------------------- | | artificial intelligence | US | Latest AI news with US-specific hashtags | | cricket world cup | IN | Sports news with India-focused content | | brexit update | GB | UK political news with British hashtags | | bushfire news | AU | Australian environmental news | Testing Process: Activate the workflow (toggle switch). Navigate to the webhook form URL. Submit test data. Monitor execution progress: News fetching (30-60 seconds) AI content generation (10-15 seconds) X publishing (5-10 seconds) Sheet update and email (5 seconds) Verify results in all platforms. üìñ Usage Guide Using the Form Interface Navigate to the webhook URL provided by the form trigger. Enter news keyword or topic (e.g., ""climate change"", ""stock market"", ""technology""). Select target country from dropdown. Click submit and wait for processing. Check email for success notification with tweet link. Example Inputs to Test | News Name | Country | Expected | | :-------------------------------- | :------ | :----------------------------------------------------- | | ""artificial intelligence breakthrough"" | ""US"" | Latest AI developments with tech hashtags | | ""football premier league"" | ""GB"" | UK football news with sports hashtags | | ""stock market updates"" | ""IN"" | Indian market news with finance hashtags | | ""hollywood movies"" | ""AU"" | Entertainment news with Australian perspective | Country-Specific Considerations United States (US)**: Focus on national news and global impact. Hashtags: #USA, #American, #Breaking, #News. Time zone considerations for optimal posting. India (IN)**: Emphasis on regional relevance. Hashtags: #India, #Indian, #News, #Breaking. Cultural context in content generation. United Kingdom (GB)**: British perspective and terminology. Hashtags: #UK, #British, #News, #Breaking. Focus on European context. Australia (AU)**: Australian angle and regional focus. Hashtags: #Australia, #Australian, #News, #Breaking. Pacific region context. üìä Reading the Results Google Sheets Data The output sheet contains: Complete tweet content with hashtags and formatting. Direct tweet URLs for easy access and sharing. Chronological record of all published content. Audit trail for content management. Email Notifications Success emails include: Confirmation that content was published. Direct link to view the tweet. Professional formatting for easy reference. X (Twitter) Posts Published content features: AI-optimized messaging within 260 character limit. Relevant hashtags based on topic and country. Engaging format designed for social media. Professional tone suitable for news sharing. üîß Customization Options Expanding Social Media Platforms Add more platforms to the publishing workflow: // Add LinkedIn publishing { ""node"": ""LinkedIn"", ""type"": ""n8n-nodes-base.linkedin"", ""parameters"": { ""text"": ""={{ $json.output }}"", ""additionalFields"": {} } } // Add Facebook posting { ""node"": ""Facebook"", ""type"": ""n8n-nodes-base.facebook"", ""parameters"": { ""pageId"": ""YOUR_PAGE_ID"", ""message"": ""={{ $json.output }}"" } }",1824,2025-06-01 11:55:19.435000+00:00,True,7
4330,Automated Resume Job Matching Engine with Bright Data MCP & OpenAI 4o mini,"Notice Community nodes can only be installed on self-hosted instances of n8n. Who this is for The Automated Resume Job Matching Engine is an intelligent workflow designed for career platforms, HR tech startups, recruiting firms, and AI developers who want to streamline job-resume matching using real-time data from LinkedIn and job boards. This workflow is tailored for: HR Tech Founders** - Building next-gen recruiting products Recruiters & Talent Sourcers** - Seeking automated candidate-job fit evaluation Job Boards & Portals** - Enriching user experience with AI-driven job recommendations Career Coaches & Resume Writers** - Offering personalized job fit analysis AI Developers** - Automating large-scale matching tasks using LinkedIn and job data What problem is this workflow solving? Manually matching a resume to job description is time-consuming, biased, and inefficient. Additionally, accessing live job postings and candidate profiles requires overcoming web scraping limitations. This workflow solves: Automated LinkedIn profile and job post data extraction using Bright Data MCP infrastructure Semantic matching between job requirements and candidate resume using OpenAI 4o mini Pagination handling for high-volume job data End-to-end automation from scraping to delivery via webhook and persisting the job matched response to disk What this workflow does Bright Data MCP for Job Data Extraction Uses Bright Data MCP Clients to extract multiple job listings (supports pagination) Pulls job data from LinkedIn with the pre-defined filtering criteria's OpenAI 4o mini LLM Matching Engine Extracts paginated job data from the Bright Data MCP extracted info via the MCP scrape_as_html tool. Extracts textual job description information via the scraped job information by leveraging the Bright Data MCP scrape_as_html tool. AI Job Matching node handles the job description and the candidate resume compare to generate match scores with insights Data Delivery Sends final match report to a Webhook Notification endpoint Persistence of AI matched job response to disk Pre-conditions Knowledge of Model Context Protocol (MCP) is highly essential. Please read this blog post - model-context-protocol You need to have the Bright Data account and do the necessary setup as mentioned in the Setup section below. You need to have the Google Gemini API Key. Visit Google AI Studio You need to install the Bright Data MCP Server @brightdata/mcp You need to install the n8n-nodes-mcp Setup Please make sure to setup n8n locally with MCP Servers by navigating to n8n-nodes-mcp Please make sure to install the Bright Data MCP Server @brightdata/mcp on your local machine. Sign up at Bright Data. Navigate to Proxies & Scraping and create a new Web Unlocker zone by selecting Web Unlocker API under Scraping Solutions. Create a Web Unlocker proxy zone called mcp_unlocker on Bright Data control panel. In n8n, configure the OpenAi account credentials. In n8n, configure the credentials to connect with MCP Client (STDIO) account with the Bright Data MCP Server as shown below. Make sure to copy the Bright Data API_TOKEN within the Environments textbox above as API_TOKEN=&lt;your-token&gt;. Update the Set input fields for candidate resume, keywords and other filtering criteria's. Update the Webhook HTTP Request node with the Webhook endpoint of your choice. Update the file name and path to persist on disk. How to customize this workflow to your needs Target Different Job Boards Set input fields with the sites like Indeed, ZipRecruiter, or Monster Customize Matching Criteria Adjust the prompt inside the AI Job Match node Include scoring metrics like skills match %, experience relevance, or cultural fit Automate Scheduling Use a Cron Node to periodically check for new jobs matching a profile Set triggers based on webhook or input form submissions Output Customization Add Markdown/PDF formatting for report summaries Extend with Google Sheets export for internal analytics Enhance Data Security Mask personal info before sending to external endpoints",1820,2025-05-23 00:01:02.829000+00:00,True,5
2571,Post new RSS feed items as BlueSky posts,"Who is this for? BlueSky users looking to automate the publication of new posts based on new items from a RSS feed. What this workflow does This will create a BlueSky post with each new RSS feed item, including the feed title, post image, link and content (up to 200 characters) Setup You'll need to generate a BlueSky app password Configure your feed URL in the first node Configure your credentials in the second node How to customize this workflow to your needs You can modify the message posted in the `Create post node, changing the JSON text` value, in case you want to include only the feed item title instead of the content. If you RSS feed doesn't provide an image, you can define a static one on the `Download image` node.",1817,2024-11-25 10:23:12.892000+00:00,False,1
2154,Classify new bugs in Linear with OpenAI's GPT-4 and move them to the right team,"Use case When working with multiple teams, bugs must get in front of the right team as quickly as possible to be resolved. Normally this includes a manual grooming of new bugs that have arrived in your ticketing system (in our case Linear). We found this way too time-consuming. That's why we built this workflow. What this workflow does This workflow triggers every time a Linear issue is created or updated within a certain team. For us at n8n, we created one general team called Engineering where all bugs get added in the beginning. The workflow then checks if the issue meets the criteria to be auto-moved to a certain team. In our case, that means that the description is filled, that it has the bug label, and that it's in the Triage state. The workflow then classifies the bug using OpenAI's GPT-4 model before updating the team property of the Linear issue. If the AI fails to classify a team, the workflow sends an alert to Slack. Setup Add your Linear and OpenAi credentials Change the team in the Linear Trigger to match your needs Customize your teams and their areas of responsibility in the Set me up node. Please use the format Teamname. Also, make sure that the team names match the names in Linear exactly. Change the Slack channel in the Set me up node to your Slack channel of choice. How to adjust it to your needs Play around with the context that you're giving to OpenAI, to make sure the model has enough knowledge about your teams and their areas of responsibility Adjust the handling of AI failures to your needs How to enhance this workflow At n8n we use this workflow in combination with some others. E.g. we have the following things on top: We're using an automation that enables everyone to add new bugs easily with the right data via a /bug command in Slack (check out this template if that's interesting to you) This workflow was built using n8n version 1.30.0",1801,2024-02-29 18:27:31.406000+00:00,True,4
4468,"Generate AI Media with ComfyUI: Images, Video, 3D & Audio Bridge","Unlock low-cost, high-control generative media workflows directly from n8n by integrating with ComfyUI. Ideal for indie creators, AI developers, or small teams seeking scalable media automation‚Äîfrom images to video, 3D, and even audio‚Äîthis workflow makes generative content production more flexible and programmable than ever. How it works Accept a media generation request via an n8n trigger (HTTP webhook, schedule, etc.) Parse input and inject it into a ComfyUI payload Send the payload to your local or remote ComfyUI instance Wait for and collect the output media files (e.g., images, videos, 3D models, or audio) Send the result to a destination like email, Telegram, S3, or upload it back to a CMS or client app ‚ú® The format and complexity of the media are entirely dependent on the ComfyUI workflow you use, meaning this n8n integration is as powerful and creative as your ComfyUI setups. Set up steps Set up and run a local or remote ComfyUI instance with API access enabled Load or create a ComfyUI workflow that suits your content goals (image gen, video stitching, etc.) Open this n8n template and set your ComfyUI server URL, input template, and output handling preferences Connect additional services for input (e.g., Airtable, HTTP) and output (e.g., Notion, Slack, S3) depending on your use case",1797,2025-05-28 18:41:49.542000+00:00,True,4
2319,Better Oauth2.0 workflow for Pipedrive CRM with Supabase,"This workflow provides an OAuth 2.0 auth token refresh process for better control. Developers can utilize it as an alternative to n8n's built-in OAuth flow to achieve improved control and visibility. In this template, I've used Pipedrive API, but users can apply it with any app that requires the authorization_code for token access. This resolves the issue of manually refreshing the OAuth 2.0 token when it expires, or when n8n's native OAuth stops working. What you need to replicate this Your database with a pre-existing table for storing authentication tokens and associated information. I'm using Supabase in this example, but you can also employ a self-hosted MySQL. Here's a quick video on setting up the Supabase table. Create a client app for your chosen application that you want to access via the API. After duplicating the template: a. Add credentials to your database and connect the DB nodes in all 3 workflows. Enable/Publish the first workflow, ""1. Generate and Save Pipedrive tokens to Database."" Open your client app and follow the Pipedrive instructions to authenticate. Click on Install and test. This will save your initial refresh token and access token to the database. Please watch the YouTube video for a detailed demonstration of the workflow: How it operates Workflow 1. Create a workflow to capture the authorization_code, generate the access_token, and refresh the token, and then save the token to the database. Workflow 2. Develop your primary workflow to fetch or post data to/from your application. Observe the logic to include an if condition when an error occurs with an invalid token. This triggers the third workflow to refresh the token. Workflow 3. This workflow will handle the token refresh. Remember to send the unique ID to the webhook to fetch the necessary tokens from your table. Detailed demonstration of the workflow: https://youtu.be/6nXi_yverss",1780,2024-07-06 18:28:02.254000+00:00,False,3
2185,Automatically optimise images added to a Google drive folder,"How it works This workflow watches a selected Google Drive folder for any images added to it. It then takes that image, sends it the the tinypng.com service which optimises and reduces its size (where possible) Tinypng then returns the updated image which is then automatically saved in your chose Google Drive folder Setting things up It's pretty simple to configure and should only take around 5-10mins. You only need to set up credentials for Google Drive and Tinypng.com For Tinypng.com you can sign up for their free tier API access which gives you 500 optimisations per month Once you have those two things, you just need choose your 'input' folder to watch for images and your 'output' folder for where these images should be stored There are a few more optional things you can do such as the naming of your final image and also lots more you could do with the Tinypng API for more advanced image optimisation",1779,2024-03-20 09:35:25.344000+00:00,False,2
2304,Convert XLSX to PDF using ConvertAPI,"Who is this for? For developers and organizations that need to convert XLSX files to PDF. What problem is this workflow solving? The file format conversion problem. What this workflow does Downloads the XLSX file from the web. Converts the XLSX file to PDF. Stores the PDF file in the local file system. How to customize this workflow to your needs Open the HTTP Request node. Adjust the URL parameter (all endpoints can be found here). Use your API Token for authentication. Pass the token in the Authorization header as a Bearer token. You can manage your API Tokens in the User panel ‚Üí Authentication. Optionally, additional Body Parameters can be added for the converter.",1779,2024-07-02 11:42:59.891000+00:00,False,1
4606,"Automate Travel Agent Outreach with Web Scraping, OpenAI, and Google Sheets","üîß Automated Workflow: Scrape Travel Agent Contacts and Send Personalized Survey Emails This workflow is designed to automate the process of scraping travel agent contact data, standardizing the information, storing it, and then sending out personalized survey emails using AI. It‚Äôs especially useful for outreach campaigns, research, or lead generation. ‚öôÔ∏è Workflow Breakdown üìç Part 1: Scraping and Storing Travel Agent Data HTTP Scrape Website Type: HTTP Request (POST) Function: Calls a third-party scraping API (https://api.firecrawl.dev...) to scrape data from a travel agent listing site. Purpose: Extract raw HTML or structured data from a website containing contact info. OpenAI Standardise Data Type: OpenAI Message Model Function: Uses AI to clean and standardize the raw scraped data into structured JSON (e.g., name, email, agency, location). Purpose: Ensures uniformity in formatting, making data easier to process downstream. Split Out Type: Item Splitter Function: Splits the standardized array of agent records into individual items. Purpose: Allows appending each agent as a separate row in Google Sheets. Google Sheet - Data Store Type: Google Sheets (Append) Function: Stores each individual record in a spreadsheet. Purpose: Maintains a centralized and accessible log of scraped and processed contacts. üìç Part 2: Read Records and Send Personalized Survey Emails Triggered Manually ‚Äì when ‚ÄúTest Workflow‚Äù button is clicked. Trigger ‚Äì When clicking 'Test workflow' Type: Manual Trigger Function: Starts the second part of the workflow manually. Use Case: Testing or running the outreach email process on demand. Google Sheet Data Store (Read) Type: Google Sheets (Read) Function: Reads the stored travel agent records from the spreadsheet. Purpose: Retrieves contact details and context for personalized messaging. OpenAI Mail Composer Type: OpenAI Message Model Function: Generates a custom email for each agent using their details. Purpose: Creates human-like, engaging emails that include a survey link (optional input). Google Sheet Update Records Type: Google Sheets (Update) Function: Optionally marks the record as ""emailed"" or logs the date of outreach. Purpose: Prevents duplicate outreach and helps track campaign status. Send Email Type: Email Node (SMTP or integrated service) Function: Sends the personalized email generated by OpenAI. Purpose: Delivers the survey to each travel agent with contextually relevant messaging. üß† Use Case: Targeted email outreach to travel agents. Collect insights or feedback via survey links. Use personalized messaging to improve response rates. üìå Benefits: ‚úÖ Fully automated scraping and processing. ‚úÖ Personalized at scale using OpenAI. ‚úÖ Easily repeatable for different domains or campaigns. ‚úÖ Centralized recordkeeping in Google Sheets. üõ†Ô∏è Tech Stack: n8n: Automation and workflow management OpenAI: AI-based text standardization and email generation Firecrawl (or similar): Web scraping API Google Sheets: Data storage and tracking Email Node: Survey email delivery",1731,2025-06-02 18:01:17.136000+00:00,True,4
2716,Store Form Submission in Airtable,"This workflow, developed by our AI developers at WeblineIndia, is designed to automate the process of capturing form submissions and storing them in Airtable. By leveraging automation, it eliminates manual data entry, ensuring a smooth and efficient way to handle form data. The purpose of creating this workflow is to streamline data management, helping businesses save time, reduce errors, and maintain an organized, structured database for easy access and future use. Steps: Trigger on Form Submission (Form Node)** What It Does: Activates the workflow whenever a form is submitted. How to Set It Up: Use the Form Submission Trigger node to detect new form submissions. This ensures the workflow starts automatically when a user fills out the form. Store Data in Airtable (Airtable Node)** What It Does: Transfers the form data into an Airtable base. How to Set It Up: Use the Airtable Node to map form fields to corresponding columns in your Airtable table, storing the data accurately. Finalize and Activate** What It Does: Completes the setup to automate data storage upon form submission. How to Set It Up: Save and activate the workflow. Once active, it will automatically record all new form submissions in Airtable.",1720,2025-01-10 15:22:18.979000+00:00,True,1
2376,Backup Tag-Selected Workflows to Gitlab,"Fetches workflow definitions from within n8n, selecting only the ones that have one or more (configurable) assigned tags and then: Derives a suitable backup filename by reducing the workflow name to a string with alphanumeric characters and no-spaces Note: This isn't bulletproof, but works as long as workflow names aren't too crazy. Determines which workflows need to be backed up based on whether each one: has been modified. (Note: Even repositioning a node counts.) ...or... is new. (Note: Renaming counts as this.) Commits JSON copies of each workflow, as necessary, to a Gitlab repository with a generated, date-stamped commit message. Setup Credentials Create a Gitlab Credentials item and assign it to all Gitlab nodes. Create an n8n Credentials item and assign it to the n8n node Note: This was tested with http://localhost:5678/api/v1 but should work with any reachable n8n instance and API key. Modify these values in the ""Globals"" Node gitlab_owner - {{your gitlab account}} gitlab_project - {{ your gitlab project name }} gitlab_workflow_path - {{ subdirectory in the project where backup files should be saved/committed }} tags_to_match_for_backup - {{tag(s) to match for backup selection}} *ALERT: According to the n8n node's Filters -&gt; tags field annotations, and API documentation, this supports a CSV list of multiple tags (e.g. tag1,tag2), but the API behavior requires workflows to have all-of the listed tags, not any-of them.* See: https://github.com/n8n-io/n8n/issues/10348 TL/DR - Don't expect a multiple tag list to be more inclusive. Possible workaround: To match more than one tag value, duplicate the n8n node into multiple single-tag matches, or split and iterate multiple values, and merge the results. Possible Enhancements Make the branch (""Reference"") for all the gitlab nodes configurable. Fixed on all as ""main"" in the template. Add an n8n node to generate an audit and store the output in gitlab along with the backups. Extend the workflow at the end to create a Gitlab release/tag whenever any backup files are actually updated or created.",1718,2024-08-11 02:28:38.951000+00:00,False,1
4528,Transcribe Voice Messages from Telegram using OpenAI Whisper-1,"This n8n workflow processes incoming Telegram messages, differentiating between text and voice messages. How it works: Message Trigger: The workflow initiates when a new message is received via the Telegram ""Message Trigger"" node. Switch Node: This node acts as a router. It examines the incoming message: If the message is text, it directs the flow along the ""text"" branch. If the message contains voice, it directs the flow along the ""voice"" branch. Get Audio File: For audio messages, this node downloads the audio file from Telegram. Transcribe Audio: The downloaded audio file is then sent to an ""OpenAI Transcribe Recording"" node, which uses OpenAI's whisper-1 speech-to-text model to convert the audio into a text transcript. Send Transcription Message: Regardless of whether the original message was text or transcribed audio, the final text content is then passed to a ""Send transcription message"" node. Setup Requirements: Telegram Bot Token**: You will need a Telegram bot token configured in the ""Message Trigger"" node to receive messages. OpenAI API Key**: An OpenAI API key is required for the ""Transcribe audio"" node to perform speech transcription. Additional Notes: This workflow provides a foundational step for building more complex AI-driven applications. The transcribed text or original text message can be easily piped into an AI agent (e.g., a large language model) for analysis, response generation, or interaction with other tools, extending the bot's capabilities beyond simple message reception and transcription. üëâ Need Help? Feel free to contact us at 1 Node. Get instant access to a library of free resources we created.",1714,2025-05-31 08:23:21.556000+00:00,True,2
2297,Convert DOCX (from URL) to PDF using ConvertAPI,"Who is this for? For developers and organizations that need to convert DOCX files to PDF. What problem is this workflow solving? The file format conversion problem. What this workflow does Downloads the DOCX file from the web. Converts the DOCX file to PDF. Stores the PDF file in the local file system. How to customize this workflow to your needs Open the HTTP Request node. Adjust the URL parameter (all endpoints can be found here). Use your API Token for authentication. Pass the token in the Authorization header as a Bearer token. You can manage your API Tokens in the User panel ‚Üí Authentication. Adjust url_to_file in the Config node to URL pointing to your file. Optionally, additional Body Parameters can be added for the converter.",1703,2024-06-21 10:46:13.599000+00:00,False,1
2138,Create Linear tickets from Notion content,"This workflow allows you to define multiple tickets/issues in a Notion page, then easily import them into Linear. Why is it useful? We use this workflow internally at n8n for collaboration between Product and Engineering teams: Engineering needs all work to be in our ticketing system (Linear) in order to keep track of it Product prefers to review features in Notion. This is because it and can be used to dump all your thoughts and organise them into themes afterwards, plus it better supports rich content like videos Features Supports rich formatting (bullets, images, videos, links, etc.) Keeps links between the Notion and Linear version, in case you need to refer back Allows you to assign each issue to a team member in the Notion definition Avoids importing the same issues twice if you run it again on the same page (meaning you can issues incrementally) You can see an example of the required format of the Notion page here.",1691,2024-02-28 12:53:41.298000+00:00,False,5
2655,Vector Database as a Big Data Analysis Tool for AI Agents [2/3 - anomaly],"Vector Database as a Big Data Analysis Tool for AI Agents Workflows from the webinar ""Build production-ready AI Agents with Qdrant and n8n"". This series of workflows shows how to build big data analysis tools for production-ready AI agents with the help of vector databases. These pipelines are adaptable to any dataset of images, hence, many production use cases. Uploading (image) datasets to Qdrant Set up meta-variables for anomaly detection in Qdrant Anomaly detection tool KNN classifier tool For anomaly detection The first pipeline to upload an image dataset to Qdrant. 2. This is the second pipeline to set up cluster (class) centres & cluster (class) threshold scores needed for anomaly detection. The third is the anomaly detection tool, which takes any image as input and uses all preparatory work done with Qdrant to detect if it's an anomaly to the uploaded dataset. For KNN (k nearest neighbours) classification The first pipeline to upload an image dataset to Qdrant. The second is the KNN classifier tool, which takes any image as input and classifies it on the uploaded to Qdrant dataset. To recreate both You'll have to upload crops and lands datasets from Kaggle to your own Google Storage bucket, and re-create APIs/connections to Qdrant Cloud (you can use Free Tier cluster), Voyage AI API & Google Cloud Storage. [This workflow] Setting Up Cluster (Class) Centres & Cluster (Class) Threshold Scores for Anomaly Detection Preparatory workflow to set cluster centres and cluster threshold scores so anomalies can be detected based on these thresholds. Here, we're using two approaches to set up these centres: the ""distance matrix approach"" and the ""multimodal embedding model approach"".",1683,2024-12-19 09:41:43.006000+00:00,True,2
2610,Smartlead to HubSpot Performance Analytics,"*Smartlead to HubSpot Performance Analytics A streamlined workflow to analyze your Smartlead performance metrics by tracking lifecycle stages in HubSpot and generating automated reports.* Who is this for? (Outbound) Automation Agencies, Sales and marketing teams using Smartlead for outreach campaigns who want to track their performance metrics and lead progression in HubSpot. What problem does this workflow solve? Manual tracking of lead performance across Smartlead and HubSpot is time-consuming and error-prone. This workflow automates performance reporting by connecting your Smartlead data with HubSpot lifecycle stages, providing clear insights into your outreach campaign effectiveness. What this workflow does Automatically pulls performance data from your Smartlead campaigns Cross-references contact status with HubSpot lifecycle stages Generates comprehensive performance reports in Google Sheets Provides customizable reporting schedules to match your team's needs Setup Requirements PostgreSQL Database Set up your PostgreSQL instance (includes $300 free GCP credits) Follow our step-by-step setup guide: Find a step-by-step guide here Google Account Integration Connect your Google Account to n8n Find the guide here Smartlead Configuration Connect your Smartlead instance: Detailed connection guide included in workflow How to customize this workflow Configure the Trigger node to adjust report frequency Modify the Google Sheets template to match your specific KPIs Customize HubSpot lifecycle stage mapping in the Function node Adjust PostgreSQL queries to track additional metrics Need assistance or have suggestions? lmk here",1662,2024-12-05 13:51:28.938000+00:00,False,5
4643,SSL Certificate Expiry Notifier (No Paid APIs),"Great ‚Äî here‚Äôs a complete Workflow Description for your n8n Creator submission based on the JSON you shared: üîí SSL Expiry Notifier (No Paid APIs) üß© How it Works This workflow automatically checks SSL certificate expiry dates for a list of websites and sends timely email alerts if any certificate is close to expiring. It‚Äôs ideal for DevOps, IT Operations, or Web Admin teams to stay ahead of certificate issues ‚Äî without relying on any paid API services. Here's the high-level logic: Trigger: Runs daily at 8 AM using the Schedule Trigger node. Fetch URLs: Reads URLs to monitor from a Google Sheet. Check SSL: Calls the free ssl-checker.io API to retrieve SSL certificate details (valid from, valid till, days left). Conditional Check: Filters any certificates expiring in 7 days or less. Send Email: Sends a styled HTML email alert to predefined recipients. Update Sheet: Updates the same Google Sheet with the latest SSL info for each domain. ‚öôÔ∏è Set Up Steps (Quick Overview) üïê Estimated setup time: \~10‚Äì15 minutes Connect Google Sheets ‚Äì OAuth2 credentials required. Sheet must include a column titled URL with domains to monitor. Set Up SMTP ‚Äì Add your email credentials under the SMTP node to enable notifications. Optional ‚Äì Customize the threshold days in the ""Expiry Alert"" IF node if you want alerts for a different timeframe. Deploy and Activate ‚Äì That‚Äôs it. Once active, the flow runs automatically every day. üí° Detailed field mappings and logic notes are included inside the workflow as sticky notes. üöÄ Ready to Get Started? Let‚Äôs start your journey on n8n by creating your free account here.",1652,2025-06-04 09:49:31.793000+00:00,False,3
4375,"Automatic News Summarization & Email Digest with GPT-4, NewsAPI and Gmail","üì∞ AI News Digest Agent: Auto News Summarizer & Email Newsletter Create an intelligent news curation system that automatically fetches breaking headlines, generates AI-powered summaries, and delivers personalized news digests to your subscriber list. Perfect for newsletter creators, team leaders, and content curators who want to keep their audience informed without the manual effort of news monitoring and summarization. üîÑ How It Works This streamlined 5-step automation delivers fresh news insights around the clock: Step 1: Automated News Collection The workflow runs on a configurable schedule (default: every 10 minutes) to fetch the latest headlines from NewsAPI, ensuring your content stays current with breaking developments. Step 2: Intelligent Content Curation The system pulls top headlines from reliable news sources, filtering by country, category, and relevance to deliver the most important stories of the day. Step 3: AI-Powered Summarization GPT-4 processes the collected headlines and creates: Concise 5-bullet point summaries Key insights and implications Easy-to-digest news overviews Professional formatting for email distribution Step 4: Subscriber Management The workflow accesses your Google Sheets subscriber list, retrieving names and email addresses for personalized delivery. Step 5: Automated Email Distribution Personalized news digests are automatically sent to each subscriber via Gmail, with custom greetings and professionally formatted content. ‚öôÔ∏è Setup Steps Prerequisites NewsAPI account (free tier available) OpenAI API access for content summarization Google Sheets for subscriber management Gmail account for email distribution n8n instance (cloud or self-hosted) Required Google Sheets Structure Create a simple subscriber database: | Name | Email | |---------------|--------------------------| | John Smith | john@example.com | | Sarah Johnson | sarah@company.com | | Mike Chen | mike.chen@startup.co | Configuration Steps Credential Setup NewsAPI Key: Sign up at newsapi.org for free headline access OpenAI API Key: Required for AI-powered news summarization Google Sheets OAuth2: Access your subscriber spreadsheet Gmail OAuth2: Enable automated email sending News Source Configuration Country Selection: Choose target region (US, UK, CA, AU, etc.) Category Filters: Focus on specific topics (technology, business, health) Source Selection: Prefer certain news outlets or avoid others Language Settings: Configure for international audiences AI Summarization Customization Default prompt creates 5-bullet summaries, but can be tailored for: Industry Focus: Technology, finance, healthcare, politics Audience Type: General public, professionals, executives Content Depth: Brief overviews vs detailed analysis Tone & Style: Formal, conversational, or technical Email Template Personalization Subject Line Formatting: Include date, breaking news indicators Greeting Customization: Use subscriber names for personal touch Content Layout: Professional formatting with clear sections Branding Elements: Add your organization's signature or logo Delivery Schedule Optimization Frequency Settings: Every 10 minutes, hourly, or daily Time Zone Considerations: Optimize for subscriber locations Breaking News Alerts: Immediate delivery for urgent stories Digest Compilation: Collect multiple stories for periodic summaries üöÄ Use Cases Newsletter Publishers Content Automation: Generate newsletter content without manual curation Consistent Publishing: Maintain regular delivery schedules automatically Audience Growth: Provide value that encourages subscriptions and shares Time Savings: Eliminate hours of daily news monitoring and writing Corporate Communications Employee Updates: Keep teams informed about industry developments Executive Briefings: Deliver curated news summaries to leadership Client Communications: Share relevant industry insights with customers Stakeholder Relations: Maintain informed investor and partner networks Educational Institutions Student Resources: Provide current events for academic discussions Faculty Updates: Keep educators informed about relevant developments Research Support: Deliver news related to specific academic fields Parent Communications: Share educational policy and school-related news Professional Services Client Value Addition: Provide industry-specific news as a service benefit Thought Leadership: Position your firm as an informed industry expert Business Development: Share insights that demonstrate market knowledge Team Knowledge Sharing: Keep entire organization current on industry trends Community Organizations Member Engagement: Keep community members informed and engaged Local News Focus: Customize for regional or local news coverage Event Planning: Stay informed about developments affecting your community Advocacy Support: Monitor news relevant to your organization's mission üîß Advanced Customization Options Multi-Source News Aggregation Expand beyond NewsAPI with additional sources: RSS Feed Integration: Add specialized industry publications Social Media Monitoring: Include trending topics from Twitter/LinkedIn Government Sources: Official announcements and policy updates International Coverage: Global perspectives on major stories Intelligent Content Filtering Implement smart curation features: Sentiment Analysis: Filter positive, negative, or neutral news Relevance Scoring: Prioritize stories based on subscriber interests Duplicate Detection: Avoid sending repetitive story coverage Quality Assessment: Ensure content meets editorial standards Subscriber Segmentation Create targeted news experiences: Interest Categories: Technology, business, sports, entertainment Geographic Preferences: Local, national, or international focus Delivery Preferences: Frequency and format customization Engagement Tracking: Monitor opens, clicks, and subscriber behavior Enhanced Email Features Professional newsletter capabilities: HTML Templates: Rich formatting with images and links Call-to-Action Buttons: Drive engagement with your content or services Social Sharing: Enable easy sharing of newsletter content Analytics Integration: Track email performance and subscriber engagement üìä Content Generation Examples Sample Email Output: Subject: üì∞ Your Daily News Digest - March 15, 2024 Hi John, Please find today's top news headlines summarized below: üìà BUSINESS & TECHNOLOGY Federal Reserve signals potential rate cuts following inflation data Major tech companies announce AI partnership for healthcare applications Renewable energy sector sees record investment levels in Q1 2024 Cryptocurrency markets stabilize after regulatory clarity announcement Supply chain disruptions ease as global shipping routes normalize üí° These developments suggest growing economic optimism and continued technology sector innovation. The healthcare AI partnership particularly signals significant advances in medical technology accessibility. Stay informed and have a great day! Powered by AI News Digest Agent Unsubscribe | Update Preferences Breaking News Alert Format: Subject: üö® Breaking News Alert - Major Development Hi Sarah, BREAKING: [Headline] Key Details: [Critical point 1] [Critical point 2] [Impact analysis] Full coverage in your next scheduled digest. AI News Digest Agent üõ†Ô∏è Troubleshooting & Best Practices Common Issues & Solutions API Rate Limiting Monitor NewsAPI quota usage and upgrade plan if needed Implement intelligent caching to reduce redundant requests Stagger requests during high-traffic periods Set up alerts for approaching rate limits Email Delivery Challenges Monitor Gmail sending limits and implement delays if needed Use professional email authentication (SPF, DKIM) Maintain clean subscriber lists to avoid spam flags Implement unsubscribe functionality for compliance Content Quality Control Review AI summaries periodically for accuracy and bias Implement feedback loops for continuous prompt improvement Create editorial guidelines for consistent tone and style Monitor subscriber feedback and engagement metrics Optimization Strategies Performance Enhancement Use parallel processing for multiple news sources Implement intelligent caching for repeated content Optimize AI prompts for faster processing and better results Monitor workflow execution time and resource usage Subscriber Growth Create compelling value propositions for newsletter signups Implement referral systems for organic growth Share sample newsletters on social media and websites Collect feedback to continuously improve content quality Content Strategy A/B test different summary formats and lengths Analyze which news categories generate most engagement Experiment with sending times for optimal open rates Create themed newsletters for special events or topics üìà Success Metrics Engagement Indicators Open Rates: Percentage of subscribers reading newsletters Click-Through Rates: Engagement with linked news sources Subscriber Growth: New signups and retention rates Forward/Share Rates: Viral coefficient of your content Content Quality Measurements Relevance Scores: Subscriber feedback on content usefulness Timeliness: How quickly breaking news reaches subscribers Accuracy: Verification of AI-summarized content Completeness: Coverage of important stories in your focus areas üìû Questions & Support Need assistance with your AI News Digest Agent setup or optimization? üìß Technical Support Email: Yaron@nofluff.online Response Time: Within 24 hours on business days Specialization: NewsAPI integration, AI content optimization, email deliverability üé• Educational Resources YouTube Channel: https://www.youtube.com/@YaronBeen/videos Complete setup and configuration tutorials Advanced customization techniques for different industries Email marketing best practices for automated newsletters Troubleshooting common integration issues Scaling strategies for growing subscriber lists ü§ù Professional Community LinkedIn: https://www.linkedin.com/in/yaronbeen/ Connect for ongoing newsletter automation support Share your news curation success stories Access exclusive templates and workflow variations Join discussions about content automation trends üí¨ Support Request Best Practices Include in your support message: Your target audience and newsletter focus Current subscriber count and growth goals Specific news categories or geographic regions of interest Any technical errors or integration challenges Current content creation workflow and pain points",1645,2025-05-25 05:58:27.949000+00:00,True,5
2305,Convert PPTX to PDF using ConvertAPI,"Who is this for? For developers and organizations that need to convert PPTX files to PDF. What problem is this workflow solving? The file format conversion problem. What this workflow does Downloads the PPTX file from the web. Converts the PPTX file to PDF. Stores the PDF file in the local file system. How to customize this workflow to your needs Open the HTTP Request node. Adjust the URL parameter (all endpoints can be found here). Use your API Token for authentication. Pass the token in the Authorization header as a Bearer token. You can manage your API Tokens in the User panel ‚Üí Authentication. Optionally, additional Body Parameters can be added for the converter.",1625,2024-07-02 11:48:16.378000+00:00,False,1
2570,Send a welcome private message to your new BlueSky followers,"Who is this for? BlueSky users who are looking to send a ""welcome message"" to their new followers as a private message. What this workflow does This worflow will check for new followers on BlueSky every 60 minutes and send a private message to the new ones. Setup You need to create a BlueSky app password with private messages access. Fill your credentials and the message text on the corresponding nodes (see sticky notes). Manually run once the `Save followers to file` node to generate your initial followers list. Enable the workflow How to customize this workflow to your needs You can adjust the check frecuency, but be careful to avoid hitting the 100 createSession per day rate limit Feedback or comments You can leave comments, feedback or improvements about this workflow on the n8n forums",1593,2024-11-25 10:13:44.297000+00:00,False,2
2371,Request and Receive Zigbee backup from zigbee2mqtt and save it via SFTP,"A single workflow with 2 flows/paths that combine to handle the backup sequence for Zigbee device configuration from HomeAssistant / zigbee2mqtt. This provides a way to automate a periodic capture of Zigbee coordinators and device pairings to speed the recovery process when/if the HomeAssistant instance needs to be rebuilt. Setting up similar automation without n8n (e.g. shell scripts and system timers) is consiterably more challenging. n8n makes it easy and this template should remove any other excuse not to do it. Flow 1 Triggered by Cron/Timer set whatever interval for backups sends mqtt message to request zigbee2mqtt backup (via separate message) Flow 2 Triggered by zigbee2mqtt backup message Extracts zip file from the message and stores somewhere, with a date-stamp in the filename, via sftp Setup Create a MQTT connection named ""MQTT Account"" with the appropriate protocol (mqtt), host, port (1883), username, and password Create an sftp connection named ""SFTP Zigbee Backups"" with the appropriate host, port (22), username, and password or key. Reference This article describes the mqtt parts.",1578,2024-08-03 00:26:43.219000+00:00,False,3
4596,QR Code Generator via Webhook,"This n8n template allows you to instantly generate QR codes from any text or URL by simply sending a webhook request. It's a versatile tool for creating dynamic QR codes for various purposes, from marketing campaigns to event registrations, directly integrated into your automated workflows. üîß How it works Receive Data Webhook: This node acts as the entry point for the workflow. It listens for incoming POST requests and expects a JSON body with a data property containing the text or URL you want to encode into the QR code. Generate QR Code: This node makes an HTTP GET request to the QR Server API (api.qrserver.com) to generate the QR code image. The content from your webhook is passed as the data parameter to the API. Respond with QR Code: This node sends the response from the QR Server API back to the service that initiated the webhook. The QR Server API directly returns the image data, so your webhook response will be the QR code image itself. üë§ Who is it for? This workflow is ideal for: Marketers: Generate QR codes for product links, event registrations, or promotional materials on the fly. Developers: Integrate QR code generation into applications, websites, or internal tools. Event Organizers: Create dynamic QR codes for ticketing, information access, or check-ins. Businesses: Streamline processes requiring physical-to-digital transitions, like menu access or contact sharing. Automation Enthusiasts: Add QR code generation capabilities to any workflow. üìë Data Structure When you trigger the webhook, send a POST request with a JSON body structured as follows: { ""data"": ""https://www.yourwebsite.com/your-specific-page-or-text-to-encode"" } The workflow will return the QR code image directly in the response. ‚öôÔ∏è Setup Instructions Import Workflow: In your n8n editor, click ""Import from JSON"" and paste the provided workflow JSON. Configure Webhook Path: Double-click the Receive Data Webhook node. In the 'Path' field, set a unique and descriptive path (e.g., /generate-qr). Customize QR Code (Optional): Double-click the Generate QR Code node. You can adjust the size parameter in the URL (e.g., size=200x200 for a larger QR code) or add other parameters supported by the QR Server API (e.g., bgcolor, color, qzone). Activate Workflow: Save and activate the workflow. üìù Tips Handling the Image Output: Since the QR Server API directly returns the image, the webhook response will be the image data. Depending on your use case, you might want to: Save to File/Cloud: Insert a node (e.g., Write Binary File, Amazon S3, Google Drive) after Generate QR Code to save the image to a file system or cloud storage. Embed in HTML/Email: If you're building an HTML response or sending an email, you might need to convert the image data to a Base64 string or provide a URL to a saved image. Error Handling: Enhance workflow robustness by adding an Error Trigger node. This allows you to catch any issues during QR code generation and set up notifications or logging. Dynamic Size/Color: You can extend the Receive Data Webhook to accept parameters for size, color, or bgcolor in the incoming JSON. Then, dynamically pass these to the url of the Generate QR Code node to create highly customizable QR codes. Input Validation: For more advanced use cases, you could add a Function node after the webhook to validate the incoming data to ensure it's in a valid format (e.g., a URL).",1573,2025-06-02 14:37:16.047000+00:00,False,1
2160,üö® Report n8n workflow errors directly to your email,Use case Error workflows are an important part of running workflows in production. Get alerts for errors directly in your inbox. How to setup Add your Gmail creds Add your target email Add this error workflow to other workflows docs here,1570,2024-03-01 15:38:07.288000+00:00,False,1
2406,Telegram User Registration Workflow,"Telegram User Registration module Offers an efficient way to manage new and returning users in your Telegram bot workflow. It checks user data against a Google Sheets database, saving essential user information and ensuring personalized interactions. Key Features: User Lookup: Searches for users in a Google Sheets database based on their unique Telegram ID. New User Handling: Automatically registers new users, capturing details such as first name, last name, username, and language code. Returning User Recognition: Detects when an existing user returns and updates their status. Data Storage: Safely stores user information in a structured format, with fields for status and date of registration. Personalized Greetings: Delivers customized welcome messages for both new and returning users, promoting engagement. Setup Instructions: Copy this Telegram User Registration Workflow Follow the instructions inside: - Use Google Sheet template - Set up your credentials - Use example data to test workflow Customization: Adjust the status and messages for users based on your bot's needs. Connecting to the bot Workflow: Copy my Telegram bot starter template Copy Workflow ID of your Telegram User Registration Workflow Find the Register module in Telegram bot starter template and paste your Workflow ID Now the user's data is entered into the Register Workflow. This module provides a scalable foundation for managing user registration, whether your bot is for meal planning, customer support, or other interactive services. My easy to set up Telegram bot modules: üèÅ Telegram bot starter template üìù Telegram callback menu (soon) Please reach out to Victor if you need further assistance with you n8n workflows and automations!",1569,2024-09-09 20:58:34.758000+00:00,False,2
2407,Store new orders to Airtable and summarize daily orders through email,"What this template does This workflow will collect order data as it is produced, then send a summary email of all orders at the end of every day, formatted in a table. It receives new orders via webhook and stores in Airtable. At 7PM every day, it sends a summary email with the day's orders in a HTML table Setup: Instructions Video Create a new table in Airtable and give it a field time with type date, orderID with type number, and orderPrice also with type number. Create a new access token if you haven't already at https://airtable.com/create/tokens/new. Make sure to give the token the scopes data.records:read, data.records:write, schema.bases:read and access to whichever table you choose to store the orders. A pop-up window appears with the token. Use this token to make Create New Credential > Access Token for Airtable in the Store Order and Airtable Get Today's Orders nodes. Create access credentials for your Gmail as described here: https://developers.google.com/workspace/guides/create-credentials. Use the credentials from your client_secret.json in the Send to Gmail node. In the Store Order node, change Base and Table to the database and table in your Airtable account you wish to use to store orders. Make sure to use these same values in the Airtable Get Today's Orders node. Every time an order is created in your system, send a POST request to Webhook from your order software. Each request must contain a single order containing fields 'orderID' and 'orderPrice' (or, edit Set Order Fields to select which incoming fields you wish to save) Change the schedule time for sending email from Everyday at 7PM to whichever time you choose. Test: Activate the workflow. From the node Webhook, copy Production URL Send the following CURL request to the URL given to you: curl -X POST -H ""Content-Type: application/json"" -d '{""orderID"": 12345, ""orderPrice"": 99.99}' YOUR_URL_HERE It should say Node executed successfully. Now check your Airtable and confirm the order was stored in the right place.",1568,2024-09-11 08:45:30.925000+00:00,False,4
2191,Verifying Email deliverability using google sheets and Effibotics API,"This workflow helps marketers verify and update data using EffiBotics Email Verifier API. Copy and create a list with emails as on this one https://docs.google.com/spreadsheets/d/1rzuojNGTaBvaUEON6cakQRDva3ueGg5kNu9v12aaSP4/edit#gid=0 The trigger checks for any updates in the number of rows that are present in a sheet and updates the verified emails on Google sheets Once you update a new cell, the new data is read, and the email is checked for its validity before. The results are then updated in real-time on the sheet. Happy Emailing!",1567,2024-03-24 11:47:52.749000+00:00,False,2
2284,Provide Real-Time Updates for Notion Databases via Webhooks with Supabase,"Purpose This enables webhooks for nearly realtime updates (every 5 seconds) from Notion Databases. Problem Notion does not offer webhooks. Even worse, the ‚ÄúLast edited time‚Äù property, we could use for polling, only updates every minute. This gives us a polling interval only as low as 2 minutes and we still need to implement a comparing mechanism to detect changes. Solution This workflow caches states in between while doing efficient polling & comparing. It brings down the update latency from 2 minutes to 5 seconds and also provides the output of the changes only. Demo How it works Database Pages are frequently polled while filtered by a last modified time stamp for more efficiency Retrieved pages get compared with previously cached versions in Supabase Only new and changed pages are pushed to a registered webhook Setup Create a new project in Supabase and import the DB schema (provided through Gumroad) Add a ""Last edited time"" property to your Notion Database, if it has none yet Define the dynamically generated settings_id from the settings table (Supabase) in the Globals node Define the Notion Database URL in the Globals node Define your custom Webhook URL in the last node where the results should be pushed to It is recommended to call this workflow using this template to prevent simultaneous workflow executions Set the Schedule Trigger to every 5 seconds or less frequent More detailed instructions provided within the workflow file and the illustrated instructions provided during the download Example output [ { ""action"": ""changed"", ""changes"": { ""property_modified_at"": ""2024-06-04T17:59:00.000Z"", ""property_priority"": ""important"" }, ""data"": { ""id"": ""ba761e03-7d6d-44c2-8e8d-c8a4fb930d0f"", ""name"": ""Try out n8n"", ""url"": ""https://www.notion.so/Try-out-n8n-ba761e037d6d44c28e8dc8a4fb930d0f"", ""property_todoist_id"": """", ""property_id"": ""ba761e037d6d44c28e8dc8a4fb930d0f"", ""property_modified_at"": ""2024-06-04T17:59:00.000Z"", ""property_status"": ""Backlog"", ""property_priority"": ""important"", ""property_due"": { ""start"": ""2024-06-05"", ""end"": null, ""time_zone"": null }, ""property_focus"": false, ""property_name"": ""Try out n8n"" }, ""updated_at"": ""2024-06-04T17:59:42.144+00:00"" } ]",1551,2024-06-08 15:50:50.481000+00:00,False,4
2353,Send Slack notifications when a new release is published for public Github repos,"This workflow checks a configured list of Github repositories daily to see if a new release has been published. How it works: Workflow has a daily trigger RepoConfig node is a JSON array that defines a list of repositories to check releases for For each of the configured repos it fetches the latest release If the release was published within the last 24 hours it is output The release is sent as a Slack message showing the repo name, release name and link Setup Update the JSON in the RepoConfig node to the Github repos you wish to get notifications for Setup your Slack connection (or replace with your choice of notification)",1525,2024-07-24 09:20:39.536000+00:00,False,3
2387,Send file to kindle with Telegram Bot and Outlook,"Workflow Overview This workflow automates the process of forwarding e-book files to a Kindle device using a Telegram bot and Outlook email. Setup Steps: Telegram Bot Setup: Create a Telegram bot via BotFather and configure its credentials in the workflow. Outlook Email Configuration: Set up your Outlook email credentials. (Currently, only Outlook is supported, but you can modify the workflow to support other email providers.) Amazon Kindle Email Setup: Find your Kindle device's email address from your Amazon account. This will be the recipient address for the e-books. Allow Email Sending to Kindle: Ensure your Amazon account is configured to allow emails from your Outlook address to send files to your Kindle. Workflow Explanation: The workflow begins with a Telegram bot trigger node that listens for new chat messages. When a new message is received, the workflow checks if the message contains a file attachment. If no file is detected, the bot will send a warning reply to the user in the chat. If a file is found, it will be renamed to ensure it appears correctly on the Kindle device when sent. The workflow then composes an email with the file attached and sends it to the Kindle's receiving address. If the email is sent successfully, the bot will notify the user with a success message in the chat. Only Amazon-supported file types will be accepted by Kindle. If sending fails, you will receive a notification email from Amazon in your Outlook inbox. In case of delivery issues, retry sending the file as network issues may occasionally interfere with the process.",1525,2024-08-23 03:03:13.336000+00:00,False,3
2590,Daily GitHub Release notification by Email,"Automating daily notifications of the latest releases from a GitHub repository. This template is ideal for developers and project managers looking to stay up-to-date with software updates. How it Works: Daily Trigger: The workflow initiates daily using the Schedule Trigger node. Fetch Repository Data: The HTTP Request node retrieves the latest release details from the specified GitHub repository. Check if new: The IF node check if the release was done in the last 24 hours. Split Content: The Split Out node processes the JSON response to extract and structure relevant data. Convert Markdown: The Markdown node converts release notes from Markdown format to HTML, making them ready to use in emails. Send a notification by email Key Features: Simple to customize by modifying the GitHub URL. Automatically processes and formats release notes for better readability. Modular design, allowing integration with other workflows like Gmail or Slack notifications. Setup Steps: Modify Repository URL: Update the Sticky Note node with the URL of the repository you want to monitor. Modify SMTP details: Update the Send Email node with your SMTP details.",1498,2024-11-30 13:14:35.977000+00:00,False,2
2657,Vector Database as a Big Data Analysis Tool for AI Agents [2/2 KNN],"Vector Database as a Big Data Analysis Tool for AI Agents Workflows from the webinar ""Build production-ready AI Agents with Qdrant and n8n"". This series of workflows shows how to build big data analysis tools for production-ready AI agents with the help of vector databases. These pipelines are adaptable to any dataset of images, hence, many production use cases. Uploading (image) datasets to Qdrant Set up meta-variables for anomaly detection in Qdrant Anomaly detection tool KNN classifier tool For anomaly detection The first pipeline to upload an image dataset to Qdrant. The second pipeline is to set up cluster (class) centres & cluster (class) threshold scores needed for anomaly detection. The third is the anomaly detection tool, which takes any image as input and uses all preparatory work done with Qdrant to detect if it's an anomaly to the uploaded dataset. For KNN (k nearest neighbours) classification The first pipeline to upload an image dataset to Qdrant. This pipeline is the KNN classifier tool, which takes any image as input and classifies it on the uploaded to Qdrant dataset. To recreate both You'll have to upload crops and lands datasets from Kaggle to your own Google Storage bucket, and re-create APIs/connections to Qdrant Cloud (you can use Free Tier cluster), Voyage AI API & Google Cloud Storage. [This workflow] KNN classification tool This tool takes any image URL, and as output, it returns a class of the object on the image based on the image uploaded to the Qdrant dataset (lands). An image URL is received via the Execute Workflow Trigger, which is then sent to the Voyage AI Multimodal Embeddings API to fetch its embedding. The image's embedding vector is then used to query Qdrant, returning a set of X similar images with pre-labeled classes. Majority voting is done for classes of neighbouring images. A loop is used to resolve scenarios where there is a tie in Majority Voting, and we increase the number of neighbours to retrieve. When the loop finally resolves, the identified class is returned to the calling workflow.",1487,2024-12-19 09:50:11.092000+00:00,True,2
4512,"Automate Microsoft Teams Meeting Analysis with GPT-4.1, Outlook & Mem.ai","Automate Microsoft Teams Meeting Analysis with GPT-4.1, Outlook & Mem.ai Watch the YouTube video to get started Follow along with the blog post Template Overview This advanced n8n template automates Microsoft Teams meeting analysis, knowledge base creation, and email drafting directly from meeting recordings and transcripts. It provides end-to-end automation for the following use cases: Meeting Analysis Connects to Microsoft Graph to retrieve meeting recordings, chat messages, and transcripts. Analyses meeting content using AI models. Extracts key points, action items, questions, and themes from meetings automatically. AI-Powered Web App Automatically creates a detailed report from past meetings. Stores meeting summaries, metadata, and insights into an easy-to-browse frontend. New meetings are added dynamically without manual work. ** Knowledge Base Indexing (via Mem.ai)** Uploads extracted meeting data into a structured knowledge base. Supports categorisation, search and chat functionality across meetings. Draft Follow-Up Emails** Draft personalised follow-up emails for meeting participants. Email drafts include: Meeting summary Key decisions Action items Emails can be sent manually with a human in the loop context via Microsoft Outlook integration. Core Components Microsoft Graph API for accessing meetings, chats, files, and user profiles. SharePoint API for file storage and search. n8n Webhooks to trigger processes dynamically. Generic OAuth2 authentication for seamless Microsoft access. JavaScript Code Nodes for flexible, intelligent parsing and structuring of meeting data. AI/LLM nodes for summarisation and content creation. Designed to be scalable, modular, and easily customisable for different organisation sizes and industries. üõ°Ô∏è Note Failure to correctly configure Azure permissions will prevent the template from functioning. Ensure admin approval is fully completed during setup. Important Prerequisites ‚ö†Ô∏è Administrator consent is required This template requires a Microsoft 365 Global Administrator or Application Administrator to grant admin consent to a set of Microsoft Graph and SharePoint API scopes. Basic Azure App Registration knowledge is required: You need to set up an Azure App Registration manually, configure OAuth2 authentication, and assign specific API permissions. A setup guide is included, but familiarity with: App registrations API permissions Client secrets OAuth2 flows is highly recommended. Knowledge of Postgres is required The template includes a SQL script to create the required Postgres table (see blog post). You are responsible for hosting your own database (You can use Supabase with the Postgres connection string).",1480,2025-05-30 12:43:51.349000+00:00,True,7
4652,"AI Agent Creates Content to Be Picked by ChatGPT, Gemini, Google","üß† Who is this for? Marketing teams, content creators, solopreneurs, and agencies who want to generate emotionally-resonant, SEO-optimized content tailored to audience psychology and buyer journey stages ‚Äî and get picked up by AI discovery engines like ChatGPT, Gemini, and Perplexity. How it works: ‚úÖ Decodes why people buy (using buyer psychology) ‚úÖ Creates SEO + emotionally resonant content for 4 formats: ‚Üí Blog Posts, Newsletters, Landing Pages, Social Media ‚úÖ Structures the content to be picked up by ChatGPT, Gemini, Perplexity & Google ‚úÖ Automatically routes it to Google Sheets, Gmail, or even WordPress This isn‚Äôt just about writing better content ‚Äî it‚Äôs about getting seen by the tools that shape the internet. How long does it take to set-up: 30 Mins",1462,2025-06-04 15:38:35.056000+00:00,True,8
2560,Send daily translated Calvin and Hobbes Comics to Discord,"How it works Automates the retrieval of Calvin and Hobbes daily comics. Extracts the comic image URL from the website. Translates comic dialogues to English and Korean. Posts the comic and translations to Discord daily. Set up steps Estimated setup time: ~10-15 minutes. Use a Schedule Trigger to automate the workflow at 9 AM daily. Add nodes for parameter setup, HTTP request, data extraction, and integration with Discord. Add detailed notes to each node in the workflow for easy understanding.",1444,2024-11-20 13:01:10.683000+00:00,False,5
2501,Replace Images in Google Docs Documents and Download as PDF/Docx,"Use Case Automate image replacement in Google Docs: You need to update document images dynamically You want to create multiple versions of a template with different images You need to batch process document images from a URL database You want to generate shareable documents with custom images What this Workflow Does The workflow automates image replacement in Google Docs: Accepts image URLs from your database Finds and replaces images in template documents Creates new document copies with updated images Optionally converts to PDF and makes documents shareable Setup Connect your image URL database (column name must be ""url"") Set up Google Docs OAuth 2 API credentials Optional: Create a template document in Google Drive with placeholder images Optional: Configure Google Drive authentication for additional features How to Adjust it to Your Needs Remove template copying for single document processing Adjust image ID selection for documents with multiple images Configure sharing settings and download formats Customize file naming and storage location More templates and n8n workflows &gt;&gt;&gt; @simonscrapes",1442,2024-10-26 10:26:29.662000+00:00,False,2
2291,"Create Leads in SuiteCRM, synchronize with Brevo and notify in NextCloud","Who is this template for? This workflow template is designed for Marketing and pre-Sales to get prospects from a form like Tally, decline data in the famous opensource CRM (SuiteCRM), synchronize contact in Brevo with linking the id from CRM, and then notify in NextCloud. Bonus : validate email with ++CaptainVerify++ and notify in NextCloud depending on response How it works For each submission in the form, a webhook is triggered. A check of the email is done with CaptainVerify. Depending on the response, and if it is ok, then a Lead is created in SuiteCRM. Else, a message in your selected discussion is sent. As the lead has been created, we can create a contact in Brevo (for future campain), ank link this contact with the lead_id from the CRM in a dedicated field. Finaly, a message in your selected discussion in NextCloud informs you about the lead. Set up instructions Complete the Set up credentials step when you first open the workflow. You'll need a CaptainVerify account (Api Key), a dedicated SuiteCRM user with Oauth, a Brevo account (Api Key) and a Nextcloud account. Set up the Webhook in the form's tool of your choice (why not Tally ?). Set each node with the explanations in sticky Notes. Enjoy ! Template was created in n8n v1.44.1",1432,2024-06-12 13:57:07.354000+00:00,False,2
2189,Assistant for Hubspot Chat using OpenAi and Airtable,"This workflow will allow you to use OpenAI Assistant API together with a chatting platform. This version is configured to work with Hubspot, however, the Hubspot modules can be replaced by other platform and it will work similarly. Prerequisites: Create a Hubspot Chat (Live chat available on free plan) or Chatflow (paid hubspot only) and configure it to send all replies toward an n8n webhook (you need to create a custom app for that. I will create a separate article on how to do it, meanwhile, feel free to message me if you need support. Setup: Create a OpenAI Assistant, define its functionality and functions Update the Hubspot modules with the Hubspot API Key Update the OpenAI modules with OpenAI API Key Create an Airtable or any other database where you keep a reference between the thread id in Hubspot and Assistant API If you need help deploying this solution don't hesitate to email me or schedule a call here.",1398,2024-03-23 18:31:45.702000+00:00,True,4
4340,"Generate Qualified Leads from LinkedIn with Apify, GPT- 4, and Airtable","Who is this for? This workflow is perfect for sales teams, business development professionals, recruitment agencies, and fractional CFO service providers who need to identify and qualify companies actively hiring. Whether you're prospecting for new clients, building a database of potential customers, or researching market opportunities, this automated solution saves hours of manual research while delivering high-quality, AI-analyzed leads. What problem is this workflow solving? Finding qualified prospects in the finance sector is time-consuming and often inefficient. Traditional methods involve: Manually browsing LinkedIn job postings for hours Difficulty distinguishing between genuine opportunities and recruitment spam Inconsistent lead categorization and qualification Risk of contacting the same companies multiple times Lack of structured data for sales team follow-up This workflow automates the entire lead generation process, from data collection to AI-powered qualification, ensuring you focus only on the most promising opportunities. What this workflow does This comprehensive lead generation system performs six key functions: Automated LinkedIn Job Scraping: Uses Apify's reliable LinkedIn Jobs Scraper to extract detailed job postings for finance positions, including company information, job descriptions, and contact details. Smart Data Processing: Removes duplicates, filters companies by size, and structures data for consistent analysis across all leads. Intelligent Lead Categorization: Compares new leads against your existing database to optimize processing and avoid duplicate work. AI-Powered Qualification: Leverages OpenAI's GPT-4 Mini to analyze each lead and determine: Company Category: Consumer companies, Fractional CFO services, Recruiting agencies, or Other Finance Role Validation: Confirms the position is genuinely finance-related Seniority Level: Entry, Mid, Senior, Director, or C-Level classification Job Summary: Concise description for quick sales team review Automated Database Management: Stores qualified leads in Airtable with comprehensive profiles, preventing duplicates while maintaining data integrity. Lead Scoring & Routing: Prioritizes leads based on processing status and qualification results for efficient sales team follow-up. Setup Prerequisites You'll need accounts for three services: Airtable** (Free tier supported) - For lead storage and management Apify** (14-day free trial available) - For LinkedIn job scraping OpenAI** (Pay-per-use) - For AI-powered lead analysis Step 1: Create Required Credentials Apify API Credential Sign up for an Apify account at apify.com Navigate to Settings ‚Üí Integrations ‚Üí API tokens Create a new API token In n8n, create a new Apify API credential with your token OpenAI API Credential Create an account at platform.openai.com Generate an API key in the API section In n8n, create a new OpenAI credential with your key Airtable Personal Access Token Go to airtable.com/create/tokens Create a personal access token with the following scopes: data.records:read data.records:write schema.bases:read In n8n, create a new Airtable Personal Access Token credential Step 2: Set Up Airtable Base Create a new Airtable base with the following structure: Table Name: Qualified Leads Required Fields: Company Name (Single line text) Job Title (Single line text) Is Finance Job (Checkbox) Seniority Level (Single select: Entry, Mid, Senior, Director, C-Level) Company Category (Single select: Consumer, Recruiting, Fractional CFO, Other) Job Summary (Long text) Company LinkedIn (URL) Job Link (URL) Posted Date (Date) Location (Single line text) Industry (Single line text) Company Employees (Number) Step 3: Configure the Workflow Import the Workflow: Copy the JSON and import it into your n8n instance Update Credentials: Replace placeholder credential IDs with your actual credential IDs in: ""Scrape LinkedIn Jobs"" node (Apify credential) ""OpenAI GPT-4 Mini"" node (OpenAI credential) ""Save to Airtable"" and ""Get Existing Leads"" nodes (Airtable credential) Configure Airtable Connection: Update the base ID and table ID in both Airtable nodes Set Search Parameters: In the ""Edit Variables"" node, configure: linkedinUrls: Your target LinkedIn job search URLs maxEmployees: Maximum company size filter (default: 200) batchSize: Processing batch size for API efficiency (default: 5) Step 4: Test the Workflow Start with a small test by setting count: 50 in the HTTP Request node Use a specific LinkedIn job search URL (e.g., ""CFO jobs in New York"") Execute the workflow manually and verify results in your Airtable base Review the AI categorization accuracy and adjust prompts if needed How to customize this workflow to your needs Targeting Different Roles Modify the LinkedIn search URLs in the ""Edit Variables"" node to target different positions: ""https://www.linkedin.com/jobs/search/?keywords=Controller"" ""https://www.linkedin.com/jobs/search/?keywords=Finance%20Director"" ""https://www.linkedin.com/jobs/search/?keywords=VP%20Finance"" Adjusting Company Size Filters Change the maxEmployees parameter to focus on different company segments: Startups: 1-50 employees SMBs: 51-500 employees Enterprise: 500+ employees Customizing AI Analysis Enhance the GPT-4 prompt in the ""AI Lead Analyzer"" node to include: Industry-specific criteria Geographic preferences Technology stack requirements Company growth stage indicators Integration Options Extend the workflow by adding: Slack notifications** for new qualified leads Email alerts** for high-priority prospects CRM integration** (Salesforce, HubSpot, Pipedrive) Lead enrichment** with additional data sources Scheduling Automation Set up the workflow to run automatically: Daily**: For active prospecting campaigns Weekly**: For ongoing market research Monthly**: For periodic database updates Performance & Cost Optimization API Efficiency**: The workflow processes leads in batches to optimize API usage Smart Deduplication**: Avoids re-processing existing leads to reduce costs Configurable Limits**: Adjust batch sizes and employee count filters based on your needs Expected Costs**: Approximately $0.05-$0.20 per 100 analysed leads (OpenAI costs) Troubleshooting Common Issues: Rate Limiting**: Increase delays between API calls if you encounter rate limits Data Quality**: Review LinkedIn search URLs for relevance to your target market AI Accuracy**: Adjust prompts if categorisation doesn't match your criteria Airtable Errors**: Verify field names match exactly between workflow and base structure Support Resources: Apify LinkedIn Scraper Documentation OpenAI API Documentation Airtable API Reference Transform your lead generation process with this powerful, AI-driven workflow that delivers qualified prospects ready for immediate outreach.",1396,2025-05-23 09:34:15.179000+00:00,True,6
2502,Monthly Spotify Track Archiving and Playlist Classification,"Monthly Spotify Track Archiving and Playlist Classification This n8n workflow allows you to automatically archive your monthly Spotify liked tracks in a Google Sheet, along with playlist details and descriptions. Based on this data, Claude 3.5 is used to classify each track into multiple playlists and add them in bulk. Who is this template for? This workflow template is perfect for Spotify users who want to systematically archive their listening history and organize their tracks into custom playlists. What problem does this workflow solve? It automates the monthly process of tracking, storing, and categorizing Spotify tracks into relevant playlists, helping users maintain well-organized music collections and keep a historical record of their listening habits. Workflow Overview Trigger Options**: Can be initiated manually or on a set schedule. Spotify Playlists Retrieval**: Fetches the current playlists and filters them by owner. Track Details Collection**: Retrieves information such as track ID and popularity from the user‚Äôs library. Audio Features Fetching**: Uses Spotify's API to get audio features for each track. Data Merging**: Combines track information with their audio features. Duplicate Checking**: Filters out tracks that have already been logged in Google Sheets. Data Logging**: Archives new tracks into a Google Sheet. AI Classification**: Uses an AI model to classify tracks into suitable playlists. Playlist Updates**: Adds classified tracks to the corresponding playlists. Setup Instructions Credentials Setup: Make sure you have valid Spotify OAuth2 and Google Sheets access credentials. Trigger Configuration: Choose between manual or scheduled triggers to start the workflow. Google Sheets Preparation: Set up a Google Sheet with the necessary structure for logging track details. Spotify Playlists Setup: Have a diverse range of playlists and exhaustive description (see example) ready to accommodate different music genres and moods. Customization Options Adjust Playlist Conditions**: Modify the AI model‚Äôs classification criteria to align with your personal music preferences. Enhance Track Analysis**: Incorporate additional audio features or external data sources for more refined track categorization. Personalize Data Logging**: Customize which track attributes to log in Google Sheets based on your archival preferences. Configure Scheduling**: Set a preferred schedule for periodic track archiving, e.g., monthly or weekly. Cost Estimate For 300 tracks, the token usage amounts to approximately 60,000 tokens (58,000 for input and 2,000 for completion), costing around 20 cents with Claude 3.5 Sonnet (as of October 2024). Playlists' Description Examples | Playlist Name | Playlist Description | |-------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------| | Classique | Indulge in the timeless beauty of classical music with this refined playlist. From baroque to romantic periods, this collection showcases renowned compositions. | | Poi | Find your flow with this dynamic playlist tailored for poi, staff, and ball juggling. Featuring rhythmic tracks that complement your movements. | | Pro Sound | Boost your productivity and focus with this carefully selected mix of concentration-enhancing music. Ideal for work or study sessions. | | ChillySleep | Drift off to dreamland with this soothing playlist of sleep-inducing tracks. Gentle melodies and ambient sounds create a peaceful atmosphere for restful sleep. | | To Sing | Warm up your vocal cords and sing your heart out with karaoke-friendly tracks. Featuring popular songs, perfect for solo performances or group sing-alongs. | | 1990s | Relive the diverse musical landscape of the 90s with this eclectic mix. From grunge to pop, hip-hop to electronic, this playlist showcases defining genres. | | 1980s | Take a nostalgic trip back to the era of big hair and neon with this 80s playlist. Packed with iconic hits and forgotten gems, capturing the energy of the decade.| | Groove Up | Elevate your mood and energy with this upbeat playlist. Featuring a mix of feel-good tracks across various genres to lift your spirits and get you moving. | | Reggae & Dub | Relax and unwind with the laid-back vibes of reggae and dub. This playlist combines classic reggae tunes with deep, spacious dub tracks for a chilled-out vibe. | | Psytrance | Embark on a mind-bending journey with this collection of psychedelic trance tracks. Ideal for late-night dance sessions or intense focus. | | Cumbia | Sway to the infectious rhythms of Cumbia with this lively playlist. Blending traditional Latin American sounds with modern interpretations for a danceable mix. | | Funky Groove | Get your body moving with this collection of funk and disco tracks. Featuring irresistible basslines and catchy rhythms, perfect for dance parties. | | French Chanson | Experience the romance and charm of France with this mix of classic and modern French songs, capturing the essence of French musical culture. | | Workout Motivation | Push your limits and power through your exercise routine with this high-energy playlist. From warm-up to cool-down, these tracks will keep you motivated. | | Cinematic Instrumentals | Immerse yourself in a world of atmospheric sounds with this collection of cinematic instrumental tracks, perfect for focus, relaxation, or contemplation. |",1387,2024-10-27 17:36:47.297000+00:00,True,7
2779,Remove Personally Identifiable Information (PII) from CSV Files with OpenAI,"What this workflow does Monitors Google Drive: The workflow triggers whenever a new CSV file is uploaded. Uses AI to Identify PII Columns: The OpenAI node analyzes the data and identifies PII-containing columns (e.g., name, email, phone). Removes PII: The workflow filters out these columns from the dataset. Uploads Cleaned File: The sanitized file is renamed and re-uploaded to Google Drive, ensuring the original data remains intact. How to customize this workflow to your needs Adjust PII Identification: Modify the prompt in the OpenAI node to align with your specific data compliance requirements. Include/Exclude File Types: Adjust the Google Drive Trigger settings to monitor specific file types (e.g., CSV only). Output Destination: Change the folder in Google Drive where the sanitized file is uploaded. Setup Prerequisites: A Google Drive account. An OpenAI API key. Workflow Configuration: Configure the Google Drive Trigger to monitor a folder for new files. Configure the OpenAI Node to connect with your API Set the Google Drive Upload folder to a different location than the Trigger folder to prevent workflow loops.",1384,2025-01-23 12:56:20.092000+00:00,True,3
2447,Time Tracking with Notion and iOS shortcut,"Who might benfit from this workflow? Do you have to record your working hours yourself? Then this n8n workflow in combination with an iOS shortcut will definitely help you. Once set up, you can use a shortcut, which can be stored as an app icon on your home screen, to record the start, end and duration of your break. How it works Once setup you can tap the iOS shortcut on your iPhone. You will see a menu containing three options: ""Track Start"", ""Track Break"" and ""Track End"". After time is tracked iOS will display you a notification about the successful operation. How to set it up Copy the notion database to your notion workspace (Top right corner). Copy the n8n workflow to your n8n workspace In the notion nodes in the n8n workflow, add your notion credentials and select the copied notion database. Download the iOS Shortcut from our documentation page Edit the shortcut and paste the url of your n8n Webhook trigger node to the first ""Text"" node of the iOS shortcut flow. It is a best practice to use authentication. You can do so by adding ""Header"" auth to the webhook node and to the shrotcut. You need help implementing this or any other n8n workflow? Feel free to contact me via LinkedIn or my business website. You want to start using n8n? Use this link to register for n8n (This is an affiliate link)",1372,2024-10-02 12:50:25.041000+00:00,False,1
2225,Verify mailing address deliverability of new contacts in Groundhogg Using Lob,"This n8n workflow verifies the deliverability of mailing addresses stored in Groundhogg CRM by integrating with Lob‚Äôs address verification service. Who is this for? This template is designed for Groundhogg CRM users who need to ensure the accuracy of mailing addresses stored in their CRM systems. What problem is this workflow solving? / Use Case This workflow addresses the challenge of maintaining accurate mailing addresses in CRM databases by verifying the deliverability of addresses. What this workflow does A new contact is created in Groundhogg CRM Webhook sent to n8n Verify if the address is deliverable via LOB Report back to Groundhogg CRM Set Up Steps Watch this setup video: https://www.youtube.com/watch?v=nrV0P0Yz8FI Takes 10-30 minutes to set up Accounts Needed: Groundhogg CRM LOB Account (https://www.lob.com $0.00/mo 300 US addresses Verifications) n8n Before using this template, ensure you have API keys for your Groundhogg CRM app and Lob. Set up authentication for both services within n8n. How to customize this workflow to your needs You can customize this workflow by adjusting the trigger settings to match Groundhogg CRM‚Äôs workflow configuration. Additionally, you can modify the actions taken based on the deliverability outcome, such as updating custom fields or sending notifications.",1355,2024-04-10 19:22:33.722000+00:00,False,1
2798,Create Threads on Bluesky,"Create engaging, structured threads on Bluesky with precise control over post timing and visibility. This workflow helps content creators and social media managers schedule and publish threaded posts that maintain proper connections and formatting, ensuring your content appears exactly as intended. How it works Creates an initial visible post that starts your thread Adds a series of hidden reply posts that form the body of your thread Maintains proper parent-child relationships between posts to ensure correct threading Enforces timing delays between posts to prevent rate limiting Concludes with two visible posts at the end of your thread The result is a clean, professional-looking thread where only the first and last two posts are immediately visible to your followers, encouraging engagement while maintaining a clean profile view. Set up steps (10-30 minutes) Create a Bluesky account Enter your Bluesky handle and app password in the ""Set Bluesky Credentials"" node Customize the post text in the Code nodes to match your content: Initial visible post Hidden reply posts Final visible posts Adjust the scheduling in the ""Run Daily at 9 AM"" node to match your preferred posting time Suggested enhancements Add error handling with retry logic for API failures Add input validation for post length and credential format Include error notifications via email or Slack Add data persistence to track successful posts and resume failed threads Make timing delays configurable with exponential backoff Add monitoring for rate limits and API quotas For Social Media Managers who want: Control over post visibility and timing Automated posting of long-form content Professional-looking content presentation",1354,2025-01-25 10:01:11.632000+00:00,False,2
2543,Use a Custom URL for Recurring Zoom Meetings,"Use case Instead of this: https://us06web.zoom.us/j/83456429326?pwd=1hVesbyHCsOfstyVU3z4CR6D46A8K.1 share this: mydomain.com/meet-me Do you ever wish you had one, simple URL that you can share with people to hop on a Zoom meeting? üòÉ You could waste time: üëéüëé creating a recurring Zoom meeting üò´ saving the link somewhere üòµ‚Äçüí´ finding it, copying it each time you need it üò≠ sharing an ugly long link with everyone ü§¢ Or... You could create a üåπ beautiful link using your own domain/website that redirects to your Zoom meeting, and share that beautified URL with everyone. üòå And it will be easy for you to remember üí° &gt; NOTE Zoom now forces a one-year max lifetime on recurring videos. üòê So I created this simple workflow to solve a few headaches. ‚ò∫Ô∏è What this workflow does Triggers once, annually (360 days) Creates a new, recurring meeting in Zoom Updates a redirect script with the new Zoom URL on a Wordpress Page Notifies you in a Slack channel What this workflow lacks in breakthrough innovation, it makes up for with usefulness and peace of mind. Have fun and make it your own! Setup Add your credentials in each node this pre-requires you have a Zoom, Wordpress and Slack account, and have gotten API access on those accounts Create a Page in Wordpress, and get its ID. (Or create a new Page in WP.) Configure node parameters according to your needs. TEST!!!! Don't ever skip this step. Ever. Set it and forget it. &gt; NOTE You can replace the Wordpress node with another website CMS node, or generic HTTP request for a non-wordpress site. You can also remove or replace the Slack node with other notification functionality (eg. sms, whatsapp, email...) Template was created in n8n v1.58.2",1345,2024-11-13 19:55:21.795000+00:00,False,3
2556,Exponential Backoff for Google APIs,"n8n Workflow: Exponential Backoff for Google APIs Overview This n8n workflow implements an Exponential Backoff mechanism to handle retries when interacting with Google APIs. It ensures that failed API requests are retried with increasing delays, up to a specified maximum retry count. This approach helps mitigate transient errors (e.g., rate limits or temporary network issues) while maintaining workflow efficiency. Key Features: Exponential Backoff Logic**: Dynamically increases wait time between retries based on the retry count. Error Handling**: Stops the workflow and raises an error after a specified number of retries. Dynamic Waiting**: Waits for a calculated duration before each retry. Scalable Design**: Modular nodes for easy debugging and customization. Workflow Details Nodes in the Workflow: Trigger (When clicking ""Test Workflow""): Manually starts the workflow for testing. Loop Over Items: Iterates over multiple input items to process Google API requests row by row. Google API Node (Example: Update Sheet): Sends a request to a Google API endpoint (e.g., updating a row in Google Sheets). On success: Moves to the next item in the loop. On error: Passes the error to the Exponential Backoff node. Exponential Backoff: Calculates the delay for the next retry based on the retry count. Logic: const retryCount = $json[""retryCount""] || 0; const maxRetries = 5; const initialDelay = 1; // in seconds if (retryCount &lt; maxRetries) { const currentDelayInSeconds = initialDelay * Math.pow(2, retryCount); return { json: { retryCount: retryCount + 1, waitTimeInSeconds: currentDelayInSeconds, status: 'retrying', } }; } else { return { json: { error: 'Max retries exceeded', retryCount: retryCount, status: 'failed' } }; } Wait: Dynamically waits for the waitTimeInSeconds value calculated in the Exponential Backoff node. Configuration: Resume: After Time Interval Wait Amount: {{ $json[""waitTimeInSeconds""] }} Unit: Seconds Check Max Retries: Evaluates whether the retry count has exceeded the maximum limit. Routes the workflow: True: Passes to the Stop and Error node. False: Loops back to the Google API node for retry. Stop and Error: Stops the workflow and logs the error when the maximum retry count is reached. Parameters Configurable Settings: Max Retries: Defined in the Exponential Backoff node (const maxRetries = 5). Adjust this value based on your requirements. Initial Delay: The starting wait time for retries, defined as 1 second. Google API Configuration: Ensure your Google API node is properly authenticated and configured with the desired endpoint and parameters. How to Use Import the Workflow: Copy the workflow JSON and import it into your n8n instance. Configure Google API Node: Set up the Google API node with your credentials and target API endpoint (e.g., Google Sheets, Gmail, etc.). Test the Workflow: Manually trigger the workflow and observe the retry behavior in case of errors. Monitor Logs: Use the console logs in the Exponential Backoff node to debug retry timings and status. Example Scenarios Scenario 1: Successful Execution The Google API processes all requests without errors. Workflow completes without triggering the retry logic. Scenario 2: Transient API Errors The Google API returns an error (e.g., 429 Too Many Requests). The workflow retries the request with increasing wait times. Scenario 3: Maximum Retries Exceeded The workflow reaches the maximum retry count (e.g., 5 retries). An error is raised, and the workflow stops. Considerations Jitter: This workflow does not implement jitter (randomized delay) since it's not required for low-volume use cases. If needed, jitter can be added to the exponential backoff calculation. Retry Storms: If multiple workflows run simultaneously, ensure your API quotas can handle potential retries. Error Handling Beyond Max Retries: Customize the Stop and Error node to notify stakeholders or log errors in a centralized system. Customization Options Adjust the maximum retry limit and delay calculation to suit your use case. Add additional logic to handle specific error codes differently. Extend the workflow to notify stakeholders when an error occurs (e.g., via Slack or email). Troubleshooting Retry Not Triggering**: Ensure the retryCount variable is passed correctly between nodes. Confirm that the error output from the Google API node flows to the Exponential Backoff node. Incorrect Wait Time**: Verify the Wait node is referencing the correct field for waitTimeInSeconds. Request for Feedback We are always looking to improve this workflow. If you have suggestions, improvements, or ideas for additional features, please feel free to share them. Your feedback helps us refine and enhance this solution!",1340,2024-11-18 21:52:56.975000+00:00,False,2
2451,Download and Compress Folder from S3 to ZIP File,"This workflow downloads all files from a specific folder in a S3 Bucket and compresses them so you can download it via n8n or do further processings. Fill in your Credentials and Settings in the Nodes marked with ""*"". Might serve well as Blueprint or as manual Download for S3 Folders. Since I found it rather tricky to compress all binary files into one zip file I figured might it be an interesting Template. Hint: This is the expression to get every binary key to compress them dynamically. (used in the ""Compress""-Node) Enjoy the Workflow! ‚ù§Ô∏è https://let-the-work-flow.com Workflow Automation & Development",1337,2024-10-04 22:59:30.391000+00:00,False,2
2126,"Automate Your UTM Campaign Tracking: Shopify, n8n to Baserow","Campaign tracking is pivotal; it enables marketers to evaluate the efficacy of various strategies and channels. UTM parameters are particularly essential as they provide granular details about the source, medium, and campaign effectiveness. However, when this data is not automatically integrated into a centralized system, it can become a tedious and error-prone process to manually collate and analyze it. Retrieving UTM data from Shopify and storing it in Baserow enables oy to do more with this data. For example you could build a campaign database in Baserow and automatically add campaign revenue to it using this workflow template. This template will help you: Automatically retrieve UTM parameters from Shopify orders using the Shopify Admin API Process marketing data through n8n Store this data into Baserow, providing you with a dynamic, responsive base for campaign tracking and decision-making This template will demonstrate the follwing concepts in n8n: use the Schedule trigger node use the GraphQL node to call the Shopify Admin API split larger incoming datasets into n8n items with the Split node transform the data structure with the Set node control flow with the If node store data in Baserow with the Baserow node How to get started? Create a custom app in Shopify get the credentials needed to connect n8n to Shopify This is needed for the Shopify Trigger Create Shopify Acces Token API credentials n n8n for the Shopify trigger node Create Header Auth credentials: Use X-Shopify-Access-Token as the name and the Acces-Token from the Shopify App you created as the value. The Header Auth is neccessary for the GraphQL nodes. You will need a running Baserow instance for this. You can also sign up for a free account at https://baserow.io/ Please make sure to read the notes in the template. For a detailed explanation please check the corresponding video: https://youtu.be/VBeN-3129RM",1331,2024-02-23 11:32:59.731000+00:00,False,2
2177,Weekly N8N executions failures report to Telegram,"How it works Weekly triggered Fetches all previous executions of a given workflow Filter for failures and aggregate them into a single report Sends them to a given Telegram chat. Set up steps Create a new N8N api token in the settings panel. Add new N8N credentials in the credentials panel. Add new Telegram credentials in the credentials panel. Select N8N credentials and select the workflow ID in the ""Get all previous executions"" node. Select Telegram credentials and enter the chat-id in the ""Telegram"" node.",1329,2024-03-15 13:48:17.362000+00:00,False,1
2279,Forward Netflix emails to multiple email addresses with GMail and Mailjet,"Who is this template for? This workflow template is designed for everyone with a Gmail address, who wants to forward all Netflix emails, including temporary login codes, to friends and family effortlessly. How it works Scans your Gmail inbox every minute for new e-mails from Netflix Forwards all Netflix e-mails to all desired e-mail addresses via the e-mail provider Mailjet Setup Steps Connect your Google Mail Account to n8n following the official n8n instructions Add all recipients you want to the recipients array at the ""Set all recipients"" node. Create and connect your Mailjet Account to n8n following the official n8n instructions. Note: You cannot use an Gmail e-mail address as the sender address, as mailjet does not support this. I recommend using your own email address from a custom domain. This works perfectly.",1320,2024-06-05 16:46:25.743000+00:00,False,1
2678,Export all Strava Activity Data to Google Sheets,"What does this template help with? Save the data of activities recorded and stored in Strava to a Google Sheets document. How it works: We have a Google Sheets spreadsheet where each row represents a Strava activity with the date, reference, distance, time, and elevation. Periodically, the workflow checks the latest activities in our Strava account to see if any are missing from the spreadsheet and adds them to the list. All fields must be properly formatted according to how they are stored in the Google Sheets spreadsheet. Set up instructions Complete the Set up credentials step when you first open the workflow. You'll need a Google Sheets and Strava account. In the 'activities' node, you must enter the name of the file and the sheet where you want to save the imported data. In the 'Strava' node, you must select the corresponding credential. You can adjust the format of dates, times, and distances according to your needs in the 'strava_last' node. The rest of the information is available at sherblog.es Template was created in n8n v1.72.1",1319,2024-12-28 22:23:06.727000+00:00,False,3
2117,Auto-assign deals in Hubspot to fitting sales reps based on geo and company size,"Use case To guarantee an effective sales process deals must be distributed between sales reps in the best way. Normally, this involves manually assigning new deals that have come in. This workflow automates it for you! What this workflow does This workflow runs once a day and checks for unassigned deals in your Hubspot CRM. Once it finds one, it enriches the deal with information about the assigned contact and their company. It then checks the region of the assigned company before looking at the company's employee size. Based on this, it assigns the deal to the right sales rep within your company. Requirements New deals in Hubspot need to be unassigned in the beginning New deals have to have an attached contact that has an attached company in Hubspot The company needs to have values for region and employee count in Hubspot Setup The setup is quite straight forward and will probably take a few minutes only. Add your Hubspot credentials Customize your criterias for assigning deals in the Assign by Region and the following Assign nodes Make sure deals are assigned to the right salesrep in the Hubspot nodes at the end Activate the workflow Customizing this to your needs Adjust the trigger interval to your needs. Currently, it defaults to once a day Adjust your region settings by adding/updating/removing options in the respective node Adjust your employee size settings by adding/updating/removing options in the respective node Ideas to enhance this flow Wrap each region's assigned criteria into different sub-workflows for easier maintainability. This will not consume additional execution counts. Add more logic on what happens once a deal does not match any criteria you've set",1310,2024-02-22 12:24:37.161000+00:00,False,2
2140,Add product ideas to Notion via a Slack command,"Use Case In most companies, employees have a lot of great ideas. That was the same for us at n8n. We wanted to make it as easy as possible to allow everyone to add their ideas to some formatted database - it should be somewhere where everyone is all the time and could add a new idea without much extra effort. Since we're using Slack, this seemed to be the perfect place to easily add ideas and collect them in Notion. What this workflow does This workflow waits for a webhook call within Slack, that gets fired when users use the /idea command on a bot that you will create as part of this template. It then checks the command, adds the idea to Notion, and notifies the user about the newly added idea as you can see below: Creating your Slack bot Visit https://api.slack.com/apps, click on New App and choose a name and workspace. Click on OAuth & Permissions and scroll down to Scopes -&gt; Bot token Scopes Add the chat:write scope Head over to Slash Commands and click on Create New Command Use /idea as the command Copy the test URL from the Webhook node into Request URL Add whatever feels best to the description and usage hint Go to Install app and click install Setup Add a Database in Notion with the columns Name and Creator Add your Notion credentials and add the integration to your Notion page. Fill the setup node below Create your Slack app (see other sticky) Click Test workflow and use the /idea comment in Slack Activate the workflow and exchange the Request URL with the production URL from the webhook How to adjust it to your needs You can adjust the table in Notion and for example, add different types of ideas or areas that they impact You might wanna add different templates in Notion to make it easier for users to fill their ideas with details Rename the Slack command as it works best for you How to enhance this workflow At n8n we use this workflow in combination with some others. E.g. we have the following things on top: We additionally have a /bug Slack command that adds a new bug to Linear. Here we're using AI to classify the bugs and move it to the right team. (see this template and this template) We also added other types, like /pain to be less solution-driven To make it easier for everyone to give input, we added a Votes column that allows everyone to vote on ideas/pain points in the list We're also running a workflow once a week that highlights the most popular new ideas and the most active voters (see here)",1310,2024-02-28 15:15:49.996000+00:00,True,2
2698,Scrape ProductHunt using Google Gemini,"Workflow Description: Product Data Extractor This workflow automates the extraction of product data from Product Hunt by combining webhook interactions, HTML processing, AI-based data analysis, and structured output formatting. It is designed to handle incoming requests dynamically and return detailed JSON responses for further usage. Overview The workflow processes a product name submitted through a webhook. It fetches the corresponding Product Hunt page, extracts and analyzes inline scripts, and structures the data into a well-defined JSON format using AI tools. The final JSON response is returned to the client through the webhook. Workflow Steps 1. Webhook Listener Node:** Receive Product Request Function:** Captures incoming requests containing the product name to process. Details:** Accepts HTTP requests and extracts the product parameter from the query string, such as &lt;custom_webhook_url&gt;/?product=epigram. 2. Fetch Product HTML Node:** Fetch Product HTML Function:** Sends an HTTP request to retrieve the HTML content of the specified Product Hunt page. Details:** Constructs a dynamic URL using the product name and fetches the page data. 3. Extract Inline Scripts Node:** Extract Inline Scripts Function:** Parses the HTML content to extract inline scripts located within the &lt;head&gt; section. Details:** Excludes scripts containing src attributes and validates the presence of inline scripts. 4. Process Data with LLM Node:** Process Script with LLM Function:** Analyzes the extracted scripts using a language model to identify key product data. Details:** Processes the script to derive structured and meaningful insights. 5. Refine Data with Google Gemini Node:** Analyze Script with Google Gemini Function:** Leverages Google Gemini AI for enhanced analysis of script data. Details:** Ensures the extracted data is precise and enriched. 6. Format Product Data to JSON Node:** Format Product Data to JSON Function:** Structures the processed data into a clean JSON format. Details:** Defines a schema to ensure all relevant fields are included in the output. 7. Send JSON Response to Client Node:** Send JSON Response to Client Function:** Returns the final structured JSON response to the client. Details:** Sends the response back via the same webhook that initiated the request. For example, &lt;custom_webhook_url&gt;. Key Features Versatile Use Cases:** This workflow can be used to gather Product Hunt data for creating blog posts or as a tool for AI agents to research products efficiently. Dynamic Processing:** Adapts to various product names through dynamic URL construction. AI Integration:** Utilizes the Gemini 1.5 8B AI model, offering reduced latency and minimal or no cost depending on the use case. Selector Independence:** Functions even if Product Hunt's DOM structure changes, as it does not rely on direct DOM selectors. Reliable Data Output:** A low temperature setting (0) and a precisely defined JSON schema ensure accurate and real data extraction. Dynamic Processing:** Adapts to various product names through dynamic URL construction. AI Integration:** Utilizes advanced language models for data extraction and refinement. Structured Output:** Ensures the output JSON adheres to a predefined schema for consistency. Error Handling:** Includes validations to handle missing or malformed data gracefully. Customization Options Limitations Dependency on Product Hunt:** Significant changes to the way Product Hunt loads data on its pages might require modifications to the workflow. Adaptability:** Even if changes occur, the workflow can be updated to maintain functionality due to its reliance on AI and not direct DOM selectors. Modify the webhook path to suit your application. Adjust the prompt for the language model to include additional fields. Extend the JSON schema to capture more data fields as needed. Expected Output Performance Metrics Response Time:** Typically ~6 seconds per product. Accuracy:** Data extracted with &gt;95% precision due to the pre-defined JSON schema. A JSON object containing detailed information about the specified product. Below is an example of a complete response for the product Epigram: { ""id"": ""861675"", ""slug"": ""epigram"", ""followersCount"": 181, ""name"": ""Epigram"", ""tagline"": ""Open-Source, Free, and AI-Powered News in Short"", ""reviewsRating"": 0, ""logoUuid"": ""735c2528-554c-467c-9dcf-745ee4b8bbdd.png"", ""postsCount"": 1, ""websiteUrl"": ""https://epigram.news"", ""websiteDomain"": ""epigram.news"", ""metaTitle"": ""Epigram - Open-source, free, and ai-powered news in short"", ""postName"": ""Epigram"", ""postTagline"": ""Open-source, free, and ai-powered news in short"", ""dailyRank"": ""3"", ""description"": ""An open-source, AI-powered news app for busy people. Stay updated with bite-sized news, real-time updates, and in-depth analysis. Experience balanced, trustworthy reporting tailored for fast-paced lifestyles in a sleek, user-friendly interface."", ""pricingType"": ""free"", ""userName"": ""Fazle Rahman"", ""userHeadline"": ""Co-founder & CEO, Hashnode"", ""userUsername"": ""fazlerocks"", ""userAvatarUrl"": ""https://ph-avatars.imgix.net/129147/f84e1796-548b-4d6f-9dcf-745ee4b8bbdd.jpeg"", ""makerName1"": ""Fazle Rahman"", ""makerHeadline1"": ""Co-founder & CEO, Hashnode"", ""makerUsername1"": ""fazlerocks"", ""makerAvatarUrl1"": ""https://ph-avatars.imgix.net/129147/f84e1796-548b-4d6f-9dcf-745ee4b8bbdd.jpeg"", ""makerName2"": ""Sandeep Panda"", ""makerHeadline2"": ""Co-Founder @ Hashnode"", ""makerUsername2"": ""sandeepg33k"", ""makerAvatarUrl2"": ""https://ph-avatars.imgix.net/101872/80b0b618-a540-4110-a6d1-74df39675ad0.jpeg"", ""primaryLinkUrl"": ""https://epigram.news/"", ""media1OriginalHeight"": 1080, ""media1OriginalWidth"": 1440, ""media1ImageUuid"": ""ac426fd1-3854-4734-b43d-34a5e06347ea.gif"", ""media1MediaType"": ""video"", ""media1MetadataUrl"": ""https://www.loom.com/share/b1a48a9b3cac4ba89ce772a3fbcc2847?sid=75efc771-25fa-4ac0-bb1b-5e38fc447deb"", ""media1VideoId"": ""b1a48a9b3cac4ba89ce772a3fbcc2847"", ""media2OriginalHeight"": 630, ""media2OriginalWidth"": 1200, ""media2ImageUuid"": ""8521a6bd-7640-487b-abd6-29b9f65fee32"", ""media2MediaType"": ""image"", ""media2MetadataUrl"": null, ""launchState"": ""featured"", ""thumbnailImageUuid"": ""735c2528-554c-467c-9dcf-745ee4b8bbdd.png"", ""link1StoreName"": ""Website"", ""link1WebsiteName"": ""epigram.news"", ""link2StoreName"": ""Github"", ""link2WebsiteName"": ""github.com"", ""latestScore"": 233, ""launchDayScore"": 233, ""userId"": ""129147"", ""topic1"": ""News"", ""topic2"": ""Open Source"", ""topic3"": ""Artificial Intelligence"", ""weeklyRank"": ""24"", ""commentsCount"": 20, ""postUrl"": ""https://www.producthunt.com/posts/epigram"" } Target Audience This workflow is ideal for developers, marketers, and data analysts seeking to automate the extraction and structuring of product data from Product Hunt for analytics, reporting, or integration with other tools.",1297,2025-01-05 05:47:39.642000+00:00,True,5
2270,Prevent simultaneous workflow executions with Redis,Purpose This ensures that executions of scheduled workflows do not overlap when they take longer than expected. How it works This is a separate workflow which monitors the execution of the main workflow Stores a flag in Redis (key dynamically named after workflow ID) which indicates if the main workflow is running or idle Only calls the main workflow if the last execution has finished Setup Update the credentials suitable for your Redis instance Replace the Schedule Trigger of your main workflow by an Execute Workflow Trigger Copy the workflow ID from the URL Paste the workflow ID in the Execute Workflow Node of this workflow Configure the Schedule Trigger Node,1290,2024-05-18 20:15:12.777000+00:00,False,1
2141,Add product ideas to Google Sheets via a Slack,"Use Case This workflow is a slight variation of a workflow we're using at n8n. In most companies, employees have a lot of great ideas. That was the same for us at n8n. We wanted to make it as easy as possible to allow everyone to add their ideas to some formatted database - it should be somewhere where everyone is all the time and could add a new idea without much extra effort. Since we're using Slack, this seemed to be the perfect place to easily add ideas. In this example, we're adding the ideas to Google Sheets instead of Notion, like we do. What this workflow does This workflow waits for a webhook call within Slack, that gets fired when users use the /idea command on a bot that you will create as part of this template. It then checks the command, adds the idea to Google Sheets and notifies the user about the newly added idea as you can see below: Creating your Slack bot Visit https://api.slack.com/apps, click on New App and choose a name and workspace. Click on OAuth & Permissions and scroll down to Scopes -&gt; Bot token Scopes Add the chat:write scope Head over to Slash Commands and click on Create New Command Use /idea as the command Copy the test URL from the Webhook node into Request URL Add whatever feels best to the description and usage hint Go to Install app and click install Setup Create a Google Sheets document with the columns Name and Creator Add your Google credentials Fill the Set me up node. Create your Slack app (see other sticky) Click Test workflow and use the /idea comment in Slack Activate the workflow and exchange the Request URL with the production URL from the webhook How to adjust it to your needs You can adjust the table in Google Sheets and for example, add different types of ideas or areas that they impact Rename the Slack command as it works best for you How to enhance this workflow At n8n we use this workflow in combination with some others. E.g. we have the following things on top: We additionally have a /bug Slack command that adds a new bug to Linear. Here we're using AI to classify the bugs and move it to the right team. (Bug command workflow and Ai Classifier workflow) We also added other types, like /pain to be less solution-driven To make it easier for everyone to give input, we added a Votes column that allows everyone to vote on ideas/pain points in the list We're also running a workflow once a week that highlights the most popular new ideas and the most active voters",1287,2024-02-28 15:38:22.223000+00:00,True,2
8779,ü§ñ Self Improving Email AI Support with Human-in-the-Loop,"How it works This workflow creates a sophisticated, self-improving customer support system that automatically handles incoming emails. It's designed to answer common questions using an AI-powered knowledge base and, crucially, to learn from human experts when new or complex questions arise, continuously expanding its capabilities. Think of it like having an AI assistant with a smart memory and a human mentor. Here's the step-by-step process: New Email Received: The workflow is triggered whenever a new email arrives in your designated support inbox (via Gmail). Classify Request: An AI model (Google Gemini 2.5 Flash Lite) first classifies the incoming email to ensure it's a genuine support request, filtering out irrelevant messages. Retrieve Knowledge Base: The workflow fetches all existing Question and Answer pairs from your dedicated Google Sheet knowledge base. AI Answer Attempt: A powerful AI model (Google Gemini 2.5 Pro) analyzes the customer's email against the entire knowledge base. It attempts to find a highly relevant answer and drafts a complete HTML email response if successful. Decision Point: An IF node checks if the AI found a confident answer. If Answer Found: The AI-generated HTML response is immediately sent back to the customer via Gmail. If No Answer Found (Human-in-the-Loop): Escalate to Human: The customer's summarized question and original email are forwarded to a human expert (you or your team) via Gmail, requesting their assistance. Human Reply & AI Learning: The workflow waits for the human expert's reply. Once received, another AI model (Google Gemini 2.5 Flash) processes both the original customer question and the expert's reply to distill them into a new, generic, and reusable Question/Answer pair. Update Knowledge Base: This newly created Q&A pair is then automatically added as a new row to your Google Sheet knowledge base, ensuring the system can answer similar questions automatically in the future. Set up steps Setup time: ~10-15 minutes This workflow requires connecting your Gmail and Google Sheets accounts, and obtaining a Google AI API key. Follow these steps carefully: Connect Your Gmail Account: Select the On New Email Received node. Click the Credential dropdown and select + Create New Credential to connect your Gmail account. Grant the necessary permissions. Repeat this for the Send AI Answer and Ask Human for Help nodes, selecting the credential you just created. Connect Your Google Sheets Account: Select the Get Knowledge Base node. Click the Credential dropdown and select + Create New Credential to connect your Google account. Grant the necessary permissions. Repeat this for the Add to Knowledge Base node, selecting the credential you just created. Set up Your Google Sheet Knowledge Base: Create a new Google Sheet in your Google Drive. Rename the first sheet (tab) to QA Database. In the first row of QA Database, add two column headers: Question (in cell A1) and Answer (in cell B1). Go back to the Get Knowledge Base node in n8n. In the Document ID field, select your newly created Google Sheet. Do the same for the Add to Knowledge Base node. Get Your Google AI API Key (for Gemini Models): Visit Google AI Studio at aistudio.google.com/app/apikey. Click ""Create API key in new project"" and copy the key. In the workflow, go to the Google Gemini 2.5 Pro node, click the Credential dropdown, and select + Create New Credential. Paste your key into the API Key field and Save. Repeat this for the Google Gemini 2.5 Flash Lite and Google Gemini 2.5 Flash nodes, selecting the credential you just created. Configure Human Expert Email: Select the Ask Human for Help node. In the Send To field, replace the placeholder email address with the actual email address of your human expert (e.g., your own email or a team support email). Activate the Workflow: Once all credentials and configurations are set, activate the workflow using the toggle switch at the top right of your n8n canvas. Start Learning! Send a test email to the Gmail account connected to the On New Email Received node. Observe how the AI responds, or how it escalates to your expert email and then learns from the reply. Check your Google Sheet to see new Q&A pairs being added!",1279,2025-09-20 15:06:17.309000+00:00,True,6
2368,Preconfigured nodes for Systeme.io API requests,"Using the Systeme API can be challenging due to its pagination settings and low rate limit. This requires a bit more knowledge about API requests than a beginner might have. This template provides preconfigured HTTP Request nodes to help you work more efficiently. Pagination settings, item limits, and rate limits are all configured for you, making it easier to get started. How to configure Systeme.io credentials The Systeme API uses the Header Auth method. So create a Header Auth credential in your n8n with the name ""X-API-Key"". . Check out my other templates üëâ https://n8n.io/creators/solomon/",1270,2024-07-26 14:32:36.750000+00:00,False,1
4651,Create a Branded AI Chatbot for Websites with Flowise Multi-Agent Chatflows,"This workflow integrates Flowise Multi-Agent Chatflows into a custom-branded n8n chatbot, enabling real-time interaction between users and AI agents powered by large language models (LLMs). Key Advantages: ‚úÖ Easy Integration with Flowise: Uses a low-code HTTP node to send user questions to Flowise's API (/api/v1/prediction/FLOWISE_ID) and receive intelligent responses. Supports multi-agent chatflows, allowing for complex, dynamic interactions. üé® Customizable Chatbot UI: Includes pre-built JavaScript for embedding the n8n chatbot into any website. Provides customization options such as welcome messages, branding, placeholder text, chat modes (e.g., popup or embedded), and language support. üîê Secure & Configurable: Authorization via Bearer token headers for Flowise API access. Clearly marked notes in the workflow for setting environment variables like FLOWISE_URL and FLOW_ID. How It Works Chat Trigger: The workflow starts with the When chat message received node, which acts as a webhook to receive incoming chat messages from users. HTTP Request to Flowise: The received message is forwarded to the Flowise node, which sends a POST request to a Flowise API endpoint (https://FLOWISEURL/api/v1/prediction/FLOWISE_ID). The request includes the user's input as a JSON payload ({""question"": ""{{ $json.chatInput }}""}) and uses HTTP header authentication (e.g., Authorization: Bearer FLOWSIE_API). Response Handling: The response from Flowise is passed to the Edit Fields node, which maps the output ($json.text) for further processing or display. Set Up Steps Configure Flowise Integration: Replace FLOWISEURL and FLOWISE_ID in the HTTP Request node with your Flowise instance URL and flow ID. Ensure the Authorization header is set correctly in the credentials (e.g., Bearer FLOWSIE_API). Embed n8n Chatbot: Use the provided JavaScript snippet in the sticky notes to embed the n8n chatbot on your website. Replace YOUR_PRODUCTION_WEBHOOK_URL with the webhook URL generated by the When chat message received node. Customize the chatbot's appearance and behavior (e.g., welcome messages, language, UI elements) using the createChat configuration options. Optional Branding: Adjust the sticky note examples to include branding details, such as custom messages, colors, or metadata for the chatbot. Activate Workflow: Toggle the workflow to ""Active"" in n8n and test the chat functionality end-to-end. Ideal Use Cases: Embedding branded AI assistants into websites. Connecting Flowise-powered agents with customer support chatbots. Creating dynamic, smart conversational flows with LLMs via n8n automation. Need help customizing? Contact me for consulting and support or add me on Linkedin.",1267,2025-06-04 13:42:57.244000+00:00,True,1
2443,Public Webhook Relay,"Disclaimer This template only works on n8n local instances! How it Works This workflow allows you to to receive webhooks from the public web and have your local workflow catch them, without any remote proxy. It is very useful for running quick tests without exposing your dev server. All you have to do is activate the workflow and use the public address as defined below. Set up steps If you use the default key-value storage, there are only three steps: Install the @horka.tv/n8n-nodes-storage-kv community node Put your n8n workflow address in Local Webhook Address Activate the workflow and, from Executions, note down your public webhook token from the inputs to Get Latest Requests. You can now use https://webhook.site/[YOUR TOKEN] as a webhook destination, to receive webhook requests from the public web.",1266,2024-09-30 13:12:10.831000+00:00,False,2
2690,Reusable Subworkflow Zip Multiple Files Dynamically (Compress),"üì¶ Zip Multiple Files Dynamically This template enables you to dynamically bundle multiple files into a ZIP archive. Designed to be used as a Subworkflow, it‚Äôs modular, flexible, and easy to integrate into various workflows. The output is a single ZIP file with a name that includes the current date, time, and fileName. Shoutout: Code from: Tom (mutedjam) üë§ Who is this for? This workflow is perfect for: üöÄ Businesses automating file archiving tasks. üíª Developers managing files programmatically. üìÇ Anyone needing a reusable solution for bundling files into ZIP archives. ‚ùì What problem is this workflow solving? Manually zipping multiple files is: üïí Time-consuming. ü§î Prone to errors. This workflow automates the process and, as a Subworkflow, ensures: ‚ö° Consistent file archiving across different workflows. üõ†Ô∏è Reduced manual effort. üìà Streamlined integration into existing automation. üîß What this workflow does üóÇÔ∏è Dynamically collects binary files from the input. üì¶ Bundles them into a single ZIP archive. üïí Names the ZIP file with the current date, time, and a customizable fileName. ‚úÖ Outputs the ZIP file, ready for storage or further processing. ‚öôÔ∏è Setup üîó Add this Subworkflow to your existing workflows. üì• Pass the binary files as input to the Subworkflow. ‚ñ∂Ô∏è Call the Subworkflow to generate a ZIP file. üõ†Ô∏è How to customize this workflow to your needs üåê File Sources**: Adjust the input nodes in your parent workflow to connect to your preferred file sources. üìù File Naming**: Customize the logic for the output fileName in the Subworkflow. üöÄ Additional Use Cases**: Use this Subworkflow in various scenarios, such as: ‚úâÔ∏è Sending ZIP files via email. ‚òÅÔ∏è Uploading ZIP files to cloud storage. üîÑ Triggering further automation. üéâ Why use this as a Subworkflow? Instead of building a fixed ZIP functionality for every workflow, this template offers a reusable solution that can be integrated into many different workflows effortlessly. Save time and ensure consistency across your automation projects! üí°",1249,2025-01-02 13:55:41.676000+00:00,False,2
2184,Validate email format from a Wordpress form and save a contact in Mautic,"When you collect leads from a form, you need to format the incoming data such as the lead's name and also apply a basic validation of the email entered. Lucky for us, N8N offers all of these features with simple expressions that can easily be applied to data. This workflow aims to show how you can process your lead data before saving it in Mautic. How it Works This workflow receives data from a Wordpress form; applies name formatting and basic validation to the email; Creates the contact in Mautic; If e-mail is invalid, add the lead in Dot Not Contact list. Setup Steps Set up credentials when you first open the workflow. You'll need a Mautic account. You need to configure a form in Wordpress (Elementor, WPForms, etc.) and send it to the N8N Webhook address; Now map the fields you need to apply formatting and validation. After testing your workflow, swap the Test URL to Production URL in Discourse and activate your workflow",1248,2024-03-20 00:53:49.339000+00:00,False,1
4693,Get Real-Time Security Insights with NixGuard RAG and Wazuh Integration,"Effortlessly integrate NixGuard API into your n8n workflows for real-time security insights using your API key. This connector enables seamless interaction with Nix, providing rapid Retrieval-Augmented Generation (RAG) event knowledge with Wazuh integration - completely free and set up in under 5 minutes! üöÄ Features: ‚úÖ Query NixGuard's AI-driven security insights via API authentication ‚úÖ Real-time security event knowledge integration ‚úÖ Plug-and-play workflow trigger for effortless automation ‚úÖ Wazuh compatibility for full security visibility üõ† How to Use: 1Ô∏è‚É£ Add your API Key to authenticate with NixGuard. 2Ô∏è‚É£ Integrate with your existing n8n workflows using the workflow trigger (default enabled). 3Ô∏è‚É£ (Optional) Activate the chat trigger to streamline security queries via chat-based inputs. 4Ô∏è‚É£ Run the workflow and get instant security intelligence! üì¢ Perfect for: Startup CTO's, SOC teams, security engineers, and developers needing real-time security automation within their infrastructure. üîó Learn more about NixGuard: thenex.world üîó Get started with a free security subscription: thenex.world/security/subscribe",1247,2025-06-05 14:45:24.254000+00:00,False,2
2241,Unsubscribe Mautic contacts from automated unsubscribe emails,"Who is this for? This template is designed for businesses and organizations that use Mautic for email marketing and want to automate the process of removing contacts from specific segments when they receive an unsubscribe request via email. What problem is this workflow solving? / use case Many email recipients, especially those who are less tech-savvy, may not follow the standard unsubscribe link provided in emails. Instead, for example in Gmail, they click the ""Unsubscribe"" button in the Gmail web interface, which in turn sends an email with a consistent format, these emails contain the word unsubscribe in the 'To' field using the following structure: hello+unsubscribe_6629823aa976f053068426@example.com This workflow automates the process of identifying such unsubscribe emails and removing the contact from the relevant Mautic segments, ensuring compliance with unsubscribe requests and maintaining a clean mailing list. What this workflow does Monitors a Gmail account for incoming emails. Identifies unsubscribe emails based on specific patterns in the ""To"" field (e.g., containing the word ""unsubscribe""). Retrieves the contact's ID from Mautic based on the email address. Removes the contact from the specified ""newsletter"" segment in Mautic. Adds the contact to the ""unsubscribed"" segment in Mautic. Sends a confirmation email to the contact, acknowledging their unsubscribe request. Setup Configure your email address and unsubscribe message in the ""Edit Fields"" node. Set your credentials in the Gmail trigger and in the Mautic nodes. Set the segments for the ""newsletter"" and ""unsubscribed"" in the Mautic nodes. Make sure your n8n installation has a public endpoint for your Gmail trigger to work correctly. Deploy the workflow. How to customize this workflow to your needs Adjust the conditions for identifying unsubscribe emails based on your specific requirements. Modify the segments or actions taken in Mautic according to your desired behavior. Customize the confirmation email message and sender details. Note: This workflow assumes a consistent structure for unsubscribe emails, where the ""From"" field contains the word ""unsubscribe"" using the ""+"" sign. If your email provider follows a different convention, adjust the conditions in the ""Is automated unsubscribe?"" node accordingly.",1237,2024-04-26 02:43:17.522000+00:00,False,3
2367,Replicate Line Items on New Deal in HubSpot and notify with Slack,"Replicate Line Items on New Deal in HubSpot Workflow Use Case This workflow solves the problem of manually copying line items from one deal to another in HubSpot, reducing manual work and minimizing errors. What this workflow does Triggers** upon receiving a webhook with deal IDs. Retrieves** the IDs of the won and created deals. Fetches** line items associated with the won deal. Extracts** product SKUs from the retrieved line items. Fetches** product details based on SKUs. Creates** new line items for the created deal and associates them. Sends** a Slack notification with success details. Step up steps Create a HubSpot Deal Workflow 1.1 Set up your trigger (ex: when deal stage = Won) 1.2 Add step : Create Record (deal) 1.3 Add Step : Send webhook. The webhook should be a Get to your n8n first trigger. Set two query parameter : deal_id_won as the Record ID of the deal triggering the HubSpot Workflow deal_id_create as the Record ID of the deal created above. Click Insert Data -&gt; The created object Set up your HubSpot App token in HubSpot -&gt; Settings -&gt; Integration -&gt; Private Apps Set up your HubSpot Token integration using the predefined model. Set up your Slack connection Add an error Workflow to monitor errors",1225,2024-07-26 13:52:02.140000+00:00,False,2
2239,Extract Domain and verify email syntax on the go,"What problem is this workflow solving? This workflow is aimed for email marketing enthusiasts looking for an easy way to either extract the domain from an email ad also check if the syntax is correct without having to use the code node. How this works For this to work, replace the debugger node with your actual data source. Map your data at match the above layout Run your workflow and check for all the emails that are either valid or not Once done, you will have a list of all your emails, domains, and whether they are valid or not.",1222,2024-04-22 06:31:37.656000+00:00,False,1
8615,Build a Telegram Chatbot with Gemini Pro AI and Google Docs Integration,"Telegram AI Chatbot with Google & Gemini Integration Simple overview This workflow connects a Telegram bot to OpenAi/Google Gemini (PaLM API) so the bot can reply to users with AI-generated answers. Useful for FAQs, assistants, classroom helpers, or bots that fetch document content to answer questions. Who is this for Educators, creators, developers, and support teams who want a low-code Telegram chatbot powered by Gemini. What it does (quick) Listens for messages sent to your Telegram bot. Sends incoming text to Google Gemini and receives a generated reply. Optionally fetches content from Google Docs or an external API to enrich replies. Sends the reply back to the original Telegram user. Processes messages in batches and adds short delays to avoid spamming. Quick setup (5 steps) Create a Telegram bot with @BotFather and copy the bot token. Add Telegram credentials to n8n (Telegram node). OpenAi API key and add it to n8n. Get a Google Gemini (PaLM) API key and add it to n8n. (Optional) Connect Google Docs OAuth2 if you want the bot to read documents. Activate the workflow and test by messaging the bot. Required items Telegram bot token OpenAi API key Google Gemini (PaLM) API key n8n instance with Telegram and HTTP nodes enabled (Optional) Google Docs OAuth2 credential How it works (step-by-step) Telegram message arrives ‚Üí Trigger node. Workflow extracts message and user info. (Optional) Pull supporting content from Google Docs or an API. Send prompt + context to Gemini ‚Üí receive reply. Send reply back to the Telegram user. Add small delays and batch processing to handle volume safely. How to customize Edit the Gemini prompt to change response style and behavior. OpenAi Chat model Switch Gemini model (Flash vs Pro) for speed vs. quality. Add conditions (If / Switch) to route different inputs to different behaviors. Append more data sources (Sheets, external APIs) to enrich replies. Add error handling to retry or log failed requests. Testing checklist Send a test message to the bot and confirm a reply. If using Google Docs, confirm the bot can read the target document. Check logs and node outputs in n8n for any errors. Tips and best practices Keep prompts concise and include only needed context to reduce costs. Use rate limiting (Wait node) and batching to avoid API throttling. Store API keys securely in n8n credentials. Start with small tests before enabling automated production runs. üôã For Help & Community üëæ Discord: n8n channel üåê Website: devcodejourney.com üîó LinkedIn: Connect with Shakil üì± WhatsApp Channel: Join Now üí¨ Direct Chat: Message Now",1205,2025-09-15 19:15:37.573000+00:00,True,3
2148,Write all Linear tickets to Google Sheets,"Use Case Track all Linear tickets in Google sheets. Useful if you want to do some custom analysis but don't want to pay for Linear's Plus features (Linear Insights) or that it does not cover. Setup Add Linear API header key Add Google sheets creds Update which teams to get tickets from in Graphql Nodes Update which Google Sheets page to write all the tickets to You only need to add one column, id, in the sheet. Google Sheets node in automatic mapping mode will handle adding the rest of the columns. Set any custom data on each ticket Activate workflow üöÄ How to adjust this template Set any custom fields you want to get out of this, that you can quickly do in n8n.",1194,2024-02-28 19:01:35.534000+00:00,False,3
2136,Verify & enrich form leads with Hunter and Clearbit then add them to Pipedrive,"Use case When collecting leads via an online form, you often need to manually add those new leads into your Pipedrive CRM. This not only takes a lot of time but is also error-prone. This workflow automates this tedious work for you. What this workflow does The workflow is triggered each time a form is submitted in n8n. It validates the email address using Hunter.io. If the email is valid, the workflow checks for an existing person with that email in Pipedrive. If no existing person is found, it utilizes Clearbit to enrich the person's information. It then verifies if the person's organization already exists in Pipedrive, creating a new organization if necessary. The workflow then registers the person in Pipedrive. Lastly, it creates a lead in Pipedrive using information from the person and organization. Setup This workflow is very quick to set up. Add your Hunter.io, Clearbit and Pipedrive credentials Click the test workflow button Activate the workflow and use the form trigger production URL to collect your leads in a smart way How to adjust it to your needs Exchange the n8n form trigger with your form of choice (Typeform, Google Forms, SurveyMonkey...) Add a filter criteria to only add new leads if they match certain requirements Remove the email check with Hunter.io if you don't own this tool and expect new form submission to have a correct email anyways Add ways to handle invalid emails or existing Persons",1190,2024-02-28 11:11:05.616000+00:00,False,3
2857,Pattern for Multiple Triggers Combined to Continue Workflow,"Overview This template describes a possible approach to handle a pseudo-callback/trigger from an independent, external process (initiated from a workflow) and combine the received input with the workflow execution that is already in progress. This requires the external system to pass through some context information (resumeUrl), but allows the ""primary"" workflow execution to continue with BOTH its own (previous-node) context, AND the input received in the ""secondary"" trigger/process. Primary Workflow Trigger/Execution The workflow path from the primary trigger initiates some external, independent process and provides ""context"" which includes the value of $execution.resumeUrl. This execution then reaches a Wait node configured with Resume - On Webhook Call and stops until a call to resumeUrl is received. External, Independent Process The external, independent process could be anything like a Telegram conversation, or a web-service as long as: it results in a single execution of the Secondary Workflow Trigger, and it can pass through the value of resumeUrl associated with the Primary Workflow Execution Secondary Workflow Trigger/Execution The secondary workflow execution can start with any kind of trigger as long as part of the input can include the resumeUrl. To combine / rejoin the primary workflow execution, this execution passes along whatever it receives from its trigger input to the resume-webhook endpoint on the Wait node. Notes IMPORTANT: The workflow ids in the Set nodes marked **Update Me have embedded references to the workflow IDs in the original system. They will need to be CHANGED to make this demo work. Note: The Resume Other Workflow Execution node in the template uses the $env.WEBHOOK_URL configuration to convert to an internal ""localhost"" call in a Docker environment. This can be done differently. ALERT:** This pattern is NOT suitable for a workflow that handles multiple items because the first workflow execution will only be waiting for one callback. The second workflow (not the second trigger in the first workflow) is just to demonstrate how the Independent, External Process needs to work.",1190,2025-02-07 01:14:59.594000+00:00,False,1
2650,"Extract Information from a Logo Sheet using forms, AI, Google Sheet and Airtable","Instructions This automation enables you to just upload any Image (via Form) of a Logo Sheet, containing multiple Images of Product Logos (most likely) which brings them in some context to one another. After submitting an AI-Agent eats that Logo Sheet, turning it into an List of ""Productname"" and ""Attributes"", also checks if Tools are kind of similar to another, given the Context of the Image. We utilize AI Vision capabilities for that. NOTE: It might not be able to extract all informations. For a ""upload and forget it"" Workflow it works for me. You can even run it multiple times, to be sure. But if you need to make sure it extracts everything you might need to think about an Multi-Agent Setup with Validation-Agent Steps. Once the Agent finishes the extraction, it will traditionally and deterministicly add those Attributes to Airtable (Creates those, if not already existing.) and also Upserts the Tool Informations. It uses MD5 Hashes for turning Product Names into.. something fancy really, you could also use it without that, but I wanted to have something that looks atleast like an ID. Setup Set Up the Airtable like shown below. Update and set Credentials for all Airtable Nodes. Check or Adjust the Prompt of the Agent matching your use-case. Activate the Workflow. Open the Form (default: https://your-n8n.io/form/logo-sheet-feeder) Enjoy growing your Airtable. Enjoy the workflow! ‚ù§Ô∏è let the work flow ‚Äî Workflow Automation & Development",1186,2024-12-17 12:35:08.190000+00:00,True,7
2130,Enrich lead captured by ConvertKit and save it in Hubspot,"Use Case Whenever someone shows interest in your offerings by subscribing to a list in ConvertKit it could be a potential new customer. Typically you need to gather more detailed information about them (data enrichment) and finally update their profile in your CRM system to better manage and nurture your relationship with them. This workflow does this all for you! What this workflow does The workflow runs every time a user is subscribed to a ConvertKit list. It then filters out personal emails, before enriching the email. If the email is attached to a company it enriches the company and upserts it in your Hubspot CRM. Setup Add Clearbit, Hubspot, and ConvertKit credentials. Click on Test workflow. Subscribe to a list on ConvertKit to trigger the workflow. Be aware that you can adapt this workflow to work with your enrichment tool, CRM, and email automation tool of choice.",1181,2024-02-23 17:34:42.498000+00:00,False,2
4509,üèõÔ∏è Daily US Congress Members Stock Trades Report via Firecrawl + OpenAI + Gmail,"üì¨ What This Workflow Does This workflow automatically scrapes recent high-value congressional stock trades from Quiver Quantitative, summarizes the key transactions, and delivers a neatly formatted report to your inbox ‚Äî every single day. It combines Firecrawl's powerful content extraction, OpenAI's GPT formatting, and n8n's automation engine to turn raw HTML data into a digestible, human-readable email. Watch Full Tutorial on how to build this workflow here: https://www.youtube.com/watch?v=HChQSYsWbGo&t=947s&pp=0gcJCb4JAYcqIYzv üîß How It Works üïí Schedule Trigger Fires daily at a set hour (e.g., 6 PM) to begin the data pipeline. üî• Firecrawl Extract API (POST) Targets the Quiver Quantitative ‚ÄúCongress Trading‚Äù page and sends a structured prompt asking for all trades over $50K in the past month. ‚è≥ Wait Node Allows time for Firecrawl to finish processing before retrieving results. üì• Firecrawl Get Result API (GET) Retrieves the extracted and structured data. üß† OpenAI Chat Model (GPT-4o) Formats the raw trading data into a readable summary that includes: Date of Transaction Stock/Asset traded Amount Congress member‚Äôs name and political party üìß Gmail Node Sends the summary to your inbox with the subject ‚ÄúCongress Trade Updates - QQ‚Äù. üß† Why This is Useful Congressional trading activity often reveals valuable signals ‚Äî especially when high-value trades are made. This workflow: Saves time manually tracking Quiver Quant updates Converts complex tables into a daily, readable email Keeps investors, researchers, and newsrooms in the loop ‚Äî hands-free üõ† Requirements Firecrawl API Key (with extract access) OpenAI API Key Gmail OAuth2 credentials n8n (self-hosted or cloud) üí¨ Sample Output: Congress Trade Summary ‚Äì May 21 Nancy Pelosi (D) sold TSLA for $85,000 on April 28 John Raynor (R) purchased AAPL worth $120,000 on May 2 ... and more ü™ú Setup Steps Add your Firecrawl, OpenAI, and Gmail credentials in n8n. Adjust the schedule node to your desired time. Customize the OpenAI system prompt if you want a different summary style. Deploy the workflow ‚Äî and enjoy your daily edge.",1178,2025-05-30 10:06:06.957000+00:00,True,4
2520,Summarize Umami data with AI (via Openrouter) and save it to Baserow,"Who's this for? Anyone who wants to improve the SEO of their website Umami users who want insights on how to improve their site SEO managers who need to generate reports weekly Case study Watch youtube tutorial here Get my SEO A.I. agent system here How it works This workflow calls the Umami API to get data Then it sends the data to A.I. for analysis It saves the data and analysis to Baserow How to use this Input your Umami credentials Input your website property ID Input your Openrouter.ai credentials Input your baserow credentials You will need to create a baserow database with columns: Date, Summary, Top Pages, Blog (name of your blog). Future development Use this as a template. There's alot more Umami stats you can pull from the API. Change the A.I. prompt to give even more detailed analysis. Created by Rumjahn",1176,2024-11-02 07:13:06.199000+00:00,True,3
2113,Enrich website visitors with Leadfeeder & Clearbit and save to Google Sheets,"Use Case When trying to maximize your outreach, website visitors are often an overlooked source of qualified new leads. This workflow allows your to track and enrich new website visitors and saves them to a Google Sheet once they meet a pre-defined criteria. What this workflow does This workflow fires once a day and gets all your leads saved in Leadfeeder. It then takes the leads that meet a pre-defined engagement criteria, e.g. that they visited your site 3 times, and enriches them additionally with Clearbit. From there it filters the leads again by a criteria on the company, e.g. a minimum employee count, and saves matching leads into a Google Sheet document. Setup Add your Leedfeeder credentials. The name should be Authorization and the value Token token=yourapitoken. You can find your token via Settings -&gt; Personal -&gt; API-Token Add your Google Sheet credentials Save the Leedfeeder account names you want to use in the Setup node Copy the Google Sheets Template and add its URL to the Setup node How to adjust this to your needs Adjust and/or remove the engagement and company criteria Add more ways to enrich a company Potential ideas to enhance the use of this workflow Automatically reach out to users that meet the criteria / that get added to the sheet Create a workflow that finds the right employee in companies that are identified by this workflow",1165,2024-02-21 18:04:56.018000+00:00,False,3
2814,2-way Sync Google Contacts and Notion,"Sync your Google Contacts with your Notion database. You can filter contacts by label or sync all contacts. All contact information is kept in sync between Google and Notion. What happens if I delete a contact? It gets deleted on the other end. Google Contact deleted ‚Üí Notion page deleted Notion page deleted ‚Üí Google contact deleted But you can easily change this behavior by disabling the respective deletion nodes. Who is this template for? Service providers looking to keep track of client information Organized individuals who like to centralize their lives in Notion Automation nerds who enjoy creating interesting projects How it works First, you perform an initial import to bring your contacts from Google to Notion. After that, the workflow listens for changes in both Notion and Google and keeps everything in sync. How to set up Instructions are included within the workflow itself. Check out my other templates üëâ https://n8n.io/creators/solomon/",1158,2025-01-28 14:00:09.385000+00:00,False,3
2345,Convert Baserow rich text markdown field to HTML,Sometimes you need the rich text field to be in HTML instead of Markdown. This template either syncs a single record or all records at once. Youtube tutorial,1119,2024-07-15 09:51:38.386000+00:00,False,1
2728,List recent ServiceNow Incidents in Slack Using Pop Up Modal,"Who is this for? This workflow is designed for IT teams, service desk personnel, and incident management professionals who need a streamlined way to monitor and report on recent ServiceNow incidents directly within Slack. What problem is this workflow solving? / Use Case Manually monitoring incidents in ServiceNow can be time-consuming, and keeping teams updated about new or specific incidents often involves additional manual effort. This workflow automates the process of querying recent incidents from ServiceNow based on user-defined parameters and delivering formatted results directly to Slack. It ensures faster response times and improved incident visibility. What this workflow does This workflow integrates Slack and ServiceNow to provide an automated system for retrieving and presenting incident details. Slack User Interaction: Users initiate the workflow via a Slack modal form, selecting incident parameters like priority and state. ServiceNow Query: The workflow queries ServiceNow for incidents matching the selected criteria. Results Delivery: Incident results are sent back to Slack as a message formatted using Block Kit. If no results are found, the workflow notifies the user with a detailed message, either in a Slack channel or via direct message. Error Handling: If no channel is selected or any issues arise, the workflow ensures graceful fallback with appropriate notifications. Setup Instructions Slack Setup: Integrate Slack with n8n using a Slack app. Configure the modal form to accept parameters like priority and state. Check out this video for setting up a modal slack app on YouTube. ServiceNow Integration: Use ServiceNow credentials to connect with n8n. Ensure appropriate permissions for querying incidents. n8n Workflow Configuration: Import this workflow into n8n. Verify all node configurations, particularly those for ServiceNow API queries and Slack outputs. Set up webhook URLs for Slack event handling. Testing: Trigger the workflow from Slack to test modal inputs and incident queries. Confirm the output is correctly formatted and delivered to the intended Slack channel or user. How to Customize this Workflow to Your Needs Modify the ServiceNow query logic to include additional filters or fields. Adjust the Slack Block Kit formatting to match your organization‚Äôs preferred notification style. Use conditional logic to add more advanced handling for specific priorities or states. Expand the workflow to include escalation steps, such as notifying a specific team or creating follow-up tasks. Workflow Highlights Slack Modal Form**: Allows users to specify search criteria for incidents interactively. Dynamic Results Delivery**: Automatically sends results to a Slack channel or direct message based on user input. Error Handling**: Provides fallback notifications when no incidents are found or user inputs are incomplete. Customizable Integration**: Easily adaptable to fit different organizational needs, including advanced filtering and formatting options.",1118,2025-01-15 04:47:19.413000+00:00,False,3
2510,Enhance Security Operations with the Qualys Slack Shortcut Bot!,"Enhance Security Operations with the Qualys Slack Shortcut Bot! Our Qualys Slack Shortcut Bot is strategically designed to facilitate immediate security operations directly from Slack. This powerful tool allows users to initiate vulnerability scans and generate detailed reports through simple Slack interactions, streamlining the process of managing security assessments. Workflow Highlights: Interactive Modals**: Utilizes Slack modals to gather user inputs for scan configurations and report generation, providing a user-friendly interface for complex operations. Dynamic Workflow Execution**: Integrates seamlessly with Qualys to execute vulnerability scans and create reports based on user-specified parameters. Real-Time Feedback**: Offers instant feedback within Slack, updating users about the status of their requests and delivering reports directly through Slack channels. Operational Flow: Parse Webhook Data**: Captures and parses incoming data from Slack to understand user commands accurately. Execute Actions**: Depending on the user's selection, the workflow triggers other sub-workflows like 'Qualys Start Vulnerability Scan' or 'Qualys Create Report' for detailed processing. Respond to Slack**: Ensures that every interaction is acknowledged, maintaining a smooth user experience by managing modal popups and sending appropriate responses. Setup Instructions: Verify that Slack and Qualys API integrations are correctly configured for seamless interaction. Customize the modal interfaces to align with your organization's operational protocols and security policies. Test the workflow to ensure that it responds accurately to Slack commands and that the integration with Qualys is functioning as expected. Need Assistance? Explore our Documentation or get help from the n8n Community for more detailed guidance on setup and customization. Deploy this bot within your Slack environment to significantly enhance the efficiency and responsiveness of your security operations, enabling proactive management of vulnerabilities and streamlined reporting. To handle the actual processing of requests, you will also need to deploy these two subworkflows: Qualys Start Vulnerability Scan Qualys Create Report To simplify deployment, use this Slack App manifest to quickly create an app with the correct permissions: { ""display_information"": { ""name"": ""Qualys n8n Bot"", ""description"": ""n8n Integration for Qualys"", ""background_color"": ""#2a2b2e"" }, ""features"": { ""bot_user"": { ""display_name"": ""Qualys n8n Bot"", ""always_online"": false }, ""shortcuts"": [ { ""name"": ""Scan Report Generator"", ""type"": ""global"", ""callback_id"": ""qualys-scan-report"", ""description"": ""Generate a report from the latest scan to review vulnerabilities and compliance."" }, { ""name"": ""Launch Qualsys VM Scan"", ""type"": ""global"", ""callback_id"": ""trigger-qualys-vmscan"", ""description"": ""Start a Qualys Vulnerability scan from the comfort of your Slack Workspace"" } ] }, ""oauth_config"": { ""scopes"": { ""bot"": [ ""commands"", ""channels:join"", ""channels:history"", ""channels:read"", ""chat:write"", ""chat:write.customize"", ""files:read"", ""files:write"" ] } }, ""settings"": { ""interactivity"": { ""is_enabled"": true, ""request_url"": ""Replace everything inside the double quotes with your workflow webhook url, for example: https://n8n.domain.com/webhook/99db3e73-57d8-4107-ab02-5b7e713894ad"""", ""message_menu_options_url"": ""Replace everything inside the double quotes with your workflow message options webhook url, for example: https://n8n.domain.com/webhook/99db3e73-57d8-4107-ab02-5b7e713894ad"""" }, ""org_deploy_enabled"": false, ""socket_mode_enabled"": false, ""token_rotation_enabled"": false } }",1116,2024-10-30 19:28:02.510000+00:00,False,1
2730,Get Daily Weather and Save It in Airtable,"This smart automation workflow created by the AI development team at WeblineIndia, helps with the daily collection and storage of weather data. Using the OpenWeatherMap API and Airtable, this solution gathers vital weather details such as temperature, humidity, and wind speed. The automation ensures daily updates, creating a dependable historical record of weather patterns for future reference and analysis. Steps: Set Schedule Trigger Configure a Cron node to trigger the workflow daily, for example, at 7 AM. Fetch Weather Data (HTTP Request) Use the HTTP Request node to retrieve weather data from the OpenWeatherMap API. Include your API key and query parameters (e.g., q=London, unit=metric) to specify the city and desired units. Parse Weather Data Utilize a JSON Parse node to extract key weather details, such as temperature, humidity, and wind speed, from the API response. Store Data in Airtable Use the Airtable node to insert the parsed data into the designated Airtable table. Ensure proper mapping of fields like temperature, humidity, and wind speed. Save and Execute Save the workflow and activate it to ensure weather data is fetched and stored automatically every day. Outcome This robust solution, developed by WeblineIndia, reliably collects and archives daily weather data, providing businesses and individuals with an accessible record of weather trends for analysis and decision-making. About WeblineIndia We specialize in creating custom automation solutions and innovative software workflows to help businesses streamline operations and achieve efficiency. This weather data fetcher is just one example of our expertise in delivering value through technology.",1099,2025-01-15 08:26:21.852000+00:00,True,2
4609,Automatic Workflow & Credentials Backup to GitHub with Change Detection,"This n8n workflow template uses community nodes and is only compatible with the self-hosted version of n8n. Title: n8n Credentials and Workflows Backup on Change Detection Purpose: Never lose track of your n8n changes again! This workflow smartly backs up all your workflows and credentials, automatically detects any changes using hash comparison, and pushes updates to GitHub‚Äîbut only when something has actually changed. Set your own interval and stop cluttering your repo with redundant commits. Walkthrough Video on YouTube Trigger: Schedule Trigger**: Executes the entire process at a user-defined interval. No need to worry about traceability or managing countless backups, as the workflow only commits changes when a difference is detected. Workflow Backup Process: Set Workflow Path: Defines the local backup file path for workflows. Get Old Workflow Hash: Executes a helper workflow to retrieve the previous hash. Execute Workflow Backup: Runs n8n export:workflow to export all workflows to the defined file path. Get New Workflow Hash: Executes a helper workflow to generate the new hash from the exported file. Compare Hashes (If Workflow Updated): Checks if the new hash differs from the old one. If Updated: Read Workflow Data ‚Üí Extract Text ‚Üí Push to GitHub: Reads, extracts, and commits the updated workflow JSON to GitHub under a timestamped filename. Credential Backup Process: Set Credential Path: Defines the local backup file path for credentials. Get Old Credential Hash: Executes a helper workflow to retrieve the previous hash. Execute Credential Backup: Runs n8n export:credentials to export all credentials. Get New Credential Hash: Executes a helper workflow to generate the new hash from the exported file. Compare Hashes (If Credential Updated): Checks for changes. If Updated: Read Credential Data ‚Üí Extract Text ‚Üí Push to GitHub: Commits the new credentials JSON to GitHub if changes are found. Hash Generator (Helper Flow): Used in both workflow and credential backup paths: Read File* ‚Üí *Extract Text* ‚Üí *Hash Data** Outputs SHA-256 hash used for comparison GitHub Integration: Commits are created with ISO timestamp in the filename and message. Repository: https://github.com/your-github-name/n8n-onchange-bachup File paths: backups/WorkFlow Backup -timestamp-.json and backups/Credential Backup -timestamp-.json Change Detection Logic: Only commits files when hash changes are detected (i.e., actual content change). Avoids unnecessary GitHub commits and storage use. Error Handling: GitHub nodes are set to continue workflow execution on error, avoiding full process interruption.",1097,2025-06-02 22:30:28.990000+00:00,False,2
2523,Elastic Alert Notification via Microsoft Graph API,"Who is this template for? This template is for teams and administrators who use n8n to monitor Elastic alerts and want to receive automated email notifications when an alert is triggered. It leverages Microsoft Graph API to send emails and provides an efficient way to notify users about alerts directly in their inbox. How it works? The template connects to the Elastic API to retrieve alert data. When a new alert is detected, the workflow processes the alert content and sends an email notification via Microsoft Graph API. The email includes alert details such as the alert name, timestamp, severity, and a summary of the message, allowing for quick action or review. Setup steps Step 1: Set up OAuth2 Credentials in n8n for Microsoft Graph API with Mail.Send permission. Step 2: Configure your Elastic API endpoint in the HTTP Request node to retrieve alerts. Step 3: Modify the email recipients in the template to specify who will receive the alert notifications. Step 4: Customize the email format, if necessary, to include additional alert details or adjust the message.",1082,2024-11-04 11:09:10.365000+00:00,False,1
2684,Check if a Twitch Stream is Live,"Introduction: This workflow template helps you determine if a Twitch user's stream is currently live or offline. Setup Instructions: The Document node holds the sample Twitch username you wish to check, you may adapt it in your workflow by replacing this with a chain that contains the Twitch username you want to check. This value is passed to the GraphQL node query as $('Document').item.json.twitch so make sure to change this based on your workflow. How it Works: The important nodes here are the GrapQL and IF nodes. The GrapQL queries the Twitch API, and then the output returns a document with the stream property. The IF node then checks if this property has a value, if null means the user is offline, otherwise the user is online or live. Common Use Cases: You can use this with other workflow templates to post live stream alerts to Twitter/X, Bluesky, and Discord via webhooks, etc to notify your community to join youR stream. You may also use an LLM node to write a custom alert based on the value of property title How to adjust this template If you want to check a list of Twitch channels, you can simply exchange the Document set node in the beginning with your list of channels. For more information on the GraphQL output please see the official Twitch API documentation: Get Streams",1080,2024-12-30 11:06:35.454000+00:00,True,1
2509,TwentyCRM event based updates on selective messaging channels with logs,"Who is this template for? This workflow template is designed for DevOps, Engineering, and Managed Service Provider professionals seeking alerts on various channels, with each channel being logically chosen based on the severity of the event. How it works Each time a new event occurs, the workflow runs (powered by TwentyCRM's native Webhooks feature). After filtering for the required data from the webhook, the filtered data is logged using Google Sheets. Based on the eventType from the webhook, we conditionally select a predefined messaging channel and send updates or alerts through it. Set up instructions Complete the Set up credentials step when you first open the workflow. You'll need a Google-OAuth2.0 with Gmail API & Google Sheets Scope, Slack with OAuth2.0 - chat:write scopes. Set up the Webhook in TwentyCRM, linking the On new TwentyCRM event Trigger with your TwentyCRM App. Set the correct channel to send to in the Post message in channel step. After testing your workflow, swap the Test URL to Production URL in TwentyCRM and activate your workflow. Template was created in n8n v1.63.4.",1079,2024-10-30 13:18:04.260000+00:00,False,3
2450,One-way sync Stripe Invoice PDFs to a S3 Bucket,"This automation syncs your Invoice PDFs from Stripe to a (AWS) S3 Bucket each month, in a folder of your choice, with the following subPath: yourFolder/invoiceYear/invoiceMonth/fileName Fill in your Credentials and Settings in the Nodes marked with ""*"". You can adjust this Workflow to your needs. You can also override the yearand month in the ENV* Node for manual syncs. It will sync every Invoice PDF which created-date is greater then the provided year and month. It will automatically set the day to the first day of the desired month. Enjoy the Workflow! ‚ù§Ô∏è https://let-the-work-flow.com Workflow Automation & Development",1068,2024-10-04 22:05:26.222000+00:00,False,2
2867,Credit Card Payment Reminder & Tracking-For Taiwan Banks,"Workflow Description This workflow automates the processing of credit card statement emails from multiple banks. It extracts important payment details, stores them in Google Sheets, and creates calendar reminders in Google Calendar. Additionally, it allows users to update the payment status once the bill has been paid. Key Features Email Processing: Retrieves credit card statement emails from eight Taiwanese banks. PDF Parsing: Extracts payment due date and amount from email content or attached PDF files. Google Sheets Integration: Logs extracted data into a Google Sheets document for record-keeping. Google Calendar Integration: Creates Google Calendar events as reminders for due payments. Webhook for Payment Updates: Allows users to update the payment status via a webhook. Node Configurations 1. Email Retrieval Purpose**: Fetches credit card statement emails from Gmail. Configuration**: Email Filters: SinoPac Bank: from:(newebill.banksinopac.com.tw) SinoPac Bank Credit Card E-Statement Notification Cathay United Bank: from:(service@pxbillrc01.cathaybk.com.tw) Cathay United Bank Monthly E-Statement CTBC Bank: from:(ebill@estats.ctbcbank.com) CTBC Bank Credit Card E-Statement Taipei Fubon Bank: from:(rs@cf.taipeifubon.com.tw) Taipei Fubon Bank Credit Card Statement E.SUN Commercial Bank: from:(estatement@esunbank.com) E.SUN Commercial Bank Credit Card E-Statement DBS Bank: from:(eservicetw@dbs.com) DBS Bank Credit Card E-Statement Union Bank of Taiwan: from:(ËÅØÈÇ¶ÈäÄË°å‰ø°Áî®Âç°) Union Bank of Taiwan Credit Card E-Statement (Year Month) Taishin International Bank: from:(webmaster@bhurecv.taishinbank.com.tw) Taishin International Bank Credit Card E-Statement 2. Extract Payment Information Purpose**: Extracts payment due date, total amount, and minimum payment amount. Methods**: Text-based Extraction: Uses regex to parse email body. PDF Parsing: Extracts text from PDF attachments. 3. Data Processing and Storage 3.1. Consolidate Extracted Data Purpose**: Standardizes extracted payment details. Data Fields**: payment_due_date payment_amount minimum_payment email_id bank email_subject 3.2. Google Sheets Integration Purpose**: Stores the extracted data in a structured format. Configuration**: Sheet Name: n8n-Credit Card Payment Reminder Columns: calendar_id Paid Billing Period Amount Minimum Payment Bank email_id 4. Google Calendar Integration 4.1. Create Calendar Reminders Purpose**: Generates reminders for upcoming payments. Configuration**: Event Title: Credit Card Payment - {{ bank }} Due Date: payment_due_date Reminders: 30 minutes before 60 minutes before 1 day before 4.2. Update Payment Status Purpose**: Updates the calendar event once payment is made. Configuration**: Webhook URL: Automatically updates the Google Calendar event title and description. 5. Webhook for Payment Updates Purpose: Users can mark a payment as **paid via a webhook. Configuration**: Webhook Path: darrell_demo_creditcard_pay_update_path Updates: Marks the payment as Paid Updates Google Calendar and Google Sheets",1066,2025-02-08 08:38:52.038000+00:00,False,2
2555,Send Webflow form data to Google Sheets,"Send Webflow form data to Google Sheets üöÄ How It Works This workflow connects your Webflow form submissions to Google Sheets with style and efficiency. Here's the magic in action: Form Submitted üéâ ‚Äì A Webflow form submission triggers the flow. Fields Prepared üõ†Ô∏è ‚Äì Data gets organized, and a submission date is added. Data Logged üìã ‚Äì Your data finds a cozy new home in Google Sheets. Set-Up Steps ‚è±Ô∏è Estimated Time: 5-10 minutes Connect Webflow üåê ‚Äì Hook up your Webflow account and disable legacy APIs (instructions inside the workflow). Set Up Google Sheets üßæ ‚Äì Choose your spreadsheet and authenticate. You're Ready! üí´ ‚Äì Let the workflow take it from here. Why You'll Love It ‚ù§Ô∏è No More Manual Copy-Paste:** Save time and keep data clean. Smart Field Mapping:** Automatically creates columns and fits your data. Helpful Notes Along the Way:** Friendly tips guide you through setup like a pro. Enjoy the smooth ride from Webflow to Google Sheets! üöÄ",1062,2024-11-17 13:03:12.374000+00:00,False,2
2186,Baserow campaign database to Shopify with image upload & dynamic template update,"Automating your marketing campaign management process can streamline your workflow and save you valuable time. With the combination of Baserow and n8n, you can efficiently handle your campaign data and seamlessly publish content to your Shopify store. In this workflow template, I demonstrate how to leverage Baserow as a centralized platform for organizing your marketing campaign assets, including copy and images. By utilizing n8n, we automate the process of fetching images and campaign descriptions from Baserow and uploading them directly to your Shopify store. With this automated solution, you can expedite the publishing process, ensuring that your campaigns are launched swiftly across your sales channels. Additionally, this workflow serves as a foundational step towards further automation in campaign management, allowing you to dynamically generate and upload content to your Shopify store with ease. This template will help you: Use n8n to get images for marketing campaigns from Baserow and upload them to your Shopify media library Dynamically inject data from Baserow into a template file Upload a template file to your Shopify theme This template will demonstrate the follwing concepts in n8n: use the Webhook node use the IF node to control the execution flow of the workflow do time calculation using expressions and javascript use the GraphQL node to upload images to your Shopify media files create a dynamic template file for your Shopify theme use the HTTP Reqest node to upload your template file to your Shopify store How to get started? Create a custom app in Shopify get the credentials needed to connect n8n to Shopify This is needed for the Shopify Trigger Create Shopify Acces Token API credentials n n8n for the Shopify trigger node Create Header Auth credentials: Use X-Shopify-Access-Token as the name and the Acces-Token from the Shopify App you created as the value. The Header Auth is neccessary for the GraphQL nodes. You will need a running Baserow instance for this. You can also sign up for a free account at https://baserow.io/ Please make sure to read the notes in the template. For a detailed explanation please check the corresponding video: https://youtu.be/Ky-dYlljGiY",1061,2024-03-20 14:46:48.643000+00:00,False,2
2512,Qualys Scan Slack Report Subworkflow,"Introducing the Qualys Scan Slack Report Subworkflow‚Äîa robust solution designed to automate the generation and retrieval of security reports from the Qualys API. This workflow is a sub workflow of the Qualys Slack Shortcut Bot workflow. It is triggered when someone fills out the modal popup in slack generated by the Qualys Slack Shortcut Bot. When deploying this workflow, use the Demo Data node to simulate the data that is input via the Execute Workflow Trigger. That data flows into the Global Variables Node which is then referenced by the rest of the workflow. It includes nodes to Fetch the Report IDs and then Launch a report, and then check the report status periodically and download the completed report, which is then posted to Slack for easy access. For Security Operations Centers (SOCs), this workflow provides significant benefits by automating tedious tasks, ensuring timely updates, and facilitating efficient data handling. How It Works Fetch Report Templates:** The ""Fetch Report IDs"" node retrieves a list of available report templates from Qualys. This automated retrieval saves time and ensures that the latest templates are used, enhancing the accuracy and relevance of reports. Convert XML to JSON:** The response is converted to JSON format for easier manipulation. This step simplifies data handling, making it easier for SOC analysts to work with the data and integrate it into other tools or processes. Launch Report:** A POST request is sent to Qualys to initiate report generation using specified parameters like template ID and report title. Automating this step ensures consistency and reduces the chance of human error, improving the reliability of the reports generated. Loop and Check Status:** The workflow loops every minute to check if the report generation is complete. Continuous monitoring automates the waiting process, freeing up SOC analysts to focus on higher-priority tasks while ensuring they are promptly notified when reports are ready. Download Report:** Once the report is ready, it is downloaded from Qualys. Automated downloading ensures that the latest data is always available without manual intervention, improving efficiency. Post to Slack:** The final report is posted to a designated Slack channel for quick access. This integration with Slack ensures that the team can promptly access and review the reports, facilitating swift action and decision-making. Get Started Ensure your Slack and Qualys integrations are properly set up. Customize the workflow to fit your specific reporting needs. Link to parent workflow Link to Vulnerability Scan Trigger Need Help? Join the discussion on our Forum or check out resources on Discord! Deploy this workflow to streamline your security report generation process, improve response times, and enhance the efficiency of your security operations.",1054,2024-10-31 00:22:38.823000+00:00,False,2
4684,"AI-Powered Meta Ads Analysis & Creation with Gemini, GPT-4.1 Mini & Ads Manager","AI-Powered Meta Ads Creation & Analysis Workflow Overview This comprehensive n8n workflow automates the entire Meta (Facebook/Instagram) advertising process, from asset analysis to ad creation. It combines AI-powered content analysis with automated ad deployment, streamlining the creation of high-converting social media advertisements. Watch a quick video walkthrough of the workflow Key Features ü§ñ AI-Powered Asset Analysis Video Analysis**: Uses Google Gemini to analyze video content, extracting transcripts, scene descriptions, hooks, offers, and branding elements Image Analysis**: Employs GPT-4.1 Mini to analyze static images, identifying visual elements, USPs, and marketing potential Structured Output**: Generates detailed descriptions and creative insights for informed ad creation ‚úçÔ∏è Automated Ad Copy Generation Creates 3 variations each of primary text, headlines, and link descriptions Optimized for Meta's character limits and best practices Maintains brand voice and messaging consistency Leverages AI analysis to create compelling, conversion-focused copy üéØ Meta Ads Manager Integration Video Ads**: Uploads videos, creates ad creatives with multiple text variations Image Ads**: Supports both single and multi-image campaigns (1:1 and 9:16 formats) Asset Feed Optimization**: Implements placement-specific customization rules Automated Preview Generation**: Creates ad previews for different placements üìä Smart Workflow Management Google Drive Integration**: Monitors designated folders for new creative assets Google Sheets Tracking**: Maintains comprehensive records of all assets and campaigns Status Management**: Tracks processing stages and prevents duplicate work Error Handling**: Includes retry logic and status checking for reliable operation Workflow Components Asset Discovery & Processing Google Drive Trigger: Monitors specified folder for new image/video files File Analysis: Extracts metadata, dimensions, and file specifications Asset Registration: Logs all assets in Google Sheets for tracking AI Analysis Pipeline Content Type Detection: Automatically identifies videos vs. images Video Processing: Uploads to Google Gemini for analysis Generates comprehensive content breakdown Extracts all marketing-relevant elements Image Processing: Analyzes visual content with GPT-4.1 Mini Identifies key messaging and visual elements Creates detailed creative descriptions Ad Copy Creation AI Agent Processing: Transforms analysis into marketing copy Multi-Variant Generation: Creates 3 versions of each text element Platform Optimization: Ensures compliance with Meta's requirements Quality Assurance: Structured output validation Meta Ads Deployment Asset Upload: Pushes images/videos to Meta Ads Manager Creative Assembly: Builds ad creatives with generated copy variations Campaign Creation: Sets up ads with proper targeting and placement rules Status Tracking: Updates spreadsheet with campaign IDs and status Setup Requirements Required Credentials Meta Developer Account**: App access token for Meta Graph API Google Drive OAuth**: For file monitoring and asset access Google Sheets OAuth**: For workflow tracking and management Google Gemini API**: For video analysis capabilities OpenAI API**: For image analysis and copy generation Configuration Steps Google Drive Setup: Create dedicated folder for creative assets Configure folder monitoring in the trigger node Google Sheets Template: Use provided template for asset and campaign tracking Configure account settings and campaign parameters Meta Developer Setup: Create Meta App and obtain access tokens Set up ad account permissions API Credentials: Configure all required authentication credentials Test connections before activation",1054,2025-06-05 09:09:39.117000+00:00,True,7
2776,Sync New Files From Google Drive with Airtable,"This workflow automatically fetches newly uploaded files from a specific folder in Google Drive, shares them via email with specified recipients, and logs the file details (name, ID, created time, modified time) into Airtable for easy tracking. It streamlines the process of file sharing and management while keeping track of important metadata in a central place. Step-by-Step Instructions Google Drive Node (Fetch New File) Action: This node fetches newly uploaded files from the specific folder you‚Äôve mentioned in your Google Drive. Configuration: Set the folder ID in the Google Drive node where the files are uploaded. Use the ‚ÄúNew File in Folder‚Äù trigger to automatically detect new files added to the folder. Send Email Node (Share File via Email) Action: After detecting the new file, this node shares the file via email with the recipient you specify. Configuration: Set the recipient's email address. Include the file URL from the Google Drive node in the email body, allowing easy access to the file. Add the file name as part of the email subject or body to notify the recipient about the new file. Airtable Node (Store File Metadata) Action: This node stores the file‚Äôs metadata, such as name, ID, creation time, modification time, and the email address to which it was sent, in your Airtable database. Configuration: Set up Airtable with a table. Map the output from the Google Drive node to store the file metadata, and use the email address from the email node for tracking. About WeblineIndia WeblineIndia specializes in delivering innovative and custom AI solutions to simplify and automate business processes. If you need any help, please reach out to us.",1046,2025-01-22 14:01:41.722000+00:00,True,2
2820,Backup Workflows to Git Repository on Gitea,"Overview This workflow automates the backup of all workflows from your system to a Git repository hosted on Gitea. It runs on a scheduled trigger, fetching, encoding, and committing workflow data, ensuring seamless version control and disaster recovery. üìå Quick Setup: Just update three global variables and configure authentication‚Äîno manual exports needed! How It Works (Quick Glance) 1Ô∏è‚É£ Scheduled Execution ‚Üí Runs automatically at defined intervals. 2Ô∏è‚É£ Fetch Workflows ‚Üí Uses the API to retrieve all workflows. 3Ô∏è‚É£ Process Workflows ‚Üí Converts workflow data into a Git-friendly format. 4Ô∏è‚É£ Commit & Push to Git ‚Üí Saves workflows in a Gitea repository. Setup Steps (‚ö° Takes ~5 min) 1Ô∏è‚É£ Set Global Variables Go to the Globals section in the workflow and update: repo.url* ‚Üí https://your-gitea-instance.com *(Replace with your actual Gitea URL) repo.name* ‚Üí workflows *(Repository name where backups will be stored) repo.owner* ‚Üí octoleo *(Gitea account that owns the repository) üìå These three variables define where the workflows are stored. 2Ô∏è‚É£ Configure Gitea Authentication Go to your Gitea account* ‚Üí Generate a *Personal Access Token** In the credential manager, create a new Gitea Token with: Name:** Authorization Value:** Bearer YOUR_PERSONAL_ACCESS_TOKEN üìå Ensure there is a space after Bearer before the token! 3Ô∏è‚É£ Link Credentials to Git Nodes Attach the Gitea credentials to these three Git nodes: GetGitea** ‚Üí Retrieves existing repository data PutGitea** ‚Üí Updates workflows PostGitea** ‚Üí Adds new workflows 4Ô∏è‚É£ Link Credentials for API Requests Add API authentication** in the node that fetches all workflows. 5Ô∏è‚É£ Test & Activate Run the workflow manually** to confirm backups work. Enable the schedule trigger for automation. üìå The workflow automatically checks for changes before committing updates. Why Use This Workflow? ‚úÖ Automated Backups ‚Üí No manual exports needed. ‚úÖ Version Control ‚Üí Easily track workflow changes. ‚úÖ Simple Setup ‚Üí Just configure globals & credentials. ‚úÖ Secure ‚Üí Uses token-based authentication. Next Steps üí¨ Have questions? Reach out on the forum! üöÄ",1032,2025-01-29 14:22:41.191000+00:00,False,2
2169,Watchdog: Update All Workflows With Default Error Workflow,"Do you consistently forget to set a Default Error Workflow when creating new workflows? Then this helper workflow is for you! When activated, this helper workflow will: Scan ALL other workflows every 4 hours Make sure ALL workflows have a default error workflow set (based on what Workflow ID you provide) This helper will SKIP OVER any workflows that have the default_error:false tag set (make sure your default error workflow has the default_error:false tag set, so that you don't end up with recursive loops during errors) Setup Nodes: Once imported, edit the Set Vars node with your default_error_workflow_id value. If you want to change the default_error:false tag to some other tag name, you can do so here as well. You need to update the Set Default Error Workflow node with your PostgreSQL credentials to access the n8n database.",1026,2024-03-08 21:48:19.946000+00:00,False,1
2887,Export PDF invoices from SmartBill to Google Drive,"This workflow automates the retrieval of invoice PDFs from the Smartbill API and saves them to Google Drive in a dynamically created folder based on last month‚Äôs date. It also generates a range of invoice numbers, ensuring proper formatting, and uploads each PDF with a structured filename. Overview Trigger and Data Setup: The workflow is manually triggered. It sets the invoice range (start and end numbers), invoice series, the parent Google Drive folder ID and the folder where to save the PDF files. Folder Handling: Folder Name Calculation: Calculates a default folder name based on last month‚Äôs date (formatted as YYYY-MM). Invoice Generation: A code node generates invoice items by iterating over a specified range of numbers. Each invoice number is padded to custom number of digits (e.g., 0013), and the invoice series and folder ID are attached to each item. Retrieving Invoice PDFs: For each generated invoice, the Smartbill API is called (using an HTTP Request node) to retrieve the corresponding invoice PDF. How to Use Credentials:** Ensure you have configured the Smartbill API (HTTP Basic Auth) and Google API credentials correctly. Parameter Adjustment:** Modify the SetData node if you need to change the invoice range, series, or target parent folder. Execution:** Click Execute Workflow to run the workflow manually.",1026,2025-02-12 11:02:03.340000+00:00,False,3
4525,"Build a Multi-functional Telegram Bot with Gemini, RAG PDF Search & Google Suite","The Ultimate Beginner's Guide to an AI-Powered Telegram Assistant (PDF, Brave search & Google Suite) This comprehensive workflow bundle is designed as a powerful starter kit, enabling you to build a multi-functional AI assistant on Telegram. It seamlessly integrates AI-powered voice interactions, an intelligent PDF document search using a Retrieval-Augmented Generation (RAG) system, and automates various Google Suite tasks like calendar management and document generation. Perfect for beginners looking to explore advanced AI and automation capabilities. Disclaimer This template is designed for self-hosted n8n instances. üöÄ Key Features Telegram Bot Interface:** Interact with your AI assistant using both text and voice commands through Telegram. AI Voice Bot:** Transcribes user voice messages using OpenAI Whisper. Processes requests with an AI agent powered by Google Gemini. Responds with AI-synthesized voice using Replicate API. PDF RAG System:** Index PDF documents from Google Drive via Telegram commands. Utilizes Mistral AI for Optical Character Recognition (OCR) on PDFs. Stores document content and embeddings in a Qdrant vector database. Answers questions about your documents using Google Gemini, based on retrieved context. Google Suite Automation:** Manage Google Calendar: Create events, find upcoming holidays, and list birthdays. Google Drive: Search for PDF files and manage document templates. Google Docs: Automatically generate invoices from templates. Intelligent Web Search:** Employs Brave Search for fetching real-time information from the web. Versatile AI Agent:** Leverages Google Gemini with a suite of tools including a calculator, date & time utilities, and custom integrations (e.g., example Airbnb tools). Command-Driven Functionality:** Easily trigger specific actions using Telegram commands like /help, /pdf, /rag, /invoice, /chat, /brave, and /birthday. ‚öôÔ∏è How It Works The workflow is initiated by messages or commands sent to your Telegram bot. A central Switch node directs the flow based on the input received. 1. Telegram Interaction & Command Routing A Telegram Trigger node listens for new messages (text or voice). A ""typing..."" indicator is sent to Telegram for better user experience. The Switch node parses the message content and routes it to the appropriate sub-workflow based on predefined commands (e.g., /pdf, /rag, /voice) or general chat. 2. AI Voice Bot Functionality For voice messages (or if the voice path is triggered): The voice file is downloaded from Telegram. OpenAI Whisper transcribes the audio into text. The transcribed text is fed to an AI Agent (powered by Google Gemini and equipped with various tools and memory). The AI Agent's text response is then sent to the Replicate API to generate a natural-sounding voice. The generated audio response is sent back to the user on Telegram. 3. PDF RAG System with Mistral OCR & Qdrant Indexing PDFs (via /qdrant &lt;Google_Drive_File_ID&gt; command):** The specified PDF is downloaded from Google Drive. The PDF is uploaded to Mistral AI for OCR processing. The extracted text (in markdown format) is retrieved. The text is split into manageable chunks. OpenAI Embeddings are generated for each chunk. These chunks and their embeddings are stored in a Qdrant vector collection. A confirmation message is sent to Telegram. Querying PDFs (via /rag &lt;your_question&gt; command):** The user's question is processed by a RetrievalQA Chain. This chain uses Google Gemini as the Language Model and retrieves relevant document chunks from Qdrant based on semantic similarity (embeddings). Google Gemini then generates a concise answer based on the user's question and the retrieved contextual information from the documents. The answer is sent back to the user on Telegram. Searching PDFs in Drive (via /pdf &lt;search_term&gt; command):** Searches your Google Drive for PDF files matching the provided search term. Sends a list of found PDF files (name and ID) to the user on Telegram, allowing them to easily identify files for indexing with /qdrant. 4. AI Chat & Task Automation (General Chat & /chat command) Handles general text messages or transcribed voice inputs that are not specific commands. An AI Agent (Google Gemini) processes these inputs. The agent is equipped with tools such as: Google Calendar Tools: To create events, find the next public holiday, or list upcoming birthdays. Brave Search Tool: To search the internet for information. Calculator Tool: For mathematical computations. Date & Time Utility: For handling and formatting dates. Example Airbnb Tools: Demonstrates extensibility for custom tool integrations. The agent utilizes a ""Think Node"" process for reasoning and Window Buffer Memory to maintain conversational context. Responses are formulated and sent back to the user on Telegram. 5. Google Automations Invoice Generation (via /invoice command):** Copies a predefined Google Docs invoice template from your Google Drive. Populates the new document with details (client name, items, etc. ‚Äì currently uses placeholder data from an 'Edit Fields' node, which can be customized to parse input from the command). Converts the populated Google Doc into a PDF file. Sends the generated PDF invoice to the user via Telegram. Calendar Management (via AI Agent or specific commands like /birthday):** The /birthday command fetches upcoming birthdays from a specified Google Calendar. The AI Agent can also interact with Google Calendar to create events or retrieve information based on natural language requests. 6. Web Search (via /brave &lt;query&gt; command or AI Agent) Performs a web search using the Brave Search API. Returns a list of search results, including titles and URLs, to the user on Telegram. 7. Help Command (/help) Provides the user with a formatted list of all available Telegram commands and their basic usage instructions. üõ†Ô∏è Setup Steps & Credentials To get this workflow bundle up and running, you'll need to configure several credentials: Telegram: Create a new bot via @BotFather on Telegram to obtain a Bot Token. Add this token to the Telegram Trigger node and all Telegram (sender) nodes in the workflow. OpenAI: Obtain an API Key from platform.openai.com. Create an OpenAI credential. Use this credential in the Convert audio to text (OpenAI node for Whisper transcription) and all Embeddings OpenAI nodes (for RAG system). Replicate API (Text-to-Speech): Sign up at replicate.com and get your API token. Create an HTTP Bearer Auth credential using this token. Assign this credential to the Call Replicate API (HTTP Request) node. Google Gemini (PaLM API): Get an API key from Google AI Studio or Google Cloud Console. Create a Google Gemini(PaLM) Api credential. Assign it to all Google Gemini Chat Model nodes. Qdrant (Vector Database): Set up a Qdrant instance (either cloud-hosted or self-managed). Note your Qdrant instance URL and API Key (if security is enabled). Create a QdrantApi credential with these details. Assign it to the Qdrant Vector Store nodes. For the Refresh collection node (an HTTP Request node used to clear the collection for /pdf command demo), create an HTTP Header Auth credential with your Qdrant API key if required by your instance. Mistral AI (PDF OCR): Obtain an API key from console.mistral.ai. Create a Mistral Cloud API credential. Assign this to the Mistral Upload, Mistral Signed URL, and Mistral DOC OCR (HTTP Request) nodes. Google Drive & Google Docs: Ensure the Google Drive API and Google Docs API are enabled in your Google Cloud Console project. Set up OAuth 2.0 credentials (Client ID and Client Secret). Create Google Drive OAuth2 Api and Google Docs OAuth2 Api credentials. Assign these to the respective Google Drive and Google Docs nodes throughout the workflow. Important: Configure relevant Folder IDs (for PDF search, invoice template source, invoice output) and the invoice template Document ID in the Google Drive and Google Docs nodes. Google Calendar: Enable the Google Calendar API in your Google Cloud Console project. Set up OAuth 2.0 credentials. Create a Google Calendar OAuth2 Api credential. Assign it to the Google CalendarTool and Google Calendar nodes. Specify the correct calendar names or IDs in the nodes (e.g., for birthdays, holidays, new event creation). Brave Search: Get a Brave Search API key from their developer portal. Create a Brave Search API credential. Assign it to the Brave Search nodes and tools. (Optional) Airbnb MCP Client: The workflow includes example nodes for Airbnb MCP tools. If you intend to use or expand these, set up the corresponding MCP Client API credentials. üí° Customization & Learning This ""Beginner Bundle"" is not just a ready-to-use solution but also a fantastic learning resource: Explore AI Agent Prompts:** Dive into the AI Agent nodes to see how prompts are structured to guide the LLM's behavior, including the ""Think Node"" process and character guidance for robust messaging. Modify Toolsets:** Easily add or remove tools for the AI agent to expand its capabilities. Invoice Customization:** Adapt the Edit Fields node and the Google Docs template to match your invoicing needs. You can extend it to parse invoice details directly from the Telegram command. RAG Parameters:** Experiment with chunk sizes, overlap, and different embedding models in the RAG pipeline. Extend Commands:** Add new commands and corresponding functionalities by expanding the main Switch node and building out new automation paths. By setting up and dissecting this workflow, beginners can gain a practical understanding of building complex, AI-driven applications, integrating various services, and managing different data flows.",1021,2025-05-30 22:34:27.586000+00:00,True,12
2787,Backup n8n Workflows to Bitbucket,"An automated backup solution designed for self-hosted n8n users to automatically backup their workflows to Bitbucket, leveraging Bitbucket's free private repository offering. Perfect for maintaining version control of your n8n workflows without additional costs. How it works: Runs on a regular schedule to check all workflows in your n8n instance Compares each workflow with its version in Bitbucket Only uploads workflows that are new or have changed Uses basic rate limiting to stay within Bitbucket's API limits Formats filenames for easy tracking and includes timestamps in commit messages Handles errors gracefully with automatic retries Set up steps (10-15 minutes): Create a free Bitbucket account and private repository Create a Bitbucket App Password with repository write access Add Bitbucket credentials to n8n (using your username and app password) Set up n8n API access (generate API key in your n8n instance) Configure your Bitbucket workspace and repository names in the Set node Optional: Adjust the backup schedule (default: 2 AM daily) Perfect for n8n self-hosters who want: Version control for their workflows Automated daily backups Free private repository storage Easy workflow recovery Change tracking over time The workflow includes basic error handling and rate limiting to ensure reliable backups even with larger numbers of workflows. Adjust your timing based on https://support.atlassian.com/bitbucket-cloud/docs/api-request-limits/.",1020,2025-01-24 00:25:33.966000+00:00,False,2
4817,Compose/Stitch Separate Images together using n8n & Gemini AI Image Editing,"This n8n template demonstrates how to use AI to compose or ""stitch"" separate images together to generate a new image which retains the source assets and consistent style. Use cases are many: Try producing storyboard scenes with consistent characters, marketing material with existing product assets or trying on different articles on fashion! Good to know At time of writing, each image generated will cost $0.039 USD. See Gemini Pricing for updated info. The model used in this workflow is geo-restricted! If it says model not found, it may not be available in your country or region. How it works We'll import our required assets via our Cloud storage using the HTTP node. The images are then converted to base64 strings and aggregated so we can use it for our AI model. Gemini's image generation model is used which takes all 3 images and a prompt that we define. Our prompt instructs the model on how to compose the final image. Gemini generates a new image but uses the original 3 assets to do so. The consistency to the source images is very high and shows little signs of hallucinations! Gemini's output is base64 so we use a ""Convert to file"" node to convert the data to binary. The final binary image is then uploaded to Google Drive to complete the demonstration. How to use The manual trigger node is used as an example but feel free to replace this with other triggers such as webhook or even a form. Technically, you should be able to compose even more images but of course, the generation will take longer and cost more. Requirements Gemini account for LLM and Image generation Google drive for upload Customising this workflow AI Image editing can be used for many use-cases. Try a popular use-case such as virtual try-on for fashion or applying branding on editing image assets.",1020,2025-06-09 13:00:49.415000+00:00,True,2
5629,"Multi-Channel Workflow Error Alerts with Telegram, Gmail & Messaging Apps","The Error Notification workflow is designed to instantly notify you whenever any other n8n workflow encounters an error, using popular communication channels like Telegram and Gmail‚Äîwith optional support for Discord, Slack, and WhatsApp. üí° Why Use Error Notification workflow? Immediate Awareness:** Get instant alerts when workflows fail, preventing unnoticed errors and downtime. Multi-Channel Flexibility:** Notify your team via Telegram, Gmail, and optionally Slack, Discord, or WhatsApp. Detailed Context:** Receive rich error information including the error message, node name, time, and execution link for quicker fixes. Easy Integration:** Built with native n8n nodes and customizable code, simple to adopt without complex setup. Open Source & Free:** Use and adapt this workflow at no cost, making professional error monitoring accessible. ‚ö° Who Is This For? n8n Workflow Developers:** Quickly spot and respond to automation issues in development or production. Operations Teams:** Maintain uptime and swiftly troubleshoot errors across multiple workflows. Small to Medium Businesses:** Gain professional error alerting without expensive monitoring tools. Automation Enthusiasts:** Enhance your automation reliability with real-time failure notifications. ‚ùì What Problem Does It Solve? This workflow embedd error detection and notification directly within your n8n instance. It automates the process of catching errors as they occur, compiling meaningful context, and delivering it instantly via your preferred messaging platforms. This drastically reduces your response time to issues and streamlines error management, improving your automation reliability and operational confidence. üîß What This Workflow Does ‚è± Trigger: Listens for any error generated in your n8n workflows using the n8n Error Trigger node. üìé Step 2: Executes a Code node that formats a detailed error message capturing workflow name, error node, description, timestamp, and an execution URL. üîç Step 3: Sends the formatted error notification to multiple communication channels: Telegram and Gmail by default, plus optionally Discord, Slack, and WhatsApp (disabled by default). üíå Step 4: Delivers rich, parsed HTML-formatted messages to ensure error readability and immediate actionability. üîê Setup Instructions Import the provided .json file into your n8n instance (Cloud or self-hosted). Set up credentials: Gmail OAuth credentials for sending emails via Gmail node Telegram API credentials for Telegram notifications (Optional) Discord Webhook URL credential for Discord notifications (Optional) Slack Webhook credential for Slack notifications (Optional) WhatsApp connection credentials (if enabled) Customize the Code node if needed to adjust the error message format or target chat IDs. Update the chat IDs and recipient details in each notification node according to your channels. Test the workflow by manually triggering an error in another workflow to verify proper notifications. üß© Pre-Requirements Active n8n instance (cloud or self-hosted) with version supporting Error Trigger node Telegram bot credentials and chat ID (Optional) Gmail, Discord, Slack, or WhatsApp accounts and webhook credentials if you want to use those channels üõ†Ô∏è Customize It Further Enable and configure additional notification nodes like Slack or WhatsApp to fit your team's communication style. Customize the error message template in the Code node to include extra metadata or format it differently (e.g., markdown). Integrate with incident management tools via webhook nodes or create tickets automatically on error. üß† Nodes Used Error Trigger Code Telegram Gmail Discord (disabled) Slack (disabled) WhatsApp (disabled) Sticky Note (for description) üìû Support Made by: khaisa Studio Tag: notification,error,monitoring,workflow,automation,alerts Category: Monitoring & Alerts Need a custom? Need a custom? contact me on LinkedIn or Web",1010,2025-07-03 10:30:15.915000+00:00,False,6
2709,Automated Email Optin Form with n8n and Hunter io for verification,"Case Study üìß Want to collect email subscribers without paying expensive monthly fees? üí∞ This workflow creates a free email collection system with built-in email verification to ensure you only collect legitimate email addresses! ‚ú® Learn more: üì∫ Watch the tutorial: What this workflow does üõ†Ô∏è Creates a customizable email collection form that can be embedded on your website üåê Verifies email addresses using Hunter.io to filter out fake or invalid emails ‚úÖ Stores verified email addresses in SendGrid for your email marketing needs üìä Completely free solution (except for Hunter.io's 50 free monthly credits) üÜì Setup ‚öôÔ∏è Set up a free Hunter.io account for email verification Configure your SendGrid account credentials Customize the email collection form fields Get the embedded form code for your website How to adjust it to your needs üîß Add additional form fields beyond just email collection Customize the form's appearance and labels Modify the verification logic based on your requirements Connect to different email marketing platforms instead of SendGrid Add additional automation steps after email verification Benefits üåü No monthly subscription fees for email collection üí∏ Built-in email verification prevents fake signups üõ°Ô∏è Scalable solution that won't lock you into expensive plans üìà Clean email list with only verified addresses ‚ú® Simple setup and customization üéØ This workflow is perfect for bloggers, small businesses, and anyone looking to build an email list without getting locked into expensive email marketing platforms as their subscriber count grows! üöÄ Built by rumjahn",1006,2025-01-09 05:40:32.489000+00:00,False,2
4873,üéØ Precision Prospecting: Automate LinkedIn Lead Gen with Bright Data,"üéØ Precision Prospecting: Automate LinkedIn Lead Gen with n8n & Bright Data üìù Overview This workflow turns n8n into an AI-powered prospector, automatically searching Google for LinkedIn profiles, scraping profile data via Bright Data, and summarizing key details. Ideal for sales and recruitment teams seeking targeted lead lists without manual research. üé• Workflow in Action Want to see this workflow in action? You have a chat window output below: üîë Key Features AI Chat Trigger**: Start prospecting via conversational prompts. Contextual Memory**: Retains the last 20 messages for coherent dialogue. Automated Google Search**: Generates site-restricted queries and fetches the top result. Bright Data Scraping**: Synchronously scrapes LinkedIn profile details by URL. Intelligent Filtering**: Extracts only valid LinkedIn profile links. Limit Control**: Returns a single, most relevant profile per request. LLM Summary**: Uses GPT-4o-mini to interpret and present scraped data. üöÄ How It Works (Step-by-Step) Prerequisites: n8n ‚â• v1.0 with community nodes: install n8n-nodes-brightdata (not verified community node). API credentials: OpenAI, Bright Data (web unlocker zone ‚Äúweb\_unlocker1‚Äù). Webhook endpoint for chat trigger. Node Configuration: When chat message received (chatTrigger): Fires on user prompt. Simple Memory1 (memoryBufferWindow): Stores the last 20 chat messages. AI Prospector Agent (agent): Orchestrates search logic. Get 1 Google Result (brightData): Performs a Google search with site:linkedin.com/in. Get Links from Body (html): Extracts all `` hrefs from the search result page. Extract Links (splitOut): Splits out individual link entries. Filter only LinkedIn Profiles (filter): Ensures the URL contains ‚Äúlinkedin.com/‚Äù and starts with ‚Äúhttps\://‚Äù. Limit (limit): Restricts output to the first valid profile URL. Search LinkedIn URI (toolWorkflow): Passes the URL to a secondary workflow to fetch the first link. Get LinkedIn Profile Data (brightDataTool): Scrapes the profile JSON. OpenAI Chat Model (lmChatOpenAi): Summarizes and formats the scraped data. Workflow Logic: User asks for a person by company & name, company & position, or LinkedIn URL. Agent builds a Google query (e.g., site:linkedin.com/in bright data cmo) and calls ‚ÄúGet 1 Google Result.‚Äù Extracted links are filtered and limited to the top valid profile. If user provided a direct LinkedIn URL, Agent skips search and scrapes immediately. Scraped profile JSON is passed to GPT-4o-mini to generate a concise summary. Testing & Optimization: Trigger via Execute Workflow for dry runs. Inspect intermediate node outputs in n8n‚Äôs Execution panel. Adjust maxIterations or memory window length for performance. Tune Bright Data zone or country settings to optimize scraping speed. Deployment & Monitoring: Activate the workflow and expose its webhook URL. Use n8n‚Äôs built-in Alerts or external monitoring (e.g., Slack notifications) on failures. Rotate credentials via n8n‚Äôs Credential Vault when needed. Version-control workflow via duplicates or Git-backed n8n instances. ‚úÖ Pre-requisites OpenAI Account**: API key for GPT-4o-mini. Bright Data Account**: Zone ‚Äúweb\_unlocker1‚Äù and dataset gd_l1viktl72bvl7bjuj0. n8n Version**: v1.0+ with community nodes installed. Permissions**: Webhook access, Credential Vault read/write. üë§ Who Is This For? Sales teams automating outbound LinkedIn prospecting. Recruiters sourcing candidates without manual scraping. Marketing ops looking to enrich CRM with accurate profile data. üìà Benefits & Use Cases Efficiency**: Reduces hours of manual search and data entry to seconds. Accuracy**: Filters out non-LinkedIn links and ensures high-quality results. Scalability**: Handle multiple prospect requests concurrently via chat or API. Integration**: Easily hook into CRMs or email sequencers downstream. Workflow created and verified by Miquel Colomer https://www.linkedin.com/in/miquelcolomersalas/ and N8nHackers https://n8nhackers.com",986,2025-06-11 09:26:01.909000+00:00,True,5
4815,Build an Image Restoration Service with n8n & Gemini AI Image Editing,"This n8n template demonstrates how to build a simple but effective vintage image restoration service using an AI model with image editing capabilities. With Gemini now capable of multimodal output, it's a great time to explore this capability for image or graphics automation. Let's see how well it does for a task such as image restoration. Good to know At time of writing, each image generated will cost $0.039 USD. See Gemini Pricing for updated info. The model used in this workflow is geo-restricted! If it says model not found, it may not be available in your country or region. How it works Images are imported into our workflow via the HTTP node and converted to base64 strings using the Extract from file node. The image data is then pipelined to Gemini's Image Generation model. A prompt is provided to instruct Gemini to ""restore"" the image to near new condition - of course, feel free to experiment with this prompt to improve the results! Gemini's responds with the image as a base64 string and hence, a convert to file node is used to transform the data to binary. With the restored image as a binary, we can then use this with our Google Drive node to upload it to our desired folder. How to use This demonstration uses 3 random images sourced from the internet but any typical image file will work. Use a webhook node to allow integration from other applications. Use a telegram trigger for instant mobile service! Requirements Google Gemini for LLM/Image generation Google Drive for Upload Storage Customising this workflow AI image editing can be applied to many use-cases not just image restoration. Try using it to add watermarks, branding or modify an existing image for marketing purposes.",985,2025-06-09 12:56:20.236000+00:00,True,2
2746,Post Hourly Crypto Market Summaries via Coingecko to X and to Email,"Description This workflow, delivers real-time cryptocurrency market updates (default: Bitcoin) by fetching data from the CoinGecko API. It formats the information into a visually engaging message and shares it on X (formerly Twitter) and via email. The workflow is set to trigger hourly but is fully customizable to suit different schedules or cryptocurrencies. Key Features Real-Time Market Data Retrieval:** Fetches up-to-date cryptocurrency market data using CoinGecko API. AI-Driven Message Formatting:** Converts raw data into a structured, engaging message. Social Media Integration:** Posts updates to X with OAuth2 authentication. Email Notifications:** Sends updates via email to specified recipients. Flexible Customization:** Adaptable for tracking multiple cryptocurrencies or adding new notification channels. Workflow Steps Trigger Workflow: Default is hourly, but the interval can be customized. Fetch Cryptocurrency Data: Retrieves market data using the CoinGecko API. Format the Message: Converts the data into a readable and visually appealing format. Post Update on X: Shares the message on X (formerly Twitter). Send Email Notifications: Sends the update to a specified email address. Optional Notifications: Expand to additional channels like Slack or Telegram.",975,2025-01-17 11:18:56.845000+00:00,False,4
2531,Save Qualys Reports to TheHive,"Automate Report Generation with n8n & Qualys Introducing the Save Qualys Reports to TheHive Workflow‚Äîa robust solution designed to automate the retrieval and storage of Qualys reports in TheHive. This workflow fetches reports from Qualys, filters out already processed reports, and creates cases in TheHive for the new reports. It runs every hour to ensure continuous monitoring and up-to-date vulnerability management, making it ideal for Security Operations Centers (SOCs). How It Works: Set Global Variables:** Initializes necessary global variables like base_url and newtimestamp. This step ensures that the workflow operates with the correct configuration and up-to-date timestamps. Ensure to change the Global Variables to match your environment. Fetch Reports from Qualys:** Sends a GET request to the Qualys API to retrieve finished reports. Automating this step ensures timely updates and consistent data retrieval. Convert XML to JSON:** Converts the XML response to JSON format for easier data manipulation. This transformation simplifies further processing and integration into TheHive. Filter Reports:** Checks if the reports have already been processed using their creation timestamps. This filtering ensures that only new reports are handled, avoiding duplicates. Process Each Report:** Loops through the list of new reports, ensuring each is processed individually. This step-by-step handling prevents issues related to bulk processing and improves reliability. Create Case in TheHive:** Generates a new case in TheHive for each report, serving as a container for the report data. Automating case creation improves efficiency and ensures that all relevant data is captured. Download and Attach Report:** Downloads the report from Qualys and attaches it to the respective case in TheHive. This automation ensures that all data is properly archived and easily accessible for review. Get Started: Ensure your Qualys and TheHive integrations are properly set up. Customize the workflow to fit your specific vulnerability management needs. Need Help? Join the discussion on our Forum or check out resources on Discord! Deploy this workflow to streamline your vulnerability management process, improve response times, and enhance the efficiency of your security operations.",971,2024-11-05 18:14:55.243000+00:00,False,2
2633,INSEE Company Data Enrichment for Agile CRM (For French companies only),How it works 1) Extracts all company entries in Agile CRM 2) Search for company name in French INSEE OpenData database to extract address and government ID (SIREN) 3) Updates entries with data extracted from French Insee OpenData dabase Workflow also has a readonly feature to make sure entry is not overwritten. Setup steps Add your AgileCRM credentials Add your INSEE OpenData credentials Add two company custom fields in your Agile CRM (for SIREN data and ReadOnly support),969,2024-12-11 16:16:09.355000+00:00,False,3
4743,Binance SM 15min Indicators Tool,"A short-term technical analysis agent for 15-minute candles on Binance Spot Market pairs. Calculates and interprets key trading indicators (RSI, MACD, BBANDS, ADX, SMA/EMA) and returns structured summaries, optimized for Telegram or downstream AI trading agents. This tool is designed to be triggered by another workflow (such as the Binance SM Financial Analyst Tool or Binance Quant AI Agent) and is not intended for standalone use. üîß Key Features ‚è±Ô∏è Uses 15-minute kline data (last 100 candles) üìà Calculates: RSI, MACD, Bollinger Bands, SMA/EMA, ADX üß† Interprets numeric data using GPT-4.1-mini üì§ Outputs concise, formatted analysis like: ‚Ä¢ RSI: 72 ‚Üí Overbought ‚Ä¢ MACD: Cross Up ‚Ä¢ BB: Expanding ‚Ä¢ ADX: 34 ‚Üí Strong Trend üß† AI Agent Purpose &gt; You are a short-term analysis tool for spotting volatility, early breakouts, and scalping setups. Used by higher agents to determine: Entry/exit precision Momentum shifts Scalping opportunities ‚öôÔ∏è How it Works Triggered externally by another workflow Accepts input: { ""message"": ""BTCUSDT"", ""sessionId"": ""123456789"" } Sends POST request to backend endpoint: https://treasurium.app.n8n.cloud/webhook/15m-indicators Fetches last 100 candles and calculates indicators Passes data to GPT for interpretation Returns summary with indicator tags for human readability üîó Dependencies This tool is triggered by: ‚úÖ Binance SM Financial Analyst Tool ‚úÖ Binance Spot Market Quant AI Agent üöÄ Setup Instructions Import into your n8n instance Make sure /15m-indicators webhook is active and calculates indicators correctly Connect your OpenAI GPT-4.1-mini credentials Trigger from upstream agent with Binance symbol and session ID Ensure all external calls (to Binance + webhook) are working üß™ Example Use Cases | Use Case | Result | | ------------------------------------- | --------------------------------------- | | Short-term trade decision for ETHUSDT | Receives 15m signal indicators summary | | Input from Financial Analyst Tool | Returns real-time volatility snapshot | | Telegram bot asks for ‚ÄúDOGE update‚Äù | Returns momentum indicators in 15m view | üé• Watch Tutorial: üßæ Licensing & Attribution ¬© 2025 Treasurium Capital Limited Company Architecture, prompts, and trade report structure are IP-protected. No unauthorized rebranding or resale permitted. üîó For support: Don Jayamaha ‚Äì LinkedIn",956,2025-06-06 18:11:59.825000+00:00,True,3
4797,Track CVE Vulnerability Details & History with NVD API and Google Sheets,"Who is this for? NVD (National Vulnerability Database) data is essential for security analysts, vulnerability managers, and DevSecOps professionals who need to perform both CVE lookups and monitor historical change logs. This workflow helps streamline those efforts by providing structured outputs for audit, triage, or compliance tracking purposes. üìù Note: While this example uses Google Sheets as the destination, you can easily modify the final destination node (e.g., send to Slack, email, database, etc.) based on your specific automation needs.? What problem is this solving? Security teams often manually look up CVE data and track changes across multiple tools. This process is inefficient and error-prone. This workflow automates the CVE lookup and historical change tracking by logging enriched vulnerability data into Google Sheets in real-time. What this workflow does This workflow is designed for CVE API lookup and change history tracking. In many vulnerability automation pipelines, it is essential to determine not only the metadata of a CVE but also how it has evolved over time. Based on the operational need‚Äîwhether it's enrichment, risk scoring, or remediation validation‚Äîthis workflow becomes particularly handy in surfacing both current and historical CVE data. This template performs the following actions: Accepts incoming webhook requests containing a CVE ID Queries the NVD CVE Lookup API to fetch vulnerability metadata Queries the NVD CVE History API to retrieve all historical changes Flattens both datasets into a sheet-compatible structure Appends vulnerability metadata to one sheet and change history to another within the same Google Spreadsheet Setup üîë Request an NVD API Key To request an NVD API Key, please provide your organization name, a valid email address, and indicate your organization type at NVD API Key Request. You must scroll to the end of the Terms of Use Agreement and check ""I agree to the Terms of Use"" to obtain an API Key. After submission, you will receive a single-use hyperlink via email to activate and view your API Key. If not activated within seven days, a new request must be submitted. üìä API Rate Limits Without an API key, you're limited to 5 requests per 30-second window. With an API key, you‚Äôre allowed up to 50 requests in the same period. To prevent request throttling, it's recommended to introduce slight delays between consecutive API calls in production setups. Clone or import this workflow into your n8n instance. Set up the following credentials: Google Sheets OAuth2 NVD API Key (via HTTP Header Auth) The workflow logs data to a Google Sheet titled NVD Database, with Sheet 1 named CVE Lookup and Sheet 2 named CVE History. Trigger each workflow using the respective webhook URL, appending ?cveId=CVE-XXXX-XXXX as a query parameter. üîç Example Webhook Request (CVE Change History) You can test this workflow with the following example: GET https://your-domain.com/webhook/cve-history?cveId=CVE-2023-34362 How to customize this workflow Use the Edit Fields node (optional) to centralize configuration like sheet name or query input Extend the CVE flattening logic to include more nested metadata if needed Integrate notification systems (e.g., Slack or email) by branching from the processing nodes Modify webhook paths for better endpoint organization üîê Production Security Tips Use HTTP Header Auth on the webhook for secure access &gt; ‚ö†Ô∏è This template uses webhooks and NVD API access with authentication headers. This template uses two flows: Webhook 1:** NVD CVE Lookup ‚Äî Lookup CVE vulnerability metadata from NVD and sync to Google Sheet Webhook 2:** NVD CVE Change History ‚Äî Track change history for CVEs via NVD and log each update Each flow: Hits NVD‚Äôs respective endpoint Uses custom JS Code node to flatten the nested JSON Syncs data to dedicated Google Sheet tabs üß© 4 nodes: Webhook ‚Üí API Call ‚Üí Parse ‚Üí Sheet Sync Make sure both flows are activated and webhooks exposed for external access. Based on your needs, ensure you have a secure setup‚Äîwhether hosted internally or in a cloud environment‚Äîwhen running n8n in production.",949,2025-06-08 15:14:27.731000+00:00,False,3
2422,Venafi Cloud Slack Cert Bot,"Enhance Security Operations with the Venafi Slack CertBot! Venafi Presentation - Watch Video Our Venafi Slack CertBot is strategically designed to facilitate immediate security operations directly from Slack. This tool allows end users to request Certificate Signing Requests that are automatically approved or passed to the Secops team for manual approval depending on the Virustotal analysis of the requested domain. Not only does this help centralize requests, but it helps an organization maintain the security certifications by allowing automated processes to log and analyze requests in real time. Workflow Highlights: Interactive Modals**: Utilizes Slack modals to gather user inputs for scan configurations and report generation, providing a user-friendly interface for complex operations. Dynamic Workflow Execution**: Integrates seamlessly with Venafi to execute CSR generation and if any issues are found, AI can generate a custom report that is then passed to a slack teams channel for manual approval with the press of a single button. Operational Flow: Parse Webhook Data**: Captures and parses incoming data from Slack to understand user commands accurately. Execute Actions**: Depending on the user's selection, the workflow triggers other actions within the flow like automatic Virustotal Scanning. Respond to Slack**: Ensures that every interaction is acknowledged, maintaining a smooth user experience by managing modal popups and sending appropriate responses. Setup Instructions: Verify that Slack and Qualys API integrations are correctly configured for seamless interaction. Customize the modal interfaces to align with your organization's operational protocols and security policies. Test the workflow to ensure that it responds accurately to Slack commands and that the integration with Qualys is functioning as expected. Need Assistance? Explore Venafi's Documentation or get help from the n8n Community for more detailed guidance on setup and customization. Deploy this bot within your Slack environment to significantly enhance the efficiency and responsiveness of your security operations, enabling proactive management of CSR's.",944,2024-09-19 23:37:20.885000+00:00,True,4
2719,Retry on fail except for known error,"Purpose This workflow snippet allows for advanced error catching during retry attempts. There are cases, where you want to check if an item exists first, so you can determine the following actions. Some API‚Äôs do not support an endpoint (e.g. Todoist: completed tasks) to do so, which is why you would work with the error branch, only that this does not work well in combination with the retry functionality. How it works Instead of the builtin retry function of a Node a custom loop is used, to get more granular control in between the iterations If the main executed node fails, the error can be filtered for an expected error, which can trigger a separate action The retries only happen, if an unexpected error happened The workflow only stops, if the defined amount of retries exceeded Setup Copy the nodes into your existing workflow Replace the ‚ÄúReplace me‚Äù placeholder with the Node you want to apply the retry logic on Follow the sticky notes for more instructions and optional settings",940,2025-01-12 13:17:18.184000+00:00,False,0
2444,Get bibliographic data from your Zotero Library,"What this template does This workflow will read your Zotero Library and extract Meta Data from the articles of one collection in your bibliography. You can personalize the output for optimized results. How it works Mainly, follow the instructions in the Post it notes: Go to https://www.zotero.org/settings/security and find your USER ID (It's right under the APPLICATIONS Section. On the same website, create a New Private Key. In the ""Collections"" Node, select Generic Credential Type &gt; Header Auth &gt; Create New Credential using: NAME: Zotero-API-Key VALUE: [Your Private Key] Run your Flow to check if it works and open the ""Select Collection"" node. See the Results of the previous node as TABLE and copy the ""KEY"" of the collection you want to use. After that you should have a working flow that reads your bibliography. You can edit or delete the last 2 nodes to personalize your results (Filter and Edit Fields)",938,2024-09-30 15:19:43.145000+00:00,False,1
4872,üè† Find your Home with Real Estate Agent and Bright Data,"üìù Overview This workflow transforms n8n into a smart real-estate concierge by combining an AI chat interface with Bright Data‚Äôs marketplace datasets. Users interact via chat to specify city, price, bedrooms, and bathrooms‚Äîand receive a curated list of three homes for sale, complete with images and briefings. üé• Workflow in Action Want to see this workflow in action? Play the video üîë Key Features AI-Powered Chat Trigger:** Instantly start conversations using LangChain‚Äôs Chat Trigger node. Contextual Memory:** Retain up to 30 recent messages for coherent back-and-forth. Bright Data Integration:** Dynamically filter ‚ÄúFOR\_SALE‚Äù properties by city, price, bedrooms, and bathrooms (limit = 3). Automated Snapshot Retrieval:** Poll for dataset readiness and fetch full snapshot content. HTML-Formatted Output:** Present results as a ` of ` items, embedding property images. üöÄ How It Works (Step-by-Step) Prerequisites: n8n ‚â• v1.0 Community nodes: install n8n-nodes-brightdata (the unverified community node) API credentials: OpenAI, Bright Data Webhook endpoint to receive chat messages Node Configuration: Chat Trigger: Listens for incoming chat messages; shows a welcome screen. Memory Buffer: Stores the last 30 messages for context. OpenAI Chat Model: Uses GPT-4o-mini to interpret user intent. Real Estate AI Agent: Orchestrates filtering logic, calls tools, and formats responses. Bright Data ‚ÄúFilter Dataset‚Äù Tool: Applies user-defined filters plus homeStatus = FOR_SALE. Wait & Recover Snapshot: Polls until snapshot is ready, then fetches content. Get Snapshot Content: Converts raw JSON into a structured list. Workflow Logic: User sends search criteria ‚Üí Agent validates inputs. Agent invokes ‚ÄúFilter Dataset‚Äù once all filters are present. Upon dataset readiness, the snapshot is retrieved and parsed. Final output rendered as a bullet list with property images. Testing & Optimization: Use the built-in Execute Workflow trigger for rapid dry runs. Inspect node outputs in n8n‚Äôs UI; adjust filter defaults or snapshot limits. Tune OpenAI model parameters (e.g., maxIterations) for faster responses. Deployment & Monitoring: Activate the main workflow and expose its webhook URL. Monitor executions in the ‚ÄúExecutions‚Äù panel; set up alerts for errors. Archive or duplicate workflows as needed; update credentials via credential manager. ‚úÖ Pre-requisites Bright Data Account:** API key for marketplaceDataset. OpenAI Account:** Access to GPT-4o-mini model. n8n Version:** v1.0 or later with community node support. Permissions:** Webhook access, credential vault read/write. üë§ Who Is This For? Real-estate agencies and brokers seeking to automate client queries. PropTech startups building conversational search tools. Data analysts who want on-demand property snapshots without manual scraping. üìà Benefits & Use Cases Time Savings:** Replace manual MLS searches with an AI-driven chat. Scalability:** Serve multiple clients simultaneously via webchat or embedded widget. Consistency:** Always report exactly three properties, ensuring concise results. Engagement:** Visual listings with images boost user satisfaction and conversion. Workflow created and verified by Miquel Colomer https://www.linkedin.com/in/miquelcolomersalas/ and N8nHackers https://n8nhackers.com",937,2025-06-11 09:15:41.161000+00:00,True,4
2561,Send Matomo analytics data to A.I. to analyze then save results in Baserow,"Who's this for? If you own a website and need to analyze your Matomo analytics data so you can increse the number of frequent visitors If you need to create an SEO report on what are the common trends amongst your most frequent visitors If you want to grow your site based on suggestions from data Matomo is an analytics tool that can give you details of each individual visitor. Much more powerful than Google analytics. Watch youtube tutorial here Get my SEO A.I. agent system here Here's the A.I. output: Keywords showing the most improvement: Openrouter N8N. Keywords needing attention: Ai Generated Reference Letter Obsidian Second Brain Suggested actions for improvement: Optimize for ""best Docker Synology"" despite stable ranking, an improvement to top 10 is an achievable goal. Since ""2nd brain app for developer"" is of interest to a developer. Consider writing a blog post on how the app addresses the specific pain points of developers. Use case Instead of hiring an SEO expert, I run this report weekly. It looks at the data for the past week and looks for visitors with more than 3 visits and recommends ideas to convert more visitors into frequent visitors. How it works The workflow gathers matomo analytics for the past 7 days. We then parse the data The data is sent to Openrouter and using a FREE LLM, it analyses the data. It stores the results in baserow How to use this Input your Matomo analytics credentials Input your Matomo site ID Input your Openrouter.ai credentials Input your baserow credentials You will need to create a baserow database with columns: Dates, Notes, Blog. Created by Rumjahn",936,2024-11-21 10:17:45.990000+00:00,True,3
9879,Auto-Send WooCommerce Invoices via WhatsApp with Rapiwa API,"Who is this for? This workflow listens for new or updated WooCommerce orders, cleans and structures the order data, processes orders in batches, and standardizes WhatsApp phone numbers. It verifies phone numbers via the Rapiwa API, sends invoice links or messages to verified numbers, and logs results into separate Google Sheets tabs for verified and unverified numbers. Throttling and looping are managed using batch processing and wait delays. What this Workflow Does Receives order events (e.g., order.updated) from WooCommerce or a similar trigger. Extracts customer, billing/shipping address, product list, and invoice link from the order payload. Processes orders/items in batches for controlled throughput. Cleans and normalizes phone numbers by removing non-digit characters. Verifies whether a phone number is registered on WhatsApp using the Rapiwa API. If verified, sends a personalized message or invoice link via Rapiwa's send-message endpoint. If not verified, logs the customer as unverified in Google Sheets. Logs every send attempt (status and validity) into Google Sheets. Uses Wait nodes and batching to avoid API rate limits. Key Features Trigger-based automation (WooCommerce trigger; adaptable to Shopify webhook). Batch processing using SplitInBatches for stable throughput. Phone number cleaning using JavaScript (waNoStr.replace(/\D/g, """")). Pre-send WhatsApp verification via Rapiwa to reduce failed sends. Conditional branching (IF node) between verified and unverified flows. Personalized message templates that include product and customer fields. Logging to Google Sheets with separate flows for verified/sent and unverified/not sent. Wait node for throttling and looping control. Requirements Running n8n instance with nodes: HTTP Request, Code, SplitInBatches, IF, Google Sheets, Wait, and a WooCommerce trigger (or equivalent). Rapiwa account and Bearer token for verify/send endpoints. Google account and Google Sheets access with OAuth2 credentials. WooCommerce store access credentials (or Shopify credentials if adapting). Incoming order payloads containing billing and line_items fields. Google Sheet format (example rows) A Google Sheet formatted like this ‚û§ Sample | Customer Name | Phone Number | Email Address | Address | Product Title | Product ID | Size | Quantity | Total Price | Product Image | Invoice Link | Status | Validity | | -------------- | ------------- | --------------------------------------------------------------------- | ----------- | ------------------------------------ | ---------- | ---- | -------- | ----------- | ------------------------------------------------------------------------------------------------------------------------------------------ | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | -------- | ---------- | | Abdul Mannan| 8801322827799 | contact@spagreen.net | mirpur| T-Shirt - XL | 110 | XL | 1 | BDT 499.00 | https://your_shop_domain/Product/gg.img | https://your_shop_domain/INV/DAS | sent | verified | | Abdul Mannan| 8801322827799 | contact@spagreen.net | mirpur| T-Shirt - XL | 110 | XL | 1 | BDT 499.00 | https://your_shop_domain/Product/gg.img | https://your_shop_domain/INV/DAS | not sent | unverified | Important Notes The Code nodes assume billing and line_items exist in the incoming payload; update mappings if your source differs. The message template references products[0]; if orders contain multiple items, update logic to summarize or iterate products. Start testing with small batches to avoid accidental mass messaging and to respect Rapiwa rate limits. Useful Links Dashboard:** https://app.rapiwa.com Official Website:** https://rapiwa.com Documentation:** https://docs.rapiwa.com Support & Help WhatsApp**: Chat on WhatsApp Discord**: SpaGreen Community Facebook Group**: SpaGreen Support Website**: https://spagreen.net Developer Portfolio**: Codecanyon SpaGreen",936,2025-10-19 04:53:59.196000+00:00,False,2
2646,Automatically prune n8n execution history,"Automated Execution Pruning This workflow is designed to help you manage and optimize your n8n instance by automatically pruning old workflow executions, ensuring a cleaner environment and improved performance. You can customize the retention period to suit your needs. Key Features: Configurable Retention Period: The workflow is preconfigured to delete workflow executions older than 10 days. You can easily adjust this duration by modifying the condition in the If node. Daily Automation: Using the Schedule Trigger, the workflow runs daily at the specified time (default: 4:44 AM), retrieving all workflow executions and identifying those that are older than the defined retention period. On-Demand Testing: The Manual Trigger allows you to test the workflow at any time, ensuring everything is working as expected. Decision Making: The If node evaluates each execution based on its start date and determines whether it should be deleted or retained. Execution Pruning: Delete Action: Executions meeting the criteria are removed via the Delete Execution node. No-Operation: Executions that don't meet the criteria remain untouched. Workflow Nodes: Manual Trigger: Enables on-demand testing of the workflow. Schedule Trigger: Runs the workflow daily at the configured time. n8n List Execution: Fetches all executions in your n8n instance. If Node: Compares the execution's start date with the configured retention period. Delete Execution: Deletes executions older than the specified retention period. No Operation: Serves as a placeholder for executions that don't meet the pruning criteria. How to Customize: Retention Period**: Update the If node's condition to modify the retention period. For instance, change 10 * 24 * 60 * 60 * 1000 to the desired number of days in milliseconds. Schedule**: Adjust the timing of the Schedule Trigger to match your preferred automation schedule. This workflow ensures your instance remains efficient by keeping only the relevant execution logs. Use it to maintain a streamlined and clutter-free environment effortlessly.",927,2024-12-17 09:10:02.525000+00:00,False,0
4803,"Personalized LinkedIn Connection Requests with Apollo, GPT-4, Apify & PhantomBuster","AI LinkedIn Outreach Automation with Apollo, OpenAI & PhantomBuster Categories:* Sales Automation Lead Generation AI Personalization This workflow creates a complete LinkedIn outreach automation system that generates targeted lead lists from Apollo using natural language, enriches profiles with AI-personalized icebreakers, and automatically sends connection requests through PhantomBuster. Built by someone who's made over $1 million with AI automation, this system demonstrates the real-world approach to building profitable automation workflows. Benefits* Natural Language Lead Targeting - Describe your ideal prospects in plain English and automatically generate Apollo search URLs AI-Powered Personalization - Creates custom icebreakers based on LinkedIn profile data, employment history, and professional background Complete Outreach Pipeline - From lead discovery to personalized connection requests, fully automated end-to-end Smart Data Management - Automatically tracks all prospects in Google Sheets with deduplication and status tracking Cost-Effective Scraping - Uses Apify to extract Apollo data without expensive subscription costs Scalable Architecture - Processes hundreds of leads while respecting LinkedIn's connection limits How It Works* Natural Language Lead Generation: Form input accepts audience descriptions in plain English AI converts descriptions into properly formatted Apollo search URLs Automatically includes location, company size, job titles, and keyword filters Apollo Data Extraction: Uses Apify actor to scrape targeted lead lists from Apollo Extracts LinkedIn URLs, email addresses, employment history, and profile data Processes 500+ leads per run with detailed professional information AI Personalization Engine: Analyzes LinkedIn profile data including job history and company information Generates personalized icebreakers using proven connection request templates Creates human-like messages that reference specific career details and achievements Google Sheets Integration: Automatically stores all lead data in organized spreadsheet format Tracks prospect information, contact details, and generated icebreakers Provides easy data management and campaign tracking PhantomBuster Automation: Connects to PhantomBuster API to trigger LinkedIn connection campaigns Sends personalized connection requests with custom icebreakers Respects LinkedIn's daily limits and mimics human behavior patterns Business Use Cases* Sales Teams - Automate prospecting for B2B outreach campaigns Agencies - Scale client acquisition through targeted LinkedIn outreach Recruiters - Find and connect with qualified candidates efficiently Entrepreneurs - Build professional networks in specific industries Business Development - Generate qualified leads for partnership opportunities Revenue Potential This system can replace expensive LinkedIn outreach tools that cost $200-500/month. Users typically see: 400% improvement in response rates through personalization 10x faster lead generation compared to manual prospecting Ability to process 500+ leads per hour vs. 10-20 manually Difficulty Level: Intermediate Estimated Build Time: 1-2 hours Monthly Operating Cost: ~$50 (Apollo + PhantomBuster + AI APIs) Watch My Complete 1-Hour Build* Want to see exactly how I built this system from scratch? I walk through the entire development process live, including all the debugging, API integrations, and real-world testing that goes into building profitable automation systems. üé• See My Live Build Process: ""Build This Automated AI LinkedIn DM System in 1 Hour (N8N)"" This comprehensive tutorial shows my actual development approach - including the detours, problem-solving, and iterative testing that real automation building involves. Required Google Sheets Setup* Create a Google Sheet with these exact column headers: Essential Lead Columns: id - Unique prospect identifier first_name - Contact's first name last_name - Contact's last name name - Full name linkedin_url - LinkedIn profile URL title - Current job title email_status - Email verification status photo_url - Profile photo URL icebreaker - AI-generated personalized message Setup Instructions: Create Google Sheet with these headers in row 1 Connect Google Sheets OAuth in n8n Update the document ID in the ""Add to Google Sheet"" node PhantomBuster will read from this sheet for automated outreach Set Up Steps* Apollo & Apify Configuration: Set up Apify account and obtain API credentials Configure Apollo scraper actor with proper parameters Test lead extraction with sample audience descriptions AI Personalization Setup: Configure OpenAI API for natural language processing and personalization Set up prompt templates for audience targeting and icebreaker generation Test personalization quality with sample LinkedIn profiles Google Sheets Integration: Create lead tracking spreadsheet with proper column structure Configure Google Sheets API credentials and permissions Set up data mapping for automatic lead storage PhantomBuster Connection: Set up PhantomBuster account and LinkedIn connection Configure LinkedIn auto-connect agent with custom message templates Connect API for automated campaign triggering Form and Workflow Setup: Configure form trigger for audience input collection Set up data flow between all components Add proper error handling and rate limiting Testing and Optimization: Start with small batches (5-10 connections daily) Monitor LinkedIn account health and response rates Optimize icebreaker templates based on performance data Important Compliance Notes* LinkedIn Limits: Respect 100 connection requests per week limit Account Safety: Use PhantomBuster's human-like behavior patterns Message Quality: Regularly update templates to avoid automation detection Response Management: Monitor and respond to replies within 24 hours Advanced Extensions* This system can be enhanced with: Multi-channel Outreach: Add email sequences for comprehensive campaigns A/B Testing: Test different icebreaker templates automatically CRM Integration: Connect to Salesforce, HubSpot, or other sales systems Response Tracking: Monitor reply rates and optimize messaging Explore My Channel* For more advanced automation systems that generate real business results, check out my YouTube channel where I share the exact strategies I've used to make over $1 million with AI automation.",925,2025-06-08 19:45:41.004000+00:00,True,3
2696,Complete Guide to Setting Up and Generating TOTP Codes in n8n üîê,"Setting Up and Generating TOTP Step 1: Receive QR Code and Extract the Link 1. Receive the QR Code from the 2FA service After enabling two-factor authentication (2FA) on services like OpenAI, Google, GitHub, etc., a QR Code will be given to you, which you need to scan. This QR Code contains the TOTP link used to generate one-time passcodes. 2. Extract the link from the received QR Code To extract the link from the QR Code, use online tools. These tools will help you extract the corresponding link. After using an online tool, the extracted link will appear in the following format: otpauth://totp/ServiceName:username?secret=secret_key&issuer=ServiceName For example: otpauth://totp/OpenAI:amir676080@gmail.com?secret=test-test-test&issuer=OpenAI Step 2: Create TOTP Credential in n8n Create a new Credential To use TOTP in n8n, you need to create a new TOTP Credential. Enter the details in the Credential In the Secret field:* Enter the *secret key** (extracted from the QR Code link). For example: test-test-test In the Label field:* Enter *ServiceName:username** For example: OpenAI:amir676080@gmail.com Save the Credential After entering the information, click Save to save the Credential. Step 3: Get the TOTP Code Click on Test Workflow After setting up the credentials in n8n, click on Test, and the corresponding code will be delivered to you. Output: [ { ""token"": ""720769"", ""secondsRemaining"": 18 } ] ==This code is exactly the same as the one generated by apps and services like Google Authenticator or Authy. üîê== Contact me on [Telegram]: https://t.me/amir676080",922,2025-01-04 21:19:45.650000+00:00,True,0
2166,Watchdog: Auto Resume Workflows,"If you have multiple users managing workflows, there may come a time where a user ‚Äúaccidentally‚Äù turns off a workflow. Or, if you have workflows that automatically turn off other workflows, that code might ‚Äúaccidentally‚Äù turn off the wrong one. In either case, here‚Äôs a workflow that can attempt to ‚Äúauto-start‚Äù accidentally disabled workflows: How it works: When activated, then every 4 hours, the workflow will search all other workflows that have the auto_resume:true tag present. If any other workflow has auto_resume:true set but is currently turned off, then this workflow will turn it back on. Of course, this watchdog won‚Äôt work if the watchdog workflow is turned off. That said, we‚Äôve found this useful in recovering from accidental actions that cause production workflows to be turned off.",917,2024-03-06 23:01:10.753000+00:00,False,0
4870,Automated Multilingual Gmail Draft Reply with OpenAI GPT-4o,"Automated Multilingual Gmail Draft Reply with OpenAI GPT-4o in n8n Who is this for? This workflow is ideal for anyone who receives a high volume of Gmail inquiries, especially those providing multilingual customer support or handling diverse client communications. What problem is this workflow solving? Managing frequent emails in multiple languages can be overwhelming. This workflow reduces manual drafting by automatically generating context-aware replies using OpenAI GPT-4o, letting users focus on personalization and quality assurance. What this workflow does Monitors your Gmail inbox for new emails with a specific label (e.g., ""Inquiry""). Uses OpenAI GPT-4o for message assessment and language detection. Parses information using a JSON parser. Generates an AI-powered draft reply in the detected language via OpenAI GPT-4o. Converts the reply to HTML and saves it as a draft in the original Gmail thread for your review. Setup Connect your Gmail account and set up relevant labels in both Gmail and the workflow. Integrate your OpenAI credentials in n8n. Configure the workflow trigger for your desired labels. How to customize this workflow to your needs Adjust label names in both Gmail and the workflow for different email categories. Define custom starting and ending phrases for draft replies per supported language. Expand supported languages or modify AI prompt instructions to suit your brand‚Äôs tone.",906,2025-06-11 08:11:17.450000+00:00,True,4
2691,Currency Conversion Workflow,"Purpose: This workflow exemplifies a sophisticated yet pragmatic mechanism for automating currency conversions by leveraging simple HTTP queries routed through a webhook. By intercepting user requests, sourcing real-time exchange rate data via Google search results, and formatting the data into actionable responses, it obviates the reliance on third-party APIs. This efficiency renders it an indispensable instrument for diverse applications, including dynamic pricing strategies for AI-driven systems, financial data automation, and real-time currency computation within complex workflows. The workflow's architectural simplicity ensures seamless integration across professional and academic domains, optimizing both scalability and reliability. Workflow Steps: Capture Conversion Query: The workflow initiates by intercepting user input delivered as a GET request through a configured webhook. Inputs should adhere to a structured syntax, such as 5usd to mxn, to ensure flawless processing. Testing Tip: Use tools like Postman or a browser to test GET requests and verify that the Webhook receives inputs correctly. Fetch Exchange Rate: Utilizing the HTTP Request node, a Google search query is executed to retrieve current exchange rate data. This method ensures the workflow remains economical and adaptable while circumventing API dependencies. Extract Conversion Data: By processing the returned HTML from Google's search results, this node extracts precise exchange rate figures and contextual information critical for accurate conversions. Error Handling: If extraction fails, verify that the input format is correct and update CSS selectors to reflect any changes in Google's HTML structure. Format Currency Response: The extracted data undergoes refinement and is formatted into a structured, user-friendly string that conveys the conversion results with clarity and precision. Send Conversion Response: The workflow culminates by dispatching the formatted response back to the request origin, completing the loop with efficiency and reliability. Required Configuration: Configure the Webhook node to accommodate GET requests. The query parameters should follow the format: https://your-webhook-url/currency-converter?q=5usd+to+mxn. Inputs must adhere strictly to the predefined syntax (e.g., 5usd to mxn). Deviations may induce processing errors or yield erroneous outputs. Customization Options: The Extract Conversion Data node‚Äôs CSS selectors can be fine-tuned to align with modifications in Google‚Äôs HTML structure, ensuring long-term operability. Adjustments to the Format Currency Response node enable bespoke output formatting, incorporating additional metadata or altering the response structure to meet specific project requisites. Advanced Features: This workflow‚Äôs modular design supports seamless integration into expansive systems. For instance, an e-commerce platform could employ it to dynamically localize product pricing based on user geolocation. Enhanced functionality can be achieved by appending nodes to log conversions, monitor performance metrics, or trigger auxiliary workflows conditioned on conversion outputs. Expected Results: For a query like 5usd to mxn, the workflow generates a response formatted as: 5 USD = 95 MXN. This output is optimized for readability and practical application. Use Case Examples: AI Integration:** Enables artificial intelligence agents to process real-time price conversions efficiently across diverse currencies, enhancing their computational capabilities. Financial Operations:** Automates precise currency conversions for corporate reports, international transactions, and market analytics. Personal Financial Planning:** Assists users in calculating currency conversions for investment decisions or travel budgeting with minimal manual effort. E-commerce Applications:** Facilitates dynamic price adjustments on online marketplaces, displaying localized prices to augment user experience and conversion rates. Workflow Integration:** Embeds seamlessly into larger systems, such as CRMs or ERPs, to streamline financial operations and enhance interoperability. Key Benefits: No API Dependency:** By leveraging publicly available data from Google, the workflow eliminates the complexities and costs associated with API integration, reducing overhead and enhancing accessibility. Precision and Currency:** Ensures accurate and real-time results by querying Google directly. Flexibility:** Designed to adapt to various operational contexts and input formats, making it a versatile asset in computational and commercial applications. Tags: currency conversion, automation, webhook, data extraction, AI integration, financial automation, e-commerce, real-time data, scalable workflows.",895,2025-01-02 18:21:14.460000+00:00,True,2
2278,Subscribe to new releases of a Github repository via Gmail,This is a very simple workflow that lets you subscribe to any github repository for the latest release (using n8n as example). How it works: daily poll to Github repository for release for latest (stable) version of n8n parses the content to HTML sends a gmail Setup steps: add your gmail credentials (or use other email node of choice) change the url to the right Github repository you want to check regularly change the To email address to the email that you want to receive the updates for Feedback & Questions If you have any questions or feedback about this workflow - Feel free to get in touch at ria@n8n.io,882,2024-06-03 18:16:46.501000+00:00,False,2
2317,Convert PDF to PDFA using ConvertAPI,"Who is this for? For developers and organizations that need to convert PDF files to PDFA for long term archiving. What problem is this workflow solving? The file format conversion problem. What this workflow does Downloads the PDF file from the web. Converts the PDF file to PDFA. Stores the PDFA file in the local file system. How to customize this workflow to your needs Open the HTTP Request node. Adjust the URL parameter (all endpoints can be found here). Use your API Token for authentication. Pass the token in the Authorization header as a Bearer token. You can manage your API Tokens in the User panel ‚Üí Authentication. Optionally, additional Body Parameters can be added for the converter.",863,2024-07-05 09:49:50.360000+00:00,False,1
2727,Display ServiceNow Incident Details in Slack using Slash Commands,"Who is this for? This workflow is designed for teams using Slack for communication and ServiceNow for incident management. It simplifies incident lookup by enabling team members to fetch incident details directly within Slack via a Slash Command. What problem is this workflow solving? Manually switching between Slack and ServiceNow to retrieve incident details can be time-consuming and disrupt workflow efficiency. This workflow bridges the two platforms, providing instant access to critical incident information in Slack, saving time, and improving response efficiency. What this workflow does? The workflow listens for a Slash Command in Slack that includes an incident ID, extracts the ID from the incoming payload, queries ServiceNow for the corresponding incident details, and sends a formatted response back to Slack. Depending on the query result, it can: Display incident details (e.g., ID, description, severity, and priority). Notify the user if no matching incident is found. Alert the user if there‚Äôs an issue connecting to ServiceNow. Setup Slack Setup: Create a Slash Command in Slack with the appropriate endpoint URL. Configure the command to send a POST request to the webhook endpoint of this workflow. For details on how to setup the Slack app using Slash commands and n8n, check out this video. ServiceNow Setup: Create or use an existing account with the necessary permissions to access incident data. Configure the ServiceNow node with your ServiceNow credentials. n8n Workflow Activation: Deploy and activate the workflow in your n8n instance. Ensure all nodes are properly configured and connected. How to customize this workflow to your needs Modify Incident Query Parameters:** Adjust the query logic in the Search For Incident in ServiceNow node to include additional filters or data points based on your organization‚Äôs needs. Slack Response Customization:** Customize the Slack response template to display additional incident details or to match your team‚Äôs tone and style. Error Handling:** Enhance the error handling nodes to include more detailed logs or send alerts to a dedicated Slack channel.",858,2025-01-14 20:55:38.962000+00:00,False,1
4863,YouTube Comment Scraper & Analyzer with GPT-4o + Email Summary Report,"How it Works This workflow automates the collection and analysis of YouTube comments from a video and sends a summary report via email, using Google Sheets, the YouTube API, OpenAI (GPT-4o), and Gmail. Whether you're a content creator, brand manager, or social media analyst, this workflow helps you automate sentiment analysis and receive insights directly in your inbox ‚Äî all triggered from a simple spreadsheet. üéØ Use Case Ideal for: YouTubers** monitoring audience sentiment Marketing teams** analyzing campaign feedback Community managers** summarizing engagement Setup Instructions 1. Upload the Spreadsheet File name: Youtube_Video Sheet structure: | ID | Video Title | YouTube Video ID | Status | Add video IDs and set their Status as Pending 2. Configure Google Sheets Nodes Connect your Google account to: Pick Video IDs from Google Sheet Update Status on Google Sheet 3. Add API Credentials YouTube API Key** ‚Üí for comment + video scraping nodes OpenAI API Key** ‚Üí for analyzing comments Gmail Account** ‚Üí for sending the summary email 4. Activate the Workflow Once live, the workflow will: Watch for new or updated rows in the spreadsheet Scrape comments using the YouTube API Analyze sentiment and key themes via GPT-4o Send a formatted HTML email with the summary Update the spreadsheet status to Mail sent üîÅ Workflow Logic Trigger: New/updated row in Google Sheet Retrieve: YouTube video metadata + comments Analyze: Comments using GPT-4o Email: Summary report via Gmail Update: Spreadsheet status to Mail sent üß© Node Descriptions | Node Name | Description | |-----------|-------------| | Pick Video IDs from Google Sheet | Watches the spreadsheet and retrieves pending video IDs | | If | Checks whether status is 'Pending' | | Limit | Restricts the number of processed rows | | Set Video Details | Prepares video info (e.g., title, channel) | | Get YouTube Video Details | Fetches metadata (title, channel, etc.) | | Get YouTube Video Comments | Pulls top-level comments using YouTube API | | Prepare Comments Data | Formats comment text for OpenAI | | AI Agent | Summarizes comments using OpenAI's GPT-4o | | Prepare HTML for Email | Converts summary into HTML for email body | | Gmail Account Configuration | Sends the email report via Gmail | | Update Status on Google Sheet | Marks the row as 'Mail sent' | üõ†Ô∏è Customization Tips Change the AI prompt for tone, length, or custom metrics Send results to Slack or Telegram instead of Gmail Export summaries to Notion, Airtable, or PDF Schedule it daily/weekly for recurring analysis üìí Suggested Sticky Notes for Workflow | Node/Section | Sticky Note Content | |--------------|---------------------| | Pick Video IDs from Google Sheet | ""Triggers on new YouTube videos in your spreadsheet"" | | AI Agent | ""Uses OpenAI to generate an analysis summary ‚Äì customize prompt as needed"" | | Gmail | ""Sends summary report ‚Äì you can update subject, recipients, or style"" | | Update Status | ""Marks video as processed to avoid duplicate runs"" | üìé Required Files | File Name | Purpose | |-----------|---------| | Youtube_Video | Google Sheet to hold YouTube video IDs and status | | Youtube_Comment_Scraper.json | Main n8n workflow export for this automation | üß™ Testing Tips Add one test video with a valid YouTube video ID and status = Pending Monitor the workflow logs to confirm API responses Confirm summary delivery in your inbox Verify that status updates in the sheet üè∑ Suggested Tags & Categories #YouTube #OpenAI #Automation #Marketing #Email #Analytics",853,2025-06-11 00:37:10.771000+00:00,True,7
2805,Todoist weekly email of completed tasks,"This workflow template helps Todoist users get a weekly overview of their completed tasks via email, making it easier to review their past week. Why use this workflow? Todoist doesn‚Äôt provide completed task reports or filters in its built-in reports or n8n app. This workflow solves that by using Todoist‚Äôs public API to fetch your completed tasks. How it works Runs every Friday afternoon (or manually). Uses the Todoist public API to retrieve completed tasks. Excludes specific projects you set (e.g., a grocery list). Sends an email summary, grouping tasks by the day they were completed. Set up steps Copy your Todoist API token (found here). Create a Todoist API credential in n8n. Create an SMTP credential in n8n. Alternatively, use a preferred email service like Brevo, Mailjet, etc. Import this workflow template. In the Get completed tasks via Todoist API step, select your Todoist API credential. In the Send Email step: Select your SMTP credential. Set the sender and recipient email addresses. Run the workflow manually and check your inbox! Ignoring specific projects If you do not want your grocery list, workouts, or other tasks from specific Todoist projects showing up in your weekly summary, modify the step called Optional: Ignore specific projects and change this line: const ignoredProjects = ['2335544024']; This should be an array with the id of each project you'd like to ignore. You can find a list of your projects (inc. their Ids) by visiting this link: https://api.todoist.com/rest/v2/projects",839,2025-01-26 16:27:59.627000+00:00,False,3
2359,Get custom_fields from the Stripe API,The Stripe API does not provide custom fields in invoice or charge data. So you have to get it from the Checkout Sessions endpoint. But that endpoint is not easy for begginners. It has dictionary parameters and pagination settings. This workflows solves that problem by having a preconfigured GET request that gets all the checkout sessions from the last 7 days. It then transforms the data to make it easier to work with and allows you to filter by the custom_fields you want to get. Want to generate Stripe invoices automatically? Open üëâ this workflow . Check out my other templates https://n8n.io/creators/solomon/,836,2024-07-25 21:16:49.042000+00:00,False,1
4864,Automated News Summarizer with GPT-4o + Email Delivery,"How it Works This workflow fetches top news headlines every 10 minutes from NewsAPI, summarizes them using OpenAI's GPT-4o model, and sends a concise email digest to a list of recipients defined in a Google Spreadsheet. It's ideal for anyone who wants to stay updated with the latest news in a short, digestible format. üéØ Use Case Professionals who want summarized daily news Newsletters or internal communication updates Teams that require contextual summaries of the latest events Setup Instructions 1. Upload the Spreadsheet File name: Emails Column: Email with recipient addresses 2. Configure Google Sheets Nodes Connect your Google account to: Email List Send Email 3. Add API Credentials NewsAPI Key** ‚Üí for fetching top headlines OpenAI API Key** ‚Üí for summarizing headlines Gmail Account** ‚Üí for sending the email digest 4. Activate the Workflow Once active, the workflow runs every 10 minutes via a cron trigger Summarized news is sent to the list of emails in the spreadsheet üîÅ Workflow Logic Trigger: Every 10 minutes via Cron Fetch News: HTTP request to NewsAPI for top headlines Summarize: Headlines are passed to OpenAI's GPT-4o for 5-bullet summary Read Recipients: Google Sheet is used to collect email recipients Send Email: Summary is formatted and sent via Gmail üß© Node Descriptions | Node Name | Description | |-----------|-------------| | Cron | Triggers the workflow every 10 minutes. | | HTTP Request - NewsAPI | Fetches top news headlines using NewsAPI. | | Set | Formats or structures raw news data before processing. | | AI Agent | Summarizes the news content using OpenAI into 5 bullet points. | | Email List | Reads recipient email addresses from the 'Emails' Google Spreadsheet. | | Send Email | Sends the email digest to all recipients using Gmail. | üõ†Ô∏è Customization Tips Modify the AI prompt for tone, length, or content type Send summaries to Slack, Telegram, or Notion instead of Gmail Adjust cron interval for more/less frequent updates Change email formatting (HTML vs plain text) üìé Required Files | File Name | Purpose | |-----------|---------| | Emails spreadsheet | Google Sheet containing the list of email recipients | | daily_news.json | Main n8n workflow file to automate daily news digest | üß™ Testing Tips Add 1‚Äì2 test email addresses in your spreadsheet Temporarily change the Cron node to run every minute for testing Check email inbox for delivery and formatting Inspect the execution logs for API errors or formatting issues üè∑ Suggested Tags & Categories #News #OpenAI #Automation #Email #Digest #Marketing",833,2025-06-11 01:06:35.204000+00:00,True,5
2121,Get a Telegram alert when a great lead submits form with MadKudu and Hunter,"Use case If you have a form where potential leads reach out, then you probably want to analyze those leads and send a notification if certain requirements are met, e.g. employee number is high enough. MadKudu is built exactly to solve this problem. We use it along with Hunter and Telegram to get a message for high quality leads. How to setup Add you MadKudu, Hunter, and Telegram credentials Set the Telegram chat id to send to Click the Test Workflow button, enter your email and check your email Activate the workflow and use the form trigger production URL to collect your leads in a smart way How to adjust this template You may want to raise or lower the threshold for your leads, as you see fit.",809,2024-02-22 16:09:12.451000+00:00,False,3
4747,Binance SM Indicators Webhook Tool,"This workflow acts as a central API gateway for all technical indicator agents in the Binance Spot Market Quant AI system. It listens for incoming webhook requests and dynamically routes them to the correct timeframe-based indicator tool (15m, 1h, 4h, 1d). Designed to power multi-timeframe analysis at scale. üé• Watch Tutorial: üéØ What It Does Accepts requests via webhook with a token symbol and timeframe Forwards requests to the correct internal technical indicator tool Returns a clean JSON payload with RSI, MACD, BBANDS, EMA, SMA, and ADX Can be used directly or as a microservice by other agents üõ†Ô∏è Input Format Webhook endpoint: POST /webhook/indicators Body format: { ""symbol"": ""DOGEUSDT"", ""timeframe"": ""15m"" } üîÑ Routing Logic | Timeframe | Routed To | | --------- | -------------------------------- | | 15m | Binance SM 15min Indicators Tool | | 1h | Binance SM 1hour Indicators Tool | | 4h | Binance SM 4hour Indicators Tool | | 1d | Binance SM 1day Indicators Tool | üîé Use Cases | Use Case | Description | | -------------------------------------------------- | ------------------------------------------------------ | | üîó Used by Binance Financial Analyst Tool | Automatically triggers all indicator tools in parallel | | ü§ñ Integrated in Binance Quant AI System | Supports reasoning, signal generation, and summaries | | ‚öôÔ∏è Can be called independently for raw data access | Useful for dashboards or advanced analytics | üì§ Output Example { ""symbol"": ""DOGEUSDT"", ""timeframe"": ""15m"", ""rsi"": 56.7, ""macd"": ""Bearish Crossover"", ""bbands"": ""Stable"", ""ema"": ""Price above EMA"", ""adx"": 19.4 } ‚úÖ Prerequisites Make sure all the following workflows are installed and operational: Binance SM 15min Indicators Tool Binance SM 1hour Indicators Tool Binance SM 4hour Indicators Tool Binance SM 1day Indicators Tool OpenAI credentials (for any agent using LLM formatting) üßæ Licensing & Attribution ¬© 2025 Treasurium Capital Limited Company All architectural routing logic and endpoint structuring is IP-protected. No unauthorized rebranding or resale permitted. üîó Need help? Connect on LinkedIn ‚Äì Don Jayamaha",797,2025-06-06 18:28:23.790000+00:00,True,2
2576,"Import Productboard Notes, Companies and Features into Snowflake","This workflow imports Productboard data into Snowflake, automating data extraction, mapping, and updates for features, companies, and notes. It supports scheduled weekly updates, data cleansing, and Slack notifications summarizing the latest insights. Features Fetches data from Productboard (features, companies, notes). Maps and processes data for Snowflake tables. Automates table creation, truncation, and updates. Summarizes new and unprocessed notes. Sends weekly Slack notifications with key insights. Setup Configure Productboard and Snowflake credentials in n8n. Update Snowflake table schemas to match your setup. Replace Slack channel ID and dashboard URL in the notification node. Activate the workflow and set the desired schedule.",792,2024-11-25 17:27:56.565000+00:00,False,3
2119,Qualify great leads from n8n Form with MadKudu and Hunter and alert on Slack,"Use case If you have a form where potential leads reach out, then you probably want to analyze those leads and send a notification if certain requirements are met, e.g. employee number is high enough. MadKudu is built exactly to solve this problem. We use it along with Hunter to alert on Slack for high quality leads. How to setup Add you MadKudu, Hunter, and Slack credentials Set the Slack channel Click the Test Workflow button, enter your email and check the Slack channel Activate the workflow and use the form trigger production URL to collect your leads in a smart way How to adjust this template You may want to raise or lower the threshold for your leads, as you see fit.",791,2024-02-22 15:38:48.588000+00:00,False,3
2214,"Grist: On row confirmed, create row in other table","In Grist, when I mark a row as confirmed (via a toggle): a webhook is set up to notify n8n, and this workflow will create derived records in the destination table. Design decisions Confirmation-based In the source table there is a boolean column ""Confirmed"" that will trigger the transfer. This way there is a manual check involved & it's a conscious step to trigger the workflow. Runs once If the destination table already contains an entry, we will not re-create/update it (as it might've already been changed manually) Setup Create a boolean column Confirmed in source table Add a webhook in Grist Settings Add grist API credentials in n8n Set document ID & source table ID/Name in the 'get existing' node Set docID, the destination table ID/Name - and the columns & values you want in the Create Row node",786,2024-04-05 09:40:08.227000+00:00,False,1
2306,Protect PDF with the password using ConvertAPI,"Who is this for? For developers and organizations that need to protect PDF files with the password. What problem is this workflow solving? PDF file protection problem. What this workflow does Downloads the PDF file from the web. Protects PDF file with the password. Stores the PDF file in the local file system. How to customize this workflow to your needs Open the HTTP Request node. Adjust the URL parameter (all endpoints can be found here). Use your API Token for authentication. Pass the token in the Authorization header as a Bearer token. You can manage your API Tokens in the User panel ‚Üí Authentication. Change the password in the parameter UserPassword Optionally, additional Body Parameters can be added for the converter.",786,2024-07-02 12:08:47.853000+00:00,False,2
2499,Integrate Xero with FileMaker using Webhooks,"Integrate Xero with FileMaker using Webhooks Workflow Description This n8n workflow automates the integration between Xero and FileMaker, allowing for seamless data transfer between the two platforms. By listening for webhooks from Xero (e.g., new invoices, payments, or contacts), this workflow ensures that data is automatically sent and recorded in a FileMaker database. Who is This For? This workflow template is ideal for: Accountants** who need a streamlined process to sync financial data between Xero and FileMaker. Business Owners** looking to automate data entry and improve accuracy across their systems. Developers** building solutions for clients that require integration between accounting software and databases. Operations Teams** focused on minimizing manual work and improving efficiency. Key Steps Xero Webhook Trigger: The workflow starts by capturing events from Xero via a webhook. Data Processing: Transforms and maps the incoming data to match FileMaker‚Äôs required format. FileMaker Node: Utilizes the FileMaker node to create or update records directly in the FileMaker database. Logging & Error Handling: Tracks successful entries and manages any errors with automated alerts. Setup Instructions Set Up the Xero Webhook: Create a webhook in Xero and point it to your n8n webhook node URL. Configure the types of events to trigger the workflow (e.g., new invoices or payments). Xero will then send some test calls to test you are doing proper hash control. Connect the FileMaker Node: Set up your FileMaker node with the appropriate credentials and database configuration. Map the fields between the incoming Xero data and your FileMaker database structure. Customize Data Processing: Adjust data transformations as needed to ensure compatibility with your FileMaker schema. Test and Deploy: Run the workflow with sample data to ensure everything is functioning correctly. Monitor the execution log to verify data transfer and make any adjustments as needed. Error Handling Configuration: Configure error-handling nodes or alerts to notify you of any issues during data processing. Benefits This setup facilitates real-time data synchronization between Xero and FileMaker, reducing the need for manual data entry and improving overall operational efficiency.",786,2024-10-25 14:24:00.538000+00:00,False,2
2438,Find out which Chrome extensions are tracked by Linkedin,"What this workflow does Linkedin tracks which Chrome extensions are installed in your browser. This workflow uses a huge raw JSON of chrome extension ids, extracted from Linkedin pages, and builds a pretty Google Sheet with the list of these extensions. This workflow web scrapes Google to search for chrome extension id - and extracts the first search result. Setup Clone this Google Sheet template: https://docs.google.com/spreadsheets/d/1nVtoqx-wxRl6ckP9rBHSL3xiCURZ8pbyywvEor0VwOY/edit?gid=0#gid=0 Get API key for Google SERP API access here: https://rapidapi.com/restyler/api/serp-api1 Create n8n header auth for Google SERP API Some context and discussion https://www.linkedin.com/feed/update/urn:li:activity:7245006911807393792/ Follow the author and get the final Google Sheet with 1300+ Chrome extensions: https://www.linkedin.com/in/anthony-sidashin/",777,2024-09-28 07:17:35.065000+00:00,False,3
2565,Summarize SERPBear data with AI (via Openrouter) and save it to Baserow,"Who's this for? If you own a website and need to analyze your keyword rankings If you need to create a keyword report on your rankings If you want to grow your keyword positions SerpBear is an opensourced SEO tool specifically for keyword analytics. Click here to watch youtube tutorial Example output of A.I. Key Observations about Ranking Performance: The top-performing keyword is ‚ÄúOpenrouter N8N‚Äù with a current position of 7 and an improving trend. Two keywords, ‚ÄúBest Docker Synology‚Äù and ‚ÄúBitwarden Synology‚Äù, are not ranking in the top 100 and have a stable trend. Three keywords, ‚ÄúObsidian Second Brain‚Äù, ‚ÄúAI Generated Reference Letter‚Äù, and ‚ÄúActual Budget Synology‚Äù, and ‚ÄúN8N Workflow Generator‚Äù are not ranking well and have a declining trend. Keywords showing the most improvement: ‚ÄúOpenrouter N8N‚Äù has an improving trend and a relatively high ranking of 7. Keywords needing attention: ‚ÄúObsidian Second Brain‚Äù has a declining trend and a low ranking of 69. ‚ÄúAI Generated Reference Letter‚Äù has a declining trend and a low ranking of 84. ‚ÄúActual Budget Synology‚Äù, ‚ÄúN8N Workflow Generator‚Äù, ‚ÄúBest Docker Synology‚Äù, and ‚ÄúBitwarden Synology‚Äù are not ranking in the top 100. Use case Instead of hiring an SEO expert, I run this report weekly. It checks the keyword rankings of the past week and gives me recommendations on what to improve. How it works The workflow gathers SerpBear analytics for the past 7 days. It passes the data to openrouter.ai for A.I. analysis. Finally it saves to baserow. How to use this Input your SerpBearcredentials Enter your domain name Input your Openrouter.ai credentials Input your baserow credentials You will need to create a baserow database with columns: Date, Note, Blog Created by Rumjahn",775,2024-11-22 12:55:27.465000+00:00,True,3
2755,Jotform to KlickTipp Integration - Webinar registration,"Community Node Disclaimer: This workflow uses KlickTipp community nodes. How It Works: Jotform Webinar Registry Integration: This workflow streamlines the process of handling webinar registrations submitted via JotForm. It ensures the data is correctly formatted and seamlessly integrates with KlickTipp. Data Transformation: Input data is validated and transformed to meet KlickTipp‚Äôs API requirements, including formatting phone numbers, converting dates, and validating URLs. Key Features JotForm Trigger: Captures new form submissions, including participant details and webinar preferences. Data Processing: Standardizes and validates input fields Converts phone numbers to numeric-only format with international prefixes. Transforms dates into UNIX timestamps. Validates LinkedIn URLs and applies fallback URLs if validation fails. Scales numerical fields, such as work experience, for specific use cases. Subscriber Management in KlickTipp: Adds or updates participants as subscribers in KlickTipp. Includes custom field mappings, such as: Personal information (name, email, phone number). Webinar details (chosen webinar, start date/time). Preferences (reminder intervals, questions for presenters). Tags contacts for segmentation: Adds fixed and dynamic tags to contacts. Error Handling: Validates critical fields like phone numbers, URLs, and dates to prevent incorrect data submissions. Setup Instructions Install and Configure Nodes: Set up the JotForm and KlickTipp nodes in your n8n instance. Authenticate your JotForm and KlickTipp accounts. Custom Field Preparation in KlickTipp: Create the necessary custom fields to match the data structure: | Field Name | Field Type | |-----------------------------------------|------------------| | Jotform \| URL Linkedin | URL | | Jotform \| Workexperience in Years | Decimal Number | | Jotform \| Webinar start timestamp | Date & Time | | Jotform \| Questions/Notes | Text | | Jotform \| Webinar | Text | | Jotform \| Reminder | Text | After creating fields, allow 10-15 minutes for them to sync. If fields don‚Äôt appear, reconnect your KlickTipp credentials. Field Mapping and Adjustments: Verify and customize field assignments in the workflow to align with your specific form and subscriber list setup. Workflow Logic Trigger via JotForm Submission: A new form submission from JotForm initiates the workflow Data Transformation: Processes raw form data to ensure compatibility with KlickTipp‚Äôs API. Add to KlickTipp Subscriber List: Adds participants to the designated KlickTipp list, including webinar-specific details. Get all tags from KlickTipp and create a list: Fetches all existing Tags and turns them into an array Define tags to dynamically set for contacts: Definiton of variables that are received from the form submission and should be converted into tags Merge tags of both lists: Checks whether the list of existing tags in KlickTipp contains the tags which should be dynamically set based on the form submission Tag creation and tagging contacts: Creates new tags if it previously did not exist and then tags the contact Benefits Efficient lead generation: Contacts from forms are automatically imported into KlickTipp and can be used immediately, saving time and increasing the conversion rate. Automated processes: Experts can start workflows directly, such as welcome emails or course admissions, reducing administrative effort. Error-free data management: The template ensures precise data mapping, avoids manual corrections and reinforces a professional appearance. Testing and Deployment: Test the workflow by filling the form on Jotform and verifying data updates in KlickTipp. Notes: Customization: Update field mappings within the KlickTipp nodes to align with your account setup. This ensures accurate data syncing. Resources: Jotform KlickTipp Knowledge Base help article Use KlickTipp Community Node in n8n Automate Workflows: KlickTipp Integration in n8n",757,2025-01-20 09:28:25.362000+00:00,False,0
4744,Binance SM 1hour Indicators Tool,"üß™ Binance SM 1hour Indicators Tool A precision trading signal engine that interprets 1-hour candlestick indicators for Binance Spot Market pairs using a GPT-4.1-mini LLM. Ideal for swing traders seeking directional bias and momentum clarity across medium timeframes. üé• Watch Tutorial: üéØ Purpose This tool provides a structured 1-hour market read using: RSI** (Relative Strength Index) MACD** (Moving Average Convergence Divergence) BBANDS** (Bollinger Bands) SMA & EMA** (Simple and Exponential Moving Averages) ADX** (Average Directional Index) It‚Äôs invoked as a sub-agent in broader AI workflows, such as the Binance Financial Analyst Tool and the Spot Market Quant AI Agent. ‚öôÔ∏è Key Features | Feature | Description | | ---------------------- | ------------------------------------------------------------- | | üîÑ Subworkflow Trigger | Runs only when called by parent agent (not standalone) | | üß† GPT-4.1-mini LLM | Translates numeric indicators into natural-language summaries | | üìä Real-time Data | Pulls latest 40√ó1h candles via internal webhook from Binance | | üì• Input Format | { ""message"": ""ETHUSDT"", ""sessionId"": ""telegram_chat_id"" } | | üì§ Output Format | JSON summary + Telegram-friendly HTML overview | üí° Example Output üìä 1h Technical Overview ‚Äì ETHUSDT ‚Ä¢ RSI: 59 (Neutral) ‚Ä¢ MACD: Bullish Crossover ‚Ä¢ BBANDS: Price at Upper Band ‚Ä¢ EMA &gt; SMA ‚Üí Positive Slope ‚Ä¢ ADX: 28 ‚Üí Moderate Trend Strength üß© Use Cases | Scenario | Result | | -------------------------------------- | ----------------------------------------------- | | Mid-frame market alignment | Verifies momentum between 15m and 4h timeframes | | Quant AI Agent input | Supplies trend context for entry/exit decisions | | Standalone medium-term signal snapshot | Validates swing trade setups or filters noise | üì¶ Installation Instructions Import workflow into your n8n instance Confirm internal webhook /1h-indicators is live and authorized Insert your OpenAI credentials for GPT-4.1-mini node Use only when triggered via: Binance Financial Analyst Tool Binance Spot Market Quant AI Agent üßæ Licensing & Support üîó Don Jayamaha ‚Äì LinkedIn linkedin.com/in/donjayamahajr ¬© 2025 Treasurium Capital Limited Company Architecture, prompts, and signal logic are proprietary. Redistribution or commercial use requires explicit licensing. No unauthorized cloning permitted.",743,2025-06-06 18:17:27.363000+00:00,True,3
4759,Send Cryptocurrency Price Threshold Alerts from CoinGecko to Discord,"Crypto Price Alert ‚Äì n8n Workflow A simple and effective crypto alert system for anyone who wants to stay up to date with coin price changes ‚Äî without refreshing charts all day. This workflow checks the current price of your chosen cryptocurrency (via CoinGecko) and sends you an alert on Discord if it goes above or below your target range. It‚Äôs lightweight, easy to set up, and runs on autopilot. What the Workflow Does Checks the live price of a selected coin using the CoinGecko API. Compares it to the max/min prices you define manually. Decides if the price is too high or too low. Sends an alert message to Discord depending on the result. How It Works The flow is triggered manually or on a schedule (your choice). It pulls the current price of the coin you set. Compares that price with your min and max values. Sends a ‚Äúhigh‚Äù or ‚Äúlow‚Äù message to your Discord webhook. Setup Steps Enter your coin ID and price thresholds in the ‚ÄúSet Low and High‚Äù node. Paste your Discord webhook URLs in the ""Message High"" and ""Message Low"" nodes. Optional: Adjust the schedule trigger to run every X minutes/hours. Run once manually to test ‚Äî takes under 1 minutes. Full instructions and config tips are in sticky notes inside the workflow.",742,2025-06-07 16:24:49.130000+00:00,False,2
9974,"Automate HVAC Service Scheduling with AI Agent, Google Calendar and Gmail","Instant, automated scheduling. This AI Scheduling Agent manages real-time appointments, availability checks, and rescheduling across Google Calendar and Sheets, eliminating human hold times. üéØ Problem Statement Traditional call center or online booking systems often lack the flexibility to handle complex, multi-step customer requests like rescheduling, checking dynamic availability across multiple time slots, or handling context-aware conversational booking. This leads to friction, missed bookings, and high administrative overhead for service companies like HVAC providers. ‚ú® Solution This workflow deploys a sophisticated AI Scheduling Agent that acts as a virtual receptionist. It uses the Language Model's (LLM) ""tool-use"" capability to intelligently execute complex, sequential business logic (e.g., check availability before booking, find existing events before rescheduling) and manages the entire lifecycle of a service appointment, from initial inquiry to final confirmation. ‚öôÔ∏è How It Works (Multi-Step Execution) Trigger: A customer request (e.g., from an external voice or text platform) hits the Webhook Trigger with intent details (e.g., tool\_request: 'reschedule\_appointment'). Agent Logic: The Receptionist Agent uses a strict system prompt and its internal tools to formulate an execution plan. It maintains conversational state via the simple-memory node. Tool Execution (Example: Reschedule): The Agent executes a predefined sequence of private tools: find\_old\_event: Locates the existing booking ID using the customer's email. check\_calendar: Verifies the proposed new time is available (2-hour window). reschedule\_appointment: Updates the calendar event. log\_lead: Updates the central Google Sheet. Synchronous Response: The Agent sends a confirmation or follow-up question via the respond\_to\_webhook node. Asynchronous Confirmation: The log\_lead action triggers a secondary workflow that composes a professional email via a second LLM (Anthropic) and sends it to the customer via Gmail, followed by an internal alert via Google Chat. üõ†Ô∏è Setup Steps Credentials: AI/LLM: Configure credentials for the Language Model used (OpenAI or Gemini) for the core Agent. Google Services: Set up OAuth2 credentials for Google Calendar (for booking/checking), Google Sheets (for logging), and Gmail (for customer confirmation). Google Calendar: Specify the technician's calendar ID (bhuvaneshx13@gmail.com in the template) in all Calendar nodes. Google Sheets: Create a new Google Sheet to serve as the Lead Log and update the Document ID and Sheet Name in the log\_lead and log\_lead\_trigger nodes. Tool Configuration: Review and customize the Agent's system prompt in the Receptionist node to align time zone rules (currently Asia/Kolkata - IST) and business hours (9:00 AM to 6:00 PM) with your operations. ‚úÖ Benefits Increased Efficiency: Fully automates complex scheduling and rescheduling, freeing up human staff. Contextual Service: AI handles multi-turn conversations and adheres to strict business rules (e.g., 2-hour slots, maximum tool usage). Data Integrity: Ensures all bookings are immediately logged to Google Sheets, maintaining a centralized record (CRM). Professional Flow: Provides immediate confirmation to the customer via email and instant notification to the internal team via chat. üöÄ Other Use Cases The underlying multi-step, tool-execution pattern is highly versatile and can be adapted for any service industry requiring complex, rules-based scheduling: Real Estate:** Scheduling property viewings (Check agent availability ‚Üí Book viewing ‚Üí Send directions). HVAC Services:** Managing maintenance and repair visits (Diagnose issue type ‚Üí Match with qualified technician ‚Üí Check part availability ‚Üí Schedule visit ‚Üí Send service confirmation). Medical/Dental:** Booking patient appointments (Check insurance eligibility ‚Üí Check doctor availability ‚Üí Book ‚Üí Send pre-visit forms). Legal Services:** Intake for consultations (Collect client issue ‚Üí Check specialist availability ‚Üí Book ‚Üí Send retainer agreement). Automotive Repair:** Scheduling service bays (Check bay and mechanic availability ‚Üí Book ‚Üí Update internal service board).",741,2025-10-21 09:41:32.806000+00:00,True,7
2702,Save Hotmart events to Google Sheets,"Portugu√™s Acompanhe todos os seus eventos do Hotmart em um s√≥ lugar e mantenha seus dados organizados para an√°lise. Com este fluxo voc√™ pode registrar compras, reembolsos, eventos de assinatura e abandono de carrinho diretamente no Google Sheets. Como funciona O fluxo recebe os eventos da Hotmart e registra automaticamente no Google Sheets. Por exemplo: Compras s√£o registradas com informa√ß√µes como nome do produto, valor pago e dados do comprador. Abandono de carrinho registra informa√ß√µes sobre o produto visualizado, dados de contato do cliente e hor√°rios. Eventos de assinatura mostram atualiza√ß√µes como cancelamentos ou renova√ß√µes. Para quem √©? Criadores, empreendedores e neg√≥cios que usam o Hotmart e precisam de uma forma clara e automatizada de acompanhar suas vendas e dados de assinatura. Remova a necessidade de transferir dados manualmente ‚Äì este fluxo de trabalho faz isso por voc√™, economizando tempo e reduzindo erros. Confira meus outros templates üëâ https://n8n.io/creators/solomon/ English Track all your Hotmart events in one place and keep your data organized for analysis. With this workflow, you can register purchases, refunds, subscription events, and cart abandonment directly in Google Sheets. How it works The workflow listens for events from Hotmart and automatically records the details in Google Sheets. For example: Purchases are logged with details like product name, amount paid, and buyer information. Cart abandonment records include the product viewed, customer contact details, and timestamps. Subscription events show updates like cancellations or renewals. Who is this for? Creators, entrepreneurs, and businesses using Hotmart who need a clear and automated way to track their sales and subscription data. No need to manually transfer data ‚Äì this workflow does it for you, saving time and reducing errors. Check out my other templates üëâ https://n8n.io/creators/solomon/",739,2025-01-06 23:55:50.691000+00:00,False,1
4742,Binance SM Price-24hrStats-OrderBook-Kline Tool,"A powerful sub-agent that collects real-time market structure data from Binance for any trading pair ‚Äî including price, volume, order book depth, and candlestick snapshots across multiple timeframes (15m, 1h, 4h, 1d). üé• Watch Tutorial: üéØ Purpose This workflow powers the Quant AI system with: ‚úÖ Real-time price feed (/ticker/price) ‚úÖ 24-hour stats (OHLC, % change, volume via /ticker/24hr) ‚úÖ Live order book depth (/depth) ‚úÖ Latest candlestick data (/klines) for all major intervals All outputs are parsed and formatted using GPT and returned to the parent agent (e.g., Financial Analyst Tool) as a Telegram-optimized summary. ‚öôÔ∏è Workflow Architecture | Node | Role | | ------------------------------------ | ------------------------------------------------------------ | | üîó Execute Workflow Trigger | Accepts input from parent workflow | | üß† Simple Memory | Stores session + symbol info | | ü§ñ Binance SM Market Agent | Parses prompt, routes tool calls | | üí° OpenAI Chat Model (gpt-4o-mini) | Converts raw data into a clean, readable format for Telegram | | üåê getCurrentPrice | Gets latest price | | üåê get24hrStats | Gets OHLC/volume over past 24 hours | | üåê getOrderBook | Gets top 100 bids and asks | | üåê getKlines | Gets latest 15m, 1h, 4h, and 1d candles | üì• Input Requirements This workflow is not called directly by the user. Instead, it is triggered by another workflow, such as: { ""message"": ""BTCUSDT"", ""sessionId"": ""539847013"" } üì§ Telegram Output Example üìä BTCUSDT Market Overview üí∞ Price: $63,220 üìà 24h Change: +2.3% | Volume: 45,210 BTC üìâ Order Book ‚Ä¢ Top Bid: $63,190 ‚Ä¢ Top Ask: $63,230 üï∞Ô∏è Latest Candles ‚Ä¢ 15m: O: $63,000 | C: $63,220 | Vol: 320 BTC ‚Ä¢ 1h : O: $62,700 | C: $63,300 | Vol: 980 BTC ‚Ä¢ 4h : O: $61,800 | C: $63,500 | Vol: 2,410 BTC ‚Ä¢ 1d : O: $59,200 | C: $63,220 | Vol: 7,850 BTC ‚úÖ Use Cases | Scenario | Output Provided | | ---------------------------------- | ------------------------------------------------------------ | | ‚ÄúShow current BTC price and trend‚Äù | Price, 24h stats, candles, and order book in one message | | ‚ÄúCandles for SOL‚Äù | 15m, 1h, 4h, 1d candlesticks for SOLUSDT | | Triggered by Quant AI system | Clean Telegram-ready summary with all structure tools merged | üß© Toolchain Breakdown | Tool Name | Endpoint | Purpose | | ----------------- | ---------------------- | ------------------------------ | | getCurrentPrice | /api/v3/ticker/price | Latest trade price | | get24hrStats | /api/v3/ticker/24hr | 24h OHLC, % change, volume | | getOrderBook | /api/v3/depth | Top 100 bids and asks | | getKlines | /api/v3/klines | 1-candle snapshot across 4 TFs | üöÄ Installation Steps Import the JSON into your n8n instance Connect your OpenAI credentials for the Chat Model node No Binance API key needed ‚Äî public endpoints Trigger this tool only via: Binance SM Financial Analyst Tool Binance Spot Market Quant AI Agent üîê Licensing & Attribution ¬© 2025 Treasurium Capital Limited Company Architecture, prompts, and trade structure are IP-protected. No unauthorized rebranding permitted. üîó For support: Don Jayamaha ‚Äì LinkedIn",730,2025-06-06 18:08:36.625000+00:00,True,4
2765,Automated Hugging Face Paper Summary Fetching & Categorization Workflow,"How the Automated Workflow Works Scheduled Fetching from Hugging Face ‚è∞ The workflow triggers every weekday at 8 AM, automatically fetching the latest papers from Hugging Face for easy access. Duplication Check to Avoid Redundant Entries üîç It ensures the paper's summary is not already stored in your Notion workspace, preventing duplicate records and keeping your database organized. Content Analysis with OpenAI üß† Using OpenAI's powerful capabilities, the workflow analyzes the fetched paper summary, extracts key insights, and categorizes the content for easier understanding. Data Storage and Notification Integration üì•üîî Once the summary is processed, it's automatically stored in your Notion workspace, and a notification containing the paper details is sent to your designated Slack channel for quick reference. Set Up Your Automated Workflow Create Your n8n Account üìù Start by registering for an n8n account and logging into the n8n cloud service. Connect OpenAI, Notion, and Slack üîó Link your OpenAI, Notion, and Slack accounts by entering the appropriate tokens. This step will take approximately 10‚Äì15 minutes to complete. Import the Workflow Template üì• Import the provided workflow template into your n8n instance to streamline the setup process. Activate the Workflow for Daily Summaries üöÄ After importing, simply enable the workflow, and you‚Äôre all set to receive daily paper summaries automatically. Setup Time ‚è≥: Approximately 15‚Äì20 minutes. Why Use This Automated Workflow? This automated workflow not only saves you time by fetching and categorizing the latest research papers but also helps streamline your Notion workspace and Slack notifications, allowing you to stay organized and efficient without manual intervention. Results Presentation",706,2025-01-21 09:33:22.365000+00:00,True,5
2888,Post to an XMLRPC API via the HTTP Request node,"What this does Show you how to us XMLRPC APIs via the generic HTTP-Request-node, by the example of posting to a wordpress blog This is also a feasible workaround if a specific n8n integration does not work or stops working (which happens e.g. with the Wordpress node) How it works First, the XML payload for the request is being prepared (in a code node, which also properly escapes special character in the values that you want to send to the XMLRPC endpoint) Then, the HTTP Request node sends the request using the HTTP post method Last, the returned XML response is converted to JSON which a conditional node uses to determine whether th operation was successful or not Setup steps: Import workflow Ensure you have a wordpress blog with a user that has an app-Password Edit the ""Settings""-node and enter your individual values for url/user/app-pw",706,2025-02-12 11:18:36.582000+00:00,False,2
2596,"Export Zammad Objects (Users, Roles, Groups, Organizations) to Excel","This n8n workflow enables you to export data from Zammad, including Users, Roles, Groups, and Organizations, into individual Excel files. It simplifies data handling and reporting by creating structured outputs for further processing or sharing. Features Export Users with associated details such as email, firstname, lastname, role_ids, and group_ids. Export Roles and Organizations with their respective identifiers and names. Convert all data into separate Excel files for easy access and use. Usage Import this workflow into your n8n instance. Configure the required Zammad API credentials (zammad_base_url and zammad_api_key) in the Basic Variables node. Run the workflow to generate Excel files containing Zammad data. Issues and Suggestions If you encounter any issues or have suggestions for improvement, please report them on the GitHub repository. We appreciate your feedback to help enhance this workflow!",696,2024-12-01 11:12:12.671000+00:00,False,2
5023,"Build a RAG System with Automatic Citations using Qdrant, Gemini & OpenAI","This workflow implements a Retrieval-Augmented Generation (RAG) system that: Stores vectorized documents in Qdrant, Retrieves relevant content based on user input, Generates AI answers using Google Gemini, Automatically cites the document sources (from Google Drive). Workflow Steps Create Qdrant Collection A REST API node creates a new collection in Qdrant with specified vector size (1536) and cosine similarity. Load Files from Google Drive The workflow lists all files in a Google Drive folder, downloads them as plain text, and loops through each. Text Preprocessing & Embedding Documents are split into chunks (500 characters, with 50-character overlap). Embeddings are created using OpenAI embeddings (text-embedding-3-small assumed). Metadata (file name and ID) is attached to each chunk. Store in Qdrant All vectors, along with metadata, are inserted into the Qdrant collection. Chat Input & Retrieval When a chat message is received, the question is embedded and matched against Qdrant. Top 5 relevant document chunks are retrieved. A Gemini model is used to generate the answer based on those sources. Source Aggregation & Response File IDs and names are deduplicated. The AI response is combined with a list of cited documents (filenames). Final output: AI Response Sources: [""Document1"", ""Document2""] Main Advantages End-to-end Automation**: From document ingestion to chat response generation, fully automated with no manual steps. Scalable Knowledge Base**: Easy to expand by simply adding files to the Google Drive folder. Traceable Responses**: Each answer includes its source files, increasing transparency and trustworthiness. Modular Design**: Each step (embedding, storage, retrieval, response) is isolated and reusable. Multi-provider AI**: Combines OpenAI (for embeddings) and Google Gemini (for chat), optimizing performance and flexibility. Secure & Customizable**: Uses API credentials and configurable chunk size, collection name, etc. How It Works Document Processing & Vectorization The workflow retrieves documents from a specified Google Drive folder. Each file is downloaded, split into chunks (using a recursive text splitter), and converted into embeddings via OpenAI. The embeddings, along with metadata (file ID and name), are stored in a Qdrant vector database under the collection negozio-emporio-verde. Query Handling & Response Generation When a user submits a chat message, the workflow: Embeds the query using OpenAI. Retrieves the top 5 relevant document chunks from Qdrant. Uses Google Gemini to generate a response based on the retrieved context. Aggregates and deduplicates the source file names from the retrieved chunks. The final output includes both the AI-generated response and a list of source documents (e.g., Sources: [""FAQ.pdf"", ""Policy.txt""]). Set Up Steps Configure Qdrant Collection Replace QDRANTURL and COLLECTION in the ""Create collection"" HTTP node to initialize the Qdrant collection with: Vector size: 1536 (OpenAI embedding dimension). Distance metric: Cosine. Ensure the ""Clear collection"" node is configured to reset the collection if needed. Google Drive & OpenAI Integration Link the Google Drive node to the target folder (Test Negozio in this example). Verify OpenAI and Google Gemini API credentials are correctly set in their respective nodes. Metadata & Output Customization Adjust the ""Aggregate"" and ""Response"" nodes if additional metadata fields are needed. Modify the ""Output"" node to format the response (e.g., changing Sources: {{...}} to match your preferred style). Testing Trigger the workflow manually to test document ingestion. Use the chat interface to verify responses include accurate source attribution. Note: Replace placeholder values (e.g., QDRANTURL) with actual endpoints before deployment. Need help customizing? Contact me for consulting and support or add me on Linkedin.",693,2025-06-18 14:37:45.594000+00:00,True,10
2147,Snooze Todoist tasks until 3 days before due date,"Use case This workflow snoozes any Todoist tasks, by moving them into a Snoozed todoist list and unsnoozes them 3 days before due date. Helps keep inbox clear only of tasks you need to worry about soon. How to setup Add your Todoist creds Create a Todoist project called snoozed Set the project ids in the relevant nodes Add due dates to your tasks in Inbox. Watch them disappear to snoozed. Set their date to tomorrow, watch it return to inbox. How to adjust this template Adjust the timeline.. Maybe 3 days is too close for you. Works mostly for me :)",689,2024-02-28 18:41:22.978000+00:00,False,3
5666,"Transform Text into AI Videos with Google Veo3, Leonardo.ai and Claude 4","üé¨ Google Veo 3 Prompt and Video Generator via Leonardo.ai + Claude 4 Transform text descriptions into cinematic videos using Google's Veo 3 model through Leonardo.ai's platform! üöÄ What This Workflow Does This advanced automation pipeline takes your creative ideas and turns them into professional-quality videos using Google's powerful Veo 3 model (accessed via Leonardo.ai), enhanced by Claude 4's sophisticated prompt engineering. ‚ú® Key Features ü§ñ AI-Powered Prompt Enhancement**: Uses Claude 4 Sonnet with Wikipedia integration to craft optimal Google Veo 3 prompts üé• Professional Video Generation**: Leverages Google's Veo 3 model through Leonardo.ai for high-quality text-to-video conversion ‚òÅÔ∏è Automatic Cloud Storage**: Videos are automatically saved to your Google Drive üìã Structured Prompting**: Follows Google Veo3 best practices with 8 essential elements (Subject, Context, Action, Style, Camera Motion, Composition, Ambiance, Audio) ‚ö° Hands-Off Processing**: Set it and forget it - the workflow handles the entire pipeline üîß How It Works Input Your Concept - Describe your video idea in the ""Video Context"" node AI Enhancement - Claude 4 transforms your description into a cinematic Google Veo 3 prompt using advanced techniques Video Generation - Google's Veo 3 model (via Leonardo.ai) creates your video (720p resolution, ~8 seconds) Smart Waiting - 4-minute processing buffer ensures completion Auto-Download - Retrieves the finished video from Leonardo's servers Cloud Storage - Uploads directly to your Google Drive folder üí° Perfect For Content Creators** looking to automate video production Marketing Teams** needing quick promotional videos Educators** creating engaging visual content Social Media Managers** generating scroll-stopping content Creative Professionals** exploring AI-assisted filmmaking üìã Requirements Leonardo AI account with API access Anthropic API key (Claude 4 Sonnet) Google Drive integration N8N instance (cloud or self-hosted) üë®‚Äçüíª About the Creator Created by: AlexK1919 - AI-Native Workflow Automation Architect, n8n Ambassador and Verified Partner, Founder @ WotAI If you'd like to review more Google Veo 3 Prompts organized by business category, check out over 9,000+ free, pre-made prompts at: Google Veo 3 Prompts üìÑ License This workflow is available under Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International (CC BY-NC-SA 4.0) license. You are free to use, adapt, and share this workflow for non-commercial purposes under the terms of this license. Full license details: https://creativecommons.org/licenses/by-nc-sa/4.0/ üéØ Example Output Input: ""Star Wars stormtrooper digging for uranium in desert, saying something funny"" The AI generates a structured prompt with: Subject**: Detailed character description Context**: Desert environment specifics Action**: Dynamic digging movements Style**: Cinematic vlog aesthetic Camera**: Appropriate angles and movement Audio**: Dialogue, sound effects, and music ‚öôÔ∏è Setup Notes Character Limit**: Prompts are optimized for Leonardo's 1,500 character API limit Processing Time**: Allow 4+ minutes for Google Veo3 video generation Quality**: 720p resolution with native audio generation Consistency**: Uses advanced Google Veo3 prompting for reliable results üîÑ Customization Options Modify the prompt engineering system message for different styles Adjust video resolution and model parameters Change storage destination (Google Drive folder) Add post-processing steps or notifications üìà Why This Workflow Rocks Unlike simple text-to-video tools, this workflow: Intelligently enhances** your prompts using AI for Google Veo 3 Follows industry best practices** for Google Veo3 prompting Automates the entire pipeline** from idea to stored video Leverages multiple AI models** for superior results Handles technical details** like API limits and timing üö® Pro Tips Be specific in your initial context - detail creates better videos The workflow includes comprehensive Google Veo3 prompting guidelines Videos are typically 5-8 seconds - plan accordingly for longer content Experiment with different styles and camera movements optimized for Veo 3 The AI can access Wikipedia for factual enhancement Ready to revolutionize your video creation process? Import this workflow and start generating professional videos with just a text description! Perfect for anyone looking to harness the power of AI for content creation. Tags: #veo3 #GoogleVeo3 #AI #VideoGeneration #Leonardo #Claude #Automation #ContentCreation #GoogleAI",687,2025-07-04 04:42:17.945000+00:00,True,5
2827,Automatically Send a Direct Message (DM) to New Followers on Bluesky using Baserow,"Send personalized welcome messages to your new Bluesky followers automatically, helping you maintain engagement while saving time. This workflow monitors your follower list and sends customized direct messages, creating a warm welcome for new connections without manual intervention. How it works Checks your Bluesky followers list daily at 9 AM Identifies new followers by comparing against a database Extracts the follower's first name** when available Sends a personalized welcome message with optional link Prevents duplicate messages** through double-verification Maintains a record of sent messages to avoid repetition Set up steps (10-15 minutes) Create a Bluesky account if you haven't already Generate an app password in your Bluesky settings Enter your Bluesky handle and app password in the ""Set Bluesky Credentials"" node Set up your database (Baserow, or adapt for Airtable/Google Sheets) Customize your welcome message in the ""Create Welcome Message"" node Optional: Adjust the regular check time (default: 9 AM) Features Personalized messaging** using follower's first name when available Database tracking** to prevent duplicate messages Basic rate limiting protection** to stay within API limits Customizable welcome message with clickable links Ability to handle up to 100 new followers per check Perfect for creators who want to Welcome new followers consistently Save time** on manual messaging Build early engagement with followers Share important links or resources Maintain a professional presence Scale** their community management Suggested enhancements Add message templates for different follower types Include email/Slack notifications for errors Add analytics tracking for message success rates Implement dynamic timing based on follower activity Create A/B testing for different welcome messages Add follower segmentation based on profile data",682,2025-01-30 21:22:55.627000+00:00,False,3
2620,Calendly to KlickTipp Integration,"Community Node Disclaimer: This workflow uses KlickTipp community nodes. How It Works Enhanced Calendly Integration: This workflow processes bookings and cancellations in Calendly, dynamically managing invitee and guest data with KlickTipp. Data Transformation: Dates and times are converted into formats (UNIX timestamps) compatible with KlickTipp‚Äôs API, ensuring seamless data integration. Key Features Calendly Trigger: Captures new bookings or cancellations of events, including participant details. Invitee and Guest Subscription in KlickTipp: Adds or updates invitees and guests in KlickTipp based on booking details (event name, time, join link, reschedule link, cancel link, etc.). Tracks and processes cancellations for both invitees and guests. Handles rescheduling intelligently to avoid redundant operations. Guest-Specific Operations: Processes guests individually for bookings and cancellations using dynamic arrays of email addresses. Recovers guest data from invitee records for cancellations since Calendly does not provide guest data upon cancellation. Data Processing: Standardizes and validates input fields Converts phone numbers to numeric-only format with international prefixes. Transforms dates into UNIX timestamps. Reads out the name of the invitee based on both possible input fields for name (name vs. firstname and lastname field setup). Error Handling: Validates critical fields like phone numbers, URLs, and dates to prevent incorrect data submissions. Setup Instructions Authentication: Set up the Calendly and KlickTipp nodes in your n8n instance. Configure authentication for both Calendly and KlickTipp nodes. Custom Field Preparation in KlickTipp: Create the following custom fields in KlickTipp to align with workflow requirements: | Field Name | Field Type | |---------------------------------------|------------------| | Calendly \| event name | Text | | Calendly \| join url | URL | | Calendly \| reschedule url | URL | | Calendly \| cancel url | URL | | Calendly \| event start datetime | Date & Time | | Calendly \| event end datetime | Date & Time | | Calendly \| invitee start date | Date | | Calendly \| invitee end date | Date | | Calendly \| invitee start time | Time | | Calendly \| invitee end time | Time | | Calendly \| invitee timezone | Text | | Calendly \| invitee guests addresses | Text | After creating fields, allow 10-15 minutes for them to sync. If fields don‚Äôt appear, reconnect your KlickTipp credentials. Field Mapping and Adjustments: Open each KlickTipp node and map fields to match your setup. The workflow includes placeholders for: Invitee details (first name, last name, email, and phone). Event details (start/end times, timezone, etc.). Workflow Logic Trigger via Calendly event Booking: A new form event booking or cancellation from Calendly initiates the workflow Data Transformation: Processes raw Calendly event data to ensure compatibility with KlickTipp‚Äôs API. Add to KlickTipp Subscriber List: Adds invitees and guests to the designated KlickTipp list, including event-specific details. Benefits Efficient lead generation: Contacts from event bookings are automatically imported into KlickTipp and can be used immediately, saving time and increasing the conversion rate. Automated processes: Experts can start workflows directly, such as reminder emails or course admissions, reducing administrative effort. Error-free data management: The template ensures precise data mapping, avoids manual corrections and reinforces a professional appearance. Testing and Deployment Test the workflow by triggering a Calendly event and verifying data updates in KlickTipp. Notes: Customization: Update field mappings within the KlickTipp nodes to align with your account setup. This ensures accurate data syncing. Resources: Calendly KlickTipp Knowledge Base help article Use KlickTipp Community Node in n8n Automate Workflows: KlickTipp Integration in n8n",675,2024-12-10 15:13:32.783000+00:00,False,0
5692,Automatic Lead Export from Fluentform to Google Sheets with Form Categorization,"‚ùì What Problem Does It Solve? Manual exporting or copying of leads and newsletter signups from web forms to spreadsheets is time-consuming, error-prone, and delays follow-ups or marketing activities. Traditional workflows can lose data due to mistakes or lack of automation. The Fluentform Export workflow automates the capture and organization of form submissions and newsletter signups into Google Sheets üí° Why Use this workflow? Save Time:** Automate tedious manual data entry for form leads and newsletter signups Avoid Data Loss:** Ensure all submissions are reliably logged with real-time updates Organized Data:** Separate sheets for newsletter and contact form data maintain clarity Easy Integration:** Works seamlessly with Fluentform submissions and Google Sheets Flexible & Scalable:** Quickly adapt to changes in form structure or spreadsheet columns ‚ö° Who Is This For? Marketers & Growth Teams:** Automatically gather leads and newsletter contacts to fuel campaigns Small to Medium Businesses:** Reduce overhead from manual data management and errors Customer Support Teams:** Keep track of form submissions in a centralized, accessible place Website Admins:** Simplify data workflow from Fluentform plugins without coding üîß What This Workflow Does ‚è± Trigger:** Listens for incoming POST requests from Fluentform via webhook üìé Step 2:** Evaluates if the submission is a newsletter signup or a form based on a specific token üîÑ Step 3 (Newsletter Path):** Maps email from newsletter submissions and appends/updates Google Sheets ""News Letter"" tab üîÑ Step 3 (Form Path):** Extracts full name, email, phone, subject, and message fields and appends/updates the Google Sheets ""form"" tab üíå Step 4:** Sends a JSON success response back to Fluentform confirming receipt üîê Setup Instructions Import the provided .json workflow file into your n8n instance Set up credentials: Google Sheets OAuth2 credential with access to your target spreadsheets Customize workflow elements: Update Fluentform webhook URL in your Fluentform settings to the n8n webhook URL generated Adjust field names or spreadsheet columns if your form structure changes Update spreadsheet IDs and sheet names used in the Google Sheets nodes to match your own Sheets Test workflow thoroughly with actual Fluentform submissions to verify data flows correctly üß© Pre-Requirements Running n8n instance (Cloud or self-hosted) Google account with access to Google Sheets and OAuth credentials Fluentform installed on your website with ability to set webhook URL Target Google Sheets prepared with tabs named ""News Letter"" and ""form"" with expected columns üß† Nodes Used Webhook (POST - Retrieve Leads) If (Form or newsletter?) Set (newsletter and form data preparation) Google Sheets (Append/update for newsletter and form sheets) Respond to Webhook üìû Support Made by: khaisa Studio Tag: automation, Google Sheets, Fluentform, Leads Category: Marketing Need a custom? Contact Me",663,2025-07-05 09:00:56.355000+00:00,False,1
2587,Sync Entra User to Zammad User,"This workflow facilitates seamless synchronization between Entra (Microsoft Azure AD) and Zammad. It automates the following processes: Fetch Entra Group Members: Retrieves users from a designated Entra group. These users are candidates for synchronization. Create Universal User Object: Extracts key user information, such as email, phone, and name, and formats it for Zammad compatibility. Synchronize with Zammad: Identifies users in Zammad who need updates based on Entra data. Adds new users from Entra to Zammad. Deactivates users in Zammad if they are no longer in the Entra group. Key Features Dynamic Matching**: Compares users from Entra with existing Zammad users based on email and updates records accordingly. Efficient Management**: Automatically creates, updates, or deactivates Zammad users based on their status in Entra. Custom Fields**: Supports custom field mapping, ensuring enriched user profiles in Zammad. Setup Instructions Microsoft Entra Integration: Ensure proper API permissions for accessing Entra groups and members. Configure Microsoft OAuth2 credentials in n8n. Zammad Integration: Set up Zammad API credentials with appropriate access rights. Customize the workflow to include additional fields or map existing fields as needed. Run Workflow: Trigger the workflow manually or set up an automation schedule (e.g., daily sync). Review created/updated/deactivated users in Zammad. Use Cases IT Administration**: Keep your support system in sync with the organization‚Äôs Entra data. User Onboarding**: Automatically onboard new hires into Zammad based on Entra groups. Access Management**: Ensure accurate and up-to-date user records in Zammad. Prerequisites Access to an Entra (Azure AD) environment with group data. A Zammad instance with API credentials for user management. A custom field in Zammad User Object (entra_key) of type String. A custom field in Zammad User Object (entra_object_type) of type `Single selection field with two key value pairs user = User contact = Contact` This workflow is fully customizable and can be adapted to your organization‚Äôs specific needs. Save time and reduce manual errors by automating your user sync process with this template! If you have found an error or have any suggestions, please report them here on Github.",642,2024-11-29 09:59:23.069000+00:00,False,2
4834,"Automate Gym Lead Nurturing with Facebook Ads, WhatsApp & Google Sheets Tracking","üìå What it does: Captures leads from Facebook Ads Sends instant WhatsApp to leads Waits & checks for bookings If booked anytime during 5-day nurturing, it stops the follow-ups and sends confirmations to both lead & gym owner If not booked, continues to nurture daily for 5 days with reminders",631,2025-06-10 06:17:05.257000+00:00,False,2
2818,Fetch Scriptures Dynamically from get Bible API,"Overview The Get Bible Query Workflow is a modular and self-standing workflow designed to retrieve scriptures dynamically based on structured input. It serves as an intermediary layer that extracts references, queries the GetBible API, and returns scriptures in a standardized JSON format. This workflow is fully prepared for integration‚Äîsimply call it from another workflow with the required JSON input, and it will return the requested scripture data. Who Is This For? This workflow is ideal for developers, Bible study apps, research tools, and dynamic scripture-based projects that need seamless access to scriptural content without direct API interaction. ‚úÖ Use Cases: Bible Study Apps** ‚Üí Embed scripture retrieval functionality. Research & Theology Tools** ‚Üí Fetch structured verse data. Dynamic Content Generation** ‚Üí Integrate real-time scripture references. Sermon Preparation** ‚Üí Automate scripture lookups. How It Works Trigger Workflow ‚Üí This workflow is designed to be called from another workflow with a structured JSON input. Receive Input ‚Üí Accepts a JSON object containing references, translation, and API version. Extract References ‚Üí Parses single verses, comma-separated lists, and ranged passages. Query API ‚Üí Sends structured requests to the GetBible API. Format Response ‚Üí Returns structured JSON output, maintaining API response consistency. JSON Input Structure References** ‚Üí Should include the book name, chapter, and verse(s). Multiple Verses** ‚Üí Separated by commas (e.g., John 3:16,18). Verse Ranges** ‚Üí Defined with a dash (e.g., John 3:16-18). Translation** ‚Üí Choose from the supported translations. API Version** ‚Üí Currently supports v2. Example JSON Input { ""references"": [ ""1 John 3:16"", ""Jn 3:16"", ""James 3:16"", ""Rom 3:16"" ], ""translation"": ""kjv"", ""version"": ""v2"" } Example API Response { ""result"": { ""kjv_62_3"": { ""translation"": ""King James Version"", ""abbreviation"": ""kjv"", ""book_name"": ""1 John"", ""chapter"": 3, ""ref"": [""1 John 3:16""], ""verses"": [ { ""chapter"": 3, ""verse"": 16, ""name"": ""1 John 3:16"", ""text"": ""Hereby perceive we the love of God, because he laid down his life for us: and we ought to lay down our lives for the brethren."" } ] } } } üí° Fully structured and formatted response ‚Äì ready for seamless integration. Integration and Usage The GetBible Query Workflow is designed for immediate use. Simply call it from another workflow and pass the appropriate JSON object as input, and it will return the requested scripture passages. ‚úîÔ∏è No additional configuration is required. ‚úîÔ∏è Designed for fast, reliable, and structured scripture retrieval. ‚úîÔ∏è Fully compatible with GetBible API responses. Why Use This Workflow? ‚úîÔ∏è Fast & Reliable ‚Üí Direct API integration for efficient queries. ‚úîÔ∏è Flexible Queries ‚Üí Supports single, multi-verse, and ranged requests. ‚úîÔ∏è Agent-Compatible ‚Üí Easily integrates into automated workflows. ‚úîÔ∏è No Code Needed ‚Üí Just configure the JSON input and run the workflow. Next Steps üîó API Support üìñ API Documentation üí¨ Need help? Join the community for support! üöÄ",628,2025-01-29 13:42:38.278000+00:00,False,2
2609,Automatically Correct Wrong Shipping Addresses in Billbee Orders,"Address Validation Workflow About This workflow automates the process of validating and correcting client shipping addresses in Billbee, ensuring accurate delivery information. It's ideal for e-commerce businesses looking to save time and reduce errors in their order fulfillment process. The workflow uses Billbee, an order management platform for small to medium-sized online retailers, and the Endereco API for address validation. Who Is This For? E-Commerce Businesses**: Streamline order fulfillment by automatically correcting common shipping address errors. Warehouse Teams**: Reduce manual work and ensure packages are shipped to the correct address. Small to Medium-Sized Retailers**: Businesses using Billbee to manage orders and requiring efficient, automated solutions for address validation. How it Works Trigger: Workflow starts via a Billbee Webhook when an order is imported. Fetch Data: Retrieve the client's shipping address using the Order ID. Validate Address: Send the address to the Endereco API for validation and correction (e.g., house number errors). Conditional Actions: Valid Address: Update the address in Billbee. Invalid Address: Tag the order with ""Validation Error."" Track Status: Add tags in Billbee for processed orders. Setup Steps API Keys: Obtain Billbee Developer/User API Keys and Endereco API Key. Billbee Rule: Create an automation rule: Trigger: Order imported. Action: Call External URL with OrderId to trigger n8n workflow. Optional: Use a secondary trigger (e.g., order state changes to ""gepackt"") for manual corrections. Customization Options Filter Delivery Addresses: Customize filters to exclude specific delivery types, such as pickup shops (""Postfiliale,"" ""Paketshop,"" or ""Packstation""). Filters can be adjusted within Billbee or in the workflow. Error Handling: Configure additional actions for orders that fail validation, such as notifying your team or flagging orders for manual review. Order Tags: Define custom tags in Billbee to better track order statuses (e.g., ""Address Corrected,"" ""Validation Error""). Trigger Types: Use additional triggers such as changes to order states (e.g., ""gepackt"" or ""In Fulfillment"") for manual corrections or validations. Address Fields: Modify the workflow to focus on specific address components, such as postal codes, city names, or country codes. Validation Rules: Adjust Endereco API settings or add custom logic to refine validation criteria based on your business needs. API Documentation Endereco**: Endereco API Docs Billbee**: Billbee API Docs",626,2024-12-05 10:23:11.248000+00:00,False,1
6916,Create Viral LinkedIn Content with O3 & GPT-4.1-mini Multi-Agent Team,"Create Viral LinkedIn Content with O3 & GPT-4.1-mini Multi-Agent Team This n8n workflow is a multi-agent LinkedIn content factory. At its heart is the Content Director Agent (O3 model), who acts as the project manager. It listens for LinkedIn chat messages, analyzes them, and coordinates a team of AI specialists (all powered by GPT-4.1-mini) to produce viral, engaging, and optimized LinkedIn content. üü¢ Section 1 ‚Äì Workflow Entry & Strategy Layer Nodes: üîî When chat message received ‚Üí Captures LinkedIn requests (your idea, draft, or prompt). üß† Content Director Agent (O3) ‚Üí Acts as the leader, deciding how the content should be structured and which specialists to call. üí° Think Node ‚Üí Helps the Director brainstorm and evaluate possible approaches before delegating. ü§ñ OpenAI Chat Model Director (O3) ‚Üí The Director‚Äôs brain, providing strategic-level thinking. ‚úÖ Beginner-friendly benefit: This section is like the ‚Äúcommand center.‚Äù Any LinkedIn content request starts here and gets transformed into a clear, strategic plan before moving to specialists. ‚úçÔ∏è Section 2 ‚Äì Content Creation Specialists Nodes: ‚úçÔ∏è LinkedIn Copywriter ‚Üí Creates viral hooks, compelling posts, and platform-friendly messaging. üéì Domain Expert ‚Üí Ensures technical accuracy and industry authority in the post. üìù Proofreader & Editor ‚Üí Polishes content for grammar, tone, and style. Each agent connects to its own GPT-4.1-mini model for cost-efficient, specialized output. ‚úÖ Beginner-friendly benefit: This section is like your content writing team‚Äîfrom drafting, to adding expertise, to polishing for professional LinkedIn standards. üöÄ Section 3 ‚Äì Engagement & Optimization Specialists Nodes: üöÄ Engagement Strategist ‚Üí Crafts hashtags, posting times, and audience growth strategies. üé® Visual Content Strategist ‚Üí Designs carousels, infographics, and visual ideas. üìä Content Performance Analyst ‚Üí Tracks analytics, measures performance, and suggests improvements. Each of these also relies on GPT-4.1-mini, keeping cost low while delivering specialized insights. ‚úÖ Beginner-friendly benefit: This section is like your growth & marketing team‚Äîthey ensure your content doesn‚Äôt just look good but also performs well and reaches the right audience. üìä Summary Table | Section | Key Nodes | Role | Beginner Benefit | | ---------------------------- | -------------------------------------- | -------------------- | --------------------------------------- | | üü¢ Entry & Strategy | Trigger, Director, Think, O3 Model | Strategy & planning | Turns your idea into a clear strategy | | ‚úçÔ∏è Content Creation | Copywriter, Domain Expert, Proofreader | Writing & refinement | Produces expert-level, polished content | | üöÄ Engagement & Optimization | Engagement, Visuals, Analytics | Growth & performance | Maximizes reach, visuals, and results | üåü Why This Workflow Rocks All-in-one content team** ‚Üí Strategy + Writing + Optimization Low cost** ‚Üí O3 only for strategy, GPT-4.1-mini for specialists Parallel agents** ‚Üí Work simultaneously for faster results Scalable** ‚Üí Reusable for any LinkedIn content need üëâ Even a beginner can use this workflow: just send a LinkedIn content idea (e.g., ‚ÄúWrite a post on AI in finance‚Äù), and your AI team handles the rest‚Äîwriting, polishing, visuals, and engagement tactics.",625,2025-08-02 18:04:42.673000+00:00,True,4
2485,Automate Droplet Snapshots on DigitalOcean,"This workflow automates the management of DigitalOcean Droplet snapshots by listing all droplets, filtering based on the number of snapshots, and deleting excess snapshots before creating new ones. It ensures your droplet snapshots stay organized and within a manageable limit, preventing unnecessary storage costs due to an excess of snapshots. Who is this for? This workflow is perfect for users managing DigitalOcean Droplets and looking to automate the process of snapshot creation and cleanup to save on storage costs and maintain efficient resource management. It‚Äôs useful for DevOps teams, cloud administrators, or any developer leveraging DigitalOcean for their infrastructure. What problem is this workflow solving? When managing multiple DigitalOcean Droplets, snapshots can quickly accumulate, taking up space and increasing storage costs. Manually deleting and creating snapshots can be time-consuming and inefficient. This automation solves this problem by automating the snapshot management process, ensuring that no more than a defined number of snapshots are kept per droplet. What this workflow does Runs every 48 hours: The workflow is triggered by a cron node that runs every 48 hours, ensuring timely snapshot management. List all droplets: The workflow retrieves all droplets in the DigitalOcean account. Retrieve snapshots: For each droplet, the workflow retrieves a list of existing snapshots. Filter snapshots: If the number of snapshots exceeds 4, the workflow filters for snapshots that need to be deleted. Delete snapshots: Excess snapshots are automatically deleted based on the filter criteria. Create new snapshot: After cleaning up, the workflow creates a new snapshot for each droplet, ensuring that backups are always up-to-date. Setup DigitalOcean API Key: You‚Äôll need to configure the HTTP Request nodes with your DigitalOcean API key. This key is required for authenticating requests to list droplets, retrieve snapshots, delete snapshots, and create new ones. Snapshot Threshold: By default, the workflow is set to keep no more than 4 snapshots per droplet. This can be adjusted by modifying the filter node conditions. Set Execution Frequency: The cron node is set to run every 48 hours, but you can adjust the timing to suit your needs. How to customize this workflow Adjust Snapshot Limit**: Change the value in the filter node if you want to keep more or fewer snapshots. Modify Run Frequency**: The workflow runs every 48 hours by default. You can change the frequency in the cron node to run more or less often. Enhance with Notifications**: You can add a notification node (e.g., Slack or email) to alert you when snapshots are deleted or created. Workflow Summary This workflow automates the management of DigitalOcean Droplet snapshots by keeping the number of snapshots under a defined limit, deleting the oldest ones, and ensuring new snapshots are created at regular intervals.",615,2024-10-23 13:17:02.170000+00:00,False,1
2308,Convert Airtable rich text markdown field to HTML,Sometimes you need the rich text field to be in HTML instead of Markdown. This template either syncs a single record or all records at once. Youtube tutorial,611,2024-07-03 07:33:12.301000+00:00,False,1
2120,Alert via Gmail when a great lead submits form with MadKudu and Hunter,"Use case If you have a form where potential leads reach out, then you probably want to analyze those leads and send a notification if certain requirements are met, e.g. employee number is high enough. MadKudu is built exactly to solve this problem. We use it along with Hunter and Gmail to get an email alert for high quality leads. How to setup Add you MadKudu, Hunter, and Gmail credentials Set the email to send to Click the Test Workflow button, enter your email and check your email Activate the workflow and use the form trigger production URL to collect your leads in a smart way How to adjust this template You may want to raise or lower the threshold for your leads, as you see fit.",597,2024-02-22 15:55:22.368000+00:00,False,3
5014,End of Turn Detection for smoother AI agent chats with Telegram and Gemini,"This n8n template demonstrates one approach to achieve a more natural and less frustration conversations with AI agents by reducing interrupts by predicting the end of user utterances. When we text or chat casually, it's not uncommon to break our sentences over multiple messages or when it comes to voice, break our speech with the odd pause or umms and ahhs. If an agent replies to every message, it's likely to interrupt us before we finish our thoughts and it can get very annoying! Previously, I demonstrated a simple technique for buffering each incoming message by 5 seconds but that approach still suffers in some scenarios when more time is needed. This technique has no arbitrary time limit and instead uses AI to figure out when its the agent's turn based on the user's message, allowing for the user to take all the time they need. How it works Telegram messages are received but no reply is generated for them by default. Instead they are sent to the prediction subworkflow to determine if a reply should be generated. The prediction subworkflow begins by checking Redis for the current user's prediction session state. If this is a new ""utterance"", it kicks off the ""predict end of utterance"" loop - the purpose of which is to buffer messages in a smart way! New users message can continue to be accepted by the workflow until enough is collected to allow our prediction classifier to determine the end of the utterance has been reached. The loop is then broken and the buffered chat messages are combined and sent to the AI agent to generate a response and sent to the user via the telegram node. The prediction session state is then deleted to signal the workflow is ready to start again with a new message. How to use This system sits between your preferred chat platform and the AI agent so all you need to do is replace the telegram nodes as required. Where LLM-only prediction isn't working well enough, consider more traditional code-based checking of heuristics to improve the detection. Ideally you'll want a fast but accurate LLM so your user isn't waiting longer than they have to - at time of writing Gemini-2.5-flash-lite was the fastest in testing but keep a look out for smaller and more powerful LLMs in the future. Requirements Gemini for LLM Redis for session management Telegram for chat platform",597,2025-06-17 22:47:24.712000+00:00,True,7
4917,Automatic Travel Itinerary Generation via Email with Llama AI,"Travel Agent that Auto Response on Mail In this guide, we‚Äôll break down how to set up an AI-powered auto-reply system that works while you sleep. Ready to 10X your efficiency? Let‚Äôs dive in! What‚Äôs the Goal? AI-driven auto-responses for Email. Instant replies to FAQs, order confirmations, and support queries. 24/7 availability - no more ‚ÄúWe‚Äôll get back to you soon‚Äù delays. Seamless integration with existing business tools. By the end, you‚Äôll have a self-running communication assistant that never takes a coffee break. Why Does It Matter? Why automate replies? Because time = money and manual typing is so 2010. Here‚Äôs why this workflow is a game changer: Zero Human Error: AI doesn‚Äôt get tired or make typos. Lightning-Fast Replies: Customers get instant answers, improving satisfaction. 24/7 Availability: No more ‚ÄúOut of Office‚Äù replies. Focus on High-Value Work: Free your team from mundane tasks. Think of it as hiring a super efficient virtual assistant - minus the salary. How It Works Here‚Äôs the step by step magic behind the automation Step 1: Trigger the Workflow Detect new messages from WhatsApp, Email, or Slack. Use n8n‚Äôs webhook or API integration to capture incoming queries. Step 2: Process the Message with AI Send the message to an AI model (like OpenAI GPT-4 or Gemini). Generate a context-aware, human-like response. Step 3: Send the Automated Reply Route the AI-generated response back to the original platform. Ensure personalization (e.g., ‚ÄúHi [Name], thanks for reaching out!‚Äù). Step 4: Log & Optimize Store interactions in a database (Airtable, Google Sheets). Continuously improve AI responses based on past data. How to use the workflow? Importing a workflow in n8n is a straightforward process that allows you to use pre-built or shared workflows to save time. Below is a step-by-step guide to importing a workflow in n8n, based on the official documentation and community resources. Steps to Import a Workflow in n8n 1. Obtain the Workflow JSON Source the Workflow:** Workflows are typically shared as JSON files or code snippets. You might receive them from: The n8n community (e.g., n8n.io workflows page). A colleague or tutorial (e.g., a .json file or copied JSON code). Exported from another n8n instance (see export instructions below if needed). Format:** Ensure you have the workflow in JSON format, either as a file (e.g., workflow.json) or as text copied to your clipboard. 2. Access the n8n Workflow Editor Log in to n8n:** Open your n8n instance (via n8n Cloud or your self-hosted instance). Navigate to the Workflows tab in the n8n dashboard. Open a New Workflow:** Click Add Workflow to create a blank workflow, or open an existing workflow if you want to merge the imported workflow. 3. Import the Workflow Option 1: Import via JSON Code (Clipboard): In the n8n editor, click the three dots (‚ãØ) in the top-right corner to open the menu. Select Import from Clipboard. Paste the JSON code of the workflow into the provided text box. Click Import to load the workflow into the editor. Option 2: Import via JSON File: In the n8n editor, click the three dots (‚ãØ) in the top-right corner. Select Import from File. Choose the .json file from your computer. Click Open to import the workflow.",586,2025-06-13 09:12:39.320000+00:00,True,3
2774,Typeform to KlickTipp Integration - Quiz,"Community Node Disclaimer: This workflow uses KlickTipp community nodes. How It Works: Typeform Quiz Integration: This workflow streamlines the process of handling quiz answers submitted via Typeform. It ensures the data is correctly formatted and seamlessly integrates with KlickTipp. Data Transformation: Input data is validated and transformed to meet KlickTipp‚Äôs API requirements, including formatting phone numbers and converting dates. Key Features Typeform Trigger: Captures new quiz submissions from Typeform, including user details and quiz responses. Data Processing and Transformation: Formats phone numbers to numeric-only format with international prefixes. Converts dates (e.g., birthdays) to UNIX timestamps. Maps multiple-choice quiz answers to string values for API compatibility. Scales numeric quiz responses for tailored use cases. Subscriber Management in KlickTipp: Adds participants as subscribers to a designated KlickTipp list, with custom field mappings for: Personal details (e.g., name, email, phone number, birthday). Quiz responses (e.g., intended usage of KlickTipp, company location, and team size). Tags contacts for segmentation: Adds fixed and dynamic tags to contacts. Error Handling: Handles empty or malformed data gracefully, ensuring clean submissions to KlickTipp. Setup Instructions Install and Configure Nodes: Set up the Typeform and KlickTipp nodes in your n8n instance. Authenticate your Typeform and KlickTipp accounts. Prepare Custom Fields in KlickTipp: Create custom fields to store quiz answers and personal details, such as: | Field Name | Field Type | |----------------------------------------------------|------------------| | Typeform \| URL Linkedin | URL | | Typeform \| Question 1 KlickTipp benefits | Text | | Typeform \| Question 2 KlickTipp headquarter | Text | | Typeform \| Question 3 Members CHT | Decimal Number | After creating fields, allow 10-15 minutes for them to sync. If fields don‚Äôt appear, reconnect your KlickTipp credentials. Field Mapping and Adjustments: Verify and customize field assignments in the workflow to align with your specific form and subscriber list setup. Workflow Logic Trigger via Typeform Submission: The workflow initiates upon receiving a new quiz submission. Transform Data for KlickTipp: Converts and validates data from Typeform to match KlickTipp‚Äôs API requirements. Add to KlickTipp Subscriber List: Submits the cleaned data to KlickTipp, including all relevant quiz answers. Get all tags from KlickTipp and create a list: Fetches all existing Tags and turns them into an array Define tags to dynamically set for contacts: Definiton of variables that are received from the form submission and should be converted into tags Merge tags of both lists: Checks whether the list of existing tags in KlickTipp contains the tags which should be dynamically set based on the form submission Tag creation and tagging contacts: Creates new tags if it previously did not exist and then tags the contact Benefits Efficient lead generation: Contacts from forms are automatically imported into KlickTipp and can be used immediately, saving time and increasing the conversion rate. Automated processes: Experts can start workflows directly, such as welcome emails or course admissions, reducing administrative effort. Error-free data management: The template ensures precise data mapping, avoids manual corrections and reinforces a professional appearance. Testing and Deployment Test the workflow by filling the form on Typeform and verifying data updates in KlickTipp. Notes Customization: Update field mappings within the KlickTipp nodes to align with your account setup. This ensures accurate data syncing. Resources: Typeform KlickTipp Knowledge Base help article Use KlickTipp Community Node in n8n Automate Workflows: KlickTipp Integration in n8n",581,2025-01-22 09:29:58.300000+00:00,False,0
2839,Calculate the Centroid of a Set of Vectors,"n8n Workflow: Calculate the Centroid of a Set of Vectors Overview This workflow receives an array of vectors in JSON format, validates that all vectors have the same dimensions, and computes the centroid. It is designed to be reusable across different projects. Workflow Structure Nodes and Their Functions: Receive Vectors (Webhook): Accepts a GET request containing an array of vectors in the vectors parameter. Expected Input: vectors parameter in JSON format. Example Request: /webhook/centroid?vectors=[[2,3,4],[4,5,6],[6,7,8]] Output: Passes the received data to the next node. Extract & Parse Vectors (Set Node): Converts the input string into a proper JSON array for processing. Ensures vectors is a valid array. If the parameter is missing, it may generate an error. Expected Output Example: { ""vectors"": [[2,3,4],[4,5,6],[6,7,8]] } Validate & Compute Centroid (Code Node): Validates vector dimensions and calculates the centroid. Validation: Ensures all vectors have the same number of dimensions. Computation: Averages each dimension to determine the centroid. If validation fails: Returns an error message indicating inconsistent dimensions. Successful Output Example: { ""centroid"": [4,5,6] } Error Output Example: { ""error"": ""Vectors have inconsistent dimensions."" } Return Centroid Response (Respond to Webhook Node): Sends the final response back to the client. If the computation is successful, it returns the centroid. If an error occurs, it returns a descriptive error message. Example Response: { ""centroid"": [4, 5, 6] } Inputs JSON array of vectors, where each vector is an array of numerical values. Example Input { ""vectors"": [ [1, 2, 3], [4, 5, 6], [7, 8, 9] ] } Setup Guide Create a new workflow in n8n. Add a Webhook node (Receive Vectors) to receive JSON input. Add a Set node (Extract & Parse Vectors) to extract and convert the data. Add a Code node (Validate & Compute Centroid) to: Validate dimensions. Compute the centroid. Add a Respond to Webhook node (Return Centroid Response) to return the result. Function Node Script Example const input = items[0].json; const vectors = input.vectors; if (!Array.isArray(vectors) || vectors.length === 0) { return [{ json: { error: ""Invalid input: Expected an array of vectors."" } }]; } const dimension = vectors[0].length; if (!vectors.every(v =&gt; v.length === dimension)) { return [{ json: { error: ""Vectors have inconsistent dimensions."" } }]; } const centroid = new Array(dimension).fill(0); vectors.forEach(vector =&gt; { vector.forEach((val, index) =&gt; { centroid[index] += val; }); }); for (let i = 0; i &lt; dimension; i++) { centroid[i] /= vectors.length; } return [{ json: { centroid } }]; Testing Use a tool like Postman or the n8n UI to send sample inputs and verify the responses. Modify the input vectors to test different scenarios. This workflow provides a simple yet flexible solution for vector centroid computation, ensuring validation and reliability.",581,2025-02-03 02:28:31.005000+00:00,False,1
4973,Automated SSL Certificate Monitoring and Renewal with Notion and Telegram,"Automatically fetch existing domains from Notion's Database and verify the validity of SSL certificates through SSL-Checker. If the validity period is less than 14 days, send a Telegram message notification and trigger SSH remote automatic refresh. Successful refresh notification will be sent through Telegram. This can prevent problems with the server-side automatic refresh program, which may cause unexpected service interruptions. Main use cases: Notion store domain. Telegram receives warning messages. Remotely trigger Certbot to refresh SSL. How it works: Record who triggered this workflow, because if there is a credential that is about to expire, this workflow will be triggered repeatedly. After getting all the domains from Notion, send an http request to SSL-Checker. After getting all the SSL-Checker results, add the validity label. And use the IF node to check if there are any certificates that are about to expire. Then there are two workflows: If there is a certificate that is about to expire: send an SSH command to the remote control server to refresh the certificate, notify through Telegram, and call this workflow again to re-verify the validity of the SSL certificate. If the validity period of SSL is normal: then refresh the data on Notion, and if a re-called workflow is detected, Telegram will be used to notify that the SSL has been updated.",576,2025-06-16 14:20:52.859000+00:00,False,4
5013,Track n8n Workflow Changes Over Time with Compare Dataset & Google Sheets,"This n8n template runs daily to track and report on any changes made to workflows on any n8n instance. Useful if a team is working within a single instance and you want to be notified of what workflows have changed since you last visited them. Another use-case might be monitoring your managed instances for clients and being alerted when changes are made without your knowledge. See a sample Gsheet here: https://docs.google.com/spreadsheets/d/1dOHSfeE0W_qPyEWj5Zz0JBJm8Vrf_cWp-02OBrA_ZYc/edit?usp=sharing How it works A scheduled trigger is set to run once a day to review all available workflows. An n8n node imports the workflows as json. The workflows are brought into a loop where each is first checked to see if it exists in the designated google sheet. If not, a new entry is created and skipped. If the workflow has been captured before, then the comparison subworkflow can be executed using the previous and current versions of the workflow json data. The subworkflow uses the compare dataset tool to calculate the changes to nodes and connections for the given workflow. The results are then recorded back to the google sheet for review. How to use Start with the n8n node and try to filter by the workflows you're interested in tracking. Set the scheduled trigger interval to match the frequency to suit how often your workflows are being edited. Customising the workflow Want to get fancy? Add in an AI agent to help determine changes between the previous and current versions of the workflow. Add contextual explanations to reveal the impact of the changes.",573,2025-06-17 22:43:15.197000+00:00,True,1
2634,Spotify Sync Liked Songs to Playlist,"Short an simple: This Workflow will sync (add and delete) your Liked Songs to an custom playlist that can be shared. Setup: Create an app on the Spotify Developer Dashboard. Create Spotify Credentials - Just click on one of the Spotify Nodes in the Workflow an click on ""create new credentials"" and follow the guide. Create the Spotify Playlist that you want to sync to. Copy the exact name of you playlist, go into Node ""Edit set Vars"" and replace the value ""CHANGE MEEEE"" with your playlist name. Set your Spotify Credentiels on every Spotify Node. (Should be marekd with Yellow and Red Notes) Do you use Gotify? - No: Delete the Gotify Nodes (all the way to the right end of the Workflow) - Yes: Customize the Gotify Nodes to your needs.",567,2024-12-11 21:47:10.634000+00:00,False,2
5039,"Stock Market Information Assistant with Telegram, Yahoo Finance, and GPT-4 Nano","How it works Listens to Telegram messages to detect stock-related queries. Extracts company name and identifies its exact stock ticker symbol. Searches Yahoo Finance for stock info using the ticker. Fetches and formats the latest stock data like price and key stats. Sends a clean, simplified reply back to the user on Telegram. Set up steps Requires Telegram Bot Token and Apify API credentials. Import workflow to n8n and link both Apify actors (Google Search + Yahoo Finance). Link OpenRouter for AI and enable Telegram trigger. Takes ~10‚Äì15 min to connect services and test. Includes sticky notes with setup links and tutorial videos.",562,2025-06-19 07:12:19.448000+00:00,True,7
2551,Add new clients from Notion to Clockify,"Who is this template for? This workflow template is ideal for anyone using Notion for project management and Clockify for time tracking. The workflow automatically adds all new clients from Notion to Clockify. How it works Scans your Notion client table every minute for new clients Adds all new clients to your Clockify workspace Set up Steps Set up the Notion trigger node by adding your Notion API credentials as described in the n8n Notion docs. Go to your Notion clients page/table and give your integration permission to acces the data on this page. Go back to n8n and select your Notion client page in the Notion trigger node. Set up the Clockify node by adding your Clockify API credentials as described in the n8n Clockify docs, select your Clockify workspace and map your client name column from Notion to the Clockify ""Client Name"" field.",551,2024-11-16 14:08:29.825000+00:00,False,1
4933,"Document Parsing & Data Extraction with Mistral OCR, AI Processing, and Multi-Channel Delivery","Tired of manual data entry? Drowning in paper documents? The Sentinel OCR & Workflow Hub is your ultimate solution for transforming physical documents into actionable digital data, seamlessly and intelligently. This isn't just an OCR machine; it's a productivity powerhouse designed to revolutionize your workflow. Imagine this: Effortless Data Extraction: Simply feed documents into the Sentinel. Our state-of-the-art OCR technology accurately extracts text, numbers, and key information, even from complex layouts, invoices, receipts, and forms. Intelligent Automation with n8n: The Sentinel integrates flawlessly with n8n, the powerful low-code automation platform. This means endless possibilities! Automatically route extracted data, trigger custom workflows, and connect to hundreds of other applications ‚Äì all without a single line of code. Instant Communication, Crystal Clear: Gmail Integration: Have critical data or processed documents sent directly to your inbox or specific team members' inboxes, organized and ready for review. Say goodbye to searching through endless attachments! Telegram Notifications: Get real-time alerts and summaries of processed documents, data extraction statuses, or even flagged discrepancies, right on your Telegram app. Stay informed, even on the go! Boost Efficiency, Reduce Errors, Save Money: By automating data capture and distribution, the Sentinel dramatically reduces manual errors, frees up valuable staff time, and accelerates your business processes. Reallocate your resources to what truly matters. Scalable and Secure: Whether you're a small business or a large enterprise, the Sentinel grows with you. And with robust security features, your sensitive data is always protected. The Sentinel OCR & Workflow Hub is more than a machine; it's a strategic asset. Stop wasting time and resources on manual tasks. Start unlocking the true potential of your data today. Transform your documents. Transform your business. Choose the Sentinel.",542,2025-06-14 15:32:58.031000+00:00,True,7
2663,YouTube Report Generator,"YouTube Subtitles Report Generator Overview This template enables users to generate analytical reports from YouTube video subtitles, providing insights into the thematic content of the video. Designed for efficiency and simplicity, it processes video subtitles without requiring an API key, making it an accessible solution for content analysis. The system assumes videos already have subtitles available, excluding live streams and videos without subtitle data. Key Features Trigger Webhook: Seamlessly receive video IDs through a webhook trigger. Fetch Video HTML: Retrieves the video‚Äôs HTML content directly from YouTube. Extract Subtitles URLs: Processes the HTML content to find and decode subtitle URLs. Fetch Subtitles Content: Downloads the subtitles from the decoded URLs in XML format. Generate Analytical Report: Utilizes an AI model to summarize and analyze the thematic essence of video content. The system supports models such as Google Gemini, OpenAI, and other compatible language models. The quality of the results may vary depending on the model used, with better models providing faster and more accurate summaries. The default prompt focuses on identifying and summarizing the main theme of the video while excluding content related to promotions, subscriptions, or sponsored content. Return Analytical Report: Delivers concise analytical reports as the final response to the webhook, suitable for various use cases like research, content creation, or consumption as plain text. Setup Instructions Step 1: Webhook Configuration Set up the webhook URL in your external system (e.g., app, API) to send YouTube video IDs to this workflow. Step 2: API Access Ensure that your environment has access to YouTube‚Äôs public HTML content. An API key is not required since the workflow parses publicly available data. Step 3: AI Integration Verify the connection to the AI model used for report generation. Supported models include Google Gemini and OpenAI. Note that the system can be customized by modifying the provided prompt to suit specific analysis needs. Step 4: Testing Run a sample test with a YouTube video ID to ensure subtitles are correctly retrieved and the report is generated successfully. Expected Outcomes Effortless Content Analysis:** Generate thematic reports with minimal setup. No API Key Dependency:** Simplified access by leveraging YouTube‚Äôs public HTML. Actionable Insights:** Gain valuable information on video content for business, research, or personal projects. Cost Considerations:** The execution cost depends primarily on the model used and the length of the video (affecting token usage). Leveraging the free tier of Google Gemini models is recommended to minimize costs. Main Theme Extraction:** The default prompt excludes irrelevant promotional content, providing clear and focused thematic summaries. Estimated setup time: 15‚Äì20 minutes with a ready environment. Prompt Description The workflow includes a customizable prompt used to process subtitles in XML format and generate analytical reports. The generated report includes: Title: A brief phrase capturing the essence of the main theme. Body: An analytical description of the main theme, structured into a maximum of three concise paragraphs. It focuses on summarizing key ideas, recurring themes, and connections without referencing the source explicitly (e.g., avoiding phrases like ‚Äúthis video‚Äù). The system also removes content related to sales, subscriptions, or promotions to maintain a clear thematic focus. Example Output Title: The Ethical Challenges of Artificial Intelligence Report: Artificial intelligence presents significant challenges in areas such as privacy, fairness, and automated decision-making. Its implementation in critical sectors such as healthcare, justice, and security has sparked debates about inherent biases and lack of transparency. Additionally, there is growing concern over the ethical implications of automation, including its impact on employment and the global economy. Finally, the importance of establishing strong regulatory frameworks is highlighted to balance technological innovation with the protection of human rights.",512,2024-12-20 02:18:31.382000+00:00,True,4
2577,New TheHive Case Slack Notification Bot,"Streamline Case Management in TheHive via Slack! Our TheHive Slack Integration empowers SOC analysts by allowing them to efficiently manage and update case attributes directly within Slack, reducing the need to switch contexts and enhancing response time. Key Features: Direct Case Management**: Modify case details such as assignee, severity, status, and more through intuitive form inputs embedded within Slack messages. Seamless Integration**: Assumes matching email addresses between TheHive and Slack users for straightforward assignee updates. Note: Ensure email consistency to avoid assignment errors. Instant Case Actions**: Quickly close cases as false positives or adjust threat levels with minimal clicks, directly impacting case status in TheHive and reflecting updates immediately in Slack. Task Management**: Add tasks to cases through a user-friendly modal popup, fostering better task tracking and delegation within your team. Operational Benefits: Efficiency**: Enables analysts to perform multiple case actions without leaving Slack, streamlining workflows and saving valuable time. Accuracy**: Reduces the chances of human error by providing a controlled interface for case updates. Agility**: Enhances the SOC team's agility by providing tools for rapid response and case management, crucial for effective security operations. Setup Tips: Verify that all SOC team members have matching email IDs in TheHive and Slack. Familiarize your team with the Slack form inputs and ensure they understand the importance of accurate data entry. Regularly review and update the integration settings to accommodate any changes in your security operations protocols. Need Help? For detailed setup instructions or troubleshooting, refer to our Integration Guide or reach out on our Support Forum. Leverage this integration to maximize your SOC team's efficiency and responsiveness, ensuring that case management is as streamlined and effective as possible.",510,2024-11-26 05:32:11.636000+00:00,False,3
2379,Validate TOTP Token (without creating a credential),"TOTP Validation with Function Node This template allows you to verify if a 6-digit TOTP code is valid using the corresponding TOTP secret. It can be used in an authentication system. The inputs need to be: a base32 totp secret (String) a 6 digits code (String) ++Important:++ The 6-digit code must be in text format. If the code starts with zeros and is treated as a number, it could cause validation issues. The function node will generate a 6-digit code from the TOTP secret, then compare it with the provided code. If they match, it will return 1 otherwise, it will return 0. Example usage: You retrieve the user's TOTP secret from a database, then you want to verify if the 2FA code provided by the user is valid. Setup Guidelines You only need the ==TOTP VALIDATION== node. You will need to modify lines 39 and 40== of the node with the correct values for your specific context. Testing the Template You can define a sample secret and code in the EXAMPLE FIELDS node of the template, then click ""Test Workflow"". If the code is valid for the provided secret, the flow will proceed to the true branch of the IF CODE IS VALID node. Otherwise, it will go to the false branch.",508,2024-08-14 18:06:42.138000+00:00,False,1
2769,Gravity Forms to KlickTipp Integration - Feedback form,"Community Node Disclaimer: This workflow uses KlickTipp community nodes. How It Works Gravity Forms Customer Feedback Form Integration: This workflow streamlines the process of handling customer feedback submitted via Gravity Forms. It ensures the data is correctly formatted and seamlessly integrates with KlickTipp. Data Transformation: Input data is validated and transformed to meet KlickTipp‚Äôs API requirements, including formatting phone numbers and converting dates. Key Features Gravity Forms Trigger Captures new form submissions from Gravity Forms via a webhook and initiates the workflow. Data Processing and Transformation Formats and validates essential data: Converts phone numbers to numeric-only format with international prefixes. Transforms dates (e.g., birthdays) to UNIX timestamps. Calculates and scales numeric responses (e.g., webinar ratings). Parses webinar selections into timestamps for structured scheduling. Subscriber Management in KlickTipp Adds or updates contacts in a KlickTipp subscriber list. Includes custom field mappings such as: Personal details (name, email, birthday, phone number). Feedback and preferences (e.g., webinar ratings, chosen sessions). Structured answers from form responses. Tags contacts for segmentation: Adds fixed and dynamic tags to contacts. Error Handling Ensures invalid or empty data is handled gracefully, preventing workflow interruptions. Setup Instructions Install and Configure Nodes: Set up the Webhook, Set, and KlickTipp nodes in your n8n instance. Authenticate your Gravity Forms and KlickTipp accounts. Prepare Custom Fields in KlickTipp: Create fields in KlickTipp to align with the form submission data, such as: | Field Name | Field Type | |----------------------------------------------|------------------| | Gravityforms \| URL Linkedin | URL | | Gravityforms \| Course/webinar start timestamp | Date & Time | | Gravityforms \| Course/webinar rating | Decimal Number | | Gravityforms \| Feedback | Text | | Gravityforms \| Contact permission | Text | After creating fields, allow 10-15 minutes for them to sync. If fields don‚Äôt appear, reconnect your KlickTipp credentials. Field Mapping and Adjustments: Verify and customize field assignments in the workflow to align with your specific form and subscriber list setup. Workflow Logic Trigger via Gravity Forms Submission: The workflow begins when a new form submission is received through the webhook. Transform Data for KlickTipp: Formats and validates raw form data for compatibility with KlickTipp‚Äôs API. Add to KlickTipp Subscriber List: Adds processed data as a new subscriber or updates an existing one. Get all tags from KlickTipp and create a list: Fetches all existing Tags and turns them into an array Define tags to dynamically set for contacts: Definiton of variables that are received from the form submission and should be converted into tags Merge tags of both lists: Checks whether the list of existing tags in KlickTipp contains the tags which should be dynamically set based on the form submission Tag creation and tagging contacts: Creates new tags if it previously did not exist and then tags the contact Benefits Efficient lead generation: Contacts from forms are automatically imported into KlickTipp and can be used immediately, saving time and increasing the conversion rate. Automated processes: Experts can start workflows directly, such as welcome emails or course admissions, reducing administrative effort. Error-free data management: The template ensures precise data mapping, avoids manual corrections and reinforces a professional appearance. Testing and Deployment Test the workflow by filling the form on Gravity Forms and verifying data updates in KlickTipp. Notes Customization: Update field mappings within the KlickTipp nodes to align with your account setup. This ensures accurate data syncing. Resources: Gravity Forms KlickTipp Knowledge Base help article Use KlickTipp Community Node in n8n Automate Workflows: KlickTipp Integration in n8n",501,2025-01-21 17:03:13.432000+00:00,False,0
4758,Perplexity-Style Iterative Research with Gemini and Google Search,"AI Comprehensive Research on User's Query with Gemini and Web Search What is this? Perform comprehensive research on a user's query by dynamically generating search terms, querying the web using Google Search (by Gemini) , reflecting on the results to identify knowledge gaps, and iteratively refining its search until it can provide a well-supported answer with citations. (like Perplexity) This workflow is a reproduction of gemini-fullstack-langgraph-quickstart in N8N. The gemini‚Äëfullstack‚Äëlanggraph‚Äëquickstart is a demo by the Google‚ÄëGemini team that showcases how to build a powerful full‚Äëstack AI agent using Gemini and LangGraph How It Works Generate Query üí¨ generates one or more search queries tasks based on the User's question. uses Gemini 2.0 Flash Web Research üåê execute web search tasks using the native Google Search API tool in combination with Gemini 2.0 Flash. Reflection üìö Identifies knowledge gaps and generates potential follow-up queries. Setup Configure API Credentials: Create Google Gemini(PaLM) Api Credential using you own Gemini key Connect the credential with three nodes: Google Gemini Chat Model and GeminiSearch and reflection Configure Redis Source: prepare a Redis service that can be accessed by n8n Create Redis Crediential and connect it with all Redis node Customize Try using different Gemini models. Try modifying the parameters number_of_initial_queries and max_research_loops. Why use Redis? Use Redis as an external storage to maintain global variables (counter, search results, etc.) This workflow contains a loop process, which need global variables (as State in LangGraph). It is difficult to achieve global variables management without external storage in n8n.",485,2025-06-07 15:02:56.126000+00:00,True,6
2153,Add a bug to Linear via Slack command,"Use Case When building a product it's important to discover and eliminate bugs as quickly as possible. Since we're using our product at n8n a lot, we wanted to make it as easy as possible for everyone to add bugs with the needed level of detail. That's why we built this workflow that allows everyone to add bugs to our Linear account easily directly from Slack What this workflow does This workflow waits for a webhook call within Slack, that gets fired when users use the /bug command on a bot that you will create as part of this template. It then adds the bug to Linear using a pre-defined description and a defined label. It then notifies the user about the newly added bug as you can see below: How to create your Slack bot Visit https://api.slack.com/apps, click on New App and choose a name and workspace. Click on OAuth & Permissions and scroll down to Scopes -&gt; Bot token Scopes Add the chat:write scope Head over to Slash Commands and click on Create New Command Use /bug as the command Copy the test URL from the Webhook node into Request URL Add whatever feels best to the description and usage hint Go to Install app and click install Setup Configure your Slack bot using the sticky to the left Fill the Set me up node. You can find the IDs easily using the Helper nodes section Make sure to exchange the Request URL in your Slack with the Prod URL of the Webhook node before activating this workflow How to adjust it to your needs Add zero, one, two or many labels when creating the new ticket Change the Slack message according to your needs Change the default description for a new bug ticket Rename the Slack command as it works best for you How to enhance this workflow At n8n we use this workflow in combination with some others. E.g. we have the following things on top: We're using AI to classify the bugs and move them to the right team as soon as they get added to Linear (see this template) We also added other commands like /pain and /idea that allow us to submit other things to Notion. You can see the workflow for that here.",484,2024-02-29 13:53:59.744000+00:00,True,1
2578,Linear to Productboard feature Sync,"Linear Project/Issue Status and End Date to Productboard feature Sync Sync project and issue data between Linear and Productboard to keep teams aligned. This workflow updates Productboard features with the status and end date from Linear projects or due date from Linear issues. It ensures consistent data and sends a Slack notification whenever changes are made. Features Listens for updates in Linear projects/issues. Maps Linear statuses to Productboard feature statuses. Updates Productboard feature details including timeframe. Sends a Slack notification summarizing the updates. Setup Linear Credentials: Add your Linear API credentials in n8n. Productboard Credentials: Configure the Productboard API credentials in n8n. Linear Projects or Issues: Select the Linear project(s) or Issue(s) you want to monitor for updates. Productboard Custom Field: Create a custom field in Productboard named ""Linear"". This field should store the URL of the Linear project or issue you want to sync. Retrieve the UUID of the custom field in Productboard and set it up in the ""Get Productboard Feature ID"" node. Slack Notification: Update the Slack node with the desired Slack channel ID. Activate the Workflow: Enable the workflow to automatically sync data when triggered by updates in Linear.",475,2024-11-26 08:14:27.367000+00:00,False,3
2448,Masked Email Management for Fastmail,"Template Description This n8n workflow is designed to manage Fastmail masked email addresses using the Fastmail API. The workflow provides the following functionalities: Retrieve all masked emails: Fetches all masked email addresses associated with the Fastmail account. Create masked email: Allows creating a new masked email with a specified state (pending, enabled, etc.). Update masked email state: Updates the state of a masked email such as enabling, disabling, or deleting it. Generate HTML template: Constructs an HTML table to display the masked emails in a user-friendly format. Steps to Make it Work Webhook Node: This node listens for incoming requests to manage masked emails. Needs Basic Authentication credentials to secure the endpoint. Session Node: Sends a request to obtain session information from Fastmail's API. Requires an HTTP Header Auth credential with your Fastmail API token. Switch Node: Routes the workflow based on the state of the incoming masked email request (pending, enabled, disabled, deleted). HTTP Request Nodes: These nodes handle various Fastmail API calls for masked emails (get, set, update, delete). All HTTP Request nodes require an HTTP Header Auth credential attached, using the Fastmail API token. Set Node: Gathers the retrieved masked email list into an array for further processing. HTML Node: Generates an HTML template to render the masked email addresses in a table format. Respond to Webhook Node: Sends back the HTML table to the client in response to the webhook request. Needed Credentials Fastmail Masked E-Mail Addresses: An API token from Fastmail's API. Each HTTP call to Fastmail requires this credential for authentication. Note Ensure that you correctly configure authentication for the API calls and webhook security. Use your actual Fastmail API credentials with the correct scope. The workflow assumes that the Fastmail API is correctly configured and accessible from your n8n instance. Update URLs and credentials IDs according to your n8n configuration.",464,2024-10-02 18:03:32.846000+00:00,False,2
2598,Update Zammad Roles by Excel,"This n8n workflow allows you to update user roles in Zammad based on data from an Excel file. The workflow automates role assignments, ensuring efficient and consistent updates. Features Excel Integration**: Import user data from an Excel file containing emails and role assignments. Dynamic Updates**: Match Zammad users by email and update their roles. Error Handling**: Continue workflow execution even if some updates fail. Customizable Variables**: Configure Zammad API URL, API key, and Excel file URL. Usage Import the Workflow: Upload the provided .json file into your n8n instance. Set Variables: zammad_base_url: Your Zammad instance URL. excel_source_url: URL of the Excel file containing user data. Authentication for Zammad Create in the Node ""Find Zammad User by email"" and ""Update User Roles"" a Header Auth Authentication Name**: Authorization Value**: Bearer &lt;put here your zammad api token&gt; Run the Workflow: Execute the workflow to update user roles based on the Excel data. Issues and Suggestions For issues or suggestions, visit the GitHub Repository.",456,2024-12-01 12:18:42.597000+00:00,False,1
2251,Verify mailing address deliverability of contacts in Keap/Infusionsoft Using Lob,"Description This n8n workflow verifies the deliverability of mailing addresses stored in Keap/Infusionsoft by integrating with Lob‚Äôs address verification service. Who is this for? This template is designed for Keap/Infusionsoft users who need to ensure the accuracy of mailing addresses stored in their CRM systems. What problem is this workflow solving? / Use Case This workflow addresses the challenge of maintaining accurate mailing addresses in CRM databases by verifying the deliverability of addresses. What this workflow does A new contact is created in Keap/Infusionsoft Webhook sent to n8n Verify if the address is deliverable via LOB Report back to Keap/Infusionsoft Set Up Steps Watch this setup video: https://www.youtube.com/watch?v=T7Baopubc-0 Takes 10-30 minutes to set up Accounts Needed: Keap/Infusionsoft LOB Account (https://www.lob.com $0.00/mo 300 US addresses Verifications) n8n Before using this template, ensure you have API keys for your Keap/Infusionsoft app and Lob. Set up authentication for both services within n8n. How to customize this workflow to your needs You can customize this workflow by adjusting the trigger settings to match Keap/Infusionsoft‚Äôs workflow configuration. Additionally, you can modify the actions taken based on the deliverability outcome, such as updating custom fields or sending notifications.",451,2024-05-01 01:48:52.110000+00:00,False,2
2588,Assign Zammad Users to Organizations Based on Email Domain,"This n8n workflow automates the assignment of existing users to organizations in Zammad based on their email domains, utilizing Zammad‚Äôs ""domain-based assignment"" feature. Use Case Automate the post-hoc assignment of existing users to organizations in Zammad by leveraging their email domains. This ensures efficient management of user-organization relationships using Zammad‚Äôs ""domain-based assignment"" feature. Requirements An active Zammad account with API access. Existing users and organizations in Zammad with proper email and domain configurations. Zammad organizations must have domain-based assignment enabled. Credentials Set up your Zammad API credentials in n8n to allow the workflow to interact with your Zammad instance securely. If you have found an error or have any suggestions, please report them here on Github.",450,2024-11-29 10:07:15.757000+00:00,False,1
2471,create single new masked email address with fastmail,"Template Description This n8n workflow template allows you to create a masked email address using the Fastmail API, triggered by a webhook. This is especially useful for generating disposable email addresses for privacy-conscious users or for testing purposes. Workflow Details: Webhook Trigger: The workflow is initiated by sending a POST request to a specific webhook. You can include state and description in your request body to customize the masked email's state and description. Session Retrieval: The workflow makes an HTTP request to the Fastmail API to retrieve session information. It uses this data to authenticate further requests. Create Masked Email: Using the retrieved session data, the workflow sends a POST request to Fastmail's JMAP API to create a masked email. It uses the provided state and description from the webhook payload. Prepare Output: Once the masked email is successfully created, the workflow extracts the email address and attaches the description for further processing. Respond to Webhook: Finally, the workflow responds to the original POST request with the newly created masked email and its description. Requirements: Fastmail API Access**: You will need valid API credentials for Fastmail configured with HTTP Header Authentication. Authorization Setup**: Optionally set up authorization if your webhook is exposed to the internet to prevent misuse. Custom Webhook Request**: Use a tool like curl or create a shortcut on macOS/iOS to send the POST request to the webhook with the necessary JSON payload, like so: curl -X POST -H 'Content-Type: application/json' https://your-n8n-instance/webhook/87f9abd1-2c9b-4d1f-8c7f-2261f4698c3c -d '{""state"": ""pending"", ""description"": ""my mega fancy masked email""}' This template simplifies the process of integrating masked email functionality into your projects or workflows and can be extended for various use cases. Feel free to use the companion shortcut I've also created. Please update the authorization header in the shortcut if needed. https://www.icloud.com/shortcuts/ac249b50eab34c04acd9fb522f9f7068",426,2024-10-19 18:36:55.889000+00:00,False,1
5021,Automated GitHub Scanner for Exposed AWS IAM Keys,"Automated GitHub Scanner for Exposed AWS IAM Keys Overview This n8n workflow automatically scans GitHub for exposed AWS IAM access keys associated with your AWS account, helping security teams quickly identify and respond to potential security breaches. When compromised keys are found, the workflow generates detailed security reports and sends Slack notifications with actionable remediation steps. üîë Key Features Automated AWS IAM Key Scanning**: Regularly checks for exposed AWS access keys on GitHub Real-time Security Alerts**: Sends immediate Slack notifications when compromised keys are detected Comprehensive Security Reports**: Generates detailed reports with exposure information and risk assessment Actionable Remediation Steps**: Provides clear instructions for securing compromised credentials Continuous Monitoring**: Maintains ongoing surveillance of your AWS environment üìã Workflow Steps List AWS Users: Retrieves all users from your AWS account Split Users for Processing: Processes each user individually Get User Access Keys: Retrieves access keys for each user Filter Active Keys Only: Focuses only on currently active access keys Search GitHub for Exposed Keys: Scans GitHub repositories for exposed access keys Aggregate Search Results: Consolidates and deduplicates search findings Check For Compromised Keys: Determines if any keys have been exposed Generate Security Report: Creates detailed security reports for compromised keys Extract AWS Usernames: Extracts usernames from AWS response for notification Format Slack Alert: Prepares comprehensive Slack notifications Send Slack Notification: Delivers alerts with actionable information Continue Scanning: Maintains continuous monitoring cycle üõ†Ô∏è Setup Requirements Prerequisites Active n8n instance AWS account with IAM permissions GitHub account/token for searching repositories Slack workspace for notifications Required Credentials AWS Credentials: IAM user with permissions to list users and access keys Access Key ID and Secret Access Key GitHub Credentials: Personal Access Token with search permissions Slack Credentials: Webhook URL for your notification channel ‚öôÔ∏è Configuration AWS Configuration: Configure the ""List AWS Users"" node with your AWS credentials Ensure proper IAM permissions for listing users and access keys GitHub Configuration: Set up the ""Search GitHub for Exposed Keys"" node with your GitHub token Adjust search parameters if needed Slack Configuration: Configure the Slack node with your webhook URL Customize notification format if desired üöÄ Usage Running the Workflow Manual Execution: Click ""Execute Workflow"" to run an immediate scan Scheduled Execution: Set up a schedule to run periodic scans (recommended daily or weekly) Repository Compatibility This workflow is compatible with both public and private GitHub repositories to which you have access. It will scan all repositories you have permission to view based on your GitHub credentials. Handling Alerts When a compromised key is detected: Review the Slack notification for details about the exposure Follow the recommended remediation steps: Deactivate the compromised key immediately Create a new key if needed Investigate the exposure source Update any services using the compromised key ‚ö†Ô∏è Disclaimer This workflow template is provided for reference purposes only to demonstrate how to automate AWS IAM key exposure scanning. Please note: The scanning process may produce false positives as it only matches potential AWS access key patterns Always verify any reported exposures manually before taking action Disabling or deleting access keys without proper verification could have significant negative impacts on your environment Understand which systems and applications rely on identified access keys before deactivating them This template should be customized to fit your specific environment and security policies IMPORTANT: Use this workflow with caution and only after thoroughly understanding your AWS environment. The authors of this template are not responsible for any disruptions or damages resulting from its use. üîí Security Considerations This workflow requires access to sensitive AWS credentials Store all credentials securely within n8n Review and rotate access keys regularly üìù Customization Options Adjust GitHub search parameters for more targeted scanning Customize Slack notification format and content Modify security report generation for your specific needs Integrate with additional notification channels (email, MS Teams, etc.) Optional: Enabling Interactive Slack Buttons The Slack Block Kit notification format supports interactive buttons that can be implemented if you want to perform actions directly from Slack: Disable Key: This button can be configured to automatically disable the compromised AWS IAM access key View Details: This button can be set up to show additional information about the exposure Acknowledge: This button can be used to mark the alert as acknowledged To make these buttons functional: Set up a Slack Socket Mode App: Create a Slack app in the Slack API Console Enable Socket Mode and Interactive Components Subscribe to the block_actions event to capture button clicks Create an n8n Webhook Endpoint: Add a new webhook node to receive Slack button click events Create separate workflows for each button action Implement AWS Key Disabling: For the ""Disable Key"" button, create a workflow that uses the n8n HTTP Request node to call the AWS IAM UpdateAccessKey API Example HTTP request that can be implemented in n8n: Method: POST URL: https://iam.amazonaws.com/ Query Parameters: Action: UpdateAccessKey AccessKeyId: AKIAIOSFODNN7EXAMPLE Status: Inactive UserName: {{$json.username}} Version: 2010-05-08 Update the Slack Message Format: Modify the Format Slack Alert node to include your webhook URL in the button action values Add callback_id and action_id values to identify which button was clicked This implementation allows for immediate response to security incidents directly from the Slack interface, reducing response time and improving security posture.",419,2025-06-18 13:22:16.072000+00:00,False,3
4935,Amazon Affiliate Links to Mastodon with AI Descriptions & Shlink Tracking,"This n8n workflow automates posting Amazon affiliate products to Mastodon ‚Äî complete with image upload, description, and a shortened tracking URL using Shlink. üîß How it works Input Source: The workflow starts by reading from a connected Google Sheet that contains: SHlink (Shortlink) Amazon Link Description (Optional) PicURL Send /NO or YES A Send column (used as a flag to check if the row was already posted) Image Upload: It fetches the product image via HTTP and uploads it directly to a Mastodon instance via the /media API endpoint. URL Shortening (Shlink): The original Amazon URL is shortened using your self-hosted or cloud-hosted Shlink instance to enable click tracking and better presentation. Text Generation: A two-line promotional text is automatically generated using a Language Model (LLM), based on the product description. Posting to Mastodon: The post is then published on Mastodon with: The image The generated text The shortened Shlink URL Row Update: Once published, the Send column in the Google Sheet is updated to ""YES"" to prevent duplicates. Requirements ‚úÖ Shlink ‚Äì Required for shortening and tracking Amazon URLs ‚úÖ Google Sheet ‚Äì Used as a product queue and post ‚úÖ Google Sheet Example https://link.unixweb.home64.de/w7VqY ‚úÖ Mastodon account ‚Äì OAuth2 credentials with write scope ‚úÖ Product image URL ‚Äì Must be valid and accessible ‚úÖ n8n credentials ‚Äì Set up for Google Sheets, Mastodon, and optionally OpenRouter or other LLM providers This workflow is ideal for content creators, affiliate marketers, and automation fans who want to save time and optimize reach across the Fediverse. #affiliate #amazon #mastodon #advertisment",416,2025-06-14 17:41:34.157000+00:00,True,4
2687,TW-Request-Agri Data Open Platform-Daily Market Sheep Pricing,"This workflow automates the process of fetching agricultural transaction data from the Taiwan Agricultural Products Open Data Platform and storing it in a Google Sheets document for further analysis. Key Features Manual Trigger: Allows manual execution of the workflow to control when data is fetched. HTTP Request: Sends a request to the Open Data Platform's API to retrieve detailed transaction data, including: Pricing (Upper, Middle, Lower, Average) Transaction quantities Crop and market details Split Out Node: Processes each record individually, ensuring accurate handling of every data entry. Google Sheets Integration: Appends the data into a structured Google Sheets document for easy access and analysis. Node Configurations 1. Manual Trigger Purpose**: Start the workflow manually. Configuration**: No setup needed. 2. HTTP Request Purpose**: Fetch agricultural data. Configuration**: URL: https://data.moa.gov.tw/api/v1/SheepQuotation Query Parameters: Start_time: 2024/12/01 End_time: 2024/12/31 MarketName: Âè∞Âåó‰∫å api_key: &lt;your_api_key&gt; Headers: accept: application/json 3. Split Out Purpose**: Split the API response data array into individual items. Configuration**: Field to Split Out: Data 4. Google Sheets Purpose**: Append the data to Google Sheets. Configuration**: Operation: Append Document ID: &lt;your_document_id&gt; Sheet Name: Sheet1 Mapped Fields: TransDate, TcType, CropCode, CropName, MarketCode, MarketName Upper_Price, Middle_Price, Lower_Price, Avg_Price, Trans_Quantity Ê≠§ Workflow Âæû Âè∞ÁÅ£Ëæ≤Ê•≠Áî¢ÂìÅÈñãÊîæË≥áÊñôÂπ≥Ëá∫ Áç≤ÂèñËæ≤Áî¢ÂìÅ‰∫§ÊòìÊï∏ÊìöÔºå‰∏¶Â∞áÂÖ∂ÂÑ≤Â≠òÂà∞ Google Sheets Êñá‰ª∂ ‰∏≠ÈÄ≤Ë°åÈÄ≤‰∏ÄÊ≠•ÂàÜÊûê„ÄÇ ‰∏ªË¶ÅÂäüËÉΩ Manual TriggerÔºöÂÖÅË®±ÊâãÂãïÂü∑Ë°åÂ∑•‰ΩúÊµÅÁ®ãÔºå‰ª•ÊéßÂà∂Êï∏ÊìöÁç≤ÂèñÁöÑÊôÇÈñì„ÄÇ HTTP RequestÔºöÂêëÈñãÊîæË≥áÊñôÂπ≥Ëá∫ÁöÑ API ÁôºÈÄÅË´ãÊ±ÇÔºåÁç≤ÂèñË©≥Á¥∞ÁöÑ‰∫§ÊòìÊï∏ÊìöÔºåÂåÖÊã¨Ôºö ÂÉπÊ†º (Upper, Middle, Lower, Average) ‰∫§ÊòìÊï∏Èáè ‰ΩúÁâ©ÂíåÂ∏ÇÂ†¥Ë©≥Á¥∞Ë≥áÊñô Split Out NodeÔºöÈÄêÁ≠ÜËôïÁêÜÊØè‰∏ÄÁ≠ÜË®òÈåÑÔºåÁ¢∫‰øùÊï∏ÊìöÊ∫ñÁ¢∫ÁÑ°Ë™§„ÄÇ Google Sheets IntegrationÔºöÂ∞áÊï∏ÊìöËøΩÂä†Âà∞ÁµêÊßãÂåñÁöÑ Google Sheets Êñá‰ª∂‰∏≠ÔºåÊñπ‰æøÂ≠òÂèñÂíåÂàÜÊûê„ÄÇ ÁØÄÈªûË®≠ÂÆö 1. Manual Trigger Áî®ÈÄî**ÔºöÊâãÂãïÂïüÂãïÂ∑•‰ΩúÊµÅÁ®ã„ÄÇ Ë®≠ÂÆö**ÔºöÁÑ°ÈúÄÈ°çÂ§ñË®≠ÂÆö„ÄÇ 2. HTTP Request Áî®ÈÄî**ÔºöÊäìÂèñËæ≤Áî¢ÂìÅÊï∏Êìö„ÄÇ Ë®≠ÂÆö**Ôºö URL: https://data.moa.gov.tw/api/v1/SheepQuotation Êü•Ë©¢ÂèÉÊï∏ (Query Parameters)Ôºö Start_time: 2024/12/01 End_time: 2024/12/31 MarketName: Âè∞Âåó‰∫å api_key: &lt;your_api_key&gt; Ê®ôÈ†≠ (Headers)Ôºö accept: application/json 3. Split Out Áî®ÈÄî**ÔºöÂ∞á API ÂõûÊáâÁöÑÊï∏ÊìöÈô£ÂàóÂàÜËß£ÁÇ∫ÂÄãÂà•È†ÖÁõÆ„ÄÇ Ë®≠ÂÆö**Ôºö Field to Split Out: Data 4. Google Sheets Áî®ÈÄî**ÔºöÂ∞áÊï∏ÊìöËøΩÂä†Ëá≥ Google Sheets„ÄÇ Ë®≠ÂÆö**Ôºö OperationÔºöAppend Document IDÔºö&lt;your_document_id&gt; Sheet NameÔºöSheet1 Êò†Â∞ÑÊ¨Ñ‰Ωç (Mapped Fields)Ôºö TransDate, TcType, CropCode, CropName, MarketCode, MarketName Upper_Price, Middle_Price, Lower_Price, Avg_Price, Trans_Quantity Ë´ãÂ§öÂà©Áî® Curl Import ÂäüËÉΩ ‰æãÂ¶Ç curl -X GET ""https://data.moa.gov.tw/api/v1/AgriProductsTransType/?Start_time=114.01.01&End_time=114.01.01&MarketName=%E5%8F%B0%E5%8C%97%E4%BA%8C"" -H ""accept: application/json"" Ëæ≤Ê•≠Ë≥áÊñôÈñãÊîæÂπ≥Âè∞ Êñá‰ª∂",411,2025-01-01 19:24:45.689000+00:00,False,2
5103,AI-Powered Vendor Policy & RSS Feed Analysis with Integrated Risk Scoring,"üß† Overview A dual-engine, AI-driven n8n workflow that automates the monitoring of both vendor policy webpages and compliance-related RSS feeds. It intelligently detects recent updates, evaluates their potential risk, and delivers a structured HTML digest categorized by severity ‚Äî right to your inbox. ‚öôÔ∏è How It Works 1Ô∏è‚É£ Scheduled Execution ‚è∞ Runs daily at 3 AM to ensure timely and consistent monitoring. 2Ô∏è‚É£ Dual Data Streams üì∞ RSS Feed Monitoring: Ingests articles from selected feeds focused on security, privacy, and compliance. üåê Vendor Webpage Monitoring: Fetches specified policy URLs and checks for updates using Last-Modified headers and in-page content analysis. 3Ô∏è‚É£ Content Filtering üßπ Applies logic to filter out stale or irrelevant data, focusing only on updates within the past 24 hours. 4Ô∏è‚É£ AI-Powered Risk Categorization ü§ñ Two dedicated AI agents (one per stream) analyze content to: üìù Generate a 2-line compliance/risk-focused summary üö¶ Assign a risk score: High, Medium, Low, or Informational 5Ô∏è‚É£ Digest Generation üñºÔ∏è Summaries are grouped by risk level and formatted into a visually polished HTML report, using category-specific styles and color coding. 6Ô∏è‚É£ Email Delivery üìß Automatically sends out two separate email reports via Gmail: üóÇÔ∏è One for vendor webpage updates üì∞ One for RSS feed-based vendor news ‚ú® Key Features üîÅ Automates daily monitoring across feeds and policy pages ü§ñ AI-driven summarization and structured risk scoring üßæ Clean, HTML-formatted digests grouped by risk category üß© Fully customizable: feed sources, vendor URLs, AI prompts, and email styling üõ°Ô∏è Designed for compliance, security, and risk teams needing proactive intelligence",411,2025-06-21 13:38:35.772000+00:00,True,5
5160,ü§ñ Build Resilient AI Workflows with Automatic GPT and Gemini Failover Chain,"This workflow contains community nodes that are only compatible with the self-hosted version of n8n. How it works This workflow demonstrates how to create a resilient AI Agent that automatically falls back to a different language model if the primary one fails. This is useful for handling API errors, rate limits, or model outages without interrupting your process. State Initialization: The Agent Variables node initializes a fail_count to 0. This counter tracks how many models have been attempted. Dynamic Model Selection: The Fallback Models (a LangChain Code node) acts as a router. It receives a list of all connected AI models and, based on the current fail_count, selects which one to use for this attempt (0 for the first model, 1 for the second, etc.). Agent Execution: The AI Agent node attempts to run your prompt using the model selected by the router. The Fallback Loop: On Success: The workflow completes successfully. On Error: If the AI Agent node fails, its ""On Error"" output is triggered. This path loops back to the Agent Variables node, which increments the fail_count by 1. The process then repeats, causing the Fallback Models router to select the next model in the list. Final Failure: If all connected models are tried and fail, the workflow will stop with an error. Set up steps Setup time: ~3-5 minutes Configure Credentials: Ensure you have the necessary credentials (e.g., for OpenAI, Google AI) configured in your n8n instance. Define Your Model Chain: Add the AI model nodes you want to use to the canvas (e.g., OpenAI, Google Gemini, Anthropic). Connect them to the Fallback Models node. Important: The order in which you connect the models determines the fallback order. The model nodes first created/connected will be tried first. Set Your Prompt: Open the AI Agent node and enter the prompt you want to execute. Test: Run the workflow. To test the fallback logic, you can temporarily disable the First Model node or configure it with invalid credentials to force an error.",411,2025-06-23 16:33:43.301000+00:00,True,4
7448,Auto-Generate LinkedIn Content with Gemini AI: Posts & Images 24/7,"üîÑ How It Works - LinkedIn Post with Image Automation Overview This n8n automation creates and publishes LinkedIn posts with AI-generated images automatically. It's a complete end-to-end solution that transforms simple post titles into engaging social media content. Step-by-Step Process 1. Content Trigger & Management Google Sheets Trigger** monitors a spreadsheet for new post titles Only processes posts with ""pending"" status Limits to one post at a time for controlled execution 2. AI Content Generation AI Agent** uses Google Gemini to create engaging LinkedIn posts Takes the post title and generates: Compelling opening hooks 3-4 informative paragraphs Engagement questions Relevant hashtags (4-6) Appropriate emojis Output is structured and formatted for LinkedIn 3. AI Image Creation Google Gemini Image Generation** creates custom visuals Uses the AI-generated post content as context Generates professional images featuring: Modern workspace with coding elements Flutter development themes Professional, LinkedIn-appropriate aesthetics 16:9 aspect ratio, high resolution No text or captions** in the generated image 4. Image Processing & Storage Generated images are uploaded to Google Drive Files are shared with public access permissions Image URLs are stored back in the spreadsheet for tracking 5. LinkedIn Publishing LinkedIn API integration** handles the posting process: Registers image uploads Uploads images to LinkedIn's servers Creates posts with text + image Publishes to your LinkedIn profile Updates spreadsheet status to ""posted"" Technical Architecture Google Sheets ‚Üí AI Content ‚Üí AI Image ‚Üí Google Drive ‚Üí LinkedIn API ‚Üí Status Update ‚Üì ‚Üì ‚Üì ‚Üì ‚Üì ‚Üì Trigger Gemini LLM Gemini File Upload Posting Tracking Content Gen Image Gen Key Features ‚úÖ Fully Automated - Runs continuously without manual intervention ‚úÖ AI-Powered - Both content and images generated by AI ‚úÖ Professional Quality - LinkedIn-optimized formatting and visuals ‚úÖ Real-time Tracking - Monitor status and performance ‚úÖ Scalable - Handle multiple posts and campaigns How to Use Setup Requirements Google Gemini API for content and image generation LinkedIn API credentials for posting Google Sheets for content management Google Drive for image storage n8n instance for workflow execution Content Management Add new post titles to your Google Sheet Set status to ""pending"" Automation automatically processes and publishes Status updates to ""posted"" upon completion Customization Options Modify AI prompts for different content styles Adjust image generation parameters Change posting frequency and timing Add multiple LinkedIn accounts Integrate with other content sources Use Cases ÔøΩÔøΩ Perfect for: Startups** wanting consistent LinkedIn presence Marketing teams** overwhelmed with content creation HR departments** building employer branding Agencies** managing multiple client accounts Solo entrepreneurs** needing professional social media presence Benefits ‚è∞ Time Savings: 20+ hours per week for content teams üìà Consistency: Daily, professional posts without gaps üé® Quality: AI-optimized content and visuals üìä Scalability: Handle unlimited content volume üí∞ Cost Effective: Reduce manual content creation costs üîÑ The automation runs continuously, ensuring your LinkedIn presence stays active and engaging 24/7! For inquiries: mfarooqiqbal143@gmail.com",404,2025-08-15 17:15:56.308000+00:00,True,8
10578,Automate Hotel Booking Requests from Gmail to Google Sheets with GPT-5-mini,"Who‚Äôs it for This workflow is for hotel managers, travel agencies, and hospitality teams who receive booking requests via email. It eliminates the need for manual data entry by automatically parsing emails and attachments, assigning booking cases to the right teams, and tracking performance metrics. What it does This workflow goes beyond simple automation by including enterprise-grade logic and security: üõ°Ô∏è Gatekeeper:** Watches your Gmail and filters irrelevant emails before spending money on AI tokens. üß† AI Brain:** Uses OpenAI (GPT-5-mini) to extract structured data from unstructured email bodies and PDF attachments. ‚öñÔ∏è Business Logic:** Automatically routes tasks to different teams based on urgency, room count, and VIP status. üîí Security:** Catches PII (like credit card numbers) and scrubs them before they hit your database. üö® Safety Net:** If anything breaks, a dedicated error handling path logs the issue immediately so no booking is lost. üìà ROI Tracking:** Calculates the time saved per booking to prove the value of automation. How to set up Create your Google Sheet: Create a new sheet and rename the tabs to: Cases, Team Assignments, Error Logs, Success Metrics. Add Credentials: Go to n8n Settings ‚Üí Credentials and add your Gmail (OAuth2), Google Sheets, and OpenAI API keys. Configure User Settings: Open the ""Configuration: User Settings"" node at the start of the workflow. Paste your specific Google Sheet ID and Admin Email there. Adjust Business Rules: Open the ""Apply Business Rules"" node (Code node) to adjust the logic for team assignment (e.g., defining what counts as a ""VIP"" booking). Customize Templates: Modify the email templates in the Gmail nodes to match your hotel's branding. Test: Send a sample booking email to yourself to verify the filters and data extraction. Setup requirements Gmail account (OAuth2 connected) Google Sheets (with the 4 tabs listed below) OpenAI API key (GPT-5-mini recommended) n8n Cloud or self-hosted instance How to customize Filter Booking Emails:** Update the trigger node keywords to match your specific email subjects (e.g., ""Reservation"", ""Booking Request""). Apply Business Rules:** Edit the Javascript in the Code node to fit your company‚Äôs internal logic (e.g., changing priority thresholds). New Metrics:** Add new columns in the Google Sheet (e.g., ‚ÄúRevenue Metrics‚Äù) and map them in the ""Update Sheet"" node. AI Model:** Switch to GPT-5 if you need higher reasoning capabilities for complex PDF layouts. Google Sheets Structure Description This workflow uses a Google Sheets document with four main tabs to track and manage hotel booking requests. 1. Cases This is the main data log for all incoming booking requests. case_id:** Unique identifier generated by the workflow. processed_date:** Timestamp when the workflow processed the booking. travel_agency / contact_details:** Extracted from the email. number_of_rooms / check_in_date:** Booking details parsed by the AI. special_requests:** Optional notes (e.g., airport transfer). assigned_team / priority:** Automatically set based on business rules. days_until_checkin:** Dynamic field showing urgency. 2. Team Assignments Stores internal routing and assignment details. timestamp:** When the case was routed. case_id:** Link to the corresponding record in the Cases tab. assigned_team / team_email:** Which department handles this request. priority:** Auto-set based on room count or urgency. 3. Error Log A critical audit trail that captures details about any failed processing steps. error_type:** Categorization of the failure (e.g., MISSING_REQUIRED_FIELDS). error_message:** Detailed technical explanation for debugging. original_sender / snippet:** Context to help you manually process the request if needed. 4. Success Metrics Tracks the results of your automation to prove its value. processing_time_seconds:** The time savings achieved by the automation (run time vs. human time). record_updated:** Confirmation that the database was updated. üôã Support If you encounter any issues during setup or have questions about customization, please reach out to our dedicated support email: foivosautomationhelp@gmail.com",398,2025-11-06 23:44:26.252000+00:00,True,7
5309,Vacation Planning Agent,"Vacation Planning Agent - n8n Workflow Overview This n8n workflow creates an intelligent vacation planning chatbot that helps users find and book the perfect hotel accommodations. The agent acts as a professional travel consultant, systematically gathering travel requirements and providing personalized hotel recommendations through an interactive chat interface. Core Functionality The workflow provides a conversational AI agent that: Conducts structured information gathering** through natural conversation Automatically searches for hotels** using real-time data from Google Hotels Provides personalized recommendations** with detailed hotel information Maintains conversation context** throughout the planning process Delivers professional travel consultation** in a friendly, accessible format User Experience Flow Initial Interaction Users are greeted with a warm welcome message in German: ""Hallo! Ich helfe dir, deinen perfekten Urlaub zu planen. Bitte beanworte mir die folgenden Fragen :)"" Information Collection Process The agent systematically collects essential travel details: Destination - City and country/state Travel Dates - Check-in and check-out dates Guest Count - Number of travelers Room Requirements - Number of rooms needed Budget Preferences - Optional price range Automated Hotel Search Once core information is gathered, the agent automatically searches for available accommodations without requiring user permission. Recommendation Delivery Results are presented in a structured format including: Hotel names and star ratings Pricing information Location details Guest ratings and reviews Key amenities and highlights Direct booking links Required Integrations OpenAI API Purpose**: Powers the conversational AI agent Model**: GPT-4o-mini for cost-effective, intelligent responses Requirement**: Valid OpenAI API credentials SerpAPI (Google Hotels) Purpose**: Real-time hotel search and pricing data Service**: Google Hotels search engine integration Requirement**: Active SerpAPI account and API key Key Features Intelligent Conversation Management Maintains conversation context with 20-message memory buffer Handles edge cases like no available hotels or unclear responses Provides alternative suggestions when initial searches yield limited results Flexible Search Parameters Supports location-based searches worldwide Accommodates date range specifications Handles guest count and room quantity requirements Optional budget filtering (min/max price ranges) Currency customization support Professional Presentation Structured hotel recommendation format Clear pricing and availability information Contextual explanations for recommendations Additional destination insights when relevant Use Cases This workflow is ideal for: Travel websites** seeking to add AI-powered hotel booking assistance Travel agencies** wanting to automate initial consultation processes Hospitality businesses** providing customer service automation Personal travel planning** applications Customer support** for travel-related inquiries User Benefits Time-saving**: Eliminates manual hotel research Personalized results**: Tailored recommendations based on specific needs Real-time data**: Current pricing and availability information Professional guidance**: Expert-level travel consultation Seamless experience**: Natural conversation flow without complex forms Technical Requirements Essential Services n8n workflow automation platform OpenAI API access (GPT-4o-mini model) SerpAPI account with Google Hotels access Configuration Needs API credential setup for both OpenAI and SerpAPI Webhook endpoint configuration for chat trigger Memory buffer configuration for conversation context Optional Enhancements Custom branding for chat interface Additional language support beyond German greeting Integration with booking platforms for direct reservations Analytics tracking for usage insights",378,2025-06-25 17:52:46.960000+00:00,True,3
6840,Extract Contacts from Business Cards to Google Sheets With GPT4o,"üìÑ Auto Extract Contacts from Business Cards to Sheet With GPT4o &gt; This smart workflow extracts names, phone numbers, emails, and more from uploaded name card photos using AI, then logs them neatly into your Google Sheet. No typing. No mess. Just upload and go. üë§ Who‚Äôs it for Sales & Business Development Teams Recruiters & Talent Acquisition Specialists Event Teams collecting business cards Admins who manage contact databases manually ‚öôÔ∏è How it works / What it does This workflow automates the extraction of contact details from uploaded name card (business card) images and stores them in a structured Google Sheet for easy tracking and follow-up. Workflow Steps: User uploads one or more name card images through a web form. The uploaded files are saved to a Google Drive folder for archiving. A smart AI agent (with OCR and GPT capabilities) scans each image and extracts relevant contact data into structured JSON format. Data is transformed, cleaned (e.g., removing + from phone numbers), and filtered. Valid contacts are appended to a Google Sheet for central tracking and future use. üõ† How to set up Create a Form Allow file upload (JPG/PNG format). Label it as ‚ÄúName Card Uploader‚Äù with a clear description. Upload to Google Drive Use the Google Drive node to store uploaded images. Configure Smart Agent Use GPT-4o or similar model with OCR capability. Apply a structured output parser to extract contact fields like name, phone, email, company, etc. Transform Data Use the Code node to clean and structure contact info. Strip out unwanted characters from phone numbers (e.g., +). Filter Invalid Records Remove entries with no meaningful contact data. Append to Google Sheets Use the Google Sheets node with ""Append Sheet Row"". Map fields to columns like Name, Phone, Email, etc. ‚úÖ Requirements n8n workflow environment Google Drive integration (for file storage) Google Sheets integration (for storing contacts) GPT-4o or any image-capable LLM Clear name card images (PNG/JPG, readable text) (Optional) Slack/email integration for notifications üß© How to customize the workflow CRM Sync**: Connect to platforms like HubSpot, Salesforce, or Zoho. Validation Logic**: Ensure records contain key fields like name or email before writing. Uploader Info**: Attach submitter metadata to each contact record. Language Adaptation**: Adjust extracted field labels/output to target your preferred language. Batch Upload**: Handle multiple cards in a single image or multiple uploads in one go.",362,2025-08-02 06:44:32.114000+00:00,True,6
10581,Automate Bank Statement and Invoice Reconciliation with GPT and Google Sheets,"üü¢ Manual Trigger Workflow starts manually to initiate the reconciliation process on demand. üìÑ Fetch Invoices & Bank Statements Retrieves invoice data and bank statement data from Google Sheets for comparison. üîÄ Merge Data Combines both datasets into a single structured dataset for processing. üß© Format Payload for AI Function node prepares and structures the merged data into a clean JSON payload for AI analysis. ü§ñ AI Reconciliation AI Agent analyzes the invoice and bank statement data to identify matches, discrepancies, and reconciled entries. üßÆ Parse AI Output Parses the AI response into a structured format suitable for adding back to Google Sheets. üìä Update Sheets Adds the reconciled data and reconciliation results into the target Google Sheet for recordkeeping. üßæ Prerequisites ‚úÖ OpenAI API Credentials Required for the AI Reconciliation node to process and match transactions. Add your OpenAI API key in n8n ‚Üí Credentials ‚Üí OpenAI. ‚úÖ Google Sheets Credentials Needed to read invoice and bank statement data and to write reconciled results. Add credentials in n8n ‚Üí Credentials ‚Üí Google Sheets. ‚úÖ Google Sheets Setup The connected spreadsheet must contain the following tabs: Invoices ‚Äì for invoice data Bank_Statement ‚Äì for bank transaction data Reconciled_Data ‚Äì for storing the AI-processed reconciliation output ‚úÖ Tab Structure & Required Headers Invoices Sheet Columns: Invoice_ID Invoice_Date Customer_Name Amount Status Bank_Statement Sheet Columns: Transaction_ID Transaction_Date Description Debit/Credit Amount Reconciled_Data Sheet Columns: Invoice_ID Transaction_ID Matched_Status Remarks Confidence_Score ‚öôÔ∏è n8n Environment Setup Ensure all nodes are connected correctly and the workflow has permission to access the required sheets. Test each fetch and write operation before running the full workflow.",361,2025-11-07 05:27:00.287000+00:00,True,4
5226,Automated Weather Reports with Bright Data & n8n,"Description This workflow automatically collects weather data from multiple sources and compiles it into comprehensive reports. It helps you make informed decisions based on accurate weather forecasts without manually checking multiple weather services. Overview This workflow automatically scrapes weather data from multiple sources and compiles it into a comprehensive report. It uses Bright Data to access weather websites and can be configured to send you regular weather updates for your locations of interest. Tools Used n8n:** The automation platform that orchestrates the workflow. Bright Data:** For scraping weather websites and forecast data without getting blocked. Notification Services:** Email, messaging apps, or other platforms. How to Install Import the Workflow: Download the .json file and import it into your n8n instance. Configure Bright Data: Add your Bright Data credentials to the Bright Data node. Set Up Notifications: Configure how you want to receive weather reports. Customize: Add your locations of interest and reporting frequency. Use Cases Event Planners:** Get weather forecasts for upcoming outdoor events. Farmers:** Monitor weather conditions for agricultural planning. Travelers:** Check weather forecasts for destinations before trips. Connect with Me Website:** https://www.nofluff.online YouTube:** https://www.youtube.com/@YaronBeen/videos LinkedIn:** https://www.linkedin.com/in/yaronbeen/ Get Bright Data:** https://get.brightdata.com/1tndi4600b25 (Using this link supports my free workflows with a small commission) #n8n #automation #weather #weatherforecasts #brightdata #webscraping #weatherreports #weatheralerts #weatherdata #weathermonitoring #n8nworkflow #workflow #nocode #weatherautomation #weatherscraping #weathertracking #weathernotifications #weatherupdates #forecastdata #weatherplanning #weatherservice #outdoorevents #weatherapi #weatherinformation #climatedata #weathertech",356,2025-06-24 15:57:38.109000+00:00,False,3
4828,Generate SEO Arabic Articles and Save to Notion,"This workflow automatically creates in-depth, SEO-friendly Arabic articles based on any keyword you provide. It researches the topic, generates a full article outline, writes every section in Arabic, and saves the final article directly to your Notion workspace‚Äîall in a few clicks. How It Works Step 1:** You submit a simple web form with your keyword and (optionally) an article title. Step 2:** The workflow researches the topic using advanced AI, gathers trending questions from Google, and creates a detailed, structured outline. Step 3:** Each section of the article is written in Arabic by AI, following best SEO practices and including real FAQs. Step 4:** The completed article is automatically formatted and saved to your Notion database, ready for review or publishing. Setup Instructions What you need:** An OpenAI API key (for AI-powered writing and outline generation) An OpenRouter API key (for research via Perplexity/Sonar AI) A Notion account and Notion API integration (for saving articles) DataForSEO account (for fetching Google ""People Also Ask"" questions) How to set up:** Import the workflow into your n8n instance. Connect your API credentials for OpenAI, OpenRouter, Notion, and (optionally) DataForSEO. Update your Notion database ID in the workflow settings. Deploy the workflow. Fill out the web form to generate your first article. Setup time:** 10‚Äì20 minutes if you already have your accounts. Tip: You can fully customize the outline and writing prompts for your target audience or topic. The workflow is modular‚Äîeasy to adapt for different languages or content styles.",355,2025-06-09 22:27:04.645000+00:00,True,4
5136,Facebook/Meta Conversion API for eCommerce Leads/Orders,"This n8n workflow helps eCommerce businesses (especially in the Cash on Delivery space) send real-time order events to the Meta (Facebook) Conversions API, ensuring accurate event tracking and better ad attribution. Features Webhook Listener**: Accepts incoming order data (name, phone, IP, user-agent, etc.) via HTTP POST/GET. Data Normalization**: Cleans and formats first_name, last_name, phone, and event_time according to Facebook's strict specs. Data Hashing**: Securely hashes sensitive user data (SHA256), as required by Meta. Full Custom Data Suppor**t: Pass order value, currency, and more. Ideal For: Shopify, WooCommerce, custom stores (Laravel, Node, etc.) Businesses using Meta Ads and needing high-quality server-side tracking Teams without access to full dev resources, but using n8n for automation How It Works: Receive Order from your store via Webhook or API. Format & Normalize fields to match Facebook‚Äôs expected structure. Encrypt Sensitive Fields using SHA256 (name, phone, email). Send to Facebook via the Conversions API endpoint. Requirements: A Meta Business Manager account with Conversions API access Your Access Token and Pixel ID set up in n8n credentials",354,2025-06-23 00:59:02.029000+00:00,False,2
10097,AI Website Scraper & Company Intelligence,"AI Website Scraper & Company Intelligence Description This workflow automates the process of transforming any website URL into a structured, intelligent company profile. It's triggered by a form, allowing a user to submit a website and choose between a ""basic"" or ""deep"" scrape. The workflow extracts key information (mission, services, contacts, SEO keywords), stores it in a structured Supabase database, and archives a full JSON backup to Google Drive. It also features a secondary AI agent that automatically finds and saves competitors for each company, building a rich, interconnected database of company intelligence. Quick Implementation Steps Import the Workflow: Import the provided JSON file into your n8n instance. Install Custom Community Node: You must install the community node from: https://www.npmjs.com/package/n8n-nodes-crawl-and-scrape FIRECRAWL N8N Documentation https://docs.firecrawl.dev/developer-guides/workflow-automation/n8n Install Additional Nodes: n8n-nodes-crawl-and-scrape and n8n-nodes-mcp fire crawl mcp . Set up Credentials: Create credentials in n8n for FIRE CRAWL API,Supabase, Mistral AI, and Google Drive. Configure API Key (CRITICAL): Open the Web Search tool node. Go to Parameters ‚Üí Headers and replace the hardcoded Tavily AI API key with your own. Configure Supabase Nodes: Assign your Supabase credential to all Supabase nodes. Ensure table names (e.g., companies, competitors) match your schema. Configure Google Drive Nodes: Assign your Google Drive credential to the Google Drive2 and save to Google Drive1 nodes. Select the correct Folder ID. Activate Workflow: Turn on the workflow and open the Webhook URL in the ‚ÄúOn form submission‚Äù node to access the form. What It Does Form Trigger Captures user input: ‚ÄúWebsite URL‚Äù and ‚ÄúScraping Type‚Äù (basic or deep). Scraping Router A Switch node routes the flow: Deep Scraping ‚Üí** AI-based MCP Firecrawler agent. Basic Scraping ‚Üí** Crawlee node. Deep Scraping (Firecrawl AI Agent) Uses Firecrawl and Tavily Web Search. Extracts a detailed JSON profile: mission, services, contacts, SEO keywords, etc. Basic Scraping (Crawlee) Uses Crawl and Scrape node to collect raw text. A Mistral-based AI extractor structures the data into JSON. Data Storage Stores structured data in Supabase tables (companies, company_basicprofiles). Archives a full JSON backup to Google Drive. Automated Competitor Analysis Runs after a deep scrape. Uses Tavily web search to find competitors (e.g., from Crunchbase). Saves competitor data to Supabase, linked by company_id. Who's It For Sales & Marketing Teams:** Enrich leads with deep company info. Market Researchers:** Build structured, searchable company databases. B2B Data Providers:** Automate company intelligence collection. Developers:** Use as a base for RAG or enrichment pipelines. Requirements n8n instance** (self-hosted or cloud) Supabase Account:** With tables like companies, competitors, social_links, etc. Mistral AI API Key** Google Drive Credentials** Tavily AI API Key** (Optional) Custom Nodes: n8n-nodes-crawl-and-scrape How It Works Flow Summary Form Trigger: Captures ‚ÄúWebsite URL‚Äù and ‚ÄúScraping Type‚Äù. Switch Node: deep ‚Üí MCP Firecrawler (AI Agent). basic ‚Üí Crawl and Scrape node. Scraping & Extraction: Deep path: Firecrawler ‚Üí JSON structure. Basic path: Crawlee ‚Üí Mistral extractor ‚Üí JSON. Storage: Save JSON to Supabase. Archive in Google Drive. Competitor Analysis (Deep Only): Finds competitors via Tavily. Saves to Supabase competitors table. End: Finishes with a No Operation node. How To Set Up Import workflow JSON. Install community nodes (especially n8n-nodes-crawl-and-scrape from npm). Configure credentials (Supabase, Mistral AI, Google Drive). Add your Tavily API key. Connect Supabase and Drive nodes properly. Fix disconnected ‚Äúbasic‚Äù path if needed. Activate workflow. Test via the webhook form URL. How To Customize Change LLMs:** Swap Mistral for OpenAI or Claude. Edit Scraper Prompts:** Modify system prompts in AI agent nodes. Change Extraction Schema:** Update JSON Schema in extractor nodes. Fix Relational Tables:** Add Items node before Supabase inserts for arrays (social links, keywords). Enhance Automation:** Add email/slack notifications, or replace form trigger with a Google Sheets trigger. Add-ons Automated Trigger:** Run on new sheet rows. Notifications:** Email or Slack alerts after completion. RAG Integration:** Use the Supabase database as a chatbot knowledge source. Use Case Examples Sales Lead Enrichment:** Instantly get company + competitor data from a URL. Market Research:** Collect and compare companies in a niche. B2B Database Creation:** Build a proprietary company dataset. WORKFLOW IMAGE Troubleshooting Guide | Issue | Possible Cause | Solution | |-------|----------------|-----------| | Form Trigger 404 | Workflow not active | Activate the workflow | | Web Search Tool fails | Missing Tavily API key | Replace the placeholder key | | FIRECRAWLER / find competitor fails | Missing MCP node | Install n8n-nodes-mcp | | Basic scrape does nothing | Switch node path disconnected | Reconnect ‚Äúbasic‚Äù output | | Supabase node error | Wrong table/column names | Match schema exactly | Need Help or More Workflows? Want to customize this workflow for your business or integrate it with your existing tools? Our team at Digital Biz Tech can tailor it precisely to your use case from automation logic to AI-powered enhancements. Contact: shilpa.raju@digitalbiz.tech For more such offerings, visit us: https://www.digitalbiz.tech",349,2025-10-24 01:38:47.520000+00:00,True,7
5204,Sentiment Analytics Visualizer,"üß† Sentiment Analyzer Google Sheets ‚Üí OpenAI GPT-4o ‚Üí QuickChart ‚Üí Gmail üöÄ What this workflow does Fetches customer reviews from a Google Sheet. Classifies each review as Positive, Neutral or Negative with GPT-4o-mini. Writes the sentiment back to your sheet. Builds a doughnut chart summarising the totals. Emails the chart to your chosen recipient so the whole team stays in the loop. Perfect for support teams, product managers or anyone who wants a zero-code mood ring for their users‚Äô feedback. üó∫Ô∏è Node-by-node tour | üî© Node | üí° Purpose | | ------------------------------------------------------- | ---------------------------------------------------------- | | Manual Trigger | Lets you test the workflow on demand. | | Select Google Sheet | Points to the spreadsheet that holds your reviews. | | Loop Over Items | Feeds each row through the analysis routine. | | Sentiment Analysis (LangChain) | Calls GPT-4o-mini and returns only the sentiment category. | | Update Google Sheet | Writes the new Sentiment value into column C. | | Read Data from Google Sheet | Pulls the full sheet again to create a summary. | | Extract Number of Answers per Sentiment (Code node) | Tallies up how many reviews fall into each category. | | Generate QuickChart | Creates a doughnut (or pie) chart as a PNG. | | Send Gmail with Sentiment Chart | Fires the chart off to your inbox. | | (Sticky Notes) | Friendly setup tips scattered around the canvas. | üõ†Ô∏è Setup checklist | ‚úÖ Step | Where | | ------------------------------------------------------------------------------------- | -------------------------------- | | Connect Google Sheets ‚Üí paste your Spreadsheet ID & choose the correct sheet. | All Google Sheets nodes | | Add OpenAI credentials (sk-‚Ä¶ key). | Sentiment Analysis node | | Configure Gmail OAuth2 + recipient address. | Gmail node | | Match your sheet columns ‚Üí ‚ÄúReview title‚Äù, ‚ÄúReview text‚Äù, empty ‚ÄúSentiment‚Äù. | Google Sheet itself | | (Optional) Switch to gpt-4o for maximum accuracy. | Sentiment Analysis ‚ÄúModel‚Äù param | üèÉ‚Äç‚ôÇÔ∏è How to run Drop a few sample reviews into the sheet. Click ‚ÄúTest workflow‚Äù on the Manual Trigger. Watch each row march through ‚Üí sentiment appears in column C. After all rows finish, check your inbox for a fresh chart. ‚úîÔ∏è ‚ú® Ideas for next level Schedule** the trigger (Cron) to auto-process new reviews daily. Feed the counts to Slack or Discord instead of email. Add a second GPT call to generate a short summary for each review. Happy automating! üéâ",346,2025-06-24 15:13:14.114000+00:00,True,6
4841,Generate Sales Emails Based on Business Events with Explorium MCP & Claude,"Explorium Event-Triggered Outreach This n8n and agent-based workflow automates outbound prospecting by monitoring Explorium event data (e.g. product launches, new office opening, new investment and more), researching companies, identifying key contacts, and generating tailored sales emails leveraging the Explorium MCP server. Template Workflow Overview Node 1: Webhook Trigger Purpose: Listens for real-time product launch events pushed from Explorium's webhook system. How it works: Explorium sends HTTP POST requests containing event data The webhook payload includes company name, business ID, domain, product name, and event type Pay attention: Product launch is just one example. You can easily enroll to many more meaningful events. to learn about events and how to enroll to events, visit the events documentation. Node 2: Company Research Agent Agent Type: Tools Agent Purpose: Enrich company data after an event occurs. How it works: Uses Explorium MCP via the MCP Client tool to gather additional company data Uses Anthropic Claude (Chat Model) to process and interpret company information for downstream personalization Node 3: Employee Data Retrieval Purpose: Retrieve prospect-level data for targeting. How it works: Uses HTTP Request node to call Explorium's fetch_prospects endpoint Filters prospects by: Company business_id Departments: Product, R&D, etc... Seniority levels: owner, cxo, vp, director, senior, manager, partner, etc... Pay Attention: Follow our fetch prospect documentation for the full list of filter and best practice. Limits results to top 5 relevant employees Code nodes handle: Filtering logic Cleaning API response Formatting data for downstream agents Node 4: Conditional Branch - Prospect Data Check If Node: Checks whether prospect data was successfully retrieved Logic: If prospects found ‚Üí personalized emails per person If no prospects ‚Üí fallback to company-level general email Node 5A: Email Writer #1 (No Prospect Data) Agent Type: Tools Agent Purpose: Write generic outbound email using only company-level research and event info. Powered by: Anthropic Chat Model Node 5B: Loop Over Prospects ‚Üí Email Writer #2 (Personalized) Agent Type: Tools Agent Purpose: Write highly personalized email for each identified employee. How it works: Loops through each individual prospect Passes company research + employee data to LLM agent Generates customized emails referencing: Prospect's title & department Product launch Role-relevant Explorium value proposition Node 6: Slack Notifications Purpose: Posts completed emails to internal Slack channel for review or testing before final deployment. Future State: Can be swapped with an email sequencing platform in production. Setup Requirements Explorium API Access MCP Client credentials for company enrichment and prospect fetching Registered webhook for event listening Get explorium api key n8n Configuration Secure environment variables for API keys & webhook secret Code nodes configured for JSON transformation, filtering & signature validation Customization Options Personalization Logic Update LLM prompt instructions to reflect ICP priorities Modify email templates based on role, department, or tenure logic Adjust fallback behavior when prospect data is unavailable API Request Tuning Adjust page_size for number of prospects retrieved Fine-tune seniority and department filters to match evolving targeting Future Expansion Swap Slack notifications for outbound email automation Integrate call task assignment directly into CRM Introduce engagement scoring feedback loop (opens, clicks, replies) Troubleshooting Tips Validate webhook signature matching to prevent unauthorized requests Ensure correct business_id is passed to prospect fetching endpoint Confirm business enrichment returns sufficient data for company researcher agents Review agent LLM responses for correct output structure and parsing consistency",332,2025-06-10 06:56:57.172000+00:00,True,6
5412,"Summarize YouTube Videos with Transcription, DeepSeek AI and Google Sheets","Tired of manually watching long YouTube videos just to extract the main points? With our YouTube Transcript & Summary Automation, you can instantly turn any video into an actionable, AI-generated summary ‚Äî all from a simple Google Sheet. What this automation does: Reads video URLs from Google Sheets (just add your links!) Generates accurate transcripts using Supadata.ai ‚Äî with 100 free uses/month Creates a smart summary using DeepSeek AI: üîπ Short summary of the video üîπ Key points üîπ Main topics Youtube tutorial: https://www.youtube.com/watch?v=uj7KaFSh95Y Automatically updates your Google Sheet with the transcript and the AI-generated summary How to set it up: Create a simple Google Sheet with these columns: Url ‚Äì link to the YouTube video Status ‚Äì set to pending to trigger the automation, updated to done after completion Transcript ‚Äì filled automatically Summary ‚Äì filled automatically Once your sheet is ready, the automation takes care of the rest ‚Äî no technical skills needed. Why you'll love it: This is the perfect tool for content creators, researchers, marketers, and educators who want to save time, extract insights faster, and never miss key ideas from long videos. Want something custom? Got feedback or want to build your own custom automation or workflow? We‚Äôd love to hear from you! Reach out to us at jacobmarketingservice@gmail.com ‚Äî let‚Äôs bring your idea to life.",320,2025-06-27 15:31:00.808000+00:00,True,5
2831,Batch Airtable requests to send data 9x faster,"Watch Demo YouTube Video Optimized Airtable Bulk Data Workflow This workflow is specifically designed to address the challenges of upserting or inserting large volumes of data into Airtable. By leveraging the Airtable Batch API, it delivers up to 9X faster performance compared to standard data insertion methods, making it an indispensable tool for high-demand data operations. Key Features ‚Ä¢ Accelerated Data Processing: Utilize the Airtable Batch API to perform bulk operations swiftly and efficiently. ‚Ä¢ Seamless Workflow Integration: Easily integrate this sub-processor into any n8n workflow that requires Airtable updates, ensuring smooth data synchronization across multiple processes. ‚Ä¢ Enhanced Reliability and Scalability: Designed to handle extensive datasets, this solution is perfect for real-time updates, database migrations, and continuous data syncing without performance degradation. Setup Instructions Add the Sub-Workflow: Import this workflow to your n8n workflows, then add it as a sub-workflow call in other workflows requiring a lot of Airtable updates. **Configure Sub-Worflow variables: ""set_Batching_vars"" SET Node** ‚Ä¢ Obtain the correct Base ID and Table ID, and insert in the ""set_Batching_vars"" SET Node. ‚Ä¢ Add or select the correct Airtable credentials in both Airtable Upsert & Insert HTTP nodes in the sub-workflow. ‚Ä¢ Ensure the API permissions are set correctly to allow data insertion/upsertion. **Adjust Batch Settings: ""set_Batching_vars"" SET Node** ‚Ä¢ In the same ""set_Batching_vars"" SET Node, put the field name in the ""merge_on"" field if you wish to upsert record, otherwise, keep it empty for insertion. ‚Ä¢ Correctly setup the fields you want to insert/upsert in the 'record' field. Test the Integration: Run a small-scale test to ensure that data is correctly processed and inserted/upserted into Airtable. Use Case Scenarios ‚Ä¢ Bulk Data Insertion: Efficiently insert large datasets into Airtable, perfect for initial data migrations or periodic data updates. ‚Ä¢ Real-Time Data Upsertion: Keep your Airtable records current by integrating this workflow with your live data pipelines. ‚Ä¢ Database Migrations & Synchronization: Seamlessly transfer data between databases and Airtable, ensuring minimal downtime and data integrity. Specific Requirements for Airtable Integration ‚Ä¢ Airtable Account: You must have an active Airtable account with appropriate permissions to modify the target base. ‚Ä¢ API Credentials: Secure a valid Airtable API connection and ensure you have the correct Base ID and Table ID for the target data store. By integrating this workflow into your system, you can significantly improve the efficiency of your Airtable operations, reducing processing time and enabling smoother data management at scale.",310,2025-02-01 06:21:25.311000+00:00,False,1
3392,Create and Join Call Sessions with Plivo and UltraVox AI Voice Assistant,This workflow template creates an audio stream session on UltraVox compatible with Plivo and sends it to Plivo. How It Works : Plivo initiates a call and requests the Answer URL. The workflow responds with Plivo XML to join the session. Note: Ensure you update the UltraVox API Key in the credentials. Update System Prompt based on your requirements. Check Youtube Video,307,2025-04-01 04:12:57.053000+00:00,True,1
6675,"Manage Construction Projects with Tasks, Photo Reports, Telegram & Google Sheets","How it works This template helps project managers collect task updates and photo reports from field workers via Telegram and stores everything in a Google Sheet. It enables daily project reporting without paper or back-office overhead. High-level flow: Workers receive daily tasks via Telegram They respond with photo reports Bot auto-saves replies (photos + status) to a Google Sheet The system tracks task completion, adds timestamps, and maintains report history Set up steps üïí Estimated setup time: 15‚Äì30 min You‚Äôll need: A Telegram bot (via BotFather) A connected Google Sheet (with specific column headers) A set of preconfigured tasks üëâ Detailed setup instructions and required table structure are documented in sticky notes inside the workflow. Consulting and Training We work with leading construction, engineering, consulting agencies and technology firms around the world to help them implement open data principles, automate CAD/BIM processing and build robust ETL pipelines. If you would like to test this solution with your own data, or are interested in adapting the workflow to real project tasks, feel free to contact us. Docs & Issues: Full Readme on GitHub",305,2025-07-30 06:10:11.318000+00:00,False,4
2597,Update all Zammad Roles to default values,"This n8n workflow allows you to reset all user roles in Zammad to specified default roles. It ensures consistency in role management across your Zammad instance. Features Retrieve all active users from Zammad. Update each user's roles to predefined default role IDs. Exclude specific users by their IDs from the update process. Simple configuration for default roles and excluded users. Usage Import the Workflow: Upload the provided .json file into your n8n instance. Configure Variables: zammad_base_url: Your Zammad instance URL. zammad_api_key: Your Zammad API key. default_roles: List of default role IDs to apply to all users. exclude_zammad_users_by_id: List of user IDs to exclude from the update. Run the Workflow: Execute the workflow to update roles automatically. Issues and Suggestions For issues or suggestions, visit the GitHub Repository.",303,2024-12-01 12:02:43.884000+00:00,False,2
2795,Programmatically Retrieve Embeddable Getty Images,"How it works: This workflow empowers marketers and content creators to seamlessly include Getty Images in their posts, provided they adhere to Getty Images' guidelines: Enter a search phrase and select criteria for Getty Images' editorial collection. Retrieve the latest images that match your input parameters. Extract embeddable links for use in your content. Automate this previously manual process to save time and maintain compliance. Set up steps: Estimated time: ~10 minutes. Configure the input node with your desired search terms and criteria. Link the workflow to your preferred CMS or storage solution. Perform a test run to ensure proper retrieval and embedding of images. Review Getty Images' usage guidelines for compliance before publishing. This solution is ideal for independent news channels and small publications looking for a legal, automated way to use Getty Images on their websites without incurring costs.",301,2025-01-24 21:51:37.266000+00:00,False,2
5158,Generate Realistic Sound Effects with CassetteAI and Save to Google Drive,"The Sound Effects Generator is an automated workflow that allows users to create realistic sound effects using AI and save them directly to Google Drive. It generates high-quality sound effects (up to 30 seconds long) based on user prompts. How It Works: User Input via Web Form A form is presented to the user asking for: A prompt describing the sound (e.g. ""waves crashing"", ""laser blast""). A duration in seconds (up to 30 seconds). API Request to Generate Audio The input is sent to CassetteAI via a POST request using API with proper authentication. Status Polling The workflow waits for 10 seconds and then checks the status of the request. Conditional Flow If the audio generation is complete (COMPLETED), it proceeds to fetch the audio file URL. If not, it waits and retries. Download & Save The audio file is downloaded from the URL. It is automatically uploaded to a specific folder in the user‚Äôs Google Drive, with a timestamped filename. Key Advantages Fast & Efficient**: Generates up to 30 seconds of audio in just 1 second of processing time. No Coding Required**: Entire flow can be triggered via a simple form interface. Automated Storage**: Files are automatically saved to a preconfigured Google Drive folder. Scalable**: Can be reused for multiple projects by simply changing the input prompts. Secure**: Uses secure API key-based authentication for interaction with Fal.run and Google Drive. Customizable**: Easy to adapt or extend‚Äîfor example, sending download links via email or Telegram. How It Works Form Submission: The workflow starts with a form where users input a prompt and the desired duration (max 30 seconds) for the sound effect. Audio Creation: The submitted data is sent to the CassetteAI Sound Effects Generator API via an HTTP request, which initiates the sound effect generation process. Status Check: The workflow periodically checks the status of the request. If the status is ""COMPLETED,"" it proceeds to fetch the audio file. Audio Retrieval: The generated audio file is downloaded from the provided URL and uploaded to a specified Google Drive folder, with a timestamped filename for organization. Set Up Steps API Key Configuration: Create an account on fal.ai and obtain an API key. In the ""Create audio"" node, set the ""Header Auth"" with: Name: Authorization Value: Key YOURAPIKEY (replace YOURAPIKEY with your actual API key). Google Drive Integration: Ensure the Google Drive node is configured with the correct OAuth2 credentials and folder ID. Adjust the folder ID in the ""Upload Audio"" node if a different destination is preferred. Need help customizing? Contact me for consulting and support or add me on Linkedin.",295,2025-06-23 15:12:57.125000+00:00,True,2
4617,Daily AI Stock Briefing Right to Your Email: OpenAI + Tavily + Gmail,"This n8n workflow template uses community nodes and is only compatible with the self-hosted version of n8n. üìà StockPulse: AI-Picked Daily News for Your Portfolio Stay ahead of the market with this automated, AI-powered stock market news briefing delivered straight to your inbox ‚Äî no code required. Watch Step-by-step Video Tutorial Here: https://www.youtube.com/watch?v=iZvPej9eLYE&t=201s ‚öôÔ∏è What it does: This workflow runs every morning and: Triggers a scheduled prompt to a Langchain AI Agent (OpenAI) Uses the Tavily Web Search API to fetch fresh financial news relevant to your watchlist or portfolio Summarizes the top stories, highlighting: üîç Key headlines üí° Investment opportunities ‚ö†Ô∏è Risks and macro trends üìä Suggested trades Sends a clean, readable email via Gmail to your preferred address üîß Built with: üß† Langchain AI Agent (OpenAI GPT-4o) üîç Tavily Search Tool üì¨ Gmail Node for Email Delivery ‚è∞ Daily Cron Trigger (customizable) üíº Who it‚Äôs for: Investors and traders who want to save time on news gathering Financial creators looking for curated, actionable insights Non-technical users interested in automating stock news monitoring Anyone who wants to combine AI + automation + market data üü¢ Customize easily: Edit your stock list or news focus inside the Agent prompt üì® Email ready: Just plug in your Gmail credentials and you‚Äôre good to go ‚è±Ô∏è 10-minute setup ‚Äî no coding required!",285,2025-06-03 10:36:48.730000+00:00,True,4
5219,Automated Competitor Price Monitoring with Bright Data & n8n,Description This workflow automatically monitors your competitors' product prices and notifies you of any changes. It helps you stay competitive in the market by providing real-time insights into pricing strategies without manual checking. Overview This workflow automatically tracks your competitors' pricing across their websites and notifies you of any changes. It uses Bright Data to scrape pricing data and can generate reports or send alerts when prices change. Tools Used n8n:** The automation platform that orchestrates the workflow. Bright Data:** For scraping competitor websites without being blocked. Spreadsheets/Databases:** For storing and analyzing price data. Notification Services:** For alerting you to significant price changes. How to Install Import the Workflow: Download the .json file and import it into your n8n instance. Configure Bright Data: Add your Bright Data credentials to the Bright Data node. Set Up Data Storage: Configure where you want to store the price data. Customize: Add your competitors' URLs and the specific products to monitor. Use Cases E-commerce Businesses:** Stay competitive with real-time price monitoring. Pricing Analysts:** Automate data collection for pricing strategy. Retailers:** Adjust your pricing strategy based on market trends. Connect with Me Website:** https://www.nofluff.online YouTube:** https://www.youtube.com/@YaronBeen/videos LinkedIn:** https://www.linkedin.com/in/yaronbeen/ Get Bright Data:** https://get.brightdata.com/1tndi4600b25 (Using this link supports my free workflows with a small commission) #n8n #automation #pricing #competitoranalysis #brightdata #ecommerce #pricemonitoring #competitiveintelligence #pricetracking #marketanalysis #n8nworkflow #workflow #nocode #pricingstrategy #competitortracking #ecommercetools #pricecomparison #businessintelligence #marketresearch #competitivepricing #retailautomation #pricealerts #datadriven #webscraping #pricinganalysis #competitorinsights,283,2025-06-24 15:54:46.724000+00:00,False,5
5138,Extract Facebook Group Posts with Airtop,"Extract Facebook Group Posts with Airtop Use Case Extracting content from Facebook Groups allows community managers, marketers, and researchers to gather insights, monitor discussions, and collect engagement metrics efficiently. This automation streamlines the process of retrieving non-sponsored post data from group feeds. What This Automation Does This automation extracts key post details from a Facebook Group feed using the following input parameters: Facebook Group URL**: The URL of the Facebook Group feed you want to scrape. Airtop Profile**: The name of your Airtop Profile authenticated to Facebook. It returns up to 5 non-sponsored posts with the following attributes for each: Post text Post URL Page/profile URL Timestamp Number of likes Number of shares Number of comments Page or profile details Post thumbnail How It Works Form Trigger: Collects the Facebook Group URL and Airtop Profile via a form. Browser Automation: Initiates a new browser session using Airtop. Navigates to the provided Facebook Group feed. Uses an AI prompt to extract post data, including interaction metrics and profile information. Structured Output: The results are returned in a defined JSON schema, ready for downstream use. Setup Requirements Airtop API Key ‚Äî Free to generate. An Airtop Profile logged into Facebook. Next Steps Integrate With Analytics Tools**: Feed the output into dashboards or analytics platforms to monitor community engagement. Automate Alerts**: Trigger notifications for posts matching certain criteria (e.g., high engagement, keywords). Combine With Comment Automation**: Extend this to reply to posts or engage with users using other Airtop automations. Let me know if you‚Äôd like this saved as a .md file or included in your Airtop automation library. Read more about how to extract posts from Facebook groups",279,2025-06-23 05:35:12.640000+00:00,True,1
5293,Audio-to-Trello Task Bot,"Telegram Tasker Bot ‚Äî —ç—Ç–æ —Å—Ü–µ–Ω–∞—Ä–∏–π n8n, –∫–æ—Ç–æ—Ä—ã–π –ø—Ä–∏–Ω–∏–º–∞–µ—Ç –≥–æ–ª–æ—Å–æ–≤—ã–µ —Å–æ–æ–±—â–µ–Ω–∏—è –≤ Telegram, –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –ø—Ä–µ–≤—Ä–∞—â–∞–µ—Ç –∏—Ö –≤ —Ç–µ–∫—Å—Ç, –∏–∑–≤–ª–µ–∫–∞–µ—Ç –∏–∑ –Ω–µ–≥–æ –∫–ª—é—á–µ–≤—ã–µ –ø–æ–ª—è –∑–∞–¥–∞—á–∏ –∏ —Å–æ–∑–¥–∞—ë—Ç –∫–∞—Ä—Ç–æ—á–∫—É –≤ –Ω—É–∂–Ω–æ–π –¥–æ—Å–∫–µ Trello. –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –ø—Ä–æ—Å—Ç–æ –≥–æ–≤–æ—Ä–∏—Ç –∑–∞–¥–∞—á—É ‚Äî –±–æ—Ç —Å–∞–º –æ—Ñ–æ—Ä–º–ª—è–µ—Ç –µ—ë –∏ –ø—Ä–∏—Å—ã–ª–∞–µ—Ç —Å—Å—ã–ª–∫—É –Ω–∞ –≥–æ—Ç–æ–≤—É—é –∫–∞—Ä—Ç–æ—á–∫—É. –î–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤–∞–º –ø–æ—Ç—Ä–µ–±—É–µ—Ç—Å—è telegram bot. –ï–≥–æ –º–æ–∂–Ω–æ —Å–æ–∑–¥–∞—Ç—å —á–µ—Ä–µ–∑ –±–æ—Ç–∞ BotFather –¢–∞–∫ –∂–µ –ø–æ–Ω–∞–¥–æ–±–∏—Ç—Å—è –¥–æ—Å—Ç—É–ø –∫ API chatgpt - –æ–Ω –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Ç–æ–ª—å–∫–æ –¥–ª—è —Ç—Ä–∞–Ω—Å–∫—Ä–∏–±—Ü–∏–∏ –∞—É–¥–∏–æ –≤ —Ä–µ—á—å. –í—ã –º–æ–∂–µ—Ç–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ª—é–±–æ–π –¥—Ä—É–≥–æ–π —Å–µ—Ä–≤–∏—Å, –ø–æ –≤–∞—à–µ–º—É –≤—ã–±–æ—Ä—É. –ò –∞–∫–∫–∞—É–Ω—Ç –≤ trello, —Å –¥–æ—Å—Ç—É–ø–æ–º –∫ API. !–í–Ω–∏–º–∞–Ω–∏–µ! ID –¥–æ—Å–∫–∏ –≤ trello –º–æ–∂–Ω–æ –≤–∑—è—Ç—å –∏–∑ url ID —Å—Ç–æ–ª–±—Ü–∞ –Ω–∞ –¥–æ—Å–∫–µ —Ç—Ä–µ–ª–ª–æ –º–æ–∂–Ω–æ –≤–∑—è—Ç—å —á–µ—Ä–µ–∑ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã —Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫–∞ (–ø–æ –∫—Ä–∞–π–Ω–µ–π –º–µ—Ä–µ —è —Ç–∞–∫ –ø–æ–ª—É—á–∞–ª —ç—Ç–∏ –¥–∞–Ω–Ω—ã–µ)",278,2025-06-25 09:39:01.975000+00:00,True,7
5193,Pinterest Keyword-Based Content Scraper with AI Agent & BrightData Automation,"Pinterest Keyword-Based Content Scraper with AI Agent & BrightData Automation Overview This n8n workflow automates Pinterest content scraping based on user-provided keywords using BrightData's API and Claude Sonnet 4 AI agent. The system intelligently processes keywords, initiates scraping jobs, monitors progress, and formats the extracted data into structured outputs. Architecture Components üß† AI-Powered Controller Claude Sonnet 4 Model**: Processes and understands keywords before initiating scrape AI Agent**: Acts as the intelligent controller coordinating all scraping steps üì• Data Input Form Trigger**: User-friendly keyword input interface Keywords Field**: Required input field for Pinterest search terms üöÄ Scraping Pipeline Launch Scraping Job: Sends keywords to BrightData API Status Monitoring: Continuously checks scraping progress Data Retrieval: Downloads completed scraped content Data Processing: Formats and structures the raw data Storage: Saves results to Google Sheets Workflow Nodes 1. Pinterest Keyword Input Type**: Form Trigger Purpose**: Entry point for user keyword submission Configuration**: Form title: ""Pinterest"" Required field: ""Keywords"" 2. Anthropic Chat Model Type**: Language Model (Claude Sonnet 4) Model**: claude-sonnet-4-20250514 Purpose**: AI-powered keyword processing and workflow orchestration 3. Keyword-based Scraping Agent Type**: AI Agent Purpose**: Orchestrates the entire scraping process Instructions**: Initiates Pinterest scraping with provided keywords Monitors scraping status until completion Downloads final scraped data Presents raw scraped data as output 4. BrightData Pinterest Scraping Type**: HTTP Request Tool Method**: POST Endpoint**: https://api.brightdata.com/datasets/v3/trigger Parameters**: dataset_id: gd_lk0sjs4d21kdr7cnlv include_errors: true type: discover_new discover_by: keyword limit_per_input: 2 Purpose**: Creates new scraping snapshot based on keywords 5. Check Scraping Status Type**: HTTP Request Tool Method**: GET Endpoint**: https://api.brightdata.com/datasets/v3/progress/{snapshot_id} Purpose**: Monitors scraping job progress Returns**: Status values like ""running"" or ""ready"" 6. Fetch Pinterest Snapshot Data Type**: HTTP Request Tool Method**: GET Endpoint**: https://api.brightdata.com/datasets/v3/snapshot/{snapshot_id} Purpose**: Downloads completed scraped data Trigger**: Executes when status is ""ready"" 7. Format & Extract Pinterest Content Type**: Code Node (JavaScript) Purpose**: Parses and structures raw scraped data Extracted Fields**: URL Post ID Title Content Date Posted User Likes & Comments Media Image URL Categories Hashtags 8. Save Pinterest Data to Google Sheets Type**: Google Sheets Node Operation**: Append Mapped Columns**: Post URL Title Content Image URL 9. Wait for 1 Minute (Disabled) Type**: Code Tool Purpose**: Adds delay between status checks (currently disabled) Duration**: 60 seconds Setup Requirements Required Credentials Anthropic API Credential ID: ANTHROPIC_CREDENTIAL_ID Required for Claude Sonnet 4 access BrightData API API Key: BRIGHT_DATA_API_KEY Required for Pinterest scraping service Google Sheets OAuth2 Credential ID: GOOGLE_SHEETS_CREDENTIAL_ID Required for data storage Configuration Placeholders Replace the following placeholders with actual values: WEBHOOK_ID_PLACEHOLDER: Form trigger webhook ID GOOGLE_SHEET_ID_PLACEHOLDER: Target Google Sheets document ID WORKFLOW_VERSION_ID: n8n workflow version INSTANCE_ID_PLACEHOLDER: n8n instance identifier WORKFLOW_ID_PLACEHOLDER: Unique workflow identifier Data Flow User Input (Keywords) ‚Üì AI Agent Processing (Claude) ‚Üì BrightData Scraping Job Creation ‚Üì Status Monitoring Loop ‚Üì Data Retrieval (when ready) ‚Üì Content Formatting & Extraction ‚Üì Google Sheets Storage Output Data Structure Each scraped Pinterest pin contains: URL**: Direct link to Pinterest pin Post ID**: Unique Pinterest identifier Title**: Pin title/heading Content**: Pin description text Date Posted**: Publication timestamp User**: Pinterest username Engagement**: Likes and comments count Media**: Media type information Image URL**: Direct image link Categories**: Pin categorization tags Hashtags**: Associated hashtags Comments**: User comments text Usage Instructions Initial Setup: Configure all required API credentials Replace placeholder values with actual IDs Create target Google Sheets document Running the Workflow: Access the form trigger URL Enter desired Pinterest keywords Submit the form to initiate scraping Monitoring Progress: The AI agent will automatically handle status monitoring No manual intervention required during scraping Accessing Results: Structured data will be automatically saved to Google Sheets Each run appends new data to existing sheet Technical Notes Rate Limiting**: BrightData API has built-in rate limiting Data Limits**: Current configuration limits 2 pins per keyword Status Polling**: Automatic status checking until completion Error Handling**: Includes error capture in scraping requests Async Processing**: Supports long-running scraping jobs Customization Options Adjust Data Limits**: Modify limit_per_input parameter Enable Wait Timer**: Activate the disabled wait node for longer jobs Custom Data Fields**: Modify the formatting code for additional fields Alternative Storage**: Replace Google Sheets with other storage options Sample Google Sheets Template Create a copy of the sample sheet structure: https://docs.google.com/spreadsheets/d/SAMPLE_SHEET_ID/edit Required columns: Post URL Title Content Image URL Troubleshooting Authentication Errors**: Verify all API credentials are correctly configured Scraping Failures**: Check BrightData API status and rate limits Data Formatting Issues**: Review the JavaScript formatting code for parsing errors Google Sheets Errors**: Ensure proper OAuth2 permissions and sheet access For any questions or support, please contact: Email or fill out this form",277,2025-06-24 08:38:03.685000+00:00,True,5
5224,Find the Best Airbnb Deals Automatically with Bright Data & n8n,"F Description This workflow automatically searches Airbnb for the best deals in your target locations and saves them for later reference. It helps travelers find affordable accommodations by continuously monitoring listings and identifying properties that match your budget and preferences. Overview This workflow automatically searches Airbnb for the best deals in your target locations and saves them for later reference. It uses Bright Data to scrape Airbnb listings and can filter results based on your preferences for price, amenities, and ratings. Tools Used n8n:** The automation platform that orchestrates the workflow. Bright Data:** For scraping Airbnb listings without being blocked. Spreadsheets/Databases:** For storing and comparing property deals. How to Install Import the Workflow: Download the .json file and import it into your n8n instance. Configure Bright Data: Add your Bright Data credentials to the Bright Data node. Set Up Data Storage: Configure where you want to store the Airbnb deals. Customize: Specify locations, date ranges, and your budget constraints. Use Cases Travelers:** Find the best accommodation deals for your trips. Digital Nomads:** Track affordable long-term stays in different locations. Property Managers:** Monitor competitor pricing in your area. Connect with Me Website:** https://www.nofluff.online YouTube:** https://www.youtube.com/@YaronBeen/videos LinkedIn:** https://www.linkedin.com/in/yaronbeen/ Get Bright Data:** https://get.brightdata.com/1tndi4600b25 (Using this link supports my free workflows with a small commission) #n8n #automation #airbnb #travel #brightdata #dealhunting #vacationrentals #traveldeals #accommodationdeals #airbnbdeals #n8nworkflow #workflow #nocode #travelhacks #budgettravel #propertydeals #travelplanning #airbnbscraper #vacationplanning #bestairbnbs #travelautomation #affordableaccommodation #staydeals #traveltech #digitalnomad #accommodationfinder",276,2025-06-24 15:56:58.236000+00:00,False,3
4471,Github Webhook-Based n8n Workflow Import Template,"This n8n workflow template uses community nodes and is only compatible with the self-hosted version of n8n. This template aims to ease the process of deploying workflows from github. It has a companion repository that developers might find useful{. See below for more details How it works Automatically import and deploy n8n workflows from your GitHub repository to your production n8n instance using a secured webhook-based approach. This template enables teams to maintain version control of their workflows while ensuring seamless deployment through a CI/CD pipeline. Receives webhook notifications from GitHub when changes are pushed to your repository Lists all files in the repository and filters for .json workflow files Downloads each workflow file and saves it locally Imports all workflows into n8n using the CLI import command Cleans up temporary files after successful import To trigger the deployment, send a POST request to your webhook with the set up credentials (basic auth) with the following body: { ""owner"": ""GITHUB_REPO_OWNER_NAME"", ""repository"": ""GITHUB_REPOSITORY_NAME"" } Set up steps Once importing this template in n8n : Setup the webhook basic auth credentials Setup the github credentials Activate the workflow ! Companion repository There is a companion repository located at https://github.com/dynamicNerdsSolutions/n8n-git-flow-template that has a Github action already setup to work with this workflow. It provides a complete development environment with: Local n8n instance via Docker Automated workflow export and commit scripts Version control integration CI/CD pipeline setup This setup allows teams to maintain a clean separation between development and production environments while ensuring reliable workflow deployment.",271,2025-05-28 21:24:41.338000+00:00,False,1
4773,"Build a Document-based AI Chatbot with Google Drive, Llama 3, and Qdrant RAG","Overview This template allows users to set up an AI-powered chatbot that retrieves and processes knowledge from Google Drive documents using Retrieval-Augmented Generation (RAG). By leveraging Llama 3 for natural language responses and Qdrant vector storage for document embeddings, this chatbot provides accurate, context-aware answers based on stored files. Problem It Solves Standard AI chatbots often rely on predefined models with limited real-time knowledge access. This workflow overcomes that limitation by: Automatically fetching new documents from Google Drive. Embedding knowledge for fast retrieval using Qdrant. Generating human-like responses with Llama 3 AI. Providing accurate, source-backed answers in conversations. Use Cases ‚úîÔ∏è Customer Support ‚Äì Retrieve and summarize FAQs stored in Google Drive. ‚úîÔ∏è Internal Knowledge Base ‚Äì Automate document-based query responses. ‚úîÔ∏è AI-powered Research Assistant ‚Äì Search and generate insights from uploaded files. ‚úîÔ∏è Business Automation ‚Äì Enhance workflows with document-aware chat interactions. Setup Instructions 1Ô∏è‚É£ Google Drive Trigger: Detect & Fetch New Documents Watches for new files added to a specific Google Drive folder. Retrieves the latest file metadata and passes it into the workflow. 2Ô∏è‚É£ Processing & Embedding the Document The document is downloaded via the Google Drive node. Text data is split into smaller, retrievable chunks using Recursive Text Splitter. Embeddings are created using Ollama‚Äôs Nomic-Embed Model. Knowledge is stored in Qdrant Vector Database for fast AI-powered lookup. 3Ô∏è‚É£ AI Chatbot & Query Handling The Chat Trigger node listens for user queries. The AI Agent retrieves context-aware answers by searching Qdrant‚Äôs vectorized documents. The Llama 3 Model generates human-like responses based on stored knowledge. Detailed Workflow Explanation üîπ Google Drive Trigger ‚úÖ Monitors a specific folder for new documents. ‚úÖ Automatically fetches document metadata when a file is uploaded. üîπ Qdrant Vector Store ‚úÖ Stores embedded document text, making retrieval instant & accurate. ‚úÖ Allows the chatbot to reference stored knowledge dynamically. üîπ Recursive Text Splitter ‚úÖ Splits long documents into manageable chunks for efficient embedding. ‚úÖ Improves chatbot response accuracy by organizing document data. üîπ Llama 3 Chat Model ‚úÖ Generates natural, human-like replies using AI. ‚úÖ Uses retrieved document data for context-aware responses. Customization Options üîπ Adjust polling frequency for document updates. üîπ Expand knowledge base by adding more storage sources. üîπ Refine chatbot responses with prompt tuning in Llama 3.",271,2025-06-08 07:38:46.273000+00:00,True,7
4911,Generate Summaries from Uploaded Files using OpenAI Assistants API,"Generate Summaries from Uploaded Files using OpenAI Assistants API üìë Overview Upload a document (PDF, DOCX, PPTX, TXT, CSV, JSON, or Markdown) and receive an AI-generated summary containing: title** ‚Äì 5-10 words summary** ‚Äì 1-2 sentences bullets** ‚Äì 3-5 key points tags** ‚Äì 3-6 short keywords The workflow: Stores the file in OpenAI. Runs an Assistant with File Search and Code Interpreter enabled. Polls until the run finishes. Retrieves the summary JSON. ‚úÖ Prerequisites OpenAI Assistant Create one at &lt;https://platform.openai.com/assistants&gt; Enable File Search and Code Interpreter Note: The assistant ID starts with asst_ OpenAI API credential setup in n8n Go to Credentials ‚Üí New ‚Üí HTTP Header Auth Header name: Authorization Value: Bearer YOUR-OPENAI-API-KEY (replace YOUR-OPENAI-API-KEY with your OpenAI API secret key for your assistant, starts with sk-) Name it: openAIApiHeader üîß Setup Import the workflow JSON. When n8n prompts for a credential, choose openAIApiHeader for every HTTP Request node. Open Run Assistant ‚Üí Body and replace ""assistant_id"": ""REPLACE_WITH_YOUR_ASSISTANT_ID"" with your real ID (starts with asst_‚Ä¶). Save. üöÄ How it works | # | Node | Purpose | |---|------|---------| | 1 | On form submission | User uploads a file (File). | | 2 | Upload File | POST /v1/files (multipart) ‚Üí returns file_id. | | 3 | Create Thread | Creates a thread and attaches the uploaded file. | | 4 | Run Assistant | Starts the run using your assistant_id. | | 5 | Poll Run Status ‚Üí Wait 2 s ‚Üí IF | Loops until status = completed. | | 6 | Fetch Summary | GET /v1/threads/{thread_id}/messages ‚Üí summary JSON. | üñåÔ∏è Customisation ideas Edit the user prompt in Create Thread to change summary length, tone, or language. Add an HTTP Response node after Fetch Summary to return plaintext to the uploader. Replace the polling loop with OpenAI‚Äôs forthcoming wait-for-run endpoint when available. No community nodes required. Works on any n8n Cloud plan (Starter, Pro, Enterprise) or self-hosted Community Edition.",262,2025-06-13 03:33:27.446000+00:00,True,1
2599,Entra Contacts to Zammad User Sync,"This workflow facilitates seamless synchronization between Entra (Microsoft Azure AD) and Zammad. It automates the following processes: Fetch Entra Contacts: Create Universal User Object: Extracts key user information, such as email, phone, and name, and formats it for Zammad compatibility. Synchronize with Zammad: Identifies users in Zammad who need updates based on Entra data. Adds new users from Entra to Zammad. Deactivates users in Zammad if they are no longer in Entra. Key Features Dynamic Matching**: Compares contacts from Entra with existing Zammad users based on email and updates records accordingly. Efficient Management**: Automatically creates, updates, or deactivates Zammad users based on their status in Entra. Custom Fields**: Supports custom field mapping, ensuring enriched user profiles in Zammad. Setup Instructions Microsoft Entra Integration: Ensure proper API permissions for accessing Entra contacts. Configure Microsoft OAuth2 credentials in n8n. Zammad Integration: Set up Zammad API credentials with appropriate access rights. Customize the workflow to include additional fields or map existing fields as needed. Run Workflow: Trigger the workflow manually or set up an automation schedule (e.g., daily sync). Review created/updated/deactivated users in Zammad. Use Cases IT Administration**: Keep your support system in sync with the organization‚Äôs Entra data. Customer Management**: Ensure accurate and up-to-date user records in Zammad. Prerequisites Access to an Entra (Azure AD) environment with contacts data. A Zammad instance with API credentials for user management. A custom field in Zammad User Object (entra_key) of type String. A custom field in Zammad User Object (entra_object_type) of type `Single selection field with two key value pairs user = User contact = Contact` This workflow is fully customizable and can be adapted to your organization‚Äôs specific needs. Save time and reduce manual errors by automating your user sync process with this template! If you have found an error or have any suggestions, please report them here on Github.",259,2024-12-01 13:44:32.857000+00:00,False,2
5142,AI-Powered Email Replies with Spam Filtering & FAQ Lookup using GPT-4o mini & Pinecone,"This n8n workflow automates the handling of incoming emails. It detects and filters out spam, searches a knowledge base (FAQ) stored in a Pinecone vector database, and sends a reply using Gmail ‚Äî all powered by an AI model (GPT-4o mini). How It Works Receiving Emails The Gmail Trigger node checks a Gmail inbox every hour. When a new email arrives, it starts the workflow. Fetching Full Email Content The get_message node retrieves all the details of the message: sender, subject, text, message ID, etc. Spam Filtering The Spam checker node uses GPT-4o mini to classify the email as either ""spam"" or ""no spam"". It detects not only classic spam but also automated messages (e.g. from Google or Microsoft). If marked as ""spam"", the workflow ends and nothing is processed. Conditional Filter The If node checks the spam result. Only ""no spam"" emails proceed to the AI Agent. AI-Based Reply The AI Agent node generates a response based on: The email content A system prompt defining the assistant‚Äôs behavior (polite, professional, under the name ‚ÄúTotal AI Solutions‚Äù) Information retrieved from the Pinecone Vector Store, which contains FAQs The AI is instructed to always check the vector store before replying. The AI prepares both the subject and the body of the reply. Sending the Reply The Gmail node sends the reply to the original sender. It uses the original email's ID to keep the thread intact. Language Model The OpenAI Chat Model node provides GPT-4o mini as the language engine for generating responses. Memory Support The Simple Memory node maintains short-term context, helpful in multi-turn conversations. Knowledge Base (FAQ) The Pinecone Vector Store node connects to a Pinecone index (faqmattabott) containing vectorized FAQ content. Vectors are created using the Embeddings OpenAI node.",255,2025-06-23 09:18:25.356000+00:00,True,7
5217,Real-Time Bitcoin Price Alerts with Bright Data & n8n,"Description This workflow monitors Bitcoin prices across multiple exchanges and sends you alerts when significant price drops occur. It helps crypto traders and investors identify buying opportunities without constantly watching the markets. Overview This workflow monitors Bitcoin prices across multiple exchanges and sends you alerts when significant price drops occur. It uses Bright Data to scrape real-time price data and can be configured to notify you through various channels. Tools Used n8n:** The automation platform that orchestrates the workflow. Bright Data:** For scraping cryptocurrency exchange data without getting blocked. Notification Services:** Email, SMS, Telegram, or other messaging platforms. How to Install Import the Workflow: Download the .json file and import it into your n8n instance. Configure Bright Data: Add your Bright Data credentials to the Bright Data node. Set Up Notifications: Configure your preferred notification method. Customize: Set your price thresholds, monitoring frequency, and which exchanges to track. Use Cases Crypto Traders:** Get notified of buying opportunities during price dips. Investors:** Monitor your crypto investments and make informed decisions. Financial Analysts:** Track Bitcoin price movements for market analysis. Connect with Me Website:** https://www.nofluff.online YouTube:** https://www.youtube.com/@YaronBeen/videos LinkedIn:** https://www.linkedin.com/in/yaronbeen/ Get Bright Data:** https://get.brightdata.com/1tndi4600b25 (Using this link supports my free workflows with a small commission) #n8n #automation #bitcoin #cryptocurrency #brightdata #pricealerts #cryptotrading #bitcoinalerts #cryptoalerts #cryptomonitoring #n8nworkflow #workflow #nocode #cryptoinvesting #bitcoinprice #cryptomarket #tradingalerts #cryptotools #bitcointrading #pricemonitoring #cryptoautomation #bitcoininvestment #cryptotracker #marketalerts #tradingopportunities #cryptoprices",248,2025-06-24 15:54:04.538000+00:00,False,5
3282,Create Notes and Comments on Any Odoo Model Record,"Introduction This workflow is designed to create and attach notes or comments to any record in your Odoo instance. It acts as a sub-workflow that can be triggered by a main workflow to log messages or comments in a centralized manner. By leveraging the powerful Odoo API, this template ensures that updates to records are handled efficiently, providing an organized way to document important information related to your business processes. Setup Instructions Import the Workflow: Import the provided JSON file into your n8n instance. Odoo Credentials: Ensure you have valid Odoo API credentials (e.g., ""Roodsys Odoo Automation Account"") configured in n8n. Node Configuration: Verify that the ""Odoo"" node (consider renaming it to ""Odoo Record Manager"" for clarity) is set up with your server details and authentication parameters. Check that the workflow trigger (""When Executed by Another Workflow"") is configured to receive input parameters from the parent workflow. Execution Trigger: This workflow is designed to be initiated by another workflow. Make sure the main workflow supplies the required inputs. Workflow Details Trigger Node: The workflow begins with the ""When Executed by Another Workflow"" node, which accepts three inputs: rec_id: A numeric identifier for the Odoo record. message: The text of the comment or note. model: The specific Odoo model (e.g., rs.deployment.action.log) where the note should be attached. Odoo Node: The second node in the workflow calls the Odoo API to create a new log message. It maps the inputs as follows: message_type is set to ""comment"". model is assigned the provided model name. res_id is assigned the record ID (rec_id). body is assigned the message content. Additional Information: A sticky note node is included to provide a brief overview of the workflow‚Äôs purpose directly within the interface. Input Parameters Record ID (rec_id): The unique identifier of the record in Odoo where the note will be added. Message (message): The content of the comment or note that is to be logged. Model (model): The Odoo model name indicating the context in which the note should be created (e.g., rs.deployment.action.log). Usage Examples Internal Logging: Use the workflow to attach internal comments or logs to specific records, such as customer profiles, orders, or deployment logs. Audit Trails: Create a comprehensive audit trail by documenting changes or important events in Odoo records. Integration with Other Workflows: Link this workflow with other automation processes in n8n (like email notifications, data synchronization, or reporting) to create a seamless integration across your systems. Pre-conditions The Odoo instance must be accessible and correctly configured. API permissions and user roles should be validated to ensure that the workflow has the necessary access rights. The workflow expects inputs from an external trigger or parent workflow. Customization & Integration This template offers several customization options to tailor it to your needs: Field Customization: Modify or add new fields to match your logging or commenting requirements. Node Renaming: Rename nodes for better clarity and consistency within your workflow ecosystem. Integration Possibilities: Easily integrate this workflow with other processes in n8n, such as triggering notifications or synchronizing data across different systems. This sub-workflow receives data from a main workflow (for example, a record ID, a message, and the Odoo model) and creates a new note (or comment) in the corresponding Odoo record. Essentially, it acts as a centralized point for logging comments or notes in a specific Odoo model, ensuring that the information remains organized and easy to track. Your model must inherit from _inherit = ['portal.mixin', 'mail.thread.main.attachment']",234,2025-03-21 17:49:37.686000+00:00,False,1
5166,Sync Leads from Webflow to Pipedrive CRM Using n8n,"This n8n template automates the process of capturing leads from Webflow form submissions and syncing them with your Pipedrive CRM. It ensures that each submission is accurately associated with the correct organization and contact in Pipedrive, streamlining lead management and minimizing duplicates. Use cases include: Sales teams that want to automate CRM data entry, marketing teams capturing qualified leads from landing pages, or any business looking to improve their Webflow-to-CRM integration workflow. Good to know The workflow assumes that Webflow form submissions include the lead‚Äôs email address. The domain is extracted from the email to match or create the organization in Pipedrive. This template does not handle lead scoring or enrichment, but can be extended for such use-cases. How it works Extract website from email The email is split to extract the domain (e.g., jane@company.com ‚Üí company.com), which is used to search for existing organizations. Check if the organization exists The Pipedrive API is queried using the domain. If the organization exists, we proceed. If not, a new organization is created. Check if the person exists -- If the person already exists in Pipedrive, a note is added to their activities to log the form submission. -- If the person does not exist, a new person is created, a note is added to the person, and a new lead is created. (Optional) Add your own actions You can extend this workflow to trigger Slack notifications, email follow-ups, or internal dashboards. How to use Start with the manual trigger node, or replace it with a webhook to connect directly to Webflow form submissions in real-time. Requirements Webflow form integration (via webhook or other method) Pipedrive account and API key Customising this workflow You can add enrichment services to auto-fill job titles or LinkedIn profiles. Perfect for growing sales pipelines without manual CRM input.",230,2025-06-23 19:40:33.279000+00:00,False,2
5379,"Create AI-Generated Music Playlists for YouTube using Suno, GPT-4, Runway & Creatomate","Automated AI Playlist Creator - Complete Music Production Pipeline Demo Youtube Video: https://www.youtube.com/watch?v=B_RDqM0kcgc Overview Transform your music creation process with this comprehensive 11-workflow automation system that generates complete music playlists from concept to YouTube publication. Using advanced AI services, this template creates original songs, lyrics, cover art, animated videos, and handles the entire publishing pipeline automatically. What This Template Does AI-Powered Song Creation**: Generates 20 unique song ideas based on your specifications Lyric Generation**: Creates custom lyrics for each song using advanced AI Music Production**: Produces high-quality audio tracks with AI music generation Visual Content**: Creates cover art and animated cover videos Video Compilation**: Renders complete playlist videos with all songs Auto-Publishing**: Uploads finished videos directly to YouTube Smart Selection**: Allows manual curation from multiple AI-generated versions Workflow Architecture Core Workflows (11 Total) Playlist Details Setup - Telegram bot interface for configuration Database & Drive Configuration - Automated folder structure and data management Song Ideas Generator - AI-powered creative song concept generation Lyrics & Task Generation (Batch 1 & 2) - Parallel lyric creation and task management Audio Fetch & Drive Upload - Retrieves generated music and organizes files Song Selection Migration - Handles user curation of AI-generated alternatives Cover Image Generation - AI-powered visual artwork creation Cover Animation - Dynamic video cover creation Video Rendering (20 Songs) - Full playlist compilation Video Rendering (15 Songs) - Alternative stable rendering option YouTube Upload & Publishing - Automated social media publishing APIs & Services Used AI & Machine Learning OpenAI GPT-4.1** - Conversational AI, lyrics generation, image creation Suno API** - High-quality AI music generation Runway ML** - AI-powered video animation and effects Media & Storage Google Sheets** - Database and workflow management Google Drive** - File storage and organization via Google Apps Script ImageBB** - Image hosting and CDN services Creatomate** - Professional video rendering and compilation Communication & Publishing Telegram Bot API** - User interface and workflow control YouTube Data API** - Automated video publishing SerpAPI** - Optional web search for creative inspiration Key Features Intelligent Automation Conversational Setup**: Natural language playlist configuration via Telegram Batch Processing**: Efficient handling of multiple songs simultaneously Quality Control**: Multiple AI-generated versions with manual selection Error Recovery**: Robust workflow design with alternative processing paths Status Tracking**: Real-time progress monitoring through Google Sheets Professional Output High-Quality Audio**: Professional-grade AI music generation Custom Visuals**: Unique cover art and animated videos for each playlist Optimized Rendering**: Smart video compilation with duration management Platform-Ready**: Direct YouTube integration with metadata handling Flexibility & Customization Adjustable AI Prompts**: Customize creativity and style parameters Multiple Rendering Options**: Choose between 15 or 20 song compilations Genre Versatility**: Works across all music genres and styles Scalable Architecture**: Easily handle multiple playlists simultaneously Cost Breakdown (Per Playlist) API Service Costs Suno API**: $2.40 (480 credits for 20 songs with 2 versions each) OpenAI Services**: $0.50-$2.00 (varies by conversation length and customizations) Runway ML**: $0.50 (10-second animated cover video) Creatomate**: $0.20-$0.27 (within monthly plan of $45-$54/month) ImageBB**: Free (within standard limits) Total Estimated Cost: $3.60-$5.17 per complete playlist Additional services (Google, YouTube, Telegram) operate within free tiers for normal usage Technical Requirements Platform Compatibility N8N Instance**: Self-hosted or cloud deployment Google Workspace**: Sheets and Drive integration API Access**: Valid keys for all integrated services Setup Complexity Beginner-Friendly**: Detailed documentation with step-by-step instructions Template Database**: Pre-configured Google Sheets structure included Guided Configuration**: Clear setup process for all API integrations Use Cases Content Creators Music channels and playlists Background music for videos Themed compilations and mixes Business Applications Brand-specific playlists Marketing campaign soundtracks Event and presentation music Personal Projects Custom playlists for special occasions Experimental music creation Learning AI music production What's Included Complete Workflow Package 11 pre-configured N8N workflows Google Sheets database template Custom Google Apps Script for file management Creatomate video template (JSON) Comprehensive setup documentation Support Resources Step-by-step API configuration guides Troubleshooting documentation Customization instructions for AI prompts Performance optimization guidelines Performance & Reliability Processing Capacity Concurrent Processing**: Multiple songs generated simultaneously Batch Operations**: Efficient API usage with rate limit management Alternative Pathways**: Backup workflows for enhanced reliability Quality Assurance Multi-Version Generation**: Compare and select best AI outputs Duration Management**: Smart handling of video length limitations Error Handling**: Comprehensive workflow error recovery This template represents a complete AI-powered music production pipeline, transforming simple text descriptions into professional playlist videos ready for publication. Perfect for content creators, musicians, and businesses looking to automate their music content creation workflow. After the Purchase in Gumroad, you will have access to all the workflows JSON files, creatomate template, google script gs code and comprehensive documentation guide. Reach out to me via email at joseph@uppfy.com incase you want a custom workflow.",212,2025-06-26 16:11:09.654000+00:00,True,12
5383,"Automate Viral Content Creation with OpenAI, ElevenLabs, and Fal.ai for Videos, Podcasts, and ASMR","üéØ Create viral TikToks, Shorts, Reels, podcasts, and ASMR videos in minutes ‚Äî all on autopilot. This is not just a workflow bundle. It's a full content creation system designed for modern creators and growth marketers. Built with n8n + AI tools, you‚Äôll automate your ideation, scripting, and even audio/video generation in just a few clicks. üéÅ What's Included? üé¨ 1. Video Analyzer ‚Äì Reverse Engineer Any Viral Video Extract topics, emotions, titles, and turn any video into a content roadmap. Great for creators, editors, and trend researchers. üß† 2. 30 Days of Viral Content Generator Instantly get 30 viral post ideas with AI-written hooks, scripts, CTAs ‚Äî all tailored to your niche. üé• 3. VEO3 Viral Video Generator Fully automate video creation in VEO3-style ‚Äî like the famous monkey vlogs. It‚Äôs storytelling + AI + automation. üçº 4. AI Baby Podcast Generator Generate quirky podcast episodes with unique baby-style voiceover using ElevenLabs and script powered by OpenAI. üî™ 5. ASMR Video Idea & Script Factory Get ASMR video ideas, visual guides, and scripts ‚Äî ready for video tools like CapCut or Runway. üîß Integrations Required &gt; ‚úÖ Don‚Äôt worry ‚Äî setup takes less than 10 minutes, even if you‚Äôre not a developer. üß† OpenAI ‚Äì for content generation üß¨ Fal.ai ‚Äì for visual assets (images/video generation) üß™ Hedra ‚Äì for advanced media workflows üéôÔ∏è ElevenLabs ‚Äì for ultra-realistic voiceovers üí° Why This Pack? üî• Based on Real Trends ‚Äì Inspired by content that‚Äôs already viral üöÄ All-in-One Bundle ‚Äì 5 powerful workflows, ready to run üß∞ Zero-Code Setup ‚Äì Just import, plug in your API keys, and run üõ†Ô∏è Customizable ‚Äì Modify prompts, tone, or niche in seconds üîç Keywords to Help You Find This #viralcontent ¬∑ #tiktokautomation ¬∑ #AIcontentcreator ¬∑ #podcastAI ¬∑ #asmrvideo ¬∑ #veo3 ¬∑ #openai ¬∑ #n8nworkflow ¬∑ #socialmediaautomation ¬∑ #nocodetools üõí Who‚Äôs It For? Content creators & influencers Social media managers Startup founders & solopreneurs AI content studios Anyone who wants to go viral without burning out ‚öôÔ∏è Setup Guide &gt; üì¶ Step 1: Import the .json workflows into n8n &gt; üîê Step 2: Add your API credentials (OpenAI, Fal.ai, ElevenLabs...) &gt; üöÄ Step 3: Run the workflow & customize your outputs üö® Note This complete automation pack is available now for just $29. After the first 7 days, the price will increase to $99 as more features and updates are added. üëâ Grab it now while it‚Äôs still in early access ‚Äî and get free future updates included. üéâ Automate your way to virality ‚Äî start today.",206,2025-06-26 16:50:39.520000+00:00,True,10
5416,Scrape AI News from Multiple Sources to Markdown & Google Drive with RSS.app,"AI News Scraping System This n8n workflow automates the process of pulling in breaking AI-related headlines from curated RSS feeds, scraping their full content, and saving readable Markdown versions directly to Google Drive. Use cases include: Creating a personal newsletter curation system Automating blog post research workflows Archiving news content for later summarization or AI use How it Works Scheduled Triggers The workflow runs every 3‚Äì4 hours using multiple Schedule Trigger nodes. Each trigger targets a different news source (e.g., Google News, OpenAI Blog, Hugging Face, etc.). Fetch and Parse Feeds RSS feeds are fetched via the HTTP Request node. Items from the feed are split into individual entries using the Split Out node. Scrape Article Content Each article URL is sent to the Firecrawl API with a prompt to extract only the main content in Markdown. The scraping skips navigation, headers, footers, and ads. Convert and Save The extracted Markdown is converted into a .md file using the Convert to File node. The file is then uploaded to a Google Drive folder. Good to Know This workflow uses the Firecrawl API for web scraping. Be sure to configure a Generic HTTP Header credential with your API key. Output files are saved in Markdown format You can add more Schedule Trigger + HTTP Request pairs to extend this workflow to additional feeds. Requirements Firecrawl API account for scraping Google Drive account (OAuth2 credentials must be configured in n8n) n8n instance (self-hosted or cloud) Customization Ideas Replace or extend RSS feeds with sources relevant to your niche Load up scraped news stories into a prompt to create new content like TikToks and Reels Add a summarization step using an LLM like GPT or Claude Send the Markdown files to Notion, Slack, or a blog CMS Example Feeds | Feed Name | URL | |------------------|----------------------------------------------------------------------| | Google News (AI) | https://rss.app/feeds/v1.1/AkOariu1C7YyUUMv.json | | OpenAI Blog | https://rss.app/feeds/v1.1/xNVg2hbY14Z7Gpva.json | | Hugging Face | https://rss.app/feeds/v1.1/sgHcE2ehHQMTWhrL.json |",191,2025-06-27 19:09:44.689000+00:00,True,2
6307,Google Maps Lead Generation with Apify & Email Extraction for Airtable,"üß† What It Does This n8n workflow collects leads from Google Maps, scrapes their websites via direct HTTP requests, and extracts valid email addresses ‚Äî all while mimicking real user behavior to improve scraping reliability. It rotates User-Agent headers, introduces randomized delays, and refines URLs by removing only query parameters and fragments to preserve valid page paths (like social media links). The workflow blends Apify actors, raw HTTP requests, HTML-to-Markdown conversion, and smart email extraction to deliver clean, actionable lead data ‚Äî ready to be sent to Airtable, Google Sheets, or any CRM. Perfect for lean, scalable B2B lead generation using n8n‚Äôs native logic and no external scrapers. üí°Why this workflow Modest lead scrapers rely on heavy tools or APIs like Firecrawl. This workflow: Uses lightweight HTTP requests (with randomized user-agents) to scrape websites. Adds natural wait times to avoid rate limits and IP bans. Avoid full-page crawlers, yet still pulls emails effectively. Works great for freelancers, marketers, or teams targeting niche B2B leads. Designed for stealth and resilience. üë§ Who it‚Äôs for Lead generation freelancers or consultants. B2B marketers looking to extract real contact info. Small businesses doing targeted outreach. Developers who want a fast, low-footprint scraper. Anyone who wants email + website leads from Google Maps. ‚öôÔ∏è How It Works 1. üì• Form Submission (Lead Input) A Form Trigger collects: Keyword Location No. of Leads (defaults to 10) This makes the workflow dynamic and user-friendly ‚Äî ready for multiple use cases and teams. 2. üìä Scrape Business Info (via Apify) Apify‚Äôs Google Maps Actor searches for matching businesses. The Dataset Node fetches all relevant business details. A Set Node parses key fields like name, phone, website, and category. A Limit Node ensures the workflow only processes the desired number of leads. 3. üîÅ First Loop ‚Äì Visit & Scrape Website Each business website is processed in a loop. A Code Node cleans the website URL by removing only query parameters/fragments ‚Äî keeping full paths like /contact. A HTTP Request Node fetches the raw HTML of the site: Uses randomized User-Agent headers (5 variants) to mimic real devices and browsers. This makes requests appear more human and reduces the risk of detection or blocking. HTML is converted to Markdown using the Markdown Node, making it easier to scan for text patterns. A Wait Node introduces a random delay between 2-7 seconds: Helps avoid triggering rate limits, Reduces likelihood of being flagged as a bot. A Merge Node combines scraped markdown + lead info for use in the second loop. 4. üîÅ Second Loop ‚Äì Extract Emails In this second loop, the markdown data is processed. A Code Node applies regex to extract the first valid email address. If no email is found, ""N/A"" is returned. A brief 1 second Wait Node simulates realistic browsing time. Another Merge Node attaches the email result to the original lead data. 5. ‚úÖ Filter, Clean & Store A Filter Node removes all entries with ""N/A"" or invalid email results. A Set Node ensures only required fields (like website, email, and company name) are passed forward. The clean leads are saved to Airtable (or optionally, Google Sheets) using an upsert-style insert to avoid duplicates. üõ°Ô∏è Anti-Flagging Design This workflow is optimized for stealth: No scraping tools or headless browsers (like Puppeteer or Firecrawl). Direct HTTP requests with rotating User-Agents. Randomized wait intervals (2-7s). Only non-intrusive parsing ‚Äî no automation footprints. üõ† How to Set It Up Open n8n (Cloud or Self-Hosted). Install Apify node search Apify and click on Install. Do this before importing your file. Import the provided .json file into your n8n editor. Set up the required credentials: üîë Apify API Key** (used for Google Maps scraping) üîë Airtable API Key** (or connect Google Sheets instead) Recommended Prepare your Airtable base or Google Sheet with fields like: Email, Website, Phone, Company Name. Review the Set node if you'd like to collect more fields from Apify (e.g., Ratings, Categories, etc.). üîÅ Customization Tips The Apify scraper returns rich business data. By default, this workflow collects name, phone, and website ‚Äî but you can add more in the ""Grab Desired Fields"" node. Need safer scraping at scale? Swap the HTTP Request for Firecrawl‚Äôs Single URL scraper (or any headless service like Browserless, Oxylabs, Bright Date, or ScrapingBee) ‚Äî they handle rendering and IP rotation. Want to extract from internal pages (like /contact or /about)? Use Firecrawl‚Äôs async crawl mode ‚Äî just note it takes longer. For speed and efficiency, this built-in HTTP + Markdown setup is usually the fastest way to grab emails.",189,2025-07-23 06:50:51.634000+00:00,False,3
5419,Process Multiple Files with Forms: A Tutorial on Binary Data and Loops,"Let a user load multiple files with a Form node, and process the binary data. A very important workflow for many tools. This is a learning example of several core concepts that are hard to grasp in n8n: $binary data Loop and $runIndex Split Out The Save File deomonstrates how to access the binary data correctly, but could be swapped to POST the files to an AI, for example.",185,2025-06-28 01:17:18.053000+00:00,False,0
5421,AI-Powered Lead Enrichment with Explorium MCP & Telegram,"ü§ñ AI-Powered Lead Enrichment with Explorium MCP & Telegram Who it's for Sales reps, agencies, and growth teams who want to turn basic company info into qualified leads with automated research . Perfect for B2B prospecting. What it does This workflow lets you send a company name or domain via Telegram, and instantly returns: ‚úÖ Enriched company profile (industry, size, tech, pain points) ‚úÖ A clean, structured JSON ‚Äî ready for your CRM or sales tools How it works üí¨ Send company info to your Telegram bot üîé Workflow pulls data from Explorium MCP + Tavily üß† AI analyzes model, tools, pain points & goals üì§ JSON response sent back via Telegram or logged to your database Requirements üîê OpenAI API (GPT-4) üß† Explorium MCP API üåê Tavily Web Search API ü§ñ Telegram Bot API üóÉÔ∏è PostgreSQL (for memory/logging) How to set up Add API keys in n8n Connect Telegram bot to webhook Set up PostgreSQL for memory persistence Customize prompts (tone, niche, etc.) Test by sending a company name via Telegram Customization Options üéØ Focus enrichment on specific industries or keywords üí¨ Adjust the email sequence structure & style üß© Add extra data sources (e.g. Clearbit, Crunchbase) üßæ Format JSON to match your CRM schema ‚öôÔ∏è Add approval step before sending emails Highlights ‚úÖ Uses multi-source enrichment ‚úÖ Works 100% from Telegram ‚úÖ Integrates into any sales pipeline",181,2025-06-28 02:07:14.053000+00:00,True,5
7401,"Automated Phone Interview Evaluation with Vapi, GPT-4o & Google Sheets","This n8n workflow template automatically processes phone interview transcripts using AI to evaluate candidates against specific criteria and saves the results to Google Sheets. Perfect for HR departments, recruitment agencies, or any business conducting phone screenings. What This Workflow Does This automated workflow: Receives phone interview transcripts via webhook Uses OpenAI GPT models to analyze candidate responses against predefined qualification criteria Extracts key information (name, phone, location, qualification status) Automatically saves structured results to a Google Sheet for easy review and follow-up The workflow is specifically designed for driving job interviews but can be easily adapted for any position with custom evaluation criteria. Tools & Services Used N8N** - Workflow automation platform OpenAI API** - AI-powered transcript analysis (GPT-4o-mini) Google Sheets** - Data storage and management Webhook** - Receiving transcript data Prerequisites Before implementing this workflow, you'll need: N8N Instance - Self-hosted or cloud version OpenAI API Account - For AI transcript processing Google Account - For Google Sheets integration Phone Interview System - That can send webhooks (like Vapi.ai) Step-by-Step Setup Instructions Step 1: Set Up OpenAI API Access Visit OpenAI's API platform Create an account or log in Navigate to API Keys section Generate a new API key Copy and securely store your API key Step 2: Create Your Google Sheet Option 1: Use Our Pre-Made Template (Recommended) Copy our template: Driver Interview Results Template Click ""File"" ‚Üí ""Make a copy"" to create your own version Rename it as desired Copy your new sheet's URL - you'll need this for the workflow Option 2: Create From Scratch Go to Google Sheets Create a new spreadsheet Name it ""Driver Interview Results"" (or your preferred name) Set up the following column headers in row 1: A1: name B1: phone C1: cityState D1: qualifies E1: reasoning Copy the Google Sheet URL - you'll need this for the workflow Step 3: Import and Configure the N8N Workflow Import the Workflow Copy the workflow JSON from the template In your N8N instance, go to Workflows ‚Üí Import from JSON Paste the JSON and import Configure OpenAI Credentials Click on either ""OpenAI Chat Model"" node Set up credentials using your OpenAI API key Test the connection to ensure it works Configure Google Sheets Integration Click on the ""Save to Google Sheets"" node Set up Google Sheets OAuth2 credentials Select your spreadsheet from the dropdown Choose the correct sheet (usually ""Sheet1"") Update the Webhook Click on the ""Webhook"" node Note the webhook URL that n8n generates This URL will receive your transcript data Step 4: Customize Evaluation Criteria The workflow includes predefined criteria for a Massachusetts driving job. To customize for your needs: Click on the ""Evaluate Candidate"" node Modify the system message to include your specific requirements Update the evaluation criteria checklist Adjust the JSON output format if needed Current Evaluation Criteria: Valid Massachusetts driver's license No felony convictions Clean driving record (no recent tickets/accidents) Willing to complete background check Can pass drug test (including marijuana) Available full-time Monday-Friday Lives in Massachusetts Step 5: Connect to Vapi.ai (Phone Interview System) This workflow is specifically designed to work with Vapi.ai's phone interview system. Here's how to connect it: Setting Up the Vapi Integration Copy Your N8N Webhook URL In your n8n workflow, click on the ""Webhook"" node Copy the webhook URL (it should look like: https://your-n8n-instance.com/webhook-test/351ffe7c-69f2-4657-b593-c848d59205c0) Configure Your Vapi Assistant Log into your Vapi.ai dashboard Create or edit your phone interview assistant In the assistant settings, find the ""Server"" section Set the Server URL to your n8n webhook URL Set timeout to 20 seconds (as configured in the workflow) Configure Server Messages In your Vapi assistant settings, enable these server messages: end-of-call-report transcript[transcriptType=""final""] Set Up the Interview Script Use the provided interview script in your Vapi assistant (found in the workflow's system message) This ensures consistent data collection for the AI evaluation Expected Data Format from Vapi The workflow expects Vapi to send data in this specific format: { ""body"": { ""message"": { ""artifact"": { ""transcript"": ""AI: Hi. Are you interested in driving for Bank of Transport?\nUser: Yes.\nAI: Great. Before we go further..."" } } } } Vapi Configuration Checklist ‚úÖ Webhook URL set in Vapi assistant server settings ‚úÖ Server messages enabled: end-of-call-report, transcript[transcriptType=""final""] ‚úÖ Interview script configured in assistant ‚úÖ Assistant set to send webhooks on call completion Alternative Phone Systems If you're not using Vapi.ai, you can adapt this workflow for other phone systems by: Modifying the ""Edit Fields2"" node to extract transcripts from your system's data format Updating the webhook data structure expectations Ensuring your phone system sends the complete interview transcript Step 6: Test the Workflow Test with Sample Data Use the ""Execute Workflow"" button with test data Verify that data appears correctly in your Google Sheet Check that the AI evaluation logic works as expected End-to-End Testing Send a test webhook with a real transcript Monitor each step of the workflow Confirm the final result is saved to Google Sheets Workflow Node Breakdown Webhook - Receives transcript data from your phone system Edit Fields2 - Extracts the transcript from the incoming data Evaluate Candidate - AI analysis using GPT-4o-mini to assess qualification Convert to JSON - Ensures proper JSON formatting with structured output parser Save to Google Sheets - Automatically logs results to your spreadsheet Customization Options Modify Evaluation Criteria Edit the system prompt in the ""Evaluate Candidate"" node Add or remove qualification requirements Adjust the scoring logic Change Output Format Modify the JSON schema in the ""Structured Output Parser"" node Update Google Sheets column mapping accordingly Add Additional Processing Insert nodes for email notifications Add Slack/Discord alerts for qualified candidates Integrate with your CRM or ATS system Troubleshooting Common Issues: OpenAI API Errors**: Check API key validity and billing status Google Sheets Not Updating**: Verify OAuth permissions and sheet access Webhook Not Receiving Data**: Confirm URL and POST format from your phone system AI Evaluation Inconsistencies**: Refine the system prompt with more specific criteria Usage Tips Monitor Token Usage**: OpenAI charges per token, so monitor your usage Regular Review**: Periodically review AI evaluations for accuracy Backup Data**: Export Google Sheets data regularly for backup Privacy Compliance**: Ensure transcript handling complies with local privacy laws Need Help with Implementation? For professional setup, customization, or troubleshooting of this workflow, contact: Robert - Ynteractive Solutions Email**: rbreen@ynteractive.com Website**: www.ynteractive.com LinkedIn**: linkedin.com/in/robert-interactive Specializing in AI-powered workflow automation, business process optimization, and custom integration solutions.",173,2025-08-14 21:51:28.556000+00:00,True,4
5393,"Convert Form Inputs to Cinematic Videos with GPT-4, Dumpling AI & ElevenLabs Audio","üìΩÔ∏è What this workflow does This workflow turns a user-submitted form with country or animal names into a cinematic video with animated scenes and immersive ambient audio. Using GPT-4 for prompt generation, Dumpling AI for visual creation,& Replicate for motion animation, ElevenLabs for sound generation, and Creatomate for video stitching, it fully automates video production ‚Äî from raw idea to rendered file. üéØ What problem is this solving? Creating engaging multimedia content can take hours. This workflow automates the entire process of ideation, design, and rendering of high-quality cinematic clips, eliminating the need for manual video editing or audio production. üë• Who is this for? Content creators and educators Digital artists and storytellers Marketers or YouTubers creating short-form visual content No-code/AI automation enthusiasts ‚öôÔ∏è Setup Instructions ‚úÖ Step 1: Google Sheet Create a Google Sheet with two columns: Title Generated videos Update the Sheet ID and tab name in the final node. ‚úÖ Step 2: Google Drive Create two folders: One for ambient audio tracks One for final generated videos Update the folder IDs in both Google Drive nodes. ‚úÖ Step 3: Credentials Setup Make sure all your API tokens are saved as credentials in n8n. This workflow uses the following integrations: OpenAI (GPT-4) Dumpling AI (via HTTP header) Replicate.com ElevenLabs Google Drive Google Sheets Creatomate ‚úÖ Step 4: Form Fields Ensure your trigger form includes these fields: Title Country 1, Country 2, Country 3, Country 4 Style (e.g., cinematic, epic, fantasy, noir, etc.) üß© How it works User Form Submission Kicks off the workflow with the required inputs. Format Inputs Combines all 4 countries/animals into a single array. GPT-4: Generate Visual Prompts Uses GPT-4 to create rich cinematic descriptions per animal/country. Dumpling AI: Create Images Each description becomes a high-quality visual. GPT-4: Create Motion Prompts Each image prompt is rewritten into motion-based video prompts. Replicate: Animate Prompts and images are sent to Replicate‚Äôs model for animation. GPT-4: Generate Sound Prompt Based on the style, GPT-4 creates an ambient sound idea. ElevenLabs: Create Ambient Audio Audio is generated and uploaded to Google Drive. Creatomate: Stitch All Media All 4 motion videos and the audio track are stitched into one cinematic output. Upload to Google Drive + Log to Sheet Final video is saved in Drive and logged in Sheets with its title and link. üõ†Ô∏è How to Customize üé® Modify GPT prompts for different themes (e.g., horror, fantasy, sci-fi). üß† Swap animals for characters, objects, or locations. üéß Replace ambient sound with ElevenLabs voiceovers or music. üìÇ Add metadata logging (generation time, duration, tags). üß™ Try using alternative video tools like Pika Labs or Runway ML. ‚úÖ Requirements n8n self-hosted or cloud instance Active accounts for: OpenAI, Dumpling AI, Replicate, ElevenLabs, Creatomate Google credentials set up for Drive + Sheets This is a perfect end-to-end automation that showcases the power of AI + automation for video storytelling.",161,2025-06-26 21:22:04.127000+00:00,True,5
10067,Automate Video Story Publishing to Facebook Page using Google Drive and Sheets,"Automated Facebook Page Story Video Publisher (Google Drive ‚Üí Facebook ‚Üí Google Sheet) &gt; Recommended: Self-hosted via tino.vn/vps-n8n?affid=388 ‚Äî use code VPSN8N for up to 39% off. This workflow is an automated solution for publishing video content from Google Drive to your Facebook Page Stories, while using Google Sheets as a posting queue manager. What This Workflow Does (Workflow Function) This automation orchestrates a complete multi-step process for uploading and publishing videos to Facebook Stories: Queue Management: Every 2 hours and 30 minutes, the workflow checks a Google Sheet (Get Row Sheet node) to find the first video whose Stories column is empty ‚Äî meaning it hasn‚Äôt been posted yet. Conditional Execution: An If node confirms that the video‚Äôs File ID exists before proceeding. Video Retrieval: Using the File ID, the workflow downloads the video from Google Drive (Google Drive node) and calculates its binary size (Set to the total size in bytes node). Facebook 3-Step Upload: It performs the Facebook Graph API‚Äôs three-step upload process through HTTP Request nodes: Step 1 ‚Äì Initialize Session: Starts an upload session and retrieves the upload_url and video_id. Step 2 ‚Äì Upload File: Uploads the binary video data to the provided upload_url. Step 3 ‚Äì Publish Video: Finalizes and publishes the uploaded video as a Facebook Story. Status Update: Once completed, the workflow updates the same row in Google Sheets (Update upload status in sheet node) using the row_number to mark the video as processed. Prerequisites (What You Need Before Running) 1. n8n Instance &gt; Recommended: Self-hosted via tino.vn/vps-n8n?affid=388 ‚Äî use code VPSN8N for up to 39% off. 2. Google Services Google Drive Credentials:** OAuth2 credentials for Google Drive to let n8n download video files. Google Sheets Credentials:** OAuth2 credentials for Google Sheets to read the posting queue and update statuses. Google Sheet:** A spreadsheet (ID: 1RnE5O06l7W6TLCLKkwEH5Oyl-EZ3OE-Uc3OWFbDohYI) containing: File ID ‚Äî the video‚Äôs unique ID in Google Drive. Stories ‚Äî posting status column (leave empty for pending videos). row_number ‚Äî used for updating the correct row after posting. 3. Facebook Setup Page ID:** Your Facebook Page ID (currently hardcoded as 115432036514099 in the info node). Access Token:* A *Page Access Token** with permissions such as pages_manage_posts and pages_read_engagement. This token is hardcoded in the info node and again in Step 3. Post video. Usage Guide and Implementation Notes How to Use Queue Videos: Add video entries to your Google Sheet. Each entry must include a valid Google Drive File ID. Leave the Stories column empty for videos that haven‚Äôt been posted. Activate: Save and activate the workflow. The Schedule Trigger will automatically handle new uploads every 2 hours and 30 minutes. Implementation Notes ‚ö†Ô∏è Token Security:* Hardcoding your *Access Token* inside the info node is *not recommended**. Tokens expire and expose your Page to risk if leaked. üëâ Action: Replace the static token with a secure Credential setup that supports token rotation. Loop Efficiency:* The *‚Äúfalse‚Äù** output of the If node currently loops back to the Get Row Sheet node. This creates unnecessary cycles if no videos are found. üëâ Action: Disconnect that branch so the workflow stops gracefully when no unposted videos remain. Status Updates:* To prevent re-posting the same video, the final Update upload status in sheet node must update the *Stories** column (e.g., write ""POSTED""). üëâ Action: Add this mapping explicitly to your Google Sheets node. Automated File ID Sync:** This workflow assumes that the Google Sheet already contains valid File IDs. üëâ You can build a secondary workflow (using Schedule Trigger1 ‚Üí Search files and folders ‚Üí Append or update row in sheet) to automatically populate new video File IDs from your Google Drive. ‚úÖ Result Once active, this workflow automatically: pulls pending videos from your Google Sheet, uploads them to Facebook Stories, and marks them as posted ‚Äî all without manual intervention.",153,2025-10-23 02:34:17.108000+00:00,False,4
5390,OpenAI Models Template: GPT-4 and DALL-E Services Overview,"Complete AI Graphic Design Suite with OpenAI, Replicate & Google Drive Categories: AI Agents, Design Automation, Business Tools This workflow creates a complete AI-powered graphic design system that replaces expensive designers with intelligent automation. Featuring a conversational AI agent that orchestrates 5 specialized design tools, this suite can generate logos, style guides, gradients, and revisions on demand. Built by someone who's scaled automation agencies to $72K/month, this system demonstrates how AI agents can deliver real business value beyond simple chatbots. Benefits Complete Design Automation** - Generate logos, style guides, gradients, and revisions through natural conversation Conversational AI Interface** - Chat-based interaction makes design accessible to non-designers Professional Quality Output** - Uses advanced AI models and proven templates for consistent results Instant Delivery** - Generate designs in seconds vs. days of traditional design processes Scalable Business Tool** - Deploy for clients, embed on websites, or use internally Cost-Effective Solution** - Replace $82K/year designers with $30/month automation How It Works AI Agent Orchestration: Central conversational AI that understands design requests in natural language Automatically selects the right tool based on user needs and context Maintains conversation memory for iterative design improvements Provides professional, helpful responses with design expertise Logo Generation System: Creates professional logos using OpenAI's advanced image generation Supports various styles: minimalistic, corporate, creative, and industry-specific Automatically uploads to Google Drive with shareable links Perfect for startups, rebranding projects, and client work Style Guide Creation: Generates comprehensive brand guidelines using template-based approach Includes color palettes, typography, logo usage, and brand elements Uses AI to customize templates with client-specific information Delivers presentation-ready style guides for professional use Gradient Background Generator: Creates beautiful background gradients for websites and marketing materials Uses proven design templates with AI-powered customization Generates multiple variations and color combinations Perfect for landing pages, social media, and brand materials Design Editor & Revision System: Intelligently revises existing designs based on feedback Handles both Google Drive files and external image URLs Maintains design consistency while implementing requested changes Supports iterative improvements and client feedback cycles Advanced Upscaling Integration: Uses Replicate API to enhance image quality up to 4x resolution Professional print-quality output for all generated designs Seamlessly integrates with all design generation tools Perfect for high-resolution marketing materials and presentations Required Setup Configuration OpenAI API Setup: Connect your OpenAI API for: GPT-4 conversation handling and design guidance DALL-E (4o) image generation for all design tools Intelligent prompt processing and tool selection Google Drive Integration: Create template files for style guides and examples Set up OAuth credentials for file management Configure sharing permissions for client access Organize folders for different design categories Replicate API Configuration: Set up account for image upscaling capabilities Replace &lt;your-replicate-api-key-here&gt; with actual API key Configure upscaling factors (2x or 4x options) AI Agent System Message: Configure the agent with business context: You are a helpful, intelligent design assistant. You generate high-quality designs using the provided tools (generate logo, generate style guide, and generate gradient background). Then you can also upscale them, and finally, you can revise them. When you receive an image from a tool, wrap it in nice looking Markdown (atx) format and present it to the user. The only things you can generate are logos, style guides, and gradient backgrounds. Make sure to clarify which (as well as any additional information needed) so the prompt you send to the image model is optimal. If you are asked to adjust or revise an image, ask the user to define their changes as explicitly as possible. Chat Integration Options: Embedded website chat widget for client-facing design services Direct chat interface for internal team use Hosted chat endpoint for external integrations Business Use Cases Design Agencies** - Offer automated design services with instant delivery and unlimited revisions Marketing Teams** - Generate brand assets, social media graphics, and campaign materials on demand Startups** - Create professional branding without expensive design budgets Consultants** - Provide design services as value-added offerings to clients Web Developers** - Offer integrated design services alongside development projects E-commerce Businesses** - Generate product graphics, banners, and promotional materials Revenue Potential This system transforms design service economics: Replace $82K/year designers** with $30/month automation costs Instant delivery advantage** - complete designs in minutes vs. days Unlimited revisions** without additional designer time costs Premium service offering** - charge $1,500-5,000 per client implementation Scalable white-label solution** for agencies and consultants 24/7 availability** for time-sensitive client requests Difficulty Level: Intermediate Estimated Build Time: 2-3 hours Monthly Operating Cost: ~$30 (OpenAI + Replicate APIs) Watch My Complete Build Process Want to see exactly how I built this entire AI design system from scratch? I walk through the complete development process, including AI agent setup, tool integration, and the business strategy behind replacing expensive designers with intelligent automation. üé• Watch My Live Build: ""This AI Agent Replaces an $82k/yr Graphic Designer (N8N)"" This comprehensive tutorial shows the real development approach - including agent design patterns, tool orchestration, and the exact prompting strategies that deliver professional-quality results. Set Up Steps Core AI Agent Configuration: Set up chat trigger with embedded and hosted options Configure OpenAI chat model with design-focused system prompts Add memory buffer for conversation context and design iterations Design Tool Integration: Configure all 5 specialized design workflows as callable tools Set up proper data flow between agent and design generators Test tool selection logic with various design requests Template and Asset Management: Upload design templates to Google Drive for style guide generation Configure file sharing permissions for client access Set up organized folder structure for different design types Quality Control Setup: Test complete design workflows from request to delivery Validate AI output quality across all design categories Optimize prompts and templates based on actual usage patterns Client Integration Options: Embed chat widget on client websites for design services Set up hosted endpoints for external system integration Configure branding and messaging for client-facing interactions Advanced Extensions Scale the system with additional capabilities: Industry-Specific Templates** - Customize design styles for different verticals Brand Consistency Engine** - Maintain design standards across all generated assets Client Portal Integration** - Automated design delivery with approval workflows Multi-Language Support** - Generate designs with international text and cultural considerations Advanced Analytics** - Track design performance and client satisfaction metrics Print Production Tools** - Generate print-ready files with proper color profiles and dimensions Why This System Works The competitive advantage lies in intelligent automation combined with professional quality: Natural conversation interface** eliminates design tool complexity Template-based generation** ensures consistent, professional results Instant iteration capability** allows real-time design improvements Cost advantage** enables competitive pricing while maintaining margins 24/7 availability** provides service levels impossible with human designers Scalable delivery** handles multiple clients simultaneously without quality degradation Check Out My Channel For more advanced AI automation systems that generate real business results, explore my YouTube channel where I share the exact strategies used to build successful automation agencies and scale to $72K+ monthly revenue.",152,2025-06-26 19:46:30.235000+00:00,True,7
5394,Turns Reddit Pain Points into Comic Ads using Dumpling AI and GPT-4o,"üìù Description ü§ñ What this workflow does This workflow turns Reddit pain points into emotionally-driven comic-style ads using AI. It takes in a product description, scrapes Reddit for real user pain points, filters relevant posts using AI, generates ad angles, rewrites them into 4-panel comic prompts, and finally uses Dumpling AI to generate comic-style images. All final creatives are uploaded to Google Drive. üß† What problem is this solving? Crafting ad content that truly speaks to customer struggles is time-consuming. This workflow automates that entire process ‚Äî from pain point discovery to visual creative output ‚Äî using AI and Reddit as a source of truth for customer language. üë§ Who is this for? Copywriters and performance marketers Startup founders and indie hackers Creatives building empathy-driven ad concepts Automation experts looking to generate scroll-stopping content ‚öôÔ∏è Setup Instructions Here‚Äôs how to set everything up, step by step: üîπ 1. Trigger: Form Input Node: üìù Form - Submit Product Info This form asks the user to enter: Brand Name Website Product Description ‚úÖ Make sure this form is active and testable. üîπ 2. Generate Reddit Keyword Node: üß† GPT-4o - Generate Reddit Keyword Uses the product description to generate a search keyword based on what your audience might be discussing on Reddit. üîπ 3. Search Reddit Node: üîç Reddit - Search Posts Uses the keyword to search Reddit for relevant threads. Make sure your Reddit integration is properly configured. üîπ 4. Filter Valid Posts Node: üîé IF - Check Upvotes & Text Length Filters out low-effort or unpopular posts. Only keeps posts with: Minimum 2 upvotes Content at least 100 characters long ‚úÖ You can adjust these thresholds in the node settings. üîπ 5. Clean Reddit Output Node: üßº Code - Structure Reddit Posts This formats the list of posts into clean JSON for the AI agents to process. üîπ 6. Check Relevance with AI Agent Node: ü§î Langchain Agent - Post Relevance Classifier This node uses a LangChain agent (tool: think2) to determine if each post is relevant to your product. Only relevant ones are passed forward. üîπ 7. Aggregate Relevant Posts Node: üì¶ Code - Merge Relevant Posts Collects all relevant posts into a clean format for the next GPT-4 call. üîπ 8. Generate Ad Angles Node: ‚úçÔ∏è GPT-4o - Generate Emotional Ad Angles Writes 10 pain-point-based marketing angles using real customer language. üîπ 9. Rank the Best Angles Node: üìä GPT-4o - Rank Top 10 Angles Scores the generated angles and ranks them from most to least powerful. Only the top 3 are passed forward. üîπ 10. Turn Angles into Comic Prompts Node: üé≠ GPT-4o - Write Comic Scene Prompts Rewrites each of the top ad angles into a 4-panel comic strip structure (pain ‚Üí tension ‚Üí product ‚Üí resolution). üîπ 11. Generate Comic Images Node: üé® Dumpling AI - Generate Comic Panels Sends each prompt to Dumpling AI to create visual comic scenes. üîπ 12. Wait for Image Generation Node: ‚è≥ Wait - Dumpling AI Response Time Adds a delay to give Dumpling AI time to finish generating all images. üîπ 13. Get Final Image URLs Node: üîó Code - Extract Image URLs from Dumpling Response Extracts all image links for preview/download. üîπ 14. Upload to Google Drive Node: ‚òÅÔ∏è Google Drive - Upload Comics Uploads the comic images to your chosen Google Drive folder. ‚úÖ Update this node with your destination folder ID. üîπ 15. Log Final Output Optional You can extend the flow to log the image links, ad angles, and Reddit sources to Google Sheets, Airtable, or Notion depending on your use case. üõ†Ô∏è How to Customize ‚úèÔ∏è Adjust tone: Update GPT-4 system prompts to sound more humorous, emotional, or brand-specific. üßµ Use different styles: Swap Dumpling AI image settings for ink sketch, manga, or cartoon renderings. üîÑ Change input source: Replace Reddit with X (Twitter), Quora, or YouTube comments. üì¶ Store results differently: Swap Google Drive for Notion, Dropbox, or Airtable. This workflow turns real audience struggles into thumb-stopping comic content ‚Äî automatically.",144,2025-06-26 21:48:51.503000+00:00,True,8
5662,Transform Voice Memos into Daily Journals & Tasks with OMI.ME & Gemini AI,"Who‚Äôs it for This template is perfect for OMI pendant users or anyone with AI-generated memory transcripts who want to: Automatically create daily journals in Markdown Extract actionable tasks from conversations Store memories in Google Drive Sync action items to Google Tasks Great for creators, ADHD professionals, techies, or productivity hackers who want to build a second brain workflow with no manual data entry. What it does / How it works This workflow: Accepts POST data from the OMI AI pendant (via webhook) Extracts structured summaries, tasks, events, and raw transcript data Converts the transcript into Markdown using metadata like emoji, category, and overview Uses Google Gemini or an AI Agent to generate a high-quality journal entry Splits out action items and creates tasks in Google Tasks Uploads both the transcription and the final journal file into separate Google Drive folders for archival Deletes processed files if needed (cleanup path is included) How to set up Connect your OMI device to send daily summaries to the webhook endpoint Authenticate your Google Drive and Google Tasks accounts Replace any hardcoded values (like folder IDs or task list IDs) with your own Review the system prompt in the AI Agent node if you'd like to personalize your journal style ## Requirements OMI pendant or device that generates .md summaries via API or webhook Google Drive & Google Tasks credentials set up in n8n Optional: Google Gemini or OpenAI for natural language journal generation ## How to customize Change the output folder IDs for GDrive in the Upload Transcription and Upload Journal nodes. One folder is for long term storage and the other is short term, the contents of which are deleted every night to generate the journal entries. Ensure your workflow timezone is set correctly in the settings. Replace Google Tasks with another todo app (e.g. Notion, Todoist) using HTTP or native nodes Customize the AI prompt in the AI Agent or Gemini Chat node to reflect your tone (e.g., poetic, minimalist, spiritual) Modify the Markdown format in the Build Markdown Transcription node for your preferred structure",143,2025-07-04 00:29:40.455000+00:00,True,4
5988,"Bilingual Newsletters with GPT-4o, AI Images & Videos for HubSpot & SP","Who‚Äôs it for Marketing, growth, and automation teams that need to ship polished bilingual newsletters‚Äîcomplete with images, optional video, and multi-channel distribution‚Äîwithout writing a line of code. How it works / What it does A Webhook receives a plain-language request (e.g. ‚ÄúCreate a newsletter about XYZ and store it in SharePoint‚Äù). The Webhook can be changed to another trigger, fr example Manual Trigger. An AI Agent (OpenAI GPT-4o) drafts German & English newsletter copy following a strict JSON schema. SharePoint fetches an HTML template; a Code node injects AI copy into the placeholders. Optional creative assets: Pollinations AI ‚Üí image (resolution check & frame overlay) FAL AI ‚Üí short video + Lyria2 audio ‚Üí merged via FFmpeg API A Gmail Approval step sends the draft to an approver; the flow continues only on ‚ÄúApprove‚Äù. Depending on user intent the workflow: Emails the newsletter to HubSpot contacts, and/or Saves HTML (+ JPG/Video URL) to a SharePoint library. How to set up Import the template Open the yellow sticky note and follow the checklist Create credentials for OpenAI, HubSpot (App Token), Microsoft 365, Gmail, and FAL AI. Enter them in n8n Credential Manager (never in the HTTP node). Edit the Configuration Settings Set node with your ENV_* variables. Activate the workflow, copy the production Webhook URL, and trigger it with JSON body { ""text"": ""‚Ä¶"" } or with a desired trigger. Requirements OpenAI account Microsoft 365 tenant (SharePoint & Outlook) HubSpot App Token FAL AI API Key (video & audio generation) How to customize Want a different language? Tweak the AI prompt inside AI Agent. Want a different storage provider? Change the SharePoint nodes to Google Drive or Dropbox. Skip video generation: set include_video to false in the incoming prompt. Change the distribution logic by adjusting the WF Result Switch node. Add more channels (e.g. Slack) by inserting additional branches after the Switch and modifying the intent determination at the beginning of the workflow.",143,2025-07-14 09:15:48.670000+00:00,True,8
5461,iMessage Food Photo Nutritional Analysis with GPT-4 Vision & Memory Storage,"iMessage AI-Powered Smart Calorie Tracker &gt; üìå What it looks like in use: &gt; This image shows a visual of the workflow in action. Use it for reference when replicating or customizing the template. This n8n template transforms a user-submitted food photo into a detailed, friendly, AI-generated nutritional report ‚Äî sent back seamlessly as a chat message. It combines OpenAI's visual reasoning, Postgres-based memory, and real-time messaging with Blooio to create a hands-free calorie and nutrition tracker. üß† Use Cases Auto-analyze meals based on user-uploaded images. Daily/weekly/monthly diet summaries with no manual input. Virtual food journaling integrated into messaging apps. Nutrition companion for healthcare, fitness, and wellness apps. üìå Good to Know ‚ö†Ô∏è This uses GPT-4 with image capabilities, which may incur higher usage costs depending on your OpenAI pricing tier. Review OpenAI‚Äôs pricing. The model uses visual reasoning and estimation to determine nutritional info ‚Äî results are estimates and should not replace medical advice. Blooio is used for sending/receiving messages. You will need a valid API key and project set up with webhook delivery. A Postgres database is required for long-term memory (optional but recommended). You can use any memory node with it. ‚öôÔ∏è How It Works Webhook Trigger The workflow begins when a message is received via Blooio. This webhook listens for user-submitted content, including any image attachments. Image Validation and Extraction A conditional check verifies the presence of attachments. If images are found, their URLs are extracted using a Code node and prepared for processing. Image Analysis via AI Agent Images are passed to an OpenAI-based agent using a custom system prompt that: Identifies the meal, Estimates portion sizes, Calculates calories, macros, fiber, sugar, and sodium, Scores the meal with a health and confidence rating, Responds in a chatty, human-like summary format. Memory Integration A Postgres memory node stores user interactions for recall and contextual continuity, allowing day/week/month reports to be generated based on cumulative messages. Response Aggregation & Summary Messages are aggregated and summarized by a second AI agent into a single concise message to be sent back to the user via Blooio. Message Dispatch The final message is posted back to the originating conversation using the Blooio Send Message API. üöÄ How to Use The included webhook can be triggered manually or programmatically by linking Blooio to a frontend chat UI. You can test the flow using a manual POST request containing mock Blooio payloads. Want to use a different messages app? Replace the Blooio nodes with your preferred messaging API (e.g., Twilio, Slack, Telegram). ‚úÖ Requirements OpenAI API access with GPT-4 Vision or equivalent multimodal support. Blooio account with access to incoming and outgoing message APIs. Optional: Postgres DB (e.g., via Neon) for tracking message context over time. üõ†Ô∏è Customising This Workflow Prompt Tuning** Tailor the system prompt in the AI Agent node to fit specific diets (e.g., keto, diabetic), age groups, or regionally-specific foods. Analytics Dashboards** Hook up your Postgres memory to a data visualization tool for nutritional trends over time. Multilingual Support** Adjust the response prompt to translate messages into other languages or regional dialects. Image Preprocessing** Insert a preprocessing node before sending images to the model to resize, crop, or enhance clarity for better results.",141,2025-06-30 00:28:05.321000+00:00,True,5
5475,"Automate Email Management with OpenAI Classification, Gmail Drafts & Slack Alerts","What it does This workflow automatically processes incoming emails with intelligent AI classification, creating draft responses and sending Slack notifications based on email content. How it works Monitors emails with the 'AI-Agent' label AI classification into categories: Inquiry, Support, Newsletter, Action Item Adds appropriate labels to emails automatically Creates draft replies for Support and Inquiry emails Sends Slack notifications for Action Items and Newsletter summaries Setup Requirements Gmail OAuth2 credentials configured OpenAI API credentials (or other AI provider) Slack OAuth2 credentials (if notifications desired) Gmail labels created (see setup instructions below) How to customize Modify classification categories in the AI Agent Adjust label mappings in the Parse Classification node Customize draft response templates Configure different Slack channels for different email types",140,2025-06-30 10:47:56.703000+00:00,True,5
5496,"Extract Email Tasks with Gmail, ChatGPT-4o and Supabase","üì© Gmail ‚Üí GPT ‚Üí Supabase | Task Extractor This n8n workflow automates the extraction of actionable tasks from unread Gmail messages using OpenAI's GPT API, stores the resulting task metadata in Supabase, and avoids re-processing previously handled emails. ‚úÖ What It Does Triggers on a schedule to check for unread emails in your Gmail inbox. Loops through each email individually using SplitInBatches. Checks Supabase to see if the email has already been processed. If it's a new email: Formats the email content into a structured GPT prompt Calls ChatGPT-4o to extract structured task data Inserts the result into your emails table in Supabase üß∞ Prerequisites Before using this workflow, you must have: An active n8n Cloud or self-hosted instance A connected Gmail account with OAuth credentials in n8n A Supabase project with an emails table and: ALTER TABLE emails ADD CONSTRAINT unique_email_id UNIQUE (email_id); An OpenAI API key with access to GPT-4o or GPT-3.5-turbo üîê Required Credentials | Name | Type | Description | |-----------------|------------|-----------------------------------| | Gmail OAuth | Gmail | To pull unread messages | | OpenAI API Key | OpenAI | To generate task summaries | | Supabase API | HTTP | For inserting rows via REST API | üîÅ Environment Variables or Replacements Supabase_TaskManagement_URI ‚Üí e.g., https://your-project.supabase.co Supabase_TaskManagement_ANON_KEY ‚Üí Your Supabase anon key These are used in the HTTP request to Supabase. ‚è∞ Scheduling / Trigger Triggered using a Schedule node Default: every X minutes (adjust to your preference) Uses a Gmail API filter: unread emails with label = INBOX üß† Intended Use Case &gt; Designed for productivity-minded professionals who want to extract, summarize, and store actionable tasks from incoming email ‚Äî without processing the same email twice or wasting GPT API credits. This is part of a larger system integrating GPT, calendar scheduling, and optional task platforms (like ClickUp). üì¶ Output (Stored in Supabase) Each processed email includes: email_id subject sender received_at body (email snippet) gpt_summary (structured task) requires_deep_work (from GPT logic) deleted (initially false)",139,2025-06-30 22:21:19.552000+00:00,True,5
8292,AI Recruiting Pipeline: Job to Candidate Shortlist with Apollo & Airtable,"Who‚Äôs it for Recruiting agencies, executive search firms, and in-house talent teams that want to automate candidate sourcing and prequalification. Instead of spending hours searching, scoring, and writing outreach, this workflow turns any job description into a ready-to-use shortlist with personalized messages. Youtube Walkthrough What it does (How it works) This workflow takes a job description (title, description, and location) and runs a complete recruiting automation pipeline: Normalize job titles** and generate variations to widen search coverage. Search candidates** in Apollo (or your CRM / database of choice). Remove duplicates** to keep clean lists. Score candidates** with AI (0‚Äì5) and provide concise reasoning across experience, industry, and seniority. Enrich LinkedIn profiles** (name, title, image, location, experience). Create structured candidate assessments** (summary, alignment, red flags, positives). Generate outreach messages** (email + LinkedIn DM) tailored to the candidate. Write to Airtable** for job/candidate tracking and downstream automation. Everything is plug-and-play, with no manual searching or copy-pasting required. Requirements n8n (Cloud or self-hosted) Airtable account + API access Apollo API or your preferred candidate source LLM provider: OpenAI or Anthropic LinkedIn enrichment API (RapidAPI, Apify, etc.) &gt; ‚ö†Ô∏è Do not hardcode API keys in HTTP nodes. Always use Credentials in n8n. Airtable table specifications Create one base (e.g., Candidate Search ‚Äì From Job Description) with two tables: Jobs Table Job Title (text) Job Description (long text) Job Location (text) Candidates (linked to Candidates table) Candidates Table Core fields: Name, LinkedIn URL, Job Title, Location, Image URL, Job Searches (linked) Assessment fields: Summary Fit Score, Executive Summary, Title Alignment, Skill Alignment, Industry Alignment, Seniority Alignment, Company Type Alignment, Educational Alignment, Potential Red Flags, Positive Signals, Final Recommendation, Next Steps Suggestion Outreach fields: Email Subject, Email Body, LinkedIn Message How to set up Connect credentials Add Airtable, Apollo/CRM, and OpenAI/Anthropic credentials under n8n Credentials. Create Airtable base/tables Follow the above spec for Jobs and Candidates. Match field names exactly to avoid mapping errors. Configure the trigger The workflow starts from a Form/Webhook node. It captures: Job Title (required) Job Description (required) Location (required) Target Companies (optional, comma-separated domains) Job title mutation The workflow uses an AI node to normalize the job title and generate up to 5 variations for broader candidate searches. Candidate search Apollo (or your CRM API) is queried with the generated titles and location filters. Results are deduped. AI scoring & structuring Candidates are scored 0‚Äì5 with clear reasoning (experience, industry, seniority, general fit). Profiles are formatted into structured JSON for Airtable. LinkedIn enrichment Enrichment API fetches missing data (geo, image, job history). Candidate assessment An AI model produces a full recruiter-ready evaluation (fit summary, strengths, red flags). Outreach generation The workflow drafts a concise cold email (&lt;75 words) and LinkedIn DM (&lt;60 words), consultative in tone. Write to Airtable All jobs and candidates (with assessments and outreach messages) are logged for review and integration. How to customize Swap Apollo with your CRM** (Greenhouse, Bullhorn, etc.). Adjust scoring prompts** to match your niche (sales, engineering, healthcare). Add custom filters** for target companies or industries. Change outreach tone** to align with your brand voice. Limit by score** (e.g., only push candidates with score ‚â•4). Security & best practices Store all keys in n8n Credentials (never in nodes). Use Set nodes to centralize editable variables (title, location, filters). Always add sticky notes in your workflow explaining steps. Rename nodes clearly for readability. Troubleshooting No candidates found?** Loosen title variations or broaden location. Low fit scores?** Refine keywords and required skills in scoring prompts. Airtable errors?** Double-check Base ID, Table ID, and field names. API rate limits?** Enable batching/pagination and increase intervals. SEO title: Build candidate shortlists from a job description to Airtable with Apollo, AI scoring, and personalized outreach Keywords: recruiting automation, Apollo people search, candidate enrichment, AI scoring, Airtable recruiting CRM, LinkedIn outreach, n8n workflow template",136,2025-09-05 10:28:25.383000+00:00,True,8
5400,"The title is very good, clearly conveying the template's purpose while mentioning key technologies","How it works Collects articles from your preferred RSS feeds. Rates and tags each article using an AI model (e.g., QWEN 14B-s4), filtering for relevance and quality. Summarizes high-rated articles with a language model (e.g., Gemma3 4B) for quick, digestible reading. Checks for duplicates to avoid sending the same article twice. Formats and sends the top articles as an HTML newsletter via Gmail, using OAuth2 authentication. Stores records in a Postgres database, tracking which articles have been sent and their ratings. Requirements Postgres Account AI Models (if you work localy use Ollama) In the cloud you have to change Ollama node to your prefered Model Node RSS Feed of your desire Google Auth2, if you want to use Gmail Recommendations Use n8n local version for this workflow Here are some more informations: https://github.com/falks-ai-workbench/n8n_newsletter",133,2025-06-27 09:16:38.121000+00:00,True,5
5492,"Generate & Send Spare Parts Price Quotes with Gmail, Sheets and Gemini AI","Who's it for This workflow is perfect for sales teams, customer service departments, and businesses that frequently handle spare parts inquiries via email. It's especially valuable for companies managing multiple products with complex pricing structures who want to automate their quotation process while maintaining professional, multilingual communication. What it does This workflow: Monitors your Gmail inbox** for incoming spare parts requests Automatically generates professional HTML price quotes** in the sender's language Sends personalized replies** Uses AI to detect the email language (supports Turkish, English, German, and more) Extracts project or part codes** Fetches pricing data from Google Sheets** Calculates totals accurately** Formats everything** into a clean, professional quote that matches your brand How it works Schedule Trigger runs every minutes to check for new emails Gmail node fetches the latest unread email Keyword detection filters for spare parts-related terms in multiple languages AI Agent processes the request by: Detecting the email's language Extracting project/part codes Querying three Google Sheets: CRM, Bill of Materials, Pricing Calculating line totals and grand total Generating a professional HTML quote in the sender's language Gmail reply sends the quote and marks the original email as read Requirements n8n self-hosted or cloud instance Gmail account with OAuth2 authentication Google Sheets with proper structure (3 sheets for CRM, BoM, and Pricing data) Google Gemini API key for AI processing Basic understanding of Google Cloud Console for OAuth setup How to set up Import the workflow into your n8n instance Create three Google Sheets with the following column structure: CRM Sheet: Email, ProjectCode, CustomerName Bill of Materials: ProjectCode, PartCode, PartDescription, Quantity Pricing Sheet: PartCode, UnitPriceEUR, PartDescription Configure credentials: Set up Gmail OAuth2 in Google Cloud Console Configure Google Sheets OAuth2 (can use same project) Get your Google Gemini API key from Google AI Studio Update the workflow: Replace placeholder Sheet IDs in the CRM, BoM, and Pricing nodes Adjust company name in the AI Agent‚Äôs system message Modify keyword detection if needed Test with a sample email before activating How to customize the workflow Add more languages**: Update the keyword detection node with additional terms Modify the quote template**: Edit the HTML in the AI Agent's message to match your branding Change data sources**: Replace Google Sheets with PostgreSQL or MySQL nodes Add approval steps**: Insert a manual approval node for quotes above a certain value Include attachments**: Add PDF or product spec file nodes Enhance notifications**: Add Slack or Teams notifications after quote is sent Implement follow-ups**: Create a separate workflow for reminder emails This template provides a solid foundation for automating your quotation process, while staying flexible to fit your specific business needs. Feel free to contact me for further implementation guidelines: LinkedIn: Berke",133,2025-06-30 19:47:37.078000+00:00,True,4
5631,Transform YouTube Videos into Interactive MCQ Quizzes with Google Forms & Gemini AI,"AI-Powered MCQ Quiz Generator from YouTube Videos Transform any YouTube video into an interactive MCQ quiz automatically! This workflow uses Google Gemini AI to analyze video content and generate comprehensive multiple-choice questions with automatic grading - perfect for educators, trainers, and content creators. Who is this For This workflow is perfect for: Educators** creating quizzes from educational YouTube content Corporate Trainers** developing assessments from training videos Content Creators** engaging their audience with interactive quizzes Students** testing their knowledge on video lectures Online Course Creators** building assessments from video content Features AI Video Analysis**: Google Gemini 2.5 Flash analyzes entire YouTube videos (up to 50 minutes) Dynamic Question Generation**: Creates up to 90 MCQ questions with 3 options each Automatic Form Creation**: Generates Google Forms with quiz functionality Smart Grading**: Built-in correct answer identification and scoring Error Handling**: Robust error management with user feedback How It Works User Input via n8n Web Form: Form Name (Quiz Title) Email Address YouTube Video URL Number of Questions (1-90) AI Processing Pipeline: Google Gemini analyzes the YouTube video content AI extracts key concepts and generates relevant questions Structured output parser formats questions into JSON Google Forms Integration: Automatically creates a new Google Form Adds all generated questions with multiple choice options Configures quiz settings with correct answers and scoring Completion & Access: User receives direct link to the generated quiz Form ready for immediate use or sharing Video Demo: See this youtube Video to explore ""how it works"". Set Up Steps Import the Workflow Create a new workflow in n8n Import the JSON file by clicking ""three dots"" (upper right corner) &gt; ""Import from file..."" Configure Google Gemini API Get your Google AI Studio API key from Google AI Studio On ‚ÄúHTTP Request to Gemini‚Äù node replace the ‚ÄúAPI_KEY‚Äù from url with your API key. Create a ""Google Gemini (PaLM) API"" credential in n8n Add your API key to the credential Connect the credential to the ""Google Gemini Chat Model"" node Set Up Google Forms Integration Enable Google Forms API in Google Cloud Console Create a ""Google OAuth2 API"" credential in n8n Authorize the credential with Forms permissions Connect the credential to both HTTP Request nodes (‚ÄúCreate a Google Form‚Äù node and ‚ÄúCreate MCQ Quizzes‚Äù node) Configure Form Trigger The workflow includes a built-in form trigger No additional setup needed - the form URL will be generated automatically Customize form fields if needed in the ‚ÄúInput YouTube URL"" node Test the Workflow Activate the workflow Submit the form to generate a test quiz Verify the Google Form is created successfully Pre-requisites Necessary Accounts:** Google Account (for Forms API access) Google AI Studio Account (for Gemini API access) n8n Instance (cloud or self-hosted) API Access:** Google Forms API enabled Google drive API enabled Google Generative AI API access Valid API keys and OAuth credentials N8N Requirements:** n8n version 1.95.2 or higher LangChain nodes package installed Internet access for API calls Customization Guidance Question Generation Prompts: Modify the prompt in ""Set Prompt and model"" node for different question styles Adjust difficulty levels or focus areas Change question format (True/False, Fill-in-blanks, etc.) Form Customization: Update form title and description templates Add additional input fields (difficulty level, subject area) Customize success/error messages Advanced Features You Can Add: Email Notifications: Send quiz links via email Analytics Integration: Track quiz performance and completion rates Multi-language Support: Generate quizzes in different languages Question Bank Storage: Save generated questions to a database Batch Processing: Generate multiple quizzes from a YouTube playlist Error Handling Enhancements: Add retry logic for API failures Implement fallback question generation Create detailed error logging Technical Specifications Video Length**: Up to 50 minutes supported Question Limit**: 1-90 questions per quiz Processing Time**: 2-10 minutes depending on video length Supported Formats**: YouTube videos (public and unlisted) Output Format**: Google Forms with automatic grading Limitations & Considerations YouTube video must be publicly accessible or unlisted Processing time increases with video length and question count API rate limits may apply for high-volume usage Some complex visual content may not be fully analyzed Ready to Transform Videos into Quizzes? This workflow streamlines the entire process from video analysis to quiz deployment. Perfect for educators and trainers looking to create engaging assessments from video content quickly and efficiently.",131,2025-07-03 11:09:11.552000+00:00,True,5
5426,Create a Webhook-Ready Conversational Assistant with Google Gemini and Session Memory,"Universal AI Assistant - Webhook-Ready Conversational AI Transform any platform into an intelligent conversational experience with this plug-and-play n8n workflow. This AI assistant can be seamlessly integrated into websites, mobile apps, or any system that supports webhook connections. Key Features: üîó Universal Integration - Connect to any platform via webhook (websites, apps, bots) üß† Powered by Google Gemini 2.0 Flash - Fast, accurate, and context-aware responses üíæ Session Memory - Maintains conversation context for natural follow-up interactions ‚ö° Real-time Responses - Instant webhook responses for smooth user experiences üéØ Customizable Personality - Easy prompt modification for brand-specific tone Perfect For: Website Chat Widgets - Add AI support to any website instantly WhatsApp/Telegram Bots - Power messaging platforms with intelligent responses Mobile App Integration - Embed conversational AI into iOS/Android apps Customer Support Systems - Automate first-line support with context retention Lead Qualification - Intelligent pre-screening of prospects before human handoff Simple Implementation: Just send POST requests to the webhook URL with: json{ ""message"": ""User's question here"", ""sessionId"": ""unique-session-identifier"" } Ready to deploy in minutes - No complex setup required. Perfect for small businesses wanting to add AI capabilities without technical overhead. This workflow gives you a production-ready foundation that you can customize for specific client needs. The session-based memory makes it ideal for customer support scenarios where context matters, and the webhook approach means it integrates with virtually any platform your clients are already using.",130,2025-06-28 04:06:54.178000+00:00,True,3
5411,Optimize SEO Meta Tags in Google Sheets with Google Gemini,"üîÅ Loop & Optimize Meta Tags with Google Gemini This workflow automates the shortening of meta titles and descriptions for SEO‚Äîdirectly from your Google Sheet, row by row, using Google Gemini. ‚úÖ What it does Reads rows from a Google Sheet (meta_title, meta_description, row_index) Loops through each row and checks if content exists Sends the data to Google Gemini for length-optimized output Cleans and parses the response Updates the original sheet with the shortened results üõ†Ô∏è Setup Requirements Google Sheets (OAuth2 credentials connected in n8n) Google Gemini API key (configured in n8n credentials) Sheet must contain: row_index meta_title meta_description Output will be written into: meta_titleFixed meta_descriptionFixed",129,2025-06-27 14:16:14.638000+00:00,False,4
5505,Capture Website Form Submissions to Notion CRM Database,"‚ö°Ô∏è How It Works This workflow captures form submissions from your website, formats the data, and automatically creates a new entry in your Notion CRM database. It eliminates manual copy-pasting and keeps your leads or requests organised in one place. üõ† Setup Steps Webhook Node ‚Ä¢ Create a webhook in n8n. ‚Ä¢ Connect your website form to POST submissions to this webhook URL. Code Node ‚Ä¢ Formats the incoming data to match your Notion database structure. ‚Ä¢ You can customise the fields in the code to suit your specific form inputs. Notion Node (Create Page) ‚Ä¢ Connect your Notion account. ‚Ä¢ Choose your target database. ‚Ä¢ Map each field from the Code node output to your Notion database properties. Test ‚Ä¢ Submit a test form entry. ‚Ä¢ Confirm the data appears correctly in Notion. ‚∏ª üë• Who It‚Äôs For ‚úÖ Freelancers collecting project inquiries ‚úÖ Agencies managing client onboarding forms ‚úÖ Business owners wanting organised lead capture ‚úÖ Teams that use Notion as their central CRM or task manager ‚úÖ Anyone tired of manually transferring form data into Notion",127,2025-07-01 06:44:09.863000+00:00,False,2
5340,Automate Lead Scraping with Scrapeless to Google Sheets with Data Cleaning,"This workflow contains community nodes that are only compatible with the self-hosted version of n8n. Prerequisites A n8n account (free trial available) A Scrapeless account and API key A Google account to access Google Sheets üõ†Ô∏è Step-by-Step Setup 1. Create a New Workflow in n8n Start by creating a new workflow in n8n. Add a Manual Trigger node to begin. 2. Add the Scrapeless Node Add the Scrapeless node and choose the Scrape operation Paste in your API key Set your target website URL Execute the node to fetch data and verify results 3. Clean Up the Data Add a Code node to clean and format the scraped data. Focus on extracting key fields like: Title Description URL 4. Set Up Google Sheets Create a new spreadsheet in Google Sheets Name the sheet (e.g., Business Leads) Add columns like Title, Description, and URL 5. Connect Google Sheets in n8n Add the Google Sheets node Choose the operation Append or update row Select the spreadsheet and worksheet Manually map each column to the cleaned data fields 6. Run and Test the Workflow Click ""Execute Workflow"" in n8n Check your Google Sheet to confirm the data is properly inserted Results With this automated workflow, you can continuously extract business lead data, clean it, and push it directly into a spreadsheet ‚Äî perfect for outbound sales, lead lists, or internal analytics. How to Use ‚öôÔ∏è Open the Variables node and plug in your Scrapeless credentials. üìÑ Confirm the Google Sheets node points to your desired spreadsheet. ‚ñ∂Ô∏è Run the workflow manually from the Start node. Perfect For: Sales teams doing outbound prospecting Marketers building lead lists Agencies running data aggregation tasks",121,2025-06-26 02:26:42.048000+00:00,False,2
6518,Automatic Job Listings Extraction and Publishing Template,"Automatically extract job listings from any website URL, format them with AI, and publish directly to WordPress. Just send a URL via Telegram, and watch as the workflow scrapes the job details, enhances the content with GPT, and creates a polished post on your site. üí° Why Use Job Repost? ‚è∞ Save countless hours Automatically extract, process, and publish job offers from any website, freeing your time from repetitive tasks. ‚úÖ Eliminate human errors Say goodbye to typos and missed fields ‚Äî every job post is validated before going live. üìà Boost engagement Fresh, well-structured job listings attract more candidates, improving your site's reach and authority. üöÄ Stay ahead Leveraging AI with GPT means your content is not just automated but polished and SEO-friendly ‚Äî the digital assistant you never knew you needed. ‚ö° Perfect For Job board managers:** Want to aggregate listings from multiple sources with minimal effort Recruiters & HR teams:** Who need to streamline job posting workflows without technical hassles Content creators & marketers:** Looking to automate publishing while maintaining style and SEO standards üîß How It Works | Step | Process | Description | |------|---------|-------------| | üì± | Trigger | Send a job URL via Telegram bot to initiate the process | | üî• | Extract | Firecrawl API scrapes and extracts clean content from the provided URL | | üìé | Process | Job data is extracted via AI, text split and cleaned, job categories and types mapped to your system | | ü§ñ | Smart Logic | GPT crafts formatted job posts, intelligent validation ensures all key data is present, default values fill in the blanks if necessary | | üíå | Output | Posts automatically published to WordPress with company logos uploaded, and success or error notifications sent via Telegram | | üóÇ | Storage | Uses Supabase vector store for managing document embeddings, ensuring quick lookup and reference compliance | üîê Quick Setup Import the provided JSON file into your n8n instances Add credentials: Firecrawl API key Google Drive OAuth2 (for RAG storage) OpenAI API WordPress API Telegram API Supabase Customize: Telegram bot token WordPress URLs Default images and category mappings if needed Update: URLs and API tokens where placeholders are used Test: Send a job URL to your Telegram bot to verify accurate extraction and posting üß© You'll Need ‚úÖ Active n8n instances ‚úÖ Firecrawl account with API access ‚úÖ Google Drive account for RAG document storage ‚úÖ OpenAI account with GPT API access ‚úÖ WordPress site with autojob plugin and API enabled ‚úÖ Telegram bot for URL submission and notifications ‚úÖ Supabase account for vector store management üõ†Ô∏è Level Up Ideas üåç Add multi-language support to expand global reach üîó Support batch URL processing for multiple jobs at once üí¨ Integrate Slack or email notifications for wider team alerts üéØ Use more AI nodes to summarize or rate job offers for quality control üîÑ Schedule periodic cleanup of vector store for performance optimization üìä Add analytics tracking for published jobs performance üß† Nodes Used Core Components: Firecrawl HTTP Request** (Web scraping and content extraction) Google Drive** (RAG document storage) Supabase Vector Store** OpenAI** (Embeddings, GPT Extraction) Code Nodes** for mapping categories Telegram Trigger & Message** HTTP Request** (for WordPress API and image uploads) Made by: Khaisa Studio Tags: automation recruitment job-posting wordpress AI web-scraping firecrawl Category: Human Resources, Recruitment, Wordpress, Scrapping Need a custom? contact me on LinkedIn or Web",117,2025-07-27 06:42:41.239000+00:00,True,9
5468,Multi-User Telegram Bot to Summarize & Repurpose YouTube Videos with GPT-4o,"üë§ Who‚Äôs it for This workflow is for content creators, marketers, educators, or anyone who wants to instantly summarize YouTube videos and repurpose them into different formats (LinkedIn post, tweet, etc.) via a simple Telegram chatbot. ‚öôÔ∏è How it works This n8n automation listens for messages in Telegram. If the message contains a YouTube link, it: Extracts the video ID Fetches the video transcript using RapidAPI Cleans the transcript of any special characters Sends it to OpenAI to generate a summary If the message is not a link, it simply acts as an AI chatbot using OpenAI with memory support. ‚úÖ Supports follow-up prompts like: ‚ÄúMake it shorter‚Äù ‚ÄúTurn this into a LinkedIn post‚Äù ‚ÄúCreate a tweet thread‚Äù üßë‚Äçü§ù‚Äçüßë Multi-User Support This Telegram bot supports multiple users simultaneously. It tracks memory and context separately for each user using Telegram's unique chat_id. ‚úÖ Each user gets personalized AI replies ‚úÖ Follow-up commands work per user ‚úÖ No interference between users üõ†Ô∏è Requirements A Telegram bot token (get via @BotFather) An OpenAI API Key (from https://platform.openai.com/account/api-keys) A RapidAPI Key and Host (typically youtube-transcript3.p.rapidapi.com) &gt; üö® API keys must be added manually ‚Äî they are not included in the template. üß© How to Set It Up Configure the Telegram Trigger node with your bot token. In the HTTP Request node, set: X-RapidAPI-Key: your RapidAPI key X-RapidAPI-Host: your RapidAPI host URL Add your OpenAI API credentials to the AI Agent node. Use the provided sticky notes for guidance inside the workflow itself. üéõÔ∏è How to Customize Modify AI prompt behavior in the AI Agent node Change the text formatting in the Code node Use a different transcript API if preferred Add commands like make it into a blog post, summarize in bullet points, etc. üìå Notes All nodes are renamed to reflect their function API credentials are removed for security Includes colored boxes and sticky notes to guide the user Compatible with n8n cloud and self-hosted setups",112,2025-06-30 08:46:22.578000+00:00,True,6
6148,"Publish LinkedIn & X Posts with Telegram Bot, Gemini AI & Vector Memory","This workflow contains community nodes that are only compatible with the self-hosted version of n8n. Overview This is a Telegram Bot capable of receiving information from the user in the form of text messages, voice messages, images or documents (e.g., presentations, PDFs, HTML pages), and publishing posts to the user's social platforms. The bot always sends the user a draft of the post for verification before publishing it. The bot saves relevant information to its long-term memory (vector store), so you don't need to repeat it in every interaction (e.g., who you are, your company, product, etc.). This template supports creating posts in LinkedIn and X. Setup Requirements To use this template your will need: Google's AI Studio API key. Get one here: https://aistudio.google.com/app/apikey Telegram Bot API key. You receive one when you register a new Telegram Bot via @BotFather bot in Telegram. LinkedIn API key. Follow the instructions here to create one: https://docs.n8n.io/integrations/builtin/credentials/linkedin/ X API key. Follow the instructions here to create one: https://docs.n8n.io/integrations/builtin/credentials/twitter/ Step-by-step instruction Import this template Create a new Telegram Bot or get an API key for existing one. Configre Telegram nodes with Telegram API key. Obtain a Google's AI Studio API key. Set it in ""Describe document"", ""Describe audio"" and ""Google Gemini Chat Model"". Create an API key for LinkedIn. Create an API key for X. Set our LinkedIn key in ""Create post in LinkedIn"" nodes. Set your X key in ""Create X (Twitter) post"" node. Other Bright-colored notes in the template highlight information that needs to be set before launching the template.",112,2025-07-18 17:34:29.119000+00:00,True,10
5517,Manage Personal Expenses with Webhooks and Google Sheets Automated Tracker,"How it works: This system functions by receiving expenses via webhook POST. It validates the data, stores it in Google Sheets, and, daily at 8 PM, generates and sends financial summaries. Automatic categorization simplifies the organization of expenses. Set up steps: Setup involves creating the Google Sheet, configuring the webhook, and defining the categorization rules. The process is quick and intuitive, taking about 10-15 minutes for the system to be ready to receive your expenses.",101,2025-07-01 14:24:38.880000+00:00,False,1
7678,Create Dynamic Crypto Market Monitors with Gemini AI and Telegram Bot,"Template Description This description details the template's purpose, how it works, and its key features. You can copy and use it directly. Overview This is a powerful n8n ""meta-workflow"" that acts as a Supervisor. Through a simple Telegram bot, you can dynamically create, manage, and delete countless independent, AI-driven market monitoring agents (Watchdogs). This template is a perfect implementation of the ""Workflowception"" (workflow managing workflows) concept in n8n, showcasing how to achieve ultimate automation by leveraging the the n8n API. How It Works ? Telegram Bot Interface: Execute all operations by sending commands to your own Telegram Bot: /add SYMBOL INTERVAL PROMPT: Add a new monitoring task. /delete SYMBOL: Delete an existing monitoring task. /list: List all currently running monitoring tasks. /help: Get help information. Use Telegram Bot to control The watchdog workfolw created in the below Dynamic Workflow Management: Upon receiving an /add command, the Supervisor system reads a ""Watchdog"" template, fills in your provided parameters (like trading pair and time interval), and then automatically creates a brand new, independent workflow via the n8n API and activates it. Persistent Storage: All monitoring tasks are stored in a PostgreSQL database, ensuring your configurations are safe even if n8n restarts. The ID of each newly created workflow is also written back to the database to facilitate future deletion operations. AI-Powered Analysis: Each created ""Watchdog"" workflow runs on schedule. It fetches the latest candlestick chart by calling a self-hosted tradingview-snapshot service. This service, available at https://github.com/0xcathiefish/tradingview-snapshot, works by simulating a login to your account and then using TradingView's official snapshot feature to generate an unrestricted, high-quality chart image. An example of a generated snapshot can be seen here: https://s3.tradingview.com/snapshots/u/uvxylM1Z.png. To use this, you need to download the Docker image from the packages in the GitHub repository mentioned above, and run it as a container. The n8n workflow then communicates directly with this container via an HTTP API to request and receive the chart snapshot. After obtaining the image, the workflow calls a multimodal AI model (Gemini). It sends both the chart image and your custom text-based conditions (e.g., ""breakout above previous high on high volume"" or ""break below 4-hour MA20"") to the AI for analysis, enabling truly intelligent chart interpretation and alert triggering. Key Features Workflowception: A prime example of one workflow using an API to create, activate, and delete other workflows. Full Control via Telegram: Manage your monitoring bots from anywhere, anytime, without needing to log into the n8n interface. AI Visual Analysis: Move beyond simple price alerts. Let an AI ""read"" the charts for you to enable complex, pattern-based, and indicator-based intelligent alerts. Persistent & Extensible: Built on PostgreSQL for stability and reliability. You can easily add more custom commands.",98,2025-08-21 03:46:49.403000+00:00,True,4
5069,LinkedIn Profile Scraper & Personalized Outreach using PhantomBuster + GPT-4,"Description This plug-and-play n8n workflow template helps you automate LinkedIn profile data extraction and transform it into structured, enriched outputs using PhantomBuster and GPT-4. Perfect for lead generation, recruiting, or growth marketing teams, this pipeline handles scraping, structuring, and messaging‚Äîall in one flow. Requirements PhantomBuster Setup Create a PhantomBuster account. Use the LinkedIn Profile Scraper Phantom (or your custom one). Get your API Key and Agent ID from PhantomBuster dashboard. Configure your LinkedIn Phantom with an active LinkedIn session cookie (available from your browser‚Äôs developer tools). OpenAI or Azure OpenAI Provide your GPT-4 / GPT-4o API Key. You can use OpenAI or Azure's hosted model. (Optional) Google Sheets Use Sheets for batch profile inputs or output logging. What This Template Does Step-by-Step Flow: üîÅ Manual Trigger / Google Sheets input ‚Äì Accepts LinkedIn profile URLs. üöÄ Launch PhantomBuster Agent ‚Äì Starts the scraping job with provided LinkedIn URLs. ‚è≥ Wait Node (45 sec) ‚Äì Allows PhantomBuster to finish execution. üì• Download Scraped Output ‚Äì Fetches the download URL for JSON/CSV from the container. üìÇ Temp File Management ‚Äì Deletes temp URLs or intermediate data as needed. AI Parser (GPT-4) ‚Äì Parses and extracts: Name Headline Company Designation Industry Location Recent roles (Optional) Create personalized outreach messages using structured output. Output Structured JSON or CSV ‚Äì Send to CRM, Google Sheets, Airtable, etc. Best For üìà Sales/BDRs building high-quality prospect lists üßë‚Äçüíº Recruiters extracting enriched candidate info üíº Founders & Marketers creating targeted lead magnets üß© Tool builders building products using LinkedIn profile data",97,2025-06-20 11:54:05.007000+00:00,True,6
8592,Fireflies Transcripts to Meeting Summaries & Task Extractor to Slack & ClickUp,"AI-powered Meeting Summaries and Action Items to Slack and ClickUp How it Works Webhook Trigger: The workflow starts when Fireflies notifies that a transcription has finished. Transcript Retrieval: The transcript is pulled from Fireflies based on the meeting ID. Pre-processing: The transcript is split into sentences and then aggregated into a raw text block. AI Summarization: The aggregated transcript is sent to Google Gemini, which generates a short summary and a structured list of action items. Post-processing: The AI response is cleaned and formatted into JSON. Action items are mapped to titles and descriptions. Distribution: The meeting summary is posted to Slack. Action items are created as tasks in ClickUp. Use Case This workflow is designed for teams that want to reduce the manual effort of writing meeting notes and extracting action items. Automatically generate a clear and concise meeting summary Share the summary instantly with your team on Slack Ensure action items are not lost by automatically creating tasks in ClickUp Ideal for distributed teams, project managers, and product teams managing recurring meetings Requirements n8n instance** set up and running Fireflies.ai account** with API access to meeting transcripts Google Gemini API (via PaLM credentials)** for AI-powered summarization Slack account** with OAuth2 credentials connected in n8n ClickUp account** with OAuth2 credentials connected in n8n",97,2025-09-15 10:20:02.945000+00:00,True,5
9393,"Automate B2B Lead Generation & Personalized Cold Emails with Apollo, Apify & GPT","This n8n template automates targeted lead discovery, AI-driven data structuring, and personalized cold-email sending at controlled intervals. It‚Äôs ideal for sales teams, founders, and agencies that want to scale outreach without losing personalization. Good to know Can run on an interval (e.g., every 10 minutes) to fetch and process new leads. Requires API keys for OpenAI (content + parsing) and Apify (lead discovery). Emails are sent one-by-one with delays (the Wait node) to reduce spam risk. Lead data is written to Google Sheets‚Äîwe recommend separate sheets for leads with and without emails. Works with Gmail, Outlook, or your own SMTP‚Äîjust plug in your credentials. How it works Form Trigger (START) A form collects: Job Title, Company Size, Keywords, Location. Apollo URL Generator (GPT) The model turns the form fields into a precise Apollo search URL. Run Apify (Actor) Apify fetches contacts/companies that match your preferences for downstream processing. Limit Caps how many records are prepared per run (e.g., max 5). Parse Lead Data (GPT) Extracts key fields (full name, email, title, LinkedIn, company, company links). Synthesizes a short 2‚Äì3 sentence sales-ready summary for each lead. Sorting (If) Splits leads into with email vs. without email. With email ‚Üí main sheet + email pathway Without email ‚Üí a separate sheet for later enrichment Email Magic (GPT) Uses the parsed data to personalize your fixed email template for each lead (keeps structure/intent, swaps in the right details). Sending Emails (Loop + Wait + Sender) Loop Over sends messages individually. Wait inserts a pause between sends (fully configurable). Delivery via Gmail or SMTP (custom domain / Outlook). Confirmation After the loop finishes, a Gmail node sends a ‚Äúcampaign complete‚Äù confirmation. How to use Enable the workflow and open the start form. Enter preferences: job title, company size, keywords, location. Add credentials: OpenAI (for parsing + email generation) Apify (Bearer token in Run Apify) Google (Sheets + optionally Gmail) SMTP/Outlook (if not using Gmail) Set limits (the Limit node) and send interval (the Wait node). Choose sheets for leads with/without email. Run‚Äîthe workflow will fetch leads, prepare emails, and send them with spacing. Requirements OpenAI API key Apify API token (access to the chosen Actor) Google Sheets for storage Gmail or SMTP/Outlook credentials for sending An operational n8n instance Customising this workflow Email template: Edit the text in ‚ÄúCreating a email‚Äù while preserving placeholders. Segmentation: Add more conditions (role, industry, country) and route to different templates/sheets. Follow-ups: Add a second loop that reads statuses and sends timed reminders. Data enrichment: Insert additional APIs before ‚ÄúParse Lead Data.‚Äù Anti-spam: Increase Wait duration, rotate senders, vary subject lines. Reporting: Add a ‚Äúsend status‚Äù sheet and an error log. Security & compliance tips Store API keys in n8n Credentials, not plain-text nodes. Respect GDPR/opt-out‚Äîtrack source and first-contact date in your sheet. Start with a small batch, validate deliverability, then scale up. In short Automated lead capture ‚Üí AI cleaning + summary ‚Üí personalized emails ‚Üí spaced sending ‚Üí completion notice. Scalable, customizable, and ready to plug into your preferred sender and template.",96,2025-10-09 05:06:36.883000+00:00,True,8
5474,"Airline Web Check-in Data Extraction with Ollama AI, Google Sheets & Postgres Vector DB","Overview This workflow retrieves airline web check-in URLs from Google Sheets, scrapes their content, employs an LLM to generate structured JSON data, refreshes the sheet, creates embeddings, and saves them in a Postgres vector DB for future semantic searches or question-answering. Quick Notes Verify that Google Sheets has accurate URLs for scraping. Ensure the Postgres vector DB is set up correctly for embedding storage. Process Flow Start the workflow with the Chat Trigger - Start node. Retrieve airline check-in URLs using the Fetch Airline URLs node. Scrape webpage data with the Scrape Airline Webpage node. Extract JSON data using the Extract info with LLM node with a Chat Model. Pause for a response with the Wait for Response node. Update Google Sheets with the Store Extracted Data node. Create embeddings with the Generate Embeddings node and store in Postgres vector DB with the Save to Vector DB node. Break down long text with the Split Long Text node and delay the next batch with the Wait Before Next Batch node. Getting Started Import the workflow into n8n and set up Google Sheets and Postgres vector DB credentials. Run a test with a sample URL to confirm scraping and embedding storage. Tailored Adjustments Tweak the Extract info with LLM node to adjust JSON output or modify the Fetch Airline URLs node to pull from different sheet fields.",95,2025-06-30 10:40:22.588000+00:00,True,8
8003,Google Maps to Airtable Lead Scraper with GPT Contact Extraction from Impressum,"How it Works This workflow automates the process of discovering companies in different cities, extracting their contact data, and storing it in Airtable. City Loop (Airtable ‚Üí Google Maps API) Reads a list of cities from Airtable. Uses each city combined with a search term (e.g., SEO Agency, Berlin) to query Google Maps. Marks processed cities as ‚Äúchecked‚Äù to allow safe restarts if interrupted. Business Discovery & Deduplication Searches for businesses via Google Maps Text Search. Checks Airtable to avoid scraping the same company multiple times. Fetches detailed info for each business via Google Maps Place Details API. Impressum Extraction (Website ‚Üí HTML Parsing) Builds an Impressum page URL for each business. Requests the HTML and cleans out ads, headers, footers, etc. Extracts relevant contact info using an AI extractor (OpenAI node). Contact Information Extraction Pulls out: Decision Maker (Name + Position in one string, if available). Email address (must be valid, containing @). Phone number (international format if possible). Filters out incomplete results (e.g., empty email). Database Storage Writes company data back into Airtable: Company name Address Website Email Phone number Decision Maker (Name + Position) Search term & city used Setup Steps 1. Prerequisites Google Maps API Key with access to: Places API ‚Üí Text Search + Place Details Airtable base with at least two tables: Cities (with columns: ID, City, Country, Status) Companies (for scraped results) OpenAI API key (for decision maker + contact extraction). 2. Authentication Configure your Airtable API credentials in n8n. Set up HTTP Query Auth with your Google Maps API key. Add your OpenAI API key in the OpenAI Chat node. 3. Configuration In the Airtable ‚ÄúCities‚Äù table, list all cities you want to scrape. Define your search term in the ‚ÄúExecute Workflow‚Äù node (e.g., SEO Agency). Adjust the batch sizes and wait intervals if you want faster/slower scraping (Google API has strict rate limits). 4. Execution Start manually or from another workflow. The workflow will scrape all companies in each city step by step. It can be safely stopped and resumed ‚Äî cities already marked as processed will be skipped. 5. Results Enriched company dataset stored in Airtable, ready for CRM import, lead generation, or further automation. Tips & Notes Always respect GDPR and local laws when handling scraped data. The workflow is modular ‚Üí you can swap Airtable with Google Sheets, Notion, or a database of your choice. Add custom filters to limit results (e.g., only companies with websites). Use sticky notes inside the workflow to understand each step (mandatory for template publishing). Keep an eye on Google Places API costs** ‚Äî queries are billed after the free quota. If you are still within the first 2 months of the Google Cloud Developer free trial, you can benefit from free credits. Questions or custom requests? üì© suliemansaid.business@gmail.com",79,2025-08-28 19:30:14.552000+00:00,True,5
8607,Fetch Real-Time Bitget Spot Market Data with GPT-4o + Telegram,"Instantly fetch real-time Bitget spot market data directly in Telegram! This workflow integrates the Bitget REST v2 API with Telegram (plus optional AI-powered formatting) to deliver the latest crypto price, order book, candles, and recent trades. Perfect for crypto traders, analysts, and investors who need reliable market data at their fingertips‚Äîno API key required.&#x20; Sign-up for Bitget for 6,200 USDT in rewards to trade: Collect Now How It Works A Telegram bot listens for user requests (e.g., BTCUSDT). The workflow connects to Bitget public endpoints to fetch: Ticker (latest price & 24h stats) Order book depth (top bids/asks) Recent trades (price, side, volume, timestamp) Candlestick data (1m, 15m, 1h, 4h, 1d) Historical candles (optional, for backfill before endTime) A Calculator node derives useful metrics like mid-price and spread. A Think node reshapes raw JSON into Telegram-ready text. A splitter ensures reports over 4000 characters are chunked safely. The final market insights are delivered instantly back to Telegram. What You Can Do with This Agent ‚úÖ Track live prices & 24h stats for any Bitget spot pair. ‚úÖ Monitor order book liquidity and spreads in real-time. ‚úÖ Analyze candlesticks across multiple timeframes. ‚úÖ Review recent trades to see execution flow. ‚úÖ Fetch historical candles for extended market context. ‚úÖ Receive clean, structured reports with optional AI-enhanced formatting. Set Up Steps Create a Telegram Bot Use @BotFather to generate a bot token. Configure in n8n Import Bitget AI Agent v1.02.json into your n8n instance. Add your Telegram credentials (bot token + your Telegram ID in the User Authentication node). Add an OpenAI key if you want AI-powered formatting. (Optional) Add an *Bitget api key** . Deploy and Test Send BTCUSDT to your bot. Get live Bitget spot data instantly in Telegram! üöÄ Unlock powerful, real-time Bitget insights in Telegram‚Äîzero setup, zero API keys required! üì∫ Setup Video Tutorial Watch the full setup guide on YouTube: üßæ Licensing & Attribution ¬© 2025 Treasurium Capital Limited Company Architecture, prompts, and trade report structure are IP-protected. No unauthorized rebranding permitted. üîó For support: Don Jayamaha ‚Äì LinkedIn",78,2025-09-15 18:39:29.722000+00:00,True,7
7731,Automate TikTok Video Posting from Google Sheets & Drive with Blotato,"Automate TikTok video posting from Google Sheets & Drive with Blotato. Perfect for content creators and social media managers. ‚ö†Ô∏è IMPORTANT Self-hosted n8n only - requires community nodes not available in cloud version. Google Sheets Structure Required columns: ID, Media URL, Caption, Status Videos must be in Google Drive Status must be ""pending"" for processing Captions can include hashtags (5 max recommended) How it works Schedule Trigger ‚Üí Runs every hour Fetch Data ‚Üí Gets pending videos from Google Sheets Process Video ‚Üí Extracts Drive ID and shares file Upload ‚Üí Transfers to Blotato platform Post ‚Üí Automatically posts to TikTok Update Status ‚Üí Marks as ""posted"" in spreadsheet Requirements Self-hosted n8n instance Blotato API account Google Drive & Sheets OAuth2 credentials Community node: @blotato/n8n-nodes-blotato.blotato Use cases Automated TikTok content posting Batch video processing Content management workflows Scheduled social media distribution The workflow processes one video per hour to avoid rate limits and maintains a clear audit trail through Google Sheets integration.",75,2025-08-22 10:13:06.656000+00:00,False,2
8572,Automated WhatsApp Welcome Messages for Sales Leads with Google Sheets & Rapiwa,"Automated WhatsApp Welcome Messages for Sales Leads with Google Sheets & Rapiwa Who is this for? This automation is ideal for sales teams, digital marketers, support agents, or small business owners who collect leads in Google Sheets and want to automatically send WhatsApp welcome messages. It's a cost-effective and easy-to-use solution built for those not using the official WhatsApp Business API but still looking to scale communication. What this Workflow Does This n8n automation reads leads from a connected Google Sheet, verifies if the provided WhatsApp numbers are valid using the Rapiwa API, and sends a personalized welcome message. It updates the sheet based on delivery success or failure, and continues this process every 5 minutes ‚Äî ensuring new leads are automatically engaged. Key Features Automatic Scheduling**: Runs every 5 minutes (adjustable) Google Sheets Integration**: Reads and updates lead data WhatsApp Number Validation**: Confirms number validity via Rapiwa Personalized Messaging**: Uses lead name for custom messages Batch Processing**: Sends up to 60 messages per cycle Safe API Usage**: Adds 5-second delay between each message Error Handling**: Marks failed messages as not sent and unverified Live Status Updates**: Sheet columns are updated after each attempt Loop Logic**: Repeats continuously to catch new rows How to Use Step-by-step Setup Prepare Your Google Sheet Copy this Sample Sheet Ensure it includes the following columns: WhatsApp No name (note: trailing space is required) row_number status, check, validity Connect Google Sheets in n8n Use OAuth2 credentials to allow n8n access Set the workflow to fetch rows where check is not empty Get a Rapiwa Account Sign up at https://rapiwa.com Add your WhatsApp number Retrieve your Bearer Token from your Rapiwa dashboard Configure HTTP Request Nodes Use Rapiwa's API endpoints: Verify Number: https://app.rapiwa.com/api/verify-whatsapp Send Message: https://app.rapiwa.com/api/send-message Add your Bearer Token to the header Start Your Workflow Run the n8n automation It will read leads, clean phone numbers, verify WhatsApp validity, send messages, and update the sheet accordingly Requirements A Google Sheet with correctly formatted columns Active Rapiwa subscription (~$5/month) A valid Bearer Token from Rapiwa Your WhatsApp number connected to Rapiwa n8n instance with: Google Sheets integration (OAuth2 setup) HTTP Request capability Google Sheet Column Reference | name | number | email | time | check | validity | status | |-----------------|--------------|-------------------|-----------------------------|---------|------------|-----------| | Abdul Mannan | 8801322827799| contact@spagreen.net| September 14th 2025, 10:34 | checked | verified | sent | | Abdul Mannan | 8801322827798| contact@spagreen.net| September 14th 2025, 10:34 | checked | unverified | not sent | Workflow Logic Summary Trigger Every 5 Minutes Fetch All Rows with Pending Status Limit to 60 Rows per Execution Clean and Format Phone Numbers Check Number Validity via Rapiwa Condition Check: If valid ‚Üí Send Message If invalid ‚Üí Update status as not sent, unverified Send WhatsApp Message via Rapiwa Update Sheet Row On success: sent, verified, checked On failure: not sent, unverified Delay 5 seconds before next message Repeat for next lead Customization Ideas Add image or document sending support via Rapiwa Customize messages based on additional fields (e.g., product, service) Log failures to a separate sheet Send admin email for failed batches Add support for multilingual messages Notes & Warnings The column name ""name "" includes a space ‚Äî do not remove or rename it. International number format is required for Rapiwa to work correctly. If you're sending many messages, increase the Wait node delay to prevent API throttling. Support WhatsApp Support: Chat Now Discord: Join SpaGreen Community Facebook Group: SpaGreen Support Website: https://spagreen.net Developer Portfolio: Codecanyon SpaGreen",71,2025-09-14 07:40:19.761000+00:00,False,3
7288,"Generate and Share Professional PDFs with OpenAI, Google Docs, and Slack","Free PDF Generator in n8n ‚Äì No External Libraries or Paid Services &gt; A 100% free n8n workflow for generating professionally formatted PDFs without relying on external libraries or paid converters. It uses OpenAI to create Markdown content, Google Docs to format and convert to PDF, and integrates with Google Drive and Slack for archiving and sharing, ideal for reports, BRDs, proposals, or any document you need directly inside n8n. Watch the demo video below: Who‚Äôs it for Teams that need auto-generated documents (reports, guides, checklists) in PDF format. Operations or enablement teams who want files archived in Google Drive and shared in Slack automatically. Anyone experimenting with LLM-powered document generation integrated into business workflows. How it works / What it does Manual trigger starts the workflow. LLM generates a sample Markdown document (via OpenAI Chat Model). Google Drive folder is configured for storage. Google Doc is created from the generated Markdown content. Document is exported to PDF using Google Drive. (Sample PDF generated from comprehensive markdown) PDF is archived in a designated Drive folder. Archived PDF is downloaded for sharing. Slack message is sent with the PDF attached. How to set up Add nodes in sequence: Manual Trigger OpenAI Chat Model (prompt to generate sample Markdown) Set/Manual input for Google Drive folder ID(s) HTTP Request or Google Drive Upload (convert to Google Docs) Google Drive Download (PDF export) Google Drive Upload (archive PDF) Google Drive Download (fetch archived file) Slack Upload (send message with attachment) Configure credentials for OpenAI, Google Drive, and Slack. Map output fields: data.markdown ‚Üí Google Docs creation docId ‚Üí PDF export fileId ‚Üí Slack upload Test run to ensure PDF is generated, archived, and posted to Slack. Requirements Credentials**: OpenAI API key (or compatible LLM provider) Google Drive (OAuth2) with read/write permissions Slack bot token with files:write permission Access**: Write access to target Google Drive folders Slack bot invited to the target channel How to customize the workflow Change the prompt** in the OpenAI Chat Model to generate different types of content (reports, meeting notes, checklists). Automate triggering**: Replace Manual Trigger with Cron for scheduled document generation. Use Webhook Trigger to run on-demand from external apps. Modify storage logic**: Save both .md and .pdf versions in Google Drive. Use separate folders for drafts vs. final versions. Enhance distribution**: Send PDFs to multiple Slack channels or via email. Integrate with project management tools for automated task creation.",66,2025-08-12 11:45:18.842000+00:00,True,5
7604,Automatic FTP File Backup to Google Drive with Scheduled Sync,"What this workflow does This workflow automatically downloads files from an FTP folder and uploads them to Google Drive. It‚Äôs useful for backup, reporting, or syncing data between systems. How it works Cron Trigger runs on a schedule (default: every hour). FTP List node checks the specified folder on your FTP server. FTP Download node fetches each file found. Google Drive Upload node saves the file into your chosen Google Drive folder. Pre-conditions / Requirements An active FTP server with accessible login credentials. A Google account with access to the Drive folder you want to sync into. Make sure the target Google Drive folder exists, and copy its Folder ID. Ensure files in your FTP folder have unique names if you want to avoid overwriting in Google Drive. Setup FTP:** Add your FTP credentials and replace {{FTP_FOLDER}} with the path of the folder you want to sync. Google Drive:** Connect your Google Drive account and replace {{GDRIVE_FOLDER_ID}} with the folder ID where files should be stored. Adjust the Cron schedule (e.g. daily, weekly) depending on how often you need the sync. Customization Options File filtering:* Add an *IF node after the FTP List node to only download files matching certain extensions (e.g., .csv, .pdf). File renaming:* Use a *Set node to rename files before uploading to Google Drive (helpful for timestamping or avoiding duplicates). Archiving:** Add a second Google Drive or FTP Upload node to store processed files in an archive folder. Error handling:* Connect an *Error Trigger to notify you via Slack or email if a file transfer fails. Benefits & Use Cases ‚Äî FTP to Google Drive File Sync Benefits Hands-free backups:** Automatically transfers files from FTP to Google Drive. Data security:** Ensures files are stored in cloud storage with built-in redundancy. Reliability:** Reduces risk of missed uploads or manual errors. Scalable:** Works with any FTP folder and Google Drive directory. Use Cases Regularly back up reports or exports from legacy systems (ERP, CRM, POS). Sync daily logs or CSVs into Google Drive for analytics and team access. Automate delivery of partner or client files without manual uploads. Centralize files from multiple FTP sources into one Drive folder.",49,2025-08-19 19:00:19.065000+00:00,False,2
8427,"Automated Range Trading with Uniswap V3, Telegram Alerts & MetaMask Delegation","Simple Range Trading w/ Uniswap V3 This workflow will monitor the price of a token trading pair (default is ETH - USDC) and automatically buy into ETH or sell into USDC based on a price window configured by the user. Additionally, the workflow will notify the user on Telegram before a trade is executed, giving the user 1 minute to cancel the trade before the workflow proceeds. Transaction are executed via MetaMask's Delegation Framework, so trades are fully non-custodial, you retain full control of your assets at all times. Check out the YouTube tutorial for a full walkthrough. Maintain Complete Trading Control with MetaMask Delegations With the MetaMask Delegation Framework, you never relinquish control of your funds to any 3rd party, not even 1Shot API. Furthermore, with this workflow, you have full control of your trading parameters and the assets you trade against. Lastly, by running your own trading setup, you can bypass fees incurred through centralized exchanges or hosted frontends that reduce your profitability. Human-in-the-Loop with Telegram Notifiactions This workflow uses Telegram to send you real-time updates on whats happening with your trades. Specifically, the Telegram human-in-the-loop node is used to give you a change to cancel trades before they happen or immediately approve them. Setup Create a free 1Shot API account, and create an API key. Import the workflow into your n8n portal. Create a credential for the 1Shot API nodes to communicate with your 1Shot API account. Trigger the sub-workflow to generate an Arbitrum server wallet and import all required smart contract methods. Point the 1Shot API nodes at the appropriate smart contract methods in you 1Shot API account. Create a Telegram credential for notifications & input the chatID of your Telegram bot. Set your desired trading price range in the Swap Configs node. Start your trading workflow.",48,2025-09-09 18:08:54.617000+00:00,False,2
7673,Daily Trello Task Tracker with Google Sheets History Log,"This workflow pulls all tasks from your Trello board every day and logs them into a Google Sheet. Use it to track project progress, due dates, and keep a daily snapshot of your board‚Äôs status. ‚öôÔ∏è Setup Instructions 1Ô∏è‚É£ Connect Trello (Developer API) Get your API key: Trello App Key On the same page, generate a Token (click Token) In n8n ‚Üí Credentials ‚Üí New ‚Üí Trello API, paste your API Key + Token, then save. Open each Trello node (Get Board, Get Lists, Get Cards) and select your Trello credential. üß† How It Works Schedule Trigger**: Runs daily (can be customized). Get Board ‚Üí Get Lists ‚Üí Get Cards**: Pulls every task, its list, due date, and description. Map Fields**: Normalizes the data (board name, list name, task name, description, due date, URL). Today's Date Node**: Adds a timestamp column so each run is logged. Google Sheets (Append): Appends all task rows into a sheet ‚Üí creating a **daily history log. üì¨ Contact Need help customizing this (e.g., filtering by list, or sending reports by email/Slack)? üìß rbreen@ynteractive.com üîó Robert Breen üåê ynteractive.com",40,2025-08-20 23:25:10.946000+00:00,False,3
8272,Dynamic MCP Server Selection with OpenAI GPT-4.1 and Contextual AI Reranker,"PROBLEM Thousands of MCP Servers exist and many are updated daily, making server selection difficult for LLMs. Current approaches require manually downloading and configuring servers, limiting flexibility. When multiple servers are pre-configured, LLMs get overwhelmed and confused about which server to use for specific tasks. This template enables dynamic server selection from a live PulseMCP directory of 5000+ servers. How it works A user query goes to an LLM that decides whether to use MCP servers to fulfill a given query and provides reasoning for its decision. Next, we fetch MCP Servers from Pulse MCP API and format them as documents for reranking Now, we use Contextual AI's Reranker to score and rank all MCP Servers based on our query and instructions How to set up Sign up for a free trial of Contextual AI here to find CONTEXTUALAI_API_KEY. Click on variables option in left panel and add a new environment variable CONTEXTUALAI_API_KEY. For the baseline model, we have used GPT 4.1 mini, you can find your OpenAI API key here How to customize the workflow We use chat trigger to initate the workflow. Feel free to replace it with a webhook or other trigger as required. We use OpenAI's GPT 4.1 mini as the baseline model and reranker prompt generator. You can swap out this section to use the LLM of your choice. We fetch 5000 MCP Servers from the PulseMCP directory as a baseline number, feel free to adjust this parameter as required. We are using Contextual AI's ctxl-rerank-v2-instruct-multilingual reranker model, which can be swapped with any one of the following rerankers: 1) ctxl-rerank-v2-instruct-multilingual 2) ctxl-rerank-v2-instruct-multilingual-mini 3) ctxl-rerank-v1-instruct You can checkout this blog for more information about rerankers to learn more about them. Good to know: Contextual AI Reranker (with full MCP docs): ~$0.035/query Includes 0.035 for reranking + ~$0.0001 for OpenAI instruction generation. OpenAI Baseline: ~$0.017/query",39,2025-09-05 03:48:50.424000+00:00,True,4
7672,Generate Daily Pipedrive Deal Summaries with GPT-4o-mini,"This workflow fetches deals and their notes from Pipedrive, cleans up stage IDs into names, aggregates the information, and uses OpenAI to generate a daily summary of your funnel. ‚öôÔ∏è Setup Instructions 1Ô∏è‚É£ Set Up OpenAI Connection Go to OpenAI Platform Navigate to OpenAI Billing Add funds to your billing account Copy your API key into the OpenAI credentials in n8n 2Ô∏è‚É£ Connect Pipedrive In Pipedrive ‚Üí Personal preferences ‚Üí API ‚Üí copy your API token URL shortcut: https://{your-company}.pipedrive.com/settings/personal/api In n8n ‚Üí Credentials ‚Üí New ‚Üí Pipedrive API Company domain: {your-company} (the subdomain in your Pipedrive URL) API Token: paste the token from step 1 ‚Üí Save In the Pipedrive nodes, select your Pipedrive credential and (optionally) set filters (e.g., owner, label, created time). üß† How It Works Trigger**: Workflow runs on manual execution (can be scheduled). Get many deals**: Pulls all deals from your Pipedrive. Code node**: Maps stage_id numbers into friendly stage names (Prospecting, Qualified, Proposal Sent, etc.). Get many notes**: Fetches notes attached to each deal. Combine Notes**: Groups notes by deal, concatenates content, and keeps deal titles. Set Field Names**: Normalizes the fields for summarization. Aggregate for Agent**: Collects data into one object. Turn Objects to Text**: Prepares text data for AI. OpenAI Chat Model + Summarize Agent: Generates a **daily natural-language summary of deals and their current stage. üí¨ Example Prompts ‚ÄúSummarize today‚Äôs deal activity.‚Äù ‚ÄúWhich deals are still in negotiation?‚Äù ‚ÄúWhat updates were added to closed-won deals this week?‚Äù üì¨ Contact Need help extending this (e.g., send summaries by Slack/Email, or auto-create tasks in Pipedrive)? üìß rbreen@ynteractive.com üîó Robert Breen üåê ynteractive.com",33,2025-08-20 23:10:38.598000+00:00,True,4
6403,"Generate Complete SEO Audits with Apify, Claude Sonnet 4, and Gmail Delivery","Description Get professional SEO audits delivered automatically in under 5 minutes. This complete n8n workflow analyzes any website and generates 4 comprehensive reports that you can send directly to clients. What You Get: ‚úÖ Complete n8n Workflow - Ready-to-import automation that runs entire SEO audits ‚úÖ Technical SEO Analysis - HTML structure, performance, and crawlability assessment ‚úÖ Content Quality Audit - Keyword optimization, readability, and E-A-T evaluation ‚úÖ Strategic Opportunities Report - Growth strategies and competitive insights ‚úÖ Executive Summary - Client-ready action plan with prioritized recommendations ‚úÖ Professional Email Templates - Branded delivery system with all reports attached ‚úÖ Setup Documentation - Complete installation guide with API configurations Perfect for agencies, consultants, and freelancers who want to offer high-value SEO audits without manual analysis. Get Started: []https://n8n.partnerlinks.io/ds9podzjls6d SEO Audit Workflow Product Window Product Name ""AI-Powered SEO Audit Automation"" Product Description Get professional SEO audits delivered automatically in under 5 minutes. This complete n8n workflow analyzes any website and generates 4 comprehensive reports that you can send directly to clients. What You Get: ‚úÖ Complete n8n Workflow - Ready-to-import automation that runs entire SEO audits ‚úÖ Technical SEO Analysis - HTML structure, performance, and crawlability assessment ‚úÖ Content Quality Audit - Keyword optimization, readability, and E-A-T evaluation ‚úÖ Strategic Opportunities Report - Growth strategies and competitive insights ‚úÖ Executive Summary - Client-ready action plan with prioritized recommendations ‚úÖ Professional Email Templates - Branded delivery system with all reports attached ‚úÖ Setup Documentation - Complete installation guide with API configurations Perfect for agencies, consultants, and freelancers who want to offer high-value SEO audits without manual analysis. Requirements: You'll need an n8n account to run this workflow. Get Started: https://n8n.partnerlinks.io/ds9podzjls6d üöÄ How to Setup Prerequisites Before importing this workflow, ensure you have: n8n account (cloud or self-hosted) Apify account for web scraping Anthropic account for AI analysis Gmail account for report delivery Step 1: API Credentials Setup 1.1 Apify API Setup Sign up at Apify Go to Account Settings ‚Üí Integrations Copy your API Token In the workflow, update the Apify Crawl Request node URL with your token 1.2 Anthropic API Setup Create account at Anthropic Console Generate an API Key In n8n, go to Credentials ‚Üí Add Credential ‚Üí Anthropic API Enter your API key 1.3 Gmail OAuth2 Setup In n8n, go to Credentials ‚Üí Add Credential ‚Üí Gmail OAuth2 Follow the OAuth2 flow to connect your Gmail account Grant permissions for sending emails with attachments Step 2: Workflow Configuration 2.1 Import Workflow Copy the provided workflow JSON In n8n, click Import from URL/File Paste the JSON and import 2.2 Update Variables Open the Variables node Set your target website URL (include https://) Set the recipient email address Save the node 2.3 Configure Credentials Assign your Anthropic API credential to all Claude model nodes Assign your Gmail OAuth2 credential to the Gmail node Update the Apify URL with your API token Step 3: Test & Run 3.1 Test Run Start with a simple website (your own site recommended) Click Execute Workflow on the manual trigger Monitor execution progress (takes 3-5 minutes) Check for any error messages 3.2 Troubleshooting Apify errors: Verify API token and URL format Anthropic errors: Check API key and usage limits Gmail errors: Re-authenticate OAuth2 connection Timeout issues: Increase node timeout settings if needed Step 4: Customization (Optional) 4.1 Audit Scope Modify maxCrawlDepth in Apify node for more pages Adjust maxCrawlPages for larger sites Update CSS selectors for specific content extraction 4.2 Email Template Customize the HTML email template in Gmail node Update sender information and branding Modify report descriptions and formatting 4.3 AI Prompts Fine-tune prompts in the three audit nodes for specific industries Adjust analysis depth and focus areas Modify scoring criteria and recommendations üìã Workflow Overview The complete process takes 3-5 minutes and includes: Website Crawling (30-60 seconds) - Extracts content and HTML AI Analysis (2-3 minutes) - Three parallel audits using Claude Sonnet 4 Report Generation (1-2 minutes) - Creates and emails 4 professional reports üí° Pro Tips Test with smaller websites first to verify setup Monitor Anthropic API usage to avoid rate limits -Keep backup copies of your customized prompts Set up email filters to organize audit reports üì¶ What Workflow does Technical SEO Audit PDF (‚âà 8-12 pp) Core Web Vitals snapshot Crawl/index errors & fixes Schema, canonical, robots, mobile UX checks Colour-coded priority list (üî¥ / üü° / üü¢) Content Quality Audit PDF (‚âà 6-10 pp) E-E-A-T scoring & gaps Keyword density + missed terms Readability metrics (Flesch, Gunning-Fog) Duplicate-content & freshness flags Strategic SEO Opportunities PDF (‚âà 5-8 pp) SERP-feature wins (Featured Snippets, PAA) Competitor content gaps & topic ideas Link-building prospects & internal-link map ROI-ranked action list Executive Summary PDF (‚âà 3-4 pp) Health scores (/100) for Tech, Content, Strategy 7-day / 30-day / 90-day action roadmap KPIs & ROI projection panel Resource/time estimates for each phase Potential Improvements If you can self-host Gotenberg, you can use it's html-to-pdf function and attach the audit as a document",29,2025-07-25 01:31:37.434000+00:00,True,5
7529,Transform Cloud Documentation into Security Baselines with OpenAI and GDrive,"What this template does Transforms provider documentation (URLs) into an auditable, enforceable multicloud security control baseline. It: Fetches and sanitizes HTML Uses AI to extract security requirements (strict 3-line TXT blocks) Composes enforceable controls** (strict 7-line TXT blocks with true-equivalence consolidation) Builds the final baseline* (TXT or JSON, see *Outputs) with a Technology: header Returns a downloadable artifact via webhook and can append/create the file in Google Drive Why it‚Äôs useful Eliminates manual copy-paste and produces a consistent, portable baseline ready for review, audit, or enforcement tooling‚Äîideal for rapidly generating or refreshing baselines across cloud providers and services. Multicloud support The workflow is multicloud by design. Provide the target cloud in the request and run the same pipeline for: AWS, **Azure, GCP (out of the box) Extensible to other providers/services by adjusting prompts and routing logic How it works (high level) POST /create (Basic Auth) with { cloudProvider, technology, urls[] } Input validation ‚Üí generate uuid ‚Üí resolve Google Drive folder (search-or-create) Download & sanitize each URL AI pipeline: Extractor ‚Üí Composer ‚Üí Baseline Builder ‚Üí (optional) Baseline Auditor Append/create file in Drive and return a downloadable artifact (TXT/JSON) via webhook Request (webhook) Method: POST URL: https://&lt;your-n8n&gt;/webhook/create Auth: Basic Auth Headers: Content-Type: application/json Example input (Postman/CLI) { ""cloudProvider"": ""aws"", ""technology"": ""Amazon S3"", ""urls"": [ ""https://docs.aws.amazon.com/AmazonS3/latest/userguide/security-best-practices.html"", ""https://www.trendmicro.com/cloudoneconformity/knowledge-base/aws/S3/"", ""https://repost.aws/knowledge-center/secure-s3-resources"" ] } Field reference cloudProvider (string, required) ‚Äî case-insensitive. Supported: aws, azure, gcp. technology (string, required) ‚Äî e.g., ""Amazon S3"", ""Azure Storage"", ""Google Cloud Storage"". urls (string\[], required) ‚Äî 1‚Äì20 http(s) URLs (official/reputable docs). Optional (Google Drive destination): gdriveTargetId (string) ‚Äî Google Drive folderId used for append/create. gdrivePath (string) ‚Äî Path like ""DefySec/Baselines"" (folders are created if missing). gdriveTargetName (string) ‚Äî Folder name to find/create under root. Optional (Assistant overrides): assistantExtractorId, assistantComposerId, assistantBaselineId, assistantAuditorId (strings) Resolution precedence Drive: gdriveTargetId ‚Üí gdrivePath ‚Üí gdriveTargetName ‚Üí default folder. Assistants: explicit IDs above ‚Üí dynamic resolution by name (expects 1_DefySec_Extractor, 2_DefySec_Control_Composer, 3_DefySec Baseline Builder, 4_DefySec_Baseline_Auditor). Validation Rejects empty urls or non-http(s) schemes; normalizes cloudProvider to aws|azure|gcp. Sanitizes fetched HTML (removes scripts/styles/headers) before AI steps. Outputs Primary:* downloadable *TXT** file controls_&lt;technology&gt;_&lt;timestamp&gt;.txt (via webhook). Composer outcomes:** if no groups to consolidate ‚Üí NO_CONTROLS_TO_BE_CONSOLIDATED; if nothing valid remains ‚Üí NO_CONTROLS_FOUND.&#x20; JSON path:* when the Builder stage is configured for *JSON-only** output (strict schema), the workflow returns a .json artifact and the Auditor validates it (see next section).&#x20; Techniques used (from the built-in assistants) Provider-aware extraction with strict TXT contract (3 lines):* Extractor limits itself to the declared provider/technology, outputs only Description/Reference/SecurityObjective, and applies a *reflexive quality check** before emitting.&#x20; Normalization & strict header parsing:** Composer normalizes whitespace/fences, requires the CloudProvider/Technology header, and ignores anything outside the exact 3-line block shape.&#x20; True-equivalence grouping & consolidation:* Composer groups *only** when intent, enforcement locus/mechanism, scope, and mode/setting all match‚Äîotherwise items remain distinct.&#x20; 7-line enforceable control format:* Composer renders each (consolidated or unique) control in *exactly seven labeled lines** to keep results auditable and automatable.&#x20; Builder with JSON-only schema & technology inference:* Builder parses 7-line blocks, infers technology, consolidates true equivalents again if needed, and returns *pure JSON** matching a canonical schema (with counters in meta).&#x20; Self-evaluation loop (Auditor):* Auditor *unwraps transport, validates **schema & content, checks provider terminology/scope/automation, and returns either GOOD_ENOUGH or a JSON instruction set for the Builder to fix and re-emit‚Äîenabling reflective improvement.&#x20; Reference prioritization:** Across stages, official provider documentation is preferred in References (AWS/Azure/GCP).&#x20; Customization & extensions Prompt-reflective techniques:** keep (or extend) the Auditor loop to add more review passes and quality gates.&#x20; Compliance assistants:* add assistants to analyze/label controls for *HIPAA, PCI DSS, SOX** (and others), emitting mappings, gaps, and remediation notes. Implementation context:* feed internal implementation docs, runbooks, or *Architecture Decision Records (ADRs); use these as **grounding to generate or refine controls (works with local/self-hosted LLMs, too). Local/self-hosted LLMs:** swap OpenAI nodes for your on-prem LLM endpoint while keeping the pipeline. Provider-specific outputs:** extend the final stage to export Policy-as-Code or IaC snippets (Rego/Sentinel, CloudFormation Guard, Bicep/ARM, Terraform validations). Assistant configuration & prompts Full assistant configurations and prompts (Extractor, Composer, Baseline Builder, Baseline Auditor) are available here: https://github.com/followdrabbit/n8nlabs/tree/main/Lab03%20-%20Multicloud%20AI%20Security%20Control%20Baseline%20Builder/Assistants Security & privacy No hardcoded secrets in HTTP nodes; use n8n‚Äôs Credential Manager. Drive operations are optional and folder-scoped. For sensitive environments, switch to a local LLM and provide only sanitized/approved inputs. Quick test (curl) curl -X POST ""https://&lt;your-n8n&gt;/webhook/create"" \ -u ""&lt;user&gt;:&lt;pass&gt;"" \ -H ""Content-Type: application/json"" \ -d '{ ""cloudProvider"":""aws"", ""technology"":""Amazon S3"", ""urls"":[ ""https://docs.aws.amazon.com/AmazonS3/latest/userguide/security-best-practices.html"" ] }' \ -OJ",29,2025-08-18 02:08:44.545000+00:00,True,4
8585,Extract Purchase Orders from Gmail using Gemini AI and Save to Google Sheets,"üöÄ AI-Powered Email to Purchase Order Workflow Automatically scan your inbox for new purchase order requests, extract order details using Gemini AI, and log them into Google Sheets ‚Äî all without manual effort. ‚ú® Core Capabilities ‚è± Runs every minute to check unread emails üìß Filters emails by subject ü§ñ Uses Gemini AI to summarize email content & extract structured order details üìÖ Formats dates into ISO calendar weeks üìä Adds product data from Google Sheets to complete order info ‚úÖ Appends final purchase order records into a Google Sheet (without replacing previous ones) üõ† Setup Essentials üì© Gmail account for fetching unread emails üîë Google Gemini (PaLM) API credentials üìí Google Sheet with predefined purchase order headers üìñ Activation Guide ‚öôÔ∏è Configure Gmail & Google Sheets credentials in n8n üéØ Adjust the subject filter to match your email rules üîå Connect Gemini AI with your API credentials üìë Create a Google Sheet with the required headers ‚ñ∂Ô∏è Activate the workflow and let it run in the background üé® Customizing the Workflow üîç Email Filters ‚Üí Change keywords in the filter node to match your purchase order email subjects üè∑ Order Fields ‚Üí Modify Set and Append to Google Sheet nodes if your schema differs ‚úçÔ∏è AI Instructions ‚Üí Adjust the AI Agent‚Äôs prompt to fit your company‚Äôs email style or product details ‚è≤ Frequency ‚Üí Update the Cron node if you want to scan emails less often üìÇ Target Google Sheet ‚Üí Point to a different sheet or tab depending on your department or customer",29,2025-09-15 05:54:18.889000+00:00,True,5
6333,Automatic Email Unsubscribe Handler: Outlook to BigQuery Integration,"üö´ Email Unsubscribe Handler for Outlook Description This n8n workflow automatically scans recent email replies from your Outlook inbox and identifies unsubscribe requests. If a contact replies with any variation of ""unsubscribe"" within the past 7 days, the system performs two key actions: Saves the contact‚Äôs email address in a BigQuery unsubscribes table (for compliance and tracking). Deletes that contact from the active leads table in BigQuery (to stop future outreach). This flow can be triggered on a schedule (e.g. every 4 hours) or run manually as needed. Key Features üì• Email Parsing from Outlook: Automatically monitors for replies that contain unsubscribe language. üß† Smart Filtering: Captures unsubscribes based on message content, not just subject lines. üîÑ BigQuery Integration: Logs unsubscribed emails and removes them from your leads dataset. ü§ù Connect with Me Description I‚Äôm Robert Breen, founder of Ynteractive ‚Äî a consulting firm that helps businesses automate operations using n8n, AI agents, and custom workflows. I‚Äôve helped clients build everything from intelligent chatbots to complex sales automations, and I‚Äôm always excited to collaborate or support new projects. If you found this workflow helpful or want to talk through an idea, I‚Äôd love to hear from you. Links üåê Website: https://www.ynteractive.com üì∫ YouTube: @ynteractivetraining üíº LinkedIn: https://www.linkedin.com/in/robert-breen üì¨ Email: rbreen@ynteractive.com",28,2025-07-23 19:07:02.461000+00:00,True,3
10750,"Automate Blog-to-Social Media with GPT-4 for LinkedIn, X, and Reddit","Turn your blog into a set-and-forget content engine: every new article is instantly repurposed into channel-specific social posts with visuals, keeping your brand visible on LinkedIn, X, and Reddit without extra copywriting time. Perfect for lean marketing teams who want consistent, always-on distribution from a single source of content. How it works ‚Ä¢ Watches your blog RSS feed (or receives a single URL) and detects new articles. ‚Ä¢ Saves each post in Postgres so every article is only processed once. ‚Ä¢ Fetches the article HTML, extracts the main body content and sends it to OpenAI (GPT-4.1). ‚Ä¢ OpenAI creates platform-optimized copy: 1 LinkedIn post, 1 X/Twitter post, 1 Reddit post + image prompts. ‚Ä¢ Generates on-brand images with OpenAI and publishes everything automatically to LinkedIn, X, and Reddit. ‚Ä¢ You can also trigger it manually or via webhook whenever you want to push a specific campaign. Setup Steps ‚Ä¢ Time: around 20‚Äì40 minutes for someone familiar with n8n and the platforms. ‚Ä¢ Create a Postgres table ‚Äúrss_items‚Äù with fields: guid (PRIMARY KEY), title, link, published_at. ‚Ä¢ Add credentials in n8n for: ‚Äì Postgres ‚Äì OpenAI ‚Äì LinkedIn OAuth2 ‚Äì X/Twitter OAuth2 + OAuth1 (for media upload) ‚Äì Reddit OAuth2 ‚Ä¢ In the RSS node, set your blog feed URL (for example: https://yourblog.com/feed). ‚Ä¢ In the webhook node, confirm the URL/path you want external tools or other workflows to call with a ‚Äúlink‚Äù field. ‚Ä¢ Run the manual trigger with one test blog URL to verify: ‚Äì Article content is extracted correctly. ‚Äì AI returns LinkedIn/X/Reddit posts and image prompts. ‚Äì Posts and images appear correctly on all social accounts. ‚Ä¢ Once tests look good, enable the Schedule Trigger so Blog2Social AI runs automatically at your chosen interval.",28,2025-11-12 09:24:53.723000+00:00,True,7
8142,Extract Amazon Book Data & Generate Purchase Reports with Decodo Scraper,"Decodo Scraper API Workflow Template (n8n Automation Amazon Book Purchase Report) Watch the demo video below: &gt; This workflow demos how to use Decodo Scraper API to crawl any public web page (headless JS, device emulation: mobile/desktop/tablet), extract structured product data from the returned HTML, generate a purchase-ready report, and automatically deliver it as a Google Doc + PDF to Slack/Drive. Who‚Äôs it for Creators / Analysts** who need quick product lists (books, gadgets, etc.) with prices/ratings. Ops & Marketing teams** building weekly ‚Äútop picks‚Äù reports. Engineers** validating the Decodo Scraper API + LLM extraction pattern before scaling. How it works / What it does Trigger ‚Äì Manually run the workflow. Edit Fields (manual) ‚Äì Provide inputs: targetUrl (e.g., an Amazon category/search/listing page) deviceType (desktop | mobile | tablet) Optional: maxItems, notes, reportTitle, reportOwner Scraper API Request (HTTP Request ‚Üí POST) Calls Decodo Scraper API with: URL to crawl, headless JS enabled Device emulation (UA + viewport) Optional waitFor / executeJS to ensure late-loading content is captured HTML Response Parser (Code/Function or HTML node) Pulls the HTML string from Decodo response and normalizes it (strip scripts/styles, collapse whitespace). Product Analyzer Agent (LLM + Structured Output Parser) Prompts an LLM to extract structured ‚Äúbook‚Äù objects from the HTML: The Structured Output Parser enforces a strict JSON schema and drops malformed items. Build üìö Book Purchase Report (Code/LLM) Converts the JSON array into a Markdown (or HTML) report with: Executive summary (top picks, average price/rating) Table of items (rank, title, author, price, rating, link) ‚ÄúRecommended to buy‚Äù shortlist (rules configurable) Notes / owner / timestamp Configure Google Drive Folder (manual) Choose/create a Drive folder for output artifacts. Create Document File (Google Docs API) Creates a Doc from the generated Markdown/HTML. Convert Document to PDF (Google Drive export) Exports the Doc to PDF. Upload report to Slack Sends the PDF (and/or Doc link) to a chosen Slack channel with a short summary. How to set up 1 Prerequisites n8n** (self-hosted or Cloud) Decodo Scraper API** key OpenAI (or compatible) API key** for the Analyzer Agent Google Drive/Docs** credentials (OAuth2) Slack** Bot/User token (files:write, chat:write) 2 Environment variables (recommended) DECODO_API_KEY OPENAI_API_KEY DRIVE_FOLDER_ID (optional default) SLACK_CHANNEL_ID 3 Nodes configuration (high level) Edit Fields (Set node) Scraper API Request (HTTP Request ‚Üí POST) HTML Response Parser (Code node) Product Analyzer Agent Build Book Purchase Report (Code/LLM) Create Document File Convert to PDF Upload to Slack Requirements Decodo**: Active API key and endpoint access. Be mindful of concurrency/rate limits. Model**: GPT-4o/4.1-mini or similar for reliable structured extraction. Google**: OAuth client (Docs/Drive scopes). Ensure n8n can write to the target folder. Slack**: Bot token with files:write + chat:write. How to customize the workflow Target site: Change targetUrl to any **public page (category, search, or listing). For other domains (not Amazon), tweak the LLM guidance (e.g., price/label patterns). Device emulation**: Switch deviceType to mobile to fetch mobile-optimized markup (often simpler DOMs). Late-loading pages**: Adjust waitFor.selector or use waitUntil: ""networkidle"" (if supported) to ensure full content loads. Client-side JS**: Extend executeJS if you need to interact (scroll, click ‚Äúnext‚Äù, expand sections). You can also loop over pagination by iterating URLs. Extraction schema**: Add fields (e.g., discount_percent, bestseller_badge, prime_eligible) and update the Structured Output schema accordingly. Filtering rules**: Modify recommendation logic (e.g., min ratings count, price bands, languages). Report branding**: Add logo, cover page, footer with company info; switch to HTML + inline CSS for richer Docs formatting. Destinations**: Besides Slack & Drive, add Email, Notion, Confluence, or a database sink. Scheduling: Add a **Cron trigger for weekly/monthly auto-reports.",27,2025-09-02 04:15:58.617000+00:00,True,7
5668,Track Top YouTube Videos by View Count with Google Sheets,"Workflow Setup Guide This workflow collects the most-viewed videos from specified YouTube channels and saves the data to a Google Sheet. Follow these steps to set it up: 1. Credentials Setup Google Sheets:** You need to have a Google Sheets credential configured in your n8n instance. If you don't have one, go to the 'Credentials' section in n8n and add a new credential for Google Sheets. YouTube API Key:** You need a YouTube Data API v3 key. Go to the Google Cloud Console. Create a new project or select an existing one. Go to 'APIs & Services' &gt; 'Library' and enable the YouTube Data API v3. Go to 'APIs & Services' &gt; 'Credentials', click 'Create Credentials', and choose 'API key'. Copy the generated API key. 2. Google Sheet Setup You will need one Google Sheet with two separate sheets (tabs) inside it. Input Sheet Use Template This sheet provides the list of YouTube channels to process. Required Columns:** Create a sheet with the following two columns: ChannelID: The ID of the YouTube channel (e.g., T7M3PpjBZzw). video_num_to_get: The number of top videos to retrieve for that channel (e.g., 5). Output Sheet This sheet is where the results will be saved. Required Columns:** The workflow will automatically append data to the following columns. You can create them beforehand or let the workflow do it. channelName title videoId videoLink 3. Node Configuration Read Channel Info from Sheet:** Select your Google Sheets credential. Enter your Spreadsheet ID. Enter the name of your Input Sheet. Fetch Most-Viewed Videos via YouTube API:** Replace YOUR_YOUTUBE_API_KEY with the API key you generated in Step 1. Append Video Details to Sheet:** Select your Google Sheets credential. Enter your Spreadsheet ID (the same one as before). Enter the name of your Output Sheet.",24,2025-07-04 04:51:14.579000+00:00,False,2
7251,Flag Bounced Emails in Google Sheets from Gmail Delivery Error Messages,"Scanning Email Inbox for Delivery Errors Prerequisite: Automate Personalized Email Campaigns with Google Docs, Sheets, and SMTP. How It Works After running your email campaign, some messages may fail to deliver. This workflow scans your email inbox for delivery errors (e.g., bounced messages), flags problematic email addresses in the Google Sheet and ensures future campaigns skip them. How to Use Ensure Prerequisite Workflow: You should have the Email Campaign Workflow configured and running. Google Sheet Setup: Use the Google Sheet Template. Identify your document‚Äôs ID (the string after /d/ and before /edit in the URL). Configure Workflow: Enter your Google Sheet ID in the settings node. Connect your Google credentials to n8n. Email Inbox: Set up the readspamfolder node to search for bounce/error messages in your mail (e.g., in the Spam or Inbox folders‚Äîadjust label/folder if emails land elsewhere). Google Sheet Update: Configure the lookupemail and update_err nodes Requirements Google Credentials** to access Gmail and sheets. Gmail Account** (bounce/error messages must be accessible here). n8n Version:** Tested with 1.105.2 (Ubuntu). Need Help? Comment this post or contact me on LinkedIn Ask in the n8n Community Forum!",24,2025-08-11 12:18:24.379000+00:00,False,3
7073,Export Dialogflow Intents with Priority Levels to Google Sheets via Telegram,"What does this workflow do? This workflow exports the names of all Dialogflow intents from your agent, together with their priority levels, directly into a Google Sheets spreadsheet. It is triggered via Telegram and includes visual indicators (emojis) for priority levels. üìú Overview üîî Activation**: Triggered when a validated user sends the keyword (e.g. ""backup"") via Telegram. üì• Data Retrieval**: Fetches all intents of the specified Dialogflow agent using the Dialogflow API. ‚öôÔ∏è Processing**: Transforms each intent into an n8n-compatible item. Extracts the displayName and priority of each intent. Assigns an emoji and descriptive label based on priority tier: üî¥ Highest, üü† High, üîµ Normal, üü¢ Low, üö´ Ignore. üìë Storage**: Appends each intent (name, priority number, emoji, and description), along with current date and time, to a Google Sheets document. üì© Notification**: Sends a single confirmation message to the Telegram user once insertion is complete (using Execute Once). üõ†Ô∏è How to install and configure Import the workflow: Upload the .json into your n8n instance. Connect Telegram: Add your Telegram bot credentials and configure the node Validaci√≥n de usuario por ID with your Telegram ID. Configure Dialogflow: Authenticate using a Google Service Account API Credential. Then, in the Obtiene datos de los intents node, replace the example project ID (TU_PROJECT_ID) with your actual Dialogflow agent's project ID. Connect Google Sheets: Authorize Google Sheets via OAuth2 and select your destination document/sheet in the node A√±adir fila en la hoja. Customize trigger keyword: Adjust the command text (default ""backup"") if needed. Activate workflow: Ensure the webhook is correctly set up in Telegram before enabling the workflow. üë• Who is this for? ü§ñ Bot administrators who need quick backups of Dialogflow intent names. üåê Teams managing multilingual or multi-intent agents wanting priority oversight. üíª Development teams needing an automated way to audit or version intent configurations regularly. üí° Use Cases ‚öôÔ∏è Backup intents periodically to monitor changes over time. üìä Visualize priority assignment in a spreadsheet for analysis or team discussion. üìñ Document conversational structure for onboarding or knowledge transfer.",20,2025-08-06 22:38:12.352000+00:00,False,4
10900,Generate LinkedIn Carousel Images from Text with Mistral AI & S3 Storage,"AI Carousel Caption & Template Editor Workflow Overview This workflow is a caption-only carousel text generator built in n8n. It turns any raw LinkedIn post or text input into 3 short, slide-ready title + subtext captions and renders those captions onto image templates. Output is a single aggregated response with markdown image embeds and download links. Workflow Structure Input:** Chat UI trigger accepts text and optional template selection. Core AI:** Agent cleans input and returns structured JSON with 3 caption pairs. Template Rendering:** Edit Image nodes render title and subtext on chosen templates. Storage:** Rendered images uploaded to S3. Aggregate Output:** Aggregate node builds final markdown response with embeds and download links. Chat Trigger (Frontend) Trigger:** When chat message received UI accepts plain text post. allowFileUploads optional for template images. SessionId preserved for context. AI Agent (Core) Node name:** AI Agent Model:** Mistral Cloud Chat Model (mistral-small-latest) Behavior:** Clean input (remove stray formatting like \n and ** but keep emojis). Produce exactly one JSON object with fields: postclean, title1, subtext1, title2, subtext2, title3, subtext3. Titles must be short (max 5 words). Subtext 1 or 2 short sentences, max 7 words per line if possible. Agent must return valid JSON to be parsed by the Structured Output Parser. Structured Output Parser Node name:** Structured Output Parser Validates agent JSON and prevents downstream errors. If parsing fails, stop and surface error. Normalize Title Nodes Nodes:** normalize title,name 1, normalize title,name 2, normalize title,name 3 (and optional 4) Map parsed output into node fields: title, subtext, safeName (safe filename for exports). Template Images Source:** Google Drive template PNGs (download via Google Drive nodes) or provided upload. Keep templates high resolution and consistent aspect ratio. Edit Image Nodes (Render Captions) Nodes:** Edit Image 1, Edit Image2, Edit Image3, Edit Image3 (or Edit Image3/Edit Image4 as available) MultiStep operations render: Title text (font, size, position) Subtext (font, size, position) This is where caption text is added to the template. Upload to S3 Nodes:** S3 Upload rendered images to bucketname using safeName filenames. Confirm public access or use signed URLs. Get S3 URLs and Aggregate Nodes:** get s3 url image 1, get s3 url image 2, get s3 url image 3, get s3 url image 4 Merge + Aggregate:** Merge1 and Aggregate collect image items. Output Format:** output format builds a single markdown message: Inline image embeds `` Download links per image. Integrations Used | Service | Purpose | Credential | |---------|---------|-----------| | Mistral Cloud | AI agent model | Mistral account | | Google Drive | Template image storage | Google Drive account | | S3 | Store rendered images and serve links | Supabase account | | n8n Core | Flow control, parsing, image editing | Native | Agent System Prompt Summary &gt; You are a data formatter and banner caption creator. Clean the user input (remove stray newlines and markup but keep emojis). Return a single JSON object with postclean, title1/subtext1, title2/subtext2, title3/subtext3. Titles must be short (max 5 words). Subtext should be 1 to 2 short sentences, useful and value adding. Respond only with JSON. Key Features Caption only output: 3 short slide-ready caption pairs. Structured JSON output enforced by a parser for reliability. Renders captions onto image templates using Edit Image nodes. Uploads images to S3 and returns markdown embeds plus download links. Template editable: swap Google Drive background templates or upload your own. Zero guess formatting: agent must produce parseable JSON to avoid downstream failures. Summary A compact n8n workflow that converts raw LinkedIn text into a caption-only carousel with rendered images. It enforces tight caption rules, validates AI JSON, places captions on templates, uploads images, and returns a single ready-to-post markdown payload. Need Help or More Workflows? We can wire this into your account, replace templates, or customize fonts, positions, and export options. We can help you set it up for free ‚Äî from connecting credentials to deploying it live. Contact: shilpa.raju@digitalbiz.tech Website: https://www.digitalbiz.tech LinkedIn: https://www.linkedin.com/company/digital-biz-tech/ You can also DM us on LinkedIn for any help.",20,2025-11-17 05:06:54.929000+00:00,True,6
8111,Analyze Google Business Reviews & Send Sentiment Reports to Slack with Gemini,"‚≠ê Google Review Sentiment Analysis & Slack Notification Workflow This workflow automates the process of collecting Google Business Profile reviews üè™, analyzing customer sentiment with Google Gemini ü§ñ‚ú®, and sending structured reports to Slack üí¨. üîë Key Advantages üì• Fetches Google Business Profile reviews for a given business and time period üß† Runs sentiment analysis using Gemini AI üìä Consolidates comments, ratings, and trends into a JSON-based summary üß© Restructures results into Slack Block Kit format for easy readability üöÄ Sends automated sentiment reports directly to a Slack channel ‚öôÔ∏è Set Up Essentials You‚Äôll Need üîë Google Business Profile API access with project approval ‚úÖ Enabled Google Business Profile API service üîê Gemini API credentials üí¨ Slack workspace & channel for receiving reports üöÄ How to Get Started üîß Configure your Google Business Profile API and enable access üë§ Set the owner name and üìç location to fetch reviews ‚è≥ Define the review time period using the Set Time Period node üîó Connect your Slack account and select a channel for notifications üïí Deploy and let the workflow run on schedule for automated insights",16,2025-09-01 08:17:37.468000+00:00,True,6
7291,"Full-Cycle Invoice Automation: Airtable, QuickBooks & Stripe","This n8n template from Intuz provides a complete and automated solution for full-cycle invoicing, orchestrating a seamless flow between Airtable, QuickBooks, and Stripe. This is the ultimate sales-to-cash automation. When a deal in Airtable is marked ""Approved for Invoicing,"" this workflow intelligently syncs customer data across QuickBooks and Stripe (creating them if they don't exist), generates an official QuickBooks invoice, creates a Stripe payment link, and then updates the original Airtable record with all the new IDs and links. Eliminate manual data entry and keep your systems perfectly in sync. Who's this workflow for? Finance, Accounting, and Operations Teams SalesOps and RevOps Teams Small Business Owners and Founders Agencies and Freelancers How It Works: 1. Airtable Trigger & Approval Gate: The workflow starts when a record in your Airtable base is updated. An If node immediately checks if the Status field is set to ""Approved for Invoicing."" If not, the workflow for that item stops. 2. Customer Sync (QuickBooks & Stripe): The workflow searches for the customer in both QuickBooks and Stripe using the details from Airtable. Using If nodes, it intelligently checks if the customer exists. If a customer is not found in either platform, it creates a new one. This ""find-or-create"" logic prevents duplicate records. 3. Update Airtable with IDs: Once the customer IDs from both QuickBooks and Stripe are secured (either found or newly created), the workflow updates the original Airtable record with these new IDs for future reference. 4. Generate Financials: Stripe Payment Link: It sends an HTTP request to Stripe to create a unique, ready-to-use payment link for the specified amount. QuickBooks Invoice: It fetches your product list from QuickBooks, finds the matching item from the Airtable record, and generates a formal, detailed invoice. 5. Close the Loop: In the final step, the workflow updates the Airtable record one last time to: Add the QuickBooks Invoice #. Add the Stripe Payment Link. Change the Status to ""Invoiced."" Step-by-Step Setup Instructions This is an advanced workflow. Follow these setup steps carefully. 1. Connect Your Credentials Airtable: Create and connect a Personal Access Token with data.records:read and data.records:write scopes. QuickBooks: Connect your QuickBooks Online account using OAuth2 credentials. Stripe: Connect your Stripe account using your Secret Key. 2. Airtable Base Setup (Crucial) Your Airtable base must have a table with the following columns. The names must match exactly: Deal Name (Text) Client Name (Text) Client Email (Email) Status (Single-select with options: Draft, Approved for Invoicing, Invoiced) QuickBooks Customer ID (Text) Stripe Customer ID (Text) Stripe Payment Link (URL) QuickBooks Invoice # (Text) Stripe Price Id (Text - The API ID of your price in Stripe, e.g., price_123...) Quantity (Number) Quickbooks Product Name (Text) Created (Created Time) - This is used by the trigger. 3. Configure the n8n Nodes All Airtable Nodes: In each Airtable node, select your Base and Table from the dropdown lists. Get all Quickbook products (HTTP Request Node): You must replace {YOUR_QUICKBOOKS_COMPANY_ID} in the URL with your actual QuickBooks Company ID (also known as a Realm ID). 4. Activate the Workflow Save the workflow and toggle the Active switch to ""on"". The workflow will now trigger whenever the Created field is updated for a record in your Airtable base. Customization Guidance Changing the Trigger Status: If you use a different status than ""Approved for Invoicing,"" simply update the value in the ""IF - Status Check"" node. Modifying Invoice Details: You can customize the Description or other line item details in the ""Create an invoice"" (QuickBooks) node by pulling more fields from your Airtable record. Adding Email Notifications: To notify a customer when their invoice is ready, add a Gmail or SendGrid node after the last Airtable Update node. You can include the Stripe Payment Link and a PDF of the QuickBooks invoice directly in the email. Advanced Error Handling: For a production environment, consider connecting the false output of the various IF nodes or using the .onError() workflow setting to send a Slack or email alert if a customer can't be found or an API call fails. Support For further support, or to develop a custom workflow, reach out to: Website: https://www.intuz.com/services Email: getstarted@intuz.com LinkedIn: https://www.linkedin.com/company/intuz Get Started: https://n8n.partnerlinks.io/intuz For Custom Worflow Automation Click here- Get Started",14,2025-08-12 12:36:34.244000+00:00,False,5
6183,Verify WhatsApp Numbers in Bulk using Google Sheets & WasenderAPI,"Bulk WhatsApp Number Verification via Unofficial API Who‚Äôs it for This workflow is perfect for marketers, CRM managers, support teams, and data verification agents who need to bulk-check WhatsApp number validity without using the official WhatsApp Business API. It is ideal for teams seeking an affordable and scalable way to clean or validate phone number lists from Google Sheets. Overview This n8n workflow verifies unlimited WhatsApp numbers from a Google Sheet using your own WhatsApp number through the WasenderAPI.com platform. How it works / What it does Fetches real-time data from Google Sheets Verifies if a number is registered on WhatsApp Implements delay and batch processing to maintain rate limits Updates verification status back to the Google Sheet Requirements Before setting up the workflow, ensure you have: An active WhatsApp Account (Personal or Business) Google Sheets API configured within your n8n instance A properly structured Google Sheet, like a sample sheet(Included with the workflow template) A WasenderAPI.com subscription (starting from ~$6/month) Google Sheet Format Use this sample sheet as your base. | WhatsApp No | Verified/Unverified | Status | |---------------|---------------------|------------| | +8801XXXXXXX | (empty) | (empty) | &gt; Ensure the Status column is initially blank for unverified rows. How to set up Step 1: Connect to Google Sheets Add a Google Sheets node. Authenticate with your Google account. Select your target document and worksheet. Apply a filter to only select rows where Status is empty. Step 2: Loop Through Rows with Delay Add a SplitInBatches or Code node to process 5 rows at a time. Add a Wait node (set to 5 seconds) between each request to respect API limits. Step 3: Verify Number via HTTP Request Add an HTTP Request node Method: POST URL: https://app.wasenderapi.com/api/send-message Headers: Content-Type: application/json Authorization: Bearer YOUR_API_KEY Workflow Highlights Triggers every 5 Minutes Reads pending contact from Google Sheets Verify WhatsApp number using WasenderAPI or WhatsApp Cloud API Updates each row‚Äôs Status to Checked Support & Community Need help setting up or customizing the workflow? Reach out here: WhatsApp: Chat with Support Discord: Join SpaGreen Server Facebook Group: SpaGreen Community Website: SpaGreen Creative Envato: SpaGreen Portfolio",11,2025-07-19 21:20:43.959000+00:00,False,3
9031,Automate Shopify Abandoned Cart WhatsApp Reminders with Product Links via Rapiwa,"Automate Shopify Abandoned Cart WhatsApp Reminders with Product Links via Rapiwa Who‚Äôs it for This n8n workflow automatically identifies customers who have abandoned their carts on your Shopify store, cleans and verifies their WhatsApp numbers, and sends them personalized reminders via the Rapiwa API. It also logs each interaction‚Äîwhether the number was valid or not‚Äîinto a connected Google Sheet. Designed for marketers, small business owners, freelancers, and support teams, this solution makes it easy to run bulk WhatsApp campaigns using just a Google Sheet and your own WhatsApp number‚Äîno need for the official WhatsApp Business API. It‚Äôs a budget-friendly, scalable, and easy-to-manage alternative for anyone looking to automate WhatsApp follow-ups without the tech hassle. How it Works / What It Does Reads rows from a Google Sheet where the Status column is marked as ""pending"". Cleans each phone number (removes special characters, spaces, etc.). Verifies whether the number is a WhatsApp user using the Rapiwa API. If valid: Sends the message via Rapiwa. Updates Status = sent and Verification = verified. If invalid: Skips sending. Updates Status = not sent and Verification = unverified. Waits a configurable delay between sends to avoid rate limits. Processes rows in small batches and repeats on schedule (default every 5 minutes). How to Set Up Duplicate the sample Google Sheet format and populate it with contacts and messages. Fill required columns such as WhatsApp No, Name, Message, Image URL, and set Status = pending. Connect Google Sheets in n8n and grant the required OAuth scopes. Create an HTTP Bearer credential in n8n and paste your Rapiwa API key. Configure the workflow nodes (Trigger, Google Sheets, Limit, SplitInBatches, Code, HTTP Request, If, Update Google Sheets, Wait). Enable the workflow in n8n. Requirements Google Sheets API credentials (OAuth2) configured in n8n Google Sheet matching the template (WhatsApp No, Name, Message, Status, Verification) Rapiwa account and Bearer token n8n instance with HTTP Request and Google Sheets nodes enabled How to Customize the Workflow Add or increase delay between messages using the Wait node (e.g., 5‚Äì10 seconds). Change message content or include media by editing the HTTP Request body. Personalize messages using sheet columns (Name, product details, coupon codes). Add error handling nodes to retry failed sends or log errors. Adjust the Limit node to control how many rows are processed per run. Workflow Highlights Triggered every 5 minutes using the Schedule Trigger node. Filters sheet rows where Status = pending. Cleans numbers and verifies WhatsApp existence before sending. Sends messages via Rapiwa (unofficial API). Updates Google Sheet rows with Status = sent/not sent and Verification = verified/unverified. Uses a Wait node to prevent rapid-fire sending. Setup in n8n 1. Connect Google Sheets Add a Google Sheets node. Authenticate with your Google account. Select the document and worksheet. Use a filter or query to fetch rows with Status = pending. 2. Loop Through Rows Use SplitInBatches to process rows in small chunks (e.g., 5 rows per batch). Add a Code node to clean phone numbers (remove non-digits). Add a Wait node to pause between individual sends (recommended 5 seconds). 3. Send Message via HTTP Node Configure an HTTP Request node to POST to: https://app.rapiwa.com/api/send-message Use Bearer Token authentication with your Rapiwa API key. Example JSON body: { ""number"": ""{{ $json['WhatsApp No'] }}"", ""message"": ""{{ $json['Message'] }}"" } After the send, update the row in Google Sheets with the result. Sample Google Sheet Structure A Google Sheet formatted like this ‚û§ sample | name | number | order id | item name | coupon | item link | total price | validity | status | | -------------- | ------------- | -------------- | --------------------------------- | ------ | -------------------------------------------------------------------------------- | ------------ | ---------- | -------- | | Abdul Mannan | 8801400620056 | 39248398811454 | S25 Ultra 5XXXXX Price Cell Phone | | Product Link | 11500.00 BDT | unverified | not sent | | Abdul Mannan | 8801400620055 | 39248402153790 | S25 Ultra 5XXXXX Price Cell Phone | | Product Link | 11500.00 BDT | verified | sent | | Abdul Mannan | 8801400620055 | 39248403431742 | S25 Ultra 5XXXXX Price Cell Phone | | Product Link | 11500.00 BDT | verified | sent | Tips Ensure numbers are in proper format (e.g., 8801XXXXXXXXX ‚Äî no +, no spaces). Use public image URLs if sending media. Store Rapiwa API key securely in n8n credentials. Test with a small batch before scaling up. Mark processed rows as ""sent"" to avoid duplicates. Increase Wait time or reduce batch size if you encounter rate limiting. Important Notes Avoid Duplicates: The Google Sheet serves as a ledger. Sent messages won‚Äôt be repeated in the next cycle. Rate Limit Awareness: Use Wait nodes to space out requests and avoid getting banned. Add Delay for Safety: If needed, increase wait times or limit message batches. Extendable: You can add product images, links, or even abandon cart discounts dynamically. Future Enhancements (Ideas) Add delay between batches (e.g., 60 messages/hour) Auto-update Shopify checkout status (via GraphQL API) Use more advanced templates or media (via Rapiwa) Add error handling to log failed API calls Use Telegram/Slack node to notify when process ends Useful Links Dashboard:** https://app.rapiwa.com Official Website:** https://rapiwa.com Documentation:** https://docs.rapiwa.com Support WhatsApp Support: Chat Now Discord: Join SpaGreen Community Facebook Group: SpaGreen Support Website: https://spagreen.net Developer Portfolio: Codecanyon SpaGreen",11,2025-09-28 05:00:04.423000+00:00,False,3
5723,Automate Vendor Due Diligence Research with Gemini & Jina AI,"This workflow contains community nodes that are only compatible with the self-hosted version of n8n. üë• Who is this for? This workflow is designed for a variety of professionals who manage vendor relationships and data security. It is especially beneficial for: üõ°Ô∏è GRC (Governance, Risk, and Compliance) Professionals**: Streamline your risk assessment processes üîí Information Security Teams**: Quickly evaluate the security posture of third-party vendors üìã Procurement Departments**: Enhance due diligence when onboarding new service providers üöÄ Startup Founders**: Efficiently assess vendors without a dedicated security team This tool is perfect for anyone looking to automate the manual review of vendor websites, policies, and company data. ‚ú® üéØ What problem is this workflow solving? Manual vendor due diligence is a time-consuming process that can take hours for a single vendor. This workflow automates over 80% of these manual tasks, which typically include: üîç Finding and organizing basic vendor information üè¢ Researching the company's background üìÑ Collecting links to key documents like Privacy Policies, Terms of Service, and Trust Pages üìñ Manually reviewing each document to extract risk-relevant information üìä Compiling all findings into a formatted report or spreadsheet for record-keeping By leveraging Gemini for structured parsing and web scraping with live internet data, this workflow frees you up to focus on critical analysis and final review. ‚ö° ‚öôÔ∏è What this workflow does This end-to-end automated n8n workflow performs the following steps: üìù Intake: Begins with a simple form to capture the vendor's name, the business use case, and the type of data they will handle üîé Background Research: Gathers essential background information on the company ‚ö†Ô∏è Risk Analysis: Conducts comprehensive research on various risk-related topics üîó URL Extraction: Finds and validates public URLs for privacy policies, security pages, and trust centers üìà Risk Assessment: Generates a structured risk score and a detailed assessment based on the collected content and context üì§ Export: Exports the final results to a Google Sheet for easy access and record-keeping üöÄ Setup To get started with this workflow, follow these steps: üîë Configure Credentials: Set up your API credentials for Gemini and Jina AI üìä Connect Google Sheets: Authenticate your Google Sheets account and configure the the Sheet where you want to store the results üîó Download the Google Sheet template for your assessment ouput from here ‚öôÔ∏è (Optional) Customize Prompts: Adjust the prompts within the workflow to better suit your specific needs üéØ (Optional) Align Risk Framework: Modify the risk questions to align with your organization's internal vendor risk framework",8,2025-07-06 18:38:17.897000+00:00,True,8
6764,Automatic LinkedIn Engagement with AI Comments using GPT-4o and Phantombuster,"Who‚Äôs it for B2B marketers, recruiters, and personal-brand builders who want to spark conversations on LinkedIn by automatically posting short, relevant comments on fresh industry content‚Äîwhile staying under daily limits. How it works / What it does Schedule Trigger fires every hour. Select Cookie picks a rotating LinkedIn session-cookie (time-slice logic). Generate Random Search Term (GPT-4o) outputs a realistic AI/BPA keyword. LinkedIn Search Agent (Phantombuster) scrapes recent posts. Get Random Post chooses one post and passes its text to Create Comment (GPT-4o) which returns a ‚â§150-character reply in the chosen language. Builds linkedin_posts_to_comment.csv, uploads to SharePoint, and launches the Auto-comment Agent to post the reply. Post URL is logged to linkedin_posts_already_commented.csv to avoid duplicates. Wait nodes throttle launches to ~120 comments/day. How to set up Add credentials: Phantombuster API, SharePoint OAuth2, OpenAI API key. In SharePoint ‚Ä∫ ‚ÄúPhantombuster‚Äù folder create: ‚Ä¢ linkedin_session_cookies.txt ‚Äì one cookie per line. ‚Ä¢ linkedin_posts_already_commented.csv with header postUrl. Edit Set ENV Variables to set default language, comment prompt, company ID, etc. Adjust schedule or comments-per-launch as needed. Activate the workflow; it will run hourly and comment on one new post each launch. Requirements n8n 1.33 + Phantombuster Growth plan (API access) OpenAI account Microsoft 365 SharePoint tenant How to customize Change tone/length: edit the prompt in Create Comment. Comment more often: raise numberOfLinesPerLaunch and schedule frequency. Use Google Drive/Dropbox instead of SharePoint by swapping storage nodes.",8,2025-08-01 08:15:36.440000+00:00,True,5
5780,UptimeRobot Alerts to Telegram with Visual Verification,"UptimeRobot Alerts to Telegram with Visual Verification Automatically sends Telegram notifications with optional screenshots when monitors change status (‚úÖ UP/üî¥ DOWN/‚è∏Ô∏è PAUSED) Example Message in Telegram: Who Is This For? Teams or individuals needing to: Get alerts when websites/services go down Verify outages with visual screenshots Monitor infrastructure from Telegram What Does This Workflow Solve? üö® Missed Alerts: Get immediate notifications in Telegram üñºÔ∏è Visual Verification: Optional screenshot confirmation of outages üìä Status Tracking: Clear records of when issues began/resolved üîó One-Click Access: Direct links to affected monitors ‚è±Ô∏è Time Savings: No need to check dashboards manually Setup Guide 1. Pre-Requisites UptimeRobot Account**: With at least one monitor configured Gmail Account**: To receive alert notifications Telegram Account**: To receive alerts (mobile/desktop app recommended) (Optional) ScreenshotMachine free/paid account 2. Credentials Setup Make sure your n8n instance is connected with: Gmail Account** (via OAuth2) UptimeRobot API** (via API key) Telegram Bot** (via bot token) (Optional) ScreenshotMachine (via customer key) 3. Configure Your n8n Workflow Nodes 1. Alert Trigger Gmail Trigger**: Configure to watch for emails from alert@uptimerobot.com Set appropriate polling interval (e.g., every 5 minutes) 2. Monitor Configuration Conf Node**: Set your preferences: { ""take_screenshot"": true, ""screenshotmachine_secret"": ""your-secret-here"", ""screenshotmachine_device"": ""desktop"", ""screenshotmachine_dimension"": ""1366xfull"", ""screenshotmachine_format"": ""png"" } 3. Notification Settings Telegram Nodes**: Set your Chat ID (find with @getidsbot) Customize message formatting if needed 4. Service-Specific Setup UptimeRobot: Go to Dashboard ‚Üí My Settings ‚Üí API Settings Create API key with ""Monitor Read"" permissions Enable email alerts in monitor settings Telegram Bot: Message @BotFather to create new bot Get your Chat ID using @getidsbot Add bot token to n8n credentials ScreenshotMachine (Optional): Sign up at screenshotmachine.com Get Customer Key from account dashboard Set your secret phrase if using hash verification Final Steps Test your workflow by manually triggering a monitor status change Verify Telegram notifications arrive as expected Check screenshot quality if enabled Monitor for a few days to fine-tune alert preferences Happy Monitoring!",7,2025-07-08 12:56:16.421000+00:00,False,5
8044,Automate Token Purchases with Dollar Cost Averaging on Uniswap V3 & 1Shot API,"This workflow contains community nodes that are only compatible with the self-hosted version of n8n. Dollar Cost Averaging with Uniswap V3 This workflow lets you set up an scheduled workflow to dollar cost average (DCA) into any token on a custom schedule using 1Shot API and the Uniswap V3 protocol. Choose your schedule input token and output token and optionally configure the workflow to send you notifications in Telegram everytime your workflow completes a swap. YouTube Tutorial You can watch the full end-to-end tutorial for this workflow on our YouTube channel. Wallet Delegation Importantly, this workflow uses Metamasks Delegation Framework which lets you DCA from an account you custody whithout ever exporting your private key or giving up control of your assets. Setup Create a free 1Shot API account, provision a server wallet to relay transactions, and generate an API key to connect to n8n. Import the following Uniswap contracts into your 1Shot API account for the chain you want to swap on: QuoterV2, SwapRouter02, and the token pool (needed to compute time-weighted average price (TWAP). Import the approve function for the ERC-20 token you want to use to purchase your target asset (this should be the token0 or token1 of the pool you imported in step 2). Click on the server wallet details you created, fund it with enough gas tokens to pay for your transactions, then generate a delegation for the SwapRouter02 contract and the ERC-20 token from steps 2 and 3. Import the DCA workflow and use your 1Shot API key/secret to create a credential. Point the 1Shot API nodes at the appropriate smart contract functions you imported in steps 2 and 3. In the Swap Configs node, set the amount of the in token you want to spend on each purchase - for example, if you are using USDC (which has 6 decimals) and you want to purchase $10 every purchase, then amountDCA should be 10000000. Also set the correct addresses for the SwapRouterV2, token0, token1 and fee of the pool you are using. Lastly, set the frequency of your DCA in the trigger node and activate! Optional Telegram Notifications You can configure a Telegarm bot to notifiy you everytime the workflow completes to send you a transaction has and you remaining balance in your purchasing funds.",7,2025-08-30 05:39:26.365000+00:00,False,2
5766,Extract YouTube Video Statistics and Save to Google Sheets,"This N8N template makes it easy to extract key YouTube video data - including title, view count, like count, comment count, and many more - and save it directly into a connected Google Sheet. Use cases are many: Whether you're YouTubers, content strategists, growth marketers, and automation engineers, this tool gives you fast, structured access to video-level insights in seconds. How It Works The workflow begins when you click Execute Workflow or Test Workflow manually in N8N. It reads the list of video URLs in the connected Google Sheet. Only the URLs marked with the Ready status will be processed. The tool loops through each video and prepares the necessary data for the YouTube API call later. For each available URL, the tool extracts the video ID and sends a request to the YouTube API to fetch key metrics. The response is checked: If successful: the video‚Äôs statistics are written back to the corresponding row in the Google Sheet and the row's status is marked as Finished. If unsuccessful: the row's status is updated to Error for later review. How To Use Download the workflow package. Import the workflow package into your N8N interface. Duplicate the YouTube - Get Video Statistics Google Sheet template into your Google Sheets account. Set up Google Cloud Console credentials in the following nodes in N8N, ensuring enabled access and suitable rights to Google Sheets and YouTube services: For Google Sheets access, ensure each node is properly connected to the correct tab in your connected Google Sheet template: Node Google Sheets - Get Video URLs ‚Üí connected to Tab Video URLs; Node Google Sheets - Update Data ‚Üí connected to Tab Video URLs; Node Google Sheets - Update Data - Error ‚Üí connected to Tab Video URLs. For YouTube access, set up a GET method to connect to YouTube API in the following node: Node HTTP - Find Video Data. In your connected Google Sheet, enter the video URLs that you want to crawl and set the rows' status to Ready. Run the workflow by clicking Execute Workflow or Test Workflow in N8N. View the results in your connected Google Sheet: Successful fetches will update the rows' status in Column A in the Video URLs tab to Finished and the video metrics will populate. If the call fails, the rows' status in Column A in the tab will be marked as Error. Requirements Basic setup in Google Cloud Console (OAuth or API Key method enabled) with enabled access to YouTube and Google Sheets. How To Customize By default, the workflow is manually triggered in N8N. However, you can automate the process by adding a Google Sheets trigger that monitors new entries automatically. If you want to fetch additional video fields or analytics (like tags, category ID, etc.), you can expand the HTTP - Find Video Data node to include those. Need Help? Join our community on different platforms for support, inspiration and tips from others. Website: https://www.agentcircle.ai/ Etsy: https://www.etsy.com/shop/AgentCircle Gumroad: http://agentcircle.gumroad.com/ Discord Global: https://discord.gg/d8SkCzKwnP FB Page Global: https://www.facebook.com/agentcircle/ FB Group Global: https://www.facebook.com/groups/aiagentcircle/ X: https://x.com/agent_circle YouTube: https://www.youtube.com/@agentcircle LinkedIn: https://www.linkedin.com/company/agentcircle",6,2025-07-08 08:37:27.218000+00:00,False,2
10523,"Task Escalation System with Google Sheets, Gmail, Telegram & Jira Automation","Description This workflow sends an instant email alert when a task in a Google Sheet is marked as Urgent, and then sends a Telegram reminder notification after 2 hours if the task still hasn‚Äôt been updated. Then a Jira ticket is created so the task enters in the formal workflow and another Telegram message is sent with the details of the issue created. It helps teams avoid missed deadlines and ensures urgent tasks get attention ‚Äî without requiring anyone to refresh or monitor the sheet manually. Context In shared task lists, urgent items can be overlooked if team members aren't actively checking the spreadsheet. This workflow solves that by: Sending an email as soon as a task becomes Urgent Waiting 2 hours Checking if the task is still open Sending a Telegram reminder only if action has not been taken Creating a Jira issue Sending a Telegram message with the details of the issue created This prevents both silence and spam, creating a smart and reliable alert system. Target Users Project Managers using Google Sheets Team leads managing shared task boards Remote teams needing lightweight coordination Anyone who wants escalation notifications without complex systems Technical Requirements Google Sheets credential Gmail credential Telegram Bot + Chat ID Google Sheet with a column named Priority Jira credential Workflow Steps Trigger: Google Sheets Trigger (on update in the ‚ÄúPriority‚Äù column) IF Node ‚Äì Checks if Priority = Urgent Send Email ‚Äì Sends alert email with task name, owner, status, deadline Mark Notified = Yes in the sheet Wait 2 hours IF Status is still not resolved Send Telegram reminder create an Issue on Jira based on the information provided Send Telegram message with the details of the ticket Key Features Real-time alerts on critical tasks Simple logic (no code required) Custom email body with dynamic fields Works on any Google Sheet with a ‚ÄúPriority‚Äù column Telegram notification ensures the task doesn‚Äôt get forgotten Expected Output Personalized email alert when a task is marked as ""Urgent"" Email includes task info: title, owner, deadline, status, next step Telegram message after 2 hours if the task is still open Automatic creation of a Jira issue with the higgest priority Telegram message to notify about the new Jira ticket How it works Trigger: Watches for ‚ÄúPriority‚Äù updates üîç Check: If Priority = Urgent AND Notified is empty üìß Email: Sends a personalized alert ‚úèÔ∏è Sheet Update: Marks the task as already notified ‚è≥ Wait: 2-hour delay ü§ñ Check Again: If Status hasn‚Äôt changed ‚Üí send Telegram reminder, create Jira ticket and send the details. Tutorial video: Watch the Youtube Tutorial video About me : I‚Äôm Yassin a Project & Product Manager Scaling tech products with data-driven project management. üì¨ Feel free to connect with me on Linkedin",6,2025-11-05 08:28:27.052000+00:00,False,4
11130,Scrape Property Listings from Zillow with Olostep API and Store in Data Tables,"Zillow Property Scraper Using Olostep API This n8n template automates Zillow property data collection by scraping Zillow search results using the Olostep API. It extracts property price, link to listing, and location, removes duplicates, and stores everything in a Google Sheet or Data Table. Who‚Äôs it for Real estate analysts and investors researching property markets. Lead generators needing structured Zillow data. Freelancers and automation builders collecting housing listings. Anyone needing fast, clean Zillow data without manual scraping. How it works / What it does Form Trigger User enters a Zillow search URL. This becomes the base Zillow search URL. Pagination Logic A list of page numbers (1‚Äì7) is generated. Each number is used to load the next Zillow search page. Scrape Zillow Pages with Olostep For each page, the Olostep API scrapes the Zillow results. Olostep‚Äôs LLM extraction schema extracts: price url (link to the Zillow listing) location Parse & Split Results Returned JSON is cleaned and converted into individual listing items. Remove Duplicates Ensures each Zillow listing appears only once. Insert into Google Sheet / Data Table Final cleaned listings are inserted row-by-row. Perfect for filtering, exporting, or further analysis. This workflow gives you a fast, scalable property scraper using Zillow + Olostep ‚Äî no browser automation, no manual copy/paste. How to set up Import the template into n8n. Add your Olostep API key. Connect your Google Sheet or n8n Data Table. Deploy the form and start scraping by entering a place name. Requirements Olostep API key Google Sheets account or Data Table n8n cloud or self-hosted instance How to customize the workflow Add more pages to the pagination array (e.g., 1‚Äì20). Expand the LLM extraction schema to include: number of bedrooms number of bathrooms square footage property type Trigger via Telegram or API instead of a form. Send results to Airtable or Notion instead of Google Sheets. üëâ This template gives you an automated Zillow scraper powered by AI extraction ‚Äî perfect for real estate lead gen or market research.",6,2025-11-22 07:56:56.079000+00:00,True,1
5733,Secure User Emails with AES-256 Encryption and Verification System,"üîê Email Encryption Masterclass Professional-Grade AES-256 Data Protection for n8n How It Works This comprehensive workflow demonstrates enterprise-level email encryption using industry-standard AES-256-CBC encryption. Perfect for organizations handling sensitive customer data who need to comply with GDPR, HIPAA, or other data protection regulations. The workflow processes data in three secure stages: Sample Data Generation - Creates test user records with emails for demonstration Email Encryption - Applies AES-256-CBC encryption with unique initialization vectors (IVs) for each record Verification System - Decrypts and validates the encrypted data to ensure integrity Each email address is encrypted with a randomly generated IV, making every encryption unique even for identical email addresses. The system includes comprehensive error handling and provides detailed processing status for audit trails. Key Features ‚úÖ Military-Grade Security: AES-256-CBC encryption with unique IVs ‚úÖ Compliance Ready: Built for GDPR, HIPAA, and SOX requirements ‚úÖ Batch Processing: Handles large datasets efficiently ‚úÖ Error Handling: Graceful failure management with detailed logging ‚úÖ Verification System: Built-in decryption validation ‚úÖ Audit Trail: Complete processing timestamps and status tracking ‚úÖ Production Ready: Includes security best practices and key management guidance Setup Steps 1. Import the Workflow Download the JSON file In n8n, go to Workflows ‚Üí Import from File Select the downloaded JSON file Click Import 2. Configure Security Settings Open the Encrypt Emails node Replace the default secret key with your own 32-character secure key: const secret = 'your_32_character_secret_key_here!'; Important**: In production, use environment variables for the secret key 3. Set Up Sample Data (Optional) The workflow includes sample user data for testing Replace with your actual data source (database, API, CSV, etc.) Ensure your data has an email field 4. Test the Workflow Click Execute workflow to run the complete process Review the encrypted results in each node Verify that emails are properly encrypted and can be decrypted 5. Production Deployment Remove** the originalEmail field from output (line 45 in Encrypt Emails node) Store your secret key in environment variables Set up proper key rotation schedule Configure monitoring and alerting for failed encryptions Security Best Practices Included üîí Unique IVs: Every encryption uses a fresh random initialization vector üîí Proper Key Length: Enforces 32-byte keys for AES-256 üîí Error Isolation: Failed encryptions don't break the entire batch üîí No Key Exposure: Secret keys are never logged or stored in output üîí Verification Loop: Ensures encrypted data can be successfully decrypted Use Cases Customer Data Protection**: Encrypt email addresses in CRM systems Compliance Requirements**: Meet GDPR, HIPAA, SOX data protection mandates Data Warehouse Security**: Protect PII in analytics databases Secure Data Transfer**: Encrypt sensitive data before API transmission Audit Preparation**: Demonstrate proper data protection controls This workflow represents years of cybersecurity expertise condensed into a ready-to-use automation solution. Perfect for developers, security professionals, and compliance teams who need reliable data encryption.",4,2025-07-07 04:26:25.713000+00:00,False,1
5744,Generate Research Questions from PDFs using InfraNodus Content Gap Analysis,"This template can be used to generate research questions from PDF documents (e.g. research papers, market reports) based on the content gaps found in text using the InfraNodus knowledge graph GraphRAG knowledge graph representation. Simply upload several PDF files (research papers, corporate or market reports, etc) and generate a research question / AI prompt in seconds. The template is useful for: generating research questions generating AI prompts that drive research further finding blind spots in any discourse and generating ideas that address them. avoiding the generic bias of LLM models and focusing on what's important in your particular context Using Content Gaps for Generating Research Questions Knowledge graphs represent any text as a network: the main concepts are the nodes, their co-occurrences are the connections between them. Based on this representation, we build a graph and apply network science metrics to rank the most important nodes (concepts) that serve as the crossroads of meaning and also the main topical clusters that they connect. Naturally, some of the clusters will be disconnected and will have gaps between them. These are the topics (groups of concepts) that exist in this context (the documents you uploaded) but that are not very well connected. Addressing those gaps can help you see which groups of concepts you could connect with your own ideas. This is exactly what InfraNodus does: builds the structure, finds the gaps, then uses the built-in AI to generate research questions that bridge those gaps. How it works 1) Step 1: First, you upload your PDF files using an online web form, which you can run from n8n or even make publicly available. 2) Steps 2-4: The documents are processed using the Code and PDF to Text nodes to extract plain text from them. 3) Step 5: This text is then sent to the InfraNodus GraphRAG node that creates a knowledge graph, identifies structural gaps in this graph, and then uses built-in AI to research questions / prompts. 4) Step 6: The ideas are then shown to the user in the same web form. Optionally, you can hook this template to your own workflow and send the question generated to an InfraNodus expert or your own AI model / agent for further processing. If you'd like to sync this workflow to PDF files in a Google Drive folder, you can copy our Google Drive PDF processing workflow for n8n. How to use You need an InfraNodus GraphRAG API account and key to use this workflow. Create an InfraNodus account Get the API key at https://infranodus.com/api-access and create a Bearer authorization key. Add this key into the InfraNodus GraphRAG HTTP node(s) you use in this workflow. You do not need any OpenAI keys for this to work. Optionally, you can change the settings in the Step 4 of this workflow and enforce it to always use the biggest gap it identifies. Requirements An InfraNodus account and API key Note: OpenAI key is not required. You will have direct access to the InfraNodus AI with the API key. Customizing this workflow You can use this same workflow with a Telegram bot or Slack (to be notified of the summaries and ideas). You can also hook up automated social media content creation workflows in the end of this template, so you can generate posts that are relevant (covering the important topics in your niche) but also novel (because they connect them in a new way). Check out our n8n templates for ideas at https://n8n.io/creators/infranodus/ Also check the full tutorial with a conceptual explanation at https://support.noduslabs.com/hc/en-us/articles/20454382597916-Beat-Your-Competition-Target-Their-Content-Gaps-with-this-n8n-Automation-Workflow Also check out the video introduction to InfraNodus to better understand how knowledge graphs and content gaps work: For support and help with this workflow, please, contact us at https://support.noduslabs.com",4,2025-07-07 10:51:35.528000+00:00,True,2
5836,Create an Offline DIGIPIN Microservice API for Precise Location Mapping in India,"This workflow contains community nodes that are only compatible with the self-hosted version of n8n. What is DIGIPIN? DIGIPIN (Digital Pincode) is a 10-character alphanumeric code introduced by India Post. It maps any 3x3 meter square in India to a unique digital address. This helps precisely locate homes, shops, or landmarks, especially in areas where physical addresses are inconsistent or missing. What this workflow does This workflow creates a fully offline DIGIPIN microservice using only JavaScript - no external APIs are used. You get two HTTP endpoints: GET /generate-digipin?lat={latitude}&lon={longitude} ‚Üí returns a DIGIPIN GET /decode-digipin?digipin={code} ‚Üí returns the latitude and longitude You can plug this into any system to: Convert GPS coordinates to a DIGIPIN Convert a DIGIPIN back to coordinates How it works An HTTP Webhook node receives the request A JS Function node either encodes or decodes based on input The result is returned as a JSON response All the logic is handled inside the workflow - no API keys, no external calls. Why use this Fast and lightweight Easily extendable: you can connect this to forms, CRMs, apps, or spreadsheets Ideal for field agents, address validation, logistics, or rural operations",4,2025-07-10 09:29:33.056000+00:00,False,1
5848,Collect & Store Restaurant Customer Feedback with Google Sheets and Email Forms,"This n8n template demonstrates how to create an automated customer feedback collection system for restaurants. The workflow triggers when new customer emails are added to an Excel sheet, automatically sends personalized feedback forms, and stores all responses in a separate Excel tracking sheet. Perfect for restaurants wanting to systematically gather customer insights and improve service quality. Good to know Each feedback form is personalized with the customer's name and email All responses are automatically timestamped and organized in Excel sheets The system handles form validation and ensures complete data capture Email notifications keep your team updated on new feedback submissions How it works Email Distribution Workflow New customer entries are detected in Excel Sheet-1 (customer database) containing customer names and email addresses The system automatically generates personalized feedback forms for each new customer Customized feedback emails are sent with embedded forms tailored to restaurant experience evaluation Wait nodes ensure proper processing timing before sending emails Feedback Collection Workflow Customer form submissions trigger the data collection process All feedback responses are captured including ratings, comments, and contact information Data is automatically appended to Excel Sheet-2 (feedback responses) with complete timestamps The system handles multiple concurrent submissions without data loss Excel Sheet Structure Sheet-1 (Customer Database) Name - Customer's full name Email - Customer's email address for form distribution Sheet-2 (Feedback Responses) Timestamp - Date and time of form submission Name - Customer's full name E-Mail - Customer's email address Contact Number - Customer's phone number How was the cleanliness of the dining area? - Cleanliness rating/feedback Did you like the taste of the food? - Food taste evaluation What dish did you enjoy the most? - Favorite dish identification Was your order accurate and timely? - Service accuracy rating Was our staff polite and helpful? - Staff service evaluation Was the food presentation appealing? - Food presentation rating How would you rate your overall dining experience? - Overall experience score Any additional comments or suggestions? - Open-ended feedback field How to use Import the workflow into your n8n instance and configure Excel integration Set up Sheet-1 with customer names and emails for feedback distribution Configure the feedback form with your restaurant's specific questions and branding Add new customer entries to Sheet-1 to automatically trigger feedback emails Monitor Sheet-2 for incoming responses and analyze customer satisfaction trends The system scales automatically with your customer database growth Requirements Google Sheets account for data storage and management Email service integration (Gmail, SMTP, or similar) n8n instance with Google Sheets and email connectors Customising this workflow Customer feedback automation can be adapted for different restaurant types and service models Try popular use-cases such as post-dining follow-ups, seasonal menu feedback, or special event evaluations The workflow can be extended to include automated response analysis, sentiment scoring, and management dashboard integration",4,2025-07-10 15:31:07.401000+00:00,False,3
5768,Monitor SSL Certificate Expiry with Google Sheets and Email Alerts,"üîí SSL Certificate Monitoring & Expiry Alert with Spreadsheet [FREE APIs] ‚úÖ What This Workflow Does This n8n template automatically monitors SSL certificates of websites listed in a Google Sheet and sends email alerts if any are expiring within 14 days. It helps ensure you avoid downtime, security issues, and trust warnings due to expired certificates. üß© Key Features üìÖ Weekly Automation: Runs every Monday at 7:00 AM (configurable). üìÑ Google Sheets Integration: Fetches and updates data in a spreadsheet. üîç SSL Check via API: Uses ssl-checker.io to get certificate details. ‚ö†Ô∏è SSL Expiry Filter: Identifies certificates expiring within 14 days. üìß Email Alerts: Sends notifications for certificates close to expiration. üìÇ Input Spreadsheet Format Your Google Sheet should have the following columns: | No | Name | Link | SSL Issued On | SSL Expired On | SSL Status | |----|-----------------|-----------------------|-------------------|-------------------|------------| | 1 | Example Site | https://example.com | 2024-07-01 | 2025-07-01 | Valid | | 2 | My Blog | https://myblog.org | 2024-07-05 | 2024-07-20 | Expiring | Each row should include a valid website URL in the Link column. üõ†Ô∏è How It Works Scheduled Trigger Executes weekly (Monday 7:00 AM). Fetch Website List Reads all website entries from the Google Sheet. Check SSL Certificates Uses ssl-checker.io API to retrieve certificate details for each website. Update Spreadsheet Writes ""Issued On"" and ""Expired On"" fields back to the spreadsheet. Evaluate SSL Expiry Filters for certificates expiring within 14 days. Check Condition Determines whether to send alerts based on filtered results. Send Email Alert Notifies via email if any certificates are expiring soon. üì¨ Example Email Output Subject: ‚ö†Ô∏è ALERT!! SSL EXPIRED SSL certificates expiring soon: example.com (expires in 5 days) anotherdomain.net (expires in 3 days) üß∞ Setup Requirements A Google Sheet with the correct columns and website links. SMTP credentials to send alert emails.",3,2025-07-08 09:26:03.496000+00:00,False,4
6775,"Extract Seed-Funded Startup Data with RSS, GPT-4.1-MINI & BrightData to Excel","This workflow contains community nodes that are only compatible with the self-hosted version of n8n. What It Does This workflow automatically discovers recently seed-funded startups by monitoring RSS feeds for funding announcements. It uses Bright Data to scrape full article content, then extracts structured company information using OpenAI (GPT). The data is exported to an Excel sheet on OneDrive, providing sales teams with a real-time list of qualified leads without any manual effort. How It Works Trigger & Article Discovery: Monitors curated RSS feeds for articles mentioning seed funding and triggers the workflow on new article detection. Content Scraping & Preparation: Scrapes full article content and converts it into clean markdown format for AI processing. Data Extraction with AI: Uses OpenAI to extract structured details like company name, website, LinkedIn profile, founders, and funding amount. Structured Data Output & Storage: Appends extracted data to an Excel sheet on OneDrive via Microsoft Graph API. Prerequisites RSS Feed URL**: A valid RSS feed source that provides seed funding articles for startups. Bright Data Credentials**: Active Bright Data account with access credentials (API token ) to enable article scraping. OpenAI API Key**: An OpenAI account with an API key and access to GPT-4.1-MINI models for data extraction. Microsoft OAuth2 API Credentials**: OAuth2 credentials (Client ID, Secret, Tenant ID) with access scopes to use Microsoft Graph API for Excel integration. Excel Sheet in SharePoint**: A pre-created Excel file hosted on OneDrive or SharePoint with the following column headers: createdAt, companyName, companyWebsite, companyLinkedIn, fundingAmount, founderName, founderLinkedIn, articleLink Excel File & Sheet Identifiers**: The Drive ID, File ID and Sheet ID of your Excel sheet stored on OneDrive or SharePoint, required by the Microsoft Graph API for appending rows using the HTTP node in n8n. Need help with the setup? Feel free to contact us How to Set It Up Follow these steps to configure and run the workflow: Import the Workflow Copy the provided n8n workflow template. In your n8n instance, go to Editor UI &gt; paste this workflow. Configure the RSS Feed Node Open the RSS trigger node. Replace the default URL with your RSS feed URL. Ensure the polling interval matches your desired frequency (e.g., every 15 minutes or 1 hour). Set Up Bright Data Node Add your Bright Data credentials. Follow the documentation to complete the setup. Configure OpenAI Integration Add your OpenAI API key as a credential in n8n. Ensure the model is set to gpt-4.1-MINI. Follow the documentation to complete the setup. Configure Excel File Integration Open the HTTP node responsible for sending data to the Excel sheet via Microsoft Graph API. Replace the placeholder values in the API endpoint URL with your actual File ID and Sheet ID from the Excel file stored on OneDrive or SharePoint. https://graph.microsoft.com/v1.0/drives/{{drive-id}}/items/{{file-id}}/workbook/tables/{ {{ sheet-id }} }/rows This URL is used to append data to the specified Excel sheet range. Next, set up Microsoft OAuth2 credentials in n8n: Go to n8n &gt; Credentials &gt; Microsoft OAuth2 API. Provide the required values: Client ID Client Secret Tenant ID Scope Follow the documentation to complete the setup. Once the credential is saved, connect it to the HTTP node making the Graph API call. Activate the Workflow Set the workflow status to Active in n8n so it runs automatically when a new article appears in the RSS feed. Need Help? Contact us for support and custom workflow development.",3,2025-08-01 11:21:08.347000+00:00,True,3
6830,Send Daily 4K Bluray Preorder Updates from Blu-ray.com to Discord,"This is a simple webpage scraper that specifically grabs today's newest 4K Bluray Preorders as listed on the Blu-ray.com website. This is a scheduled workflow that can run every day and will post a formatted summary message of links to a Discord channel of your choice. Minimal setup required: Just create a webhook for the channel you want posted to in Discord and provide that in the final step. The timezone format step is set to East Coast (NYC) by default, feel free to change. No API keys or any special configuration needed (beyond your Discord webhook) Feel free to customize the formatting of the message that gets posted üëç How it works: First format todays date to match the formatting used on the website Grab the HTML for the preorders page at www.blu-ray.com Filter only the hyperlinks for each Bluray on the page Then further filter only those with an html header matching today's date Format how you want the message to be sent to your Discord channel (in this case a simple list of Hyperlinks for each Title) Send to Discord! Disclaimer: This should be only for personal use.** The links go back to the blu-ray.com website, which is a good thing! Don't abuse this by slamming their site with some crazy level of automation frequency. Support the blu-ray.com website by using their affiliate links whenever you do want to preorder a title ;) This is one of my first shared templates, so it may not be super optimal or perfect but it works for my needs and hopefully you'll find some use out of it! Discord currently has a 2000 character limit on webhook messages. Some of the messages may get truncated as a result.",3,2025-08-02 00:38:20.759000+00:00,False,4
7578,"Create Photorealistic Social Media Images with Flux Realism, LoRa Models & Airtable","AI Image Generation Workflow for Social Media Content Overview This n8n workflow automates the creation of photorealistic AI-generated images for social media content. The workflow uses RunComfy (ComfyUI cloud service) combined with Airtable for data management to create high-quality images based on custom prompts and LoRa models. Key Features Automated Image Generation: Creates photorealistic images using Flux Realism model and custom LoRa models Airtable Integration: Centrally manages content requests, model information, and image status Cloud-based Processing: Utilizes RunComfy servers for powerful GPU processing without local hardware requirements Status Tracking: Monitors generation process and automatically updates database entries Telegram Notifications: Sends success notifications after image completion Technical Workflow Server Initialization: Starts RunComfy server with configured specifications Data Retrieval: Fetches content requests from Airtable database Image Generation: Sends prompts to ComfyUI with Flux Realism + LoRa models Status Monitoring: Checks generation progress in 30-second intervals Download: Downloads completed images Database Update: Updates Airtable with image links and status Server Cleanup: Deletes RunComfy server for cost optimization Prerequisites RunComfy Membership** with API access Airtable Account** with configured database Telegram Bot** for notifications Flux Realism Workflow** in RunComfy library Uploaded LoRa Models** in RunComfy Airtable Schema The database must contain these fields: topic: Content description pose_1: Detailed image prompt LoRa Name Flux: LoRa model name Model: Character name pose_1_drive_fotolink: Link to generated image Bilder erstellt: Generation status Configuration Options Image Resolution: Default 832x1216px (adjustable in ComfyUI parameters) Generation Parameters: 35 steps, Euler sampler, Guidance 2.0 Server Size: ""Large"" for optimal performance (adjustable based on requirements) Time Intervals: 30s status checks, 50s server initialization This workflow is ideal for content creators who need regular, high-quality, character-consistent images for social media campaigns.",3,2025-08-19 09:06:43.943000+00:00,True,4
8173,Automate LinkedIn Job Alerts with J-Search API and SMTP Email Notifications,"Automate LinkedIn Job Alerts with Google Sheets, J-Search (RapidAPI), and SMTP Description Fetch LinkedIn-style job listings from the J-Search API using predefined parameters, then deliver a professional HTML job alert digest to your inbox via SMTP. Get actionable ‚ÄúApply Now‚Äù links in your email‚Äîno manual searching required. ‚úâÔ∏èüíº What This Template Does Manual Trigger:** Initiates the workflow on demand. Set Preferences:** Captures job search parameters (role, date, page count, country, language) from a Set node. Job Listing Extraction:** Uses HTTP Request to J-Search API with dynamic parameters from the Set node. Categorizing Job Listings:** Filters and structures job data (job_id, job_title, employment_type, apply_link) via JavaScript Code node. Drafting HTML for Mail:** Generates a LinkedIn-themed HTML email with job cards and ‚ÄúApply Now‚Äù buttons. Sending Job Listings via Mail:** Delivers the job alert digest using SMTP, with a dynamic subject line. Key Benefits Save time** with one-click manual or scheduled alerts. ‚è±Ô∏è Stay organized with clean, mobile-friendly job summaries. üìä Act faster using one-click ‚ÄúApply Now‚Äù links. üîó Refine job searches using flexible parameters in the workflow. üß≠ Features Manual or scheduled workflow execution. Set node for custom job search preferences. HTTP Request node for dynamic API calls. JavaScript nodes for filtering and HTML formatting. LinkedIn-inspired, responsive HTML email. SMTP email delivery with customizable sender/recipient. Requirements n8n instance (cloud or self-hosted). üß© J-Search API access via RapidAPI; add x-rapidapi-key in n8n Credentials. üîë SMTP email account (e.g., Gmail/Outlook) configured in n8n; set valid From/To addresses. üìß Set node configured with search parameters (query, page, num_pages, date posted, country, language). No Google Sheets integration required in this version (parameters set directly in workflow). Target Audience Job seekers automating their search. üë©‚Äçüíº Recruiters streamlining lead discovery. üßë‚Äçüíº HR teams wanting timely candidate outreach. üß† Step-by-Step Setup Instructions (Concise) Configure the Set node with desired job search parameters. RapidAPI: Subscribe to J-Search API and add the API key in n8n Credentials. üîë SMTP: Connect your email account in n8n Credentials; set From/To in the workflow. ‚úâÔ∏è Import the workflow, assign credentials, and replace placeholders. üì• Rename the email node to ‚ÄúSending Job Listings via Mail.‚Äù üè∑Ô∏è Run manually or schedule with a Cron node as needed. ‚è≤Ô∏è Security Best Practices Never hardcode** API keys‚Äîuse n8n Credentials. üîê Restrict access to credentials within your workspace. üõ°Ô∏è Remove personal email addresses before sharing templates. üßπ Monitor executions for failures or anomalies. üìú",3,2025-09-02 13:55:46.959000+00:00,False,3
9141,Automated Error Handling with Email Alerts & Smart Workflow Deactivation,"How it works This workflow automatically handles errors in your n8n workflows by: Detecting when an error occurs and capturing the error details Sending an email notification with the error message and affected node Allowing you to deactivate the workflow or ignore the error via email response Optionally rerouting the error notification to another user for approval The workflow includes two templates: a basic version with simple deactivate/ignore options, and an advanced version that allows error rerouting to a second user. Set up steps Estimated time: 5-10 minutes You'll need to configure SMTP credentials for sending emails, set up n8n API access for workflow management, and update email addresses and workflow URLs throughout the nodes. Choose which template (basic or reroute logic) fits your needs, then activate the workflow to start monitoring for errors.",3,2025-10-01 07:46:23.271000+00:00,False,1
9350,Automatic Topic & Sentiment Extraction from Jotform Responses with Google Gemini,"Who this is for This workflow is designed for teams that collect feedback or survey responses via Jotform and want to automatically: Analyze sentiment (positive, neutral, negative) of each response. Extract key topics and keywords from qualitative text. Generate AI summaries and structured insights. Store results in Google Sheets and n8n DataTables for easy reporting and analysis. Use Cases Customer experience analysis Market research & survey analysis Product feedback clustering Support ticket prioritization AI-powered blog or insight generation from feedback What this workflow does This n8n automation connects Jotform, Google Gemini, and Google Sheets to turn raw responses into structured insights with sentiment, topics, and keywords. Pipeline Overview Jotform ‚Üí Webhook ‚Üí Gemini (Topics + Keywords) ‚Üí Gemini (Sentiment) ‚Üí Output Parser ‚Üí Merge ‚Üí Google Sheets Jotform Trigger Captures each new submission from your Jotform (e.g., a feedback or survey form). Extracts raw fields ($json.body.pretty) such as name, email, and response text. Format Form Data (Code Node) Converts the Jotform JSON structure into a clean string for AI input. Ensures the text is readable and consistent for Gemini. Topics & Keyword Extraction (Google Gemini + Output Parser) Goal: Identify the main themes and important keywords from responses. { ""topics"": [ { ""topic"": ""Product Features"", ""summary"": ""Users request more automation templates."", ""keywords"": [""AI templates"", ""automation"", ""workflow""], ""sentiment"": ""positive"", ""importance_score"": 0.87 } ], ""global_keywords"": [""AI automation"", ""developer tools""], ""insights"": [""Developers desire more creative, ready-to-use AI templates.""], ""generated_at"": ""2025-10-08T10:30:00Z"" } Sentiment Analyzer (Google Gemini + Output Parser) Goal: Evaluate overall emotional tone and priority. { ""customer_name"": ""Ranjan Dailata"", ""customer_email"": ""ranjancse@gmail.com"", ""feedback_text"": ""Please build more interesting AI automation templates."", ""sentiment"": ""positive"", ""confidence_score"": 0.92, ""key_phrases"": [""AI automation templates"", ""developer enablement""], ""summary"": ""Customer requests more AI automation templates to boost developer productivity."", ""alert_priority"": ""medium"", ""timestamp"": ""2025-10-08T10:30:00Z"" } Merge + Aggregate Combines the topic/keyword extraction and sentiment output into a single structured dataset. Aggregates both results for unified reporting. Persist Results (Google Sheets) Writes combined output into your connected Google Sheet. Two columns recommended: feedback_analysis ‚Üí Sentiment + Summary JSON topics_keywords ‚Üí Extracted Topics + Keywords JSON Enables easy visualization, filtering, and reporting. Visualization (Optional) Add Sticky Notes or a logo image node in your workflow to: Visually describe sections (e.g., ‚ÄúSentiment Analysis‚Äù, ‚ÄúTopic Extraction‚Äù). Embed brand logo: Example AI Output (Combined) { ""feedback_analysis"": { ""customer_name"": ""Ranjan Dailata"", ""sentiment"": ""positive"", ""summary"": ""User appreciates current templates and suggests building more advanced AI automations."", ""key_phrases"": [""AI automation"", ""developer templates""] }, ""topics_keywords"": { ""topics"": [ { ""topic"": ""AI Template Expansion"", ""keywords"": [""AI automation"", ""workflow templates""], ""sentiment"": ""positive"", ""importance_score"": 0.9 } ], ""global_keywords"": [""automation"", ""AI development""] } } Setup Instructions Pre-requisite If you are new to Jotform, Please do signup using Jotform Signup For the purpose of demonstation, we are considering the Jotforms Prebuilt New Customer Registration Form as a example. However, you are free to consider for any of the form submissions. Step 0: Local n8n (Optional) If using local n8n, set up ngrok: ngrok http 5678 Use the generated public URL as your Webhook URL base for Jotform integration. Step 1: Configure the Webhook Copy the Webhook URL generated by n8n (e.g., /webhook-test/f3c34cda-d603-4923-883b-500576200322). You can copy the URL by double clicking on the Webhook node. Make sure to replace the base url with the above Step 0, if you are running the workflow from your local machine. In Jotform, go to your form ‚Üí Settings ‚Üí Integrations ‚Üí Webhooks ‚Üí paste this URL. Now, every new form submission will trigger the n8n workflow. Step 2: Connect Google Gemini Create a Google Gemini API Credential in n8n. Select the model models/gemini-2.0-flash-exp. Step 3: Create Data Storage Create a DataTable named JotformFeedbackInsights with columns: feedback_analysis (string) topics_keywords (string) Step 4: Connect Google Sheets Add credentials under Google Sheets OAuth2. Link to your feedback tracking sheet. Step 5: Test the Workflow Submit a form via Jotform. Check results: AI nodes return structured JSON. Google Sheet updates with new records. Customization Tips Change the Prompt You can modify the topic extraction prompt to highlight specific themes: You are a research analyst. Extract main topics, keywords, and actionable insights from this feedback: {{ $json.body }} Extend the Output Schema Add more fields like: { ""suggested_blog_title"": """", ""tone"": """", ""recommendations"": [] } Then update your DataTable or Sheets schema accordingly. Integration Ideas Send sentiment alerts to Slack for high-priority feedback. Push insights into Notion, Airtable, or HubSpot. Generate weekly reports summarizing trends across all submissions. Summary This workflow turns raw Jotform submissions into actionable insights using Google Gemini AI ‚Äî extracting topics, keywords, and sentiment while automatically logging everything to Google Sheets.",3,2025-10-07 13:51:05.566000+00:00,True,5
9905,"Automate Invoice Processing with Gmail, OCR.space, Slack & Xero","How It Works Trigger: Watches for new emails in Gmail with PDF/image attachments. OCR: Sends the attachment to OCR.space API (https://ocr.space/OCRAPI) to extract invoice text. Parsing: Extracts key fields: Vendor Invoice number Amount Currency Invoice date Due date Description Validation Logic: Checks if amount is valid Ensures vendor and invoice number are present Flags high-value invoices (e.g., over $10,000) Routing: If invalid: Sends a Slack message highlighting issues Labels email as Rejected If valid: Logs the invoice into Google Sheets Sends a Slack message to the finance team for approval After approval, creates a draft invoice in Xero Labels the email as Processed in Gmail Set up steps ‚Ä¢ Estimated setup time: 45-60 mins ‚Ä¢ You‚Äôll need connected credentials for Gmail, Slack, Google Sheets, and Xero ‚Ä¢ Replace the default API key for OCR.space with your own (in the HTTP Request node) ‚Ä¢ Update Slack channel IDs and label IDs to match your workspace ‚Ä¢ Adjust invoice validation rules as needed (e.g. currency, red flag conditions) All detailed explanations and field mappings are provided in sticky notes within the workflow.",3,2025-10-19 17:03:01.846000+00:00,False,6
8429,AI Video Automation Engine - Generate & Publish YT Shorts with Veo-3 or Sora 2,"Product Overview Scale short-form content without scaling your team. This template turns idea discovery ‚Üí prompt & text generation ‚Üí Veo-3 video rendering ‚Üí vertical formatting ‚Üí multi-channel publishing into one cohesive pipeline. It‚Äôs built natively on n8n, so you keep full control, self-host if you want, and extend every step with your own logic. What‚Äôs Inside (Modular Building Blocks) Idea Engine** ‚Äì AI generates concise, surprising facts and a visual metaphor per topic (swappable LLM/provider). Prompt & Text Composer** ‚Äì Crafts an 8-second, cinematic Veo-3 prompt + title, caption, description, hashtags, tags in a consistent brand voice. Renderer Integration** ‚Äì Kie.ai ‚Üí Veo-3 (16:9, 1080p) for fast, cinematic clips. Vertical Formatter** ‚Äì FFmpeg pipeline converts to 1080√ó1920, adds top/bottom text bands (15‚Äì20 words), and preserves loop smoothness. Multi-Channel Distribution** ‚Äì Uploads to YouTube Shorts and Telegram for internal review/approvals. Scheduler & Orchestration** ‚Äì CRON triggers, retries, and lightweight polling make the system dependable for daily publishing. Why These Components (and why they work together) Veo-3 via Kie.ai* provides a sweet spot of *quality + speed**, ideal for short loops where style coherence matters. n8n* brings *vendor-neutral orchestration**: swap LLMs, add rate limits, branch flows, or trigger on events‚Äîwithout rewriting your stack. FFmpeg* ensures a *platform-native look** (9:16, crisp text bands, no blurry hacks). Separation of concerns* (idea ‚Üí text ‚Üí render ‚Üí edit ‚Üí publish) makes it easy to *A/B test** any stage independently. Key Benefits Hands-Free Consistency** ‚Äì Daily output with a repeatable creative pattern that audiences recognize. Brand-Safe Copy** ‚Äì Titles, descriptions, and hashtags follow a controlled structure‚Äîpolished by design. Time Savings** ‚Äì Replace fragmented tooling with one pipeline; no manual re-formatting, captioning, or re-uploads. Creator-Grade Quality** ‚Äì Cinematic loops optimized for watch time and retention on Shorts/Reels. Own Your Stack** ‚Äì Self-hosted orchestration and storage; no lock-in. Reuse & Extension Ideas Topic CMS** ‚Äì Feed topics from Google Sheets / Notion and mark each as ‚Äúpublished‚Äù‚Äîperfect for content calendars. Multi-Language** ‚Äì Duplicate the text composer for EN/DE outputs; upload the same clip with localized metadata. Thumbnail/Preview Track** ‚Äì Add an image generator step for platform cards or YouTube custom thumbnails. Analytics Feedback Loop** ‚Äì Fetch video performance, score ideas, and prioritize winning angles automatically. Compliance & Fact Trails** ‚Äì Store sources next to each post for quick audits and edits. Storage & Delivery** ‚Äì Push masters to S3/Backblaze, serve via CDN for faster IG ingestion. More Channels** ‚Äì Add TikTok, X, LinkedIn publishers or a newsletter export. A/B Creative** ‚Äì Render multiple Veo-3 seeds, rotate captions/CTAs, and compare retention. Who It‚Äôs For Agencies** wanting repeatable, branded Shorts at scale. Creators & Media Teams** who need daily output with minimal overhead. Product Marketing** looking to convert docs/reports into snackable, evergreen video facts. Startups** building audience fast without adding headcount. Technical Notes (at a glance) Orchestration:** n8n (self-hosted or cloud) Video:** Veo-3 via Kie.ai (16:9 render ‚Üí FFmpeg vertical remaster 1080√ó1920) Text Stack:** Any LLM provider (swappable); outputs normalized for each platform Distribution:** YouTube Data API v3, Telegram Bot API Security:** API keys via n8n credentials/ENV; public URL endpoint only serves whitelisted .mp4 filenames with proper headers Reliability:** CRON triggers, bounded retries/polling, and safe fallbacks to avoid stuck runs Who to use it To enhance the completeness, add specific step-by-step setup instructions for users to implement the workflow easily: Install Prerequisites Set up n8n on your server or local machine (production mode with reverse proxy). Install FFmpeg and ensure fonts (e.g., DejaVu Sans) are available. Configure API Credentials Get a Perplexity API key for idea generation. Request a Kie.ai API key for Veo-3 video rendering. Set up YouTube OAuth2 (enable YouTube Data API v3 in Google Cloud Console). Create a Telegram bot token via BotFather. Store all keys as environment variables in n8n (avoid hardcoding). Prepare Storage Create a working directory accessible by n8n with read/write rights. Ensure enough disk space for video caching (approx. 50‚Äì200 MB per run). Set Scheduling Add CRON triggers for automatic execution (e.g., twice daily). Example: 0 12 04 * * * ‚Üí 04:12 AM daily 0 12 16 * * * ‚Üí 04:12 PM daily Define Variables File naming conventions: name_vorher = raw 16:9 video name_nachher = final 9:16 video Set topic = content niche (e.g., history, ocean, futurism). Build Workflow Steps in n8n Idea Generation: Perplexity node for viral fact. Prompt Agent: Expand into full package (prompt, caption, hashtags). Video Generation: Kie.ai Veo-3 API request (poll until success). Download & Edit: Save raw file, convert to 9:16 with FFmpeg overlays. Notifications & Uploads: Telegram node ‚Üí send video. YouTube node ‚Üí upload with title, description, tags. Test End-to-End Run the workflow manually once. Verify video rendering, FFmpeg subtitles, Telegram message, YouTube & IG uploads. Go Live Enable CRON triggers. Monitor first runs for errors (API limits, FFmpeg paths, token expiry). Why Buy This Template You‚Äôre not buying a script‚Äîyou‚Äôre buying a production-ready system with a deliberate creative pattern, best-practice formatting, and shipping-grade integrations. It reduces ramp-up from weeks to hours and gives you a flexible base you can keep evolving. Questions or custom requests? üì© suliemansaid.business@gmail.com",1,2025-09-09 19:02:30.830000+00:00,True,7
8809,Telegram Appointment Scheduler Bot with Google Calendar & Sheets,"Telegram Appointment Scheduling Bot with n8n üìÉ Description Tired of managing appointments manually? This template transforms your Telegram account into a smart virtual assistant that handles the entire scheduling process for you, 24/7. This workflow allows you to deploy a fully functional Telegram bot that not only schedules appointments but also checks real-time availability in your Google Calendar, logs a history in Google Sheets, and allows your clients to cancel or view their upcoming appointments. It's the perfect solution for professionals, small businesses, or anyone looking to automate their booking system professionally and effortlessly. ‚ú® Key Features Complete Appointment Management:** Allows users to schedule, cancel, and list their future appointments. Conflict Prevention:** Integrates with Google Calendar to check availability before confirming a booking, eliminating the risk of double-booking. Automatic Logging:** Every confirmed appointment is saved to a row in Google Sheets, creating a perfect database for tracking and analysis. Smart Interaction:** The bot handles unrecognized commands and guides the user, ensuring a smooth experience. Easy to Adapt:** Connect your own accounts, customize messages, and tailor it to your business needs in minutes. üöÄ Setup Follow these steps to deploy your own instance of this bot: 1. Prerequisites An n8n instance (Cloud or self-hosted). A Telegram account. A Google account. 2. Telegram Bot Talk to @BotFather on Telegram. Create a new bot using /newbot. Give it a name and a username. Copy and save the API token it provides. 3. Google Cloud & APIs Go to the Google Cloud Console. Create a new project. Enable the Google Calendar API and Google Sheets API. Create OAuth 2.0 Client ID credentials. Make sure to add your n8n instance's OAuth redirect URL. Save the Client ID and Client Secret. 4. Google Sheets Create a new spreadsheet in Google Sheets. Define the column headers in the first row. For example: id, Client Name, Date and Time, ISO Date. 5. n8n Import the workflow JSON file into your n8n instance. Set up the credentials: Telegram: Create a new credential and paste your bot's token. Google Calendar & Google Sheets (OAuth2): Create a new credential and paste the Client ID and Client Secret from the Google Cloud Console. Review the Google Calendar and Google Sheets nodes to select your correct calendar and spreadsheet. Activate the workflow! üí¨ Usage Once the bot is running, you can interact with it using the following commands in Telegram: To start the bot:** /start To schedule a new appointment:** agendar YYYY-MM-DD HH:MM Your Full Name To cancel an existing appointment:** cancelar YYYY-MM-DD HH:MM Your Full Name To view your future appointments:** mis citas Your Full Name üë• Authors Jaren Pazmi√±o President of the Polytechnic Artificial Intelligence Club (CIAP)",1,2025-09-22 00:29:54.087000+00:00,False,4
9148,Real-time Sales Quote Creation in Odoo via Telegram with Google Gemini AI,"Overview This template connects Telegram with Odoo to let your sales team create sales quotes and check product availability in real-time ‚Äî just by sending chat messages. It‚Äôs designed for sales representatives, distributors, and small business owners who want to manage quotes and product information quickly without logging into Odoo. ‚öôÔ∏è How It Works Once configured, this workflow listens to your Telegram bot for incoming messages. Based on the message text, it performs different actions in Odoo: Product Queries Sales reps can ask about products directly in Telegram: ‚ÄúWhat‚Äôs the price of Product B?‚Äù ‚ÄúHow many units of Product A are available?‚Äù The workflow fetches real-time data from Odoo and replies instantly. Sales Quote Creation Sales reps can also create new sales quotes by typing messages like: ‚ÄúMy customer Amazon, his email address is abc@amazon.com wants to buy 10 pcs of Product A and 15 pcs of Product B.‚Äù The workflow extracts relevant details, creates a sales quote in Odoo, and sends confirmation back in Telegram. üß∞ Setup Instructions Create a Telegram Bot Go to @BotFather on Telegram. Create a new bot and copy the API Token. Prepare Odoo Enable the Sales and Product modules. Generate an API Key from your Odoo user account. Note your Odoo URL (e.g., https://yourcompany.odoo.com). Import Workflow Open your n8n instance (self-hosted or cloud). Click Import Workflow and upload the provided JSON file. Add Credentials Configure your Telegram credentials (Bot Token). Configure your Odoo credentials (Base URL + API Key). Activate the Workflow Set the workflow to active to start listening for Telegram messages. Send a sample message to your bot to test. üß† Use Cases Sales reps capturing orders in the field SMEs managing customer inquiries directly from Telegram Real-time price and stock lookups without opening Odoo Automation of repetitive sales quote tasks üéõÔ∏è Customization Options This workflow can be easily adapted to your business needs: Change trigger platform: Replace Telegram with WhatsApp, Slack, or Discord using the respective n8n nodes. Extend data fields: Add fields like delivery date, salesperson, or payment terms. Auto-confirm orders: Add a node to automatically confirm a Sales Quote once approved. ‚úÖ Requirements Odoo v14 or later (with Sales module enabled) Telegram Bot Token n8n instance (Cloud or self-hosted) üí¨ Example Prompts Product Query: ‚ÄúWhat‚Äôs the price of Product B?‚Äù ‚ÄúHow many units of Product A are available?‚Äù Order Entry: ‚ÄúMy customer Amazon, his email address is abc@amazon.com wants to buy 10 pcs of Product A and 15 pcs of Product B.‚Äù",1,2025-10-01 10:41:35.700000+00:00,True,8
9234,"Customer Support & Lead Collection Chatbot with RAG, GPT-4o, Sheets & Telegram","Who‚Äôs it for This template is designed for small and medium businesses, startups, and agencies that want to automate customer inquiries, provide instant support, and capture leads without losing valuable conversations. It‚Äôs especially useful for teams that get many repetitive questions about products, services, or locations but don‚Äôt want to miss out on collecting contact details for follow-up. What it does / How it works The workflow creates a 24/7 AI-powered chatbot that answers company-related questions and collects customer information. It uses: ‚Ä¢ GPT-4o for natural conversations ‚Ä¢ Pinecone Vector Store for Retrieval-Augmented Generation (RAG) with your company knowledge base ‚Ä¢ Google Sheets to store structured lead data ‚Ä¢ Telegram to instantly notify your team When a customer asks about products, services, or hours, the AI answers using the Pinecone database. Afterwards, it politely asks for their name, email, phone number, and interest. The details are saved to Google Sheets and your team receives a Telegram message with a summary. How to set up Connect your OpenAI account. Create a Pinecone index with company FAQs, documents, or policies. Link your Google Sheet with columns: Name, Email, Phone, Interested in. Add your Telegram bot token and chat/group ID. Replace [INSERT_YOUR_COMPANY_NAME_HERE] in the system prompt with your company name. Requirements ‚Ä¢ OpenAI API key ‚Ä¢ Pinecone account ‚Ä¢ Google Sheets access ‚Ä¢ Telegram bot & chat ID How to customize ‚Ä¢ Change the system prompt to match your brand‚Äôs tone. ‚Ä¢ Update the Pinecone namespace and embeddings model if needed. ‚Ä¢ Add extra fields in Google Sheets (e.g., ‚ÄúBudget‚Äù or ‚ÄúPreferred product‚Äù). ‚Ä¢ Extend the flow with CRM integrations or automated email follow-ups. With this setup, you get a smart, RAG-powered chatbot that not only answers questions but also turns every conversation into a potential lead.",1,2025-10-03 12:09:29.884000+00:00,True,6
9489,"Process Customer Feedback with OpenAI, PDF Reports, Gmail & Slack Notifications","AI-Powered Feedback Automation with PDF Reports & Team Notifications Transform customer feedback into actionable insights automatically with AI analysis, professional PDF reports, personalized emails, and real-time team notifications. Table of Contents Overview Features Demo Prerequisites Quick Start Configuration Usage Troubleshooting License Overview AI-Powered Feedback Automation is a complete, production-ready n8n workflow that automatically processes customer feedback submissions with artificial intelligence, generates beautiful branded PDF reports, sends personalized email responses, logs data for analytics, and notifies your team in real-time. What Problem Does This Solve? Manual feedback processing is time-consuming and inconsistent. This workflow eliminates all manual work by: Automatically analyzing** sentiment and extracting key insights using OpenAI Generating professional** PDF reports with custom branding Sending personalized** thank-you emails to customers Logging everything** to Google Sheets for analytics and reporting Notifying your team** instantly via Slack with actionable summaries Perfect For Product Teams** - Collect and analyze user feedback systematically Educational Institutions** - Process student/parent feedback efficiently Customer Support** - Track customer satisfaction and sentiment trends E-commerce** - Manage product reviews and customer suggestions Healthcare** - Collect patient feedback and satisfaction scores Event Management** - Gather attendee feedback post-event Consulting Firms** - Streamline client feedback collection Features AI-Powered Analysis Sentiment Classification** - Automatically categorizes feedback as Positive, Neutral, or Negative Key Highlights Extraction** - Identifies the most important points from customer comments Actionable Recommendations** - AI generates specific suggestions based on feedback Executive Summaries** - Creates concise 2-3 sentence overviews of each submission Professional Report Generation Beautiful PDF Reports** - Branded, professional documents with custom styling Visual Elements** - Star ratings, color-coded sentiment badges, organized sections Responsive Design** - Mobile-friendly and print-optimized layouts 30-Day Hosting** - PDF reports automatically hosted with expiration dates Automated Email Communications Personalized Messages** - Thank-you emails customized with customer name and feedback PDF Attachments** - Direct download links to full feedback reports Sentiment Indicators** - Color-coded visual feedback summaries Professional Templates** - Modern, responsive email designs Data Logging & Analytics Google Sheets Integration** - Automatic logging of all feedback submissions Complete Audit Trail** - Tracks submission IDs, timestamps, and processing status Analytics Ready** - Structured data perfect for dashboards and trend analysis Historical Records** - Permanent storage of all feedback data Team Notifications Slack Integration** - Real-time alerts to team channels Rich Formatting** - Structured messages with highlights and action items Direct Links** - Quick access to full PDF reports from Slack Thread Discussions** - Enable team conversations around feedback Robust Error Handling Email Validation** - Automatically checks and handles invalid email addresses Fallback Mechanisms** - Continues workflow even if email sending fails Data Cleaning** - Sanitizes and normalizes all input data Graceful Degradation** - AI parsing failures handled with intelligent fallbacks Demo Workflow Overview User Submits Feedback ‚Üì Data Cleaning & Validation ‚Üì AI Sentiment Analysis (OpenAI) ‚Üì HTML Report Generation ‚Üì PDF Conversion ‚Üì Email Validation ‚îÄ‚î¨‚îÄ Valid ‚Üí Send Email ‚îî‚îÄ Invalid ‚Üí Skip ‚Üì Log to Google Sheets ‚Üì Notify Team (Slack) ‚Üì Webhook Response Sample Input { ""name"": ""Sarah Johnson"", ""email"": ""sarah@example.com"", ""rating"": 4, ""comments"": ""Great product! Delivery was a bit slow but customer service was helpful."", ""suggestions"": ""Improve shipping speed and tracking updates."" } Sample Output ‚úÖ AI Analysis: ""Positive"" sentiment with 3 key highlights ‚úÖ PDF Report: Professional 2-page document with branding ‚úÖ Email Sent: Personalized thank-you message delivered ‚úÖ Data Logged: New row added to Google Sheet ‚úÖ Team Notified: Slack message with summary posted ‚úÖ Webhook Response: 200 OK with submission details Prerequisites Required Services & Accounts n8n Instance (v0.220.0 or higher) Self-hosted or n8n Cloud Installation Guide OpenAI Account API key with GPT-3.5-turbo or GPT-4 access Sign Up Google Account (Gmail + Google Sheets) OAuth2 setup for Gmail API OAuth2 setup for Google Sheets API Setup Guide Slack Workspace Admin access to create apps or OAuth Bot token with chat:write and channels:read scopes Create Slack App HTML to PDF API Service GET at: PDFMunk API key required VerifiEmail API GET at: VerfiEmail API key required Quick Start 1. Import Template Option A: Import via URL Copy the workflow JSON URL and paste in n8n: Settings ‚Üí Import from URL ‚Üí [Paste URL] Option B: Import via File Download workflow.json In n8n: Workflows ‚Üí Import from File Select the downloaded JSON file Click ""Import"" 2. Configure Credentials (5 minutes) Navigate to: Settings ‚Üí Credentials and add: ‚úÖ OpenAI API - Add API key from OpenAI dashboard ‚úÖ Gmail OAuth2 - Connect and authorize your Gmail account ‚úÖ Google Sheets OAuth2 - Use same Google account ‚úÖ Slack OAuth2 - Install app to workspace and authorize ‚úÖ HTML to PDF API - Add API key from your PDF service ‚úÖ VerifiEmail API - Add API key from VerifiEmail dashboard 3. Create Google Sheet (2 minutes) Create a new Google Sheet named ""Feedback Log"" with these column headers: Submission ID | Timestamp | Name | Email | Rating | Sentiment | Comments | Suggestions | AI Summary | PDF URL | PDF Available Until | Email Sent 4. Configure Workflow (3 minutes) Open the imported workflow Click ""Log Feedback Data"" node Select your ""Feedback Log"" spreadsheet Click ""Notify Team"" node Select your Slack channel (e.g., #feedback) 5. Test & Activate (5 minutes) Execute the ""Webhook"" node to get test URL Send test POST request (see test data below) Verify all nodes execute successfully Check email, Google Sheet, and Slack Click ""Active"" toggle to enable workflow Total Setup Time: ~15-20 minutes Configuration Webhook Configuration The workflow receives feedback via POST webhook: URL Format: https://your-n8n-domain.com/webhook/feedback-submission Expected Payload: { ""name"": ""string (required)"", ""email"": ""string (optional, validated)"", ""rating"": ""integer 1-5 (required)"", ""comments"": ""string (optional)"", ""suggestions"": ""string (optional)"" } Usage Testing the Workflow Using Postman/Insomnia: Create new POST request URL: https://your-n8n-domain.com/webhook/feedback-submission Headers: Content-Type: application/json Body (raw JSON): { ""name"": ""Test User"", ""email"": ""your-email@example.com"", ""rating"": 5, ""comments"": ""This is a test feedback submission. Everything works great!"", ""suggestions"": ""Maybe add more features in the future."" } Send request Expected response (200 OK): { ""success"": true, ""message"": ""Thank you for your feedback! We've sent you a detailed report via email."", ""data"": { ""submissionId"": ""FB-1234567890123-abc123xyz"", ""name"": ""Test User"", ""email"": ""your-email@example.com"", ""rating"": ""5"", ""sentiment"": ""Positive"", ""emailSent"": ""true"", ""reportUrl"": ""https://generated-pdf-url.com/report.pdf"", ""reportAvailableUntil"": ""2025-11-10"" } } Using cURL: curl -X POST https://your-n8n-domain.com/webhook/feedback-submission \ -H ""Content-Type: application/json"" \ -d '{ ""name"": ""Sarah Johnson"", ""email"": ""sarah@example.com"", ""rating"": 4, ""comments"": ""Great product! Delivery was a bit slow but customer service was helpful."", ""suggestions"": ""Improve shipping speed and tracking updates."" }' Monitoring & Maintenance Daily: Check Slack for new feedback notifications Review Google Sheet for any anomalies Weekly: Verify workflow execution success rate Check OpenAI API usage and costs Review sentiment trends in Google Sheet Monthly: Analyze feedback patterns and trends Update AI prompts if needed Check PDF service usage limits Review and optimize workflow performance Best Practices Rate Limiting Monitor for spam submissions Add rate limiting to webhook if needed Use n8n's built-in throttling Data Privacy Ensure GDPR/privacy compliance Add data retention policies Implement data deletion workflow Error Handling Set up error notifications Create error logging workflow Monitor execution failures Performance Keep Google Sheet under 50,000 rows Archive old data quarterly Use database for high volume (1000+/month) Troubleshooting Common Issues Issue 1: Webhook Not Receiving Data Symptoms: Webhook node shows no executions Forms submit but nothing happens Solutions: ‚úÖ Verify workflow is Active (toggle at top right) ‚úÖ Check webhook URL is correct in form ‚úÖ Test webhook with Postman/cURL first ‚úÖ Check n8n logs for errors: Settings ‚Üí Log Streaming ‚úÖ Verify firewall/network allows incoming webhooks Issue 2: OpenAI Node Fails Symptoms: Error: ""API key invalid"" Error: ""Insufficient credits"" Node times out Solutions: ‚úÖ Verify API key is correct and active ‚úÖ Check OpenAI account has sufficient credits ‚úÖ Check API usage limits: platform.openai.com/usage ‚úÖ Increase node timeout in workflow settings ‚úÖ Try with shorter feedback text Issue 3: PDF Not Generating Symptoms: ""PDF generation failed"" error Empty PDF URL 404 when accessing PDF Solutions: ‚úÖ Verify PDF API key is valid ‚úÖ Check API service status ‚úÖ Verify HTML content is valid (test in browser) ‚úÖ Check API usage limits/quota ‚úÖ Try alternative PDF service Issue 4: Email Not Sending Symptoms: Gmail node shows error Email doesn't arrive ""Permission denied"" error Solutions: ‚úÖ Re-authenticate Gmail OAuth2 credential ‚úÖ Check email address is valid ‚úÖ Check spam/junk folder ‚úÖ Verify Gmail API is enabled in Google Console ‚úÖ Check daily sending limits not exceeded ‚úÖ Test with different email address Issue 5: Google Sheets Not Updating Symptoms: No new rows added ""Spreadsheet not found"" error Permission errors Solutions: ‚úÖ Verify spreadsheet ID is correct ‚úÖ Check sheet name matches exactly (case-sensitive) ‚úÖ Verify column headers match exactly ‚úÖ Re-authenticate Google Sheets credential ‚úÖ Check spreadsheet isn't protected/locked ‚úÖ Verify spreadsheet isn't full (limit: 10M cells) Issue 6: Slack Not Posting Symptoms: Slack node fails Message doesn't appear in channel ""Channel not found"" error Solutions: ‚úÖ Verify bot is invited to channel: /invite @BotName ‚úÖ Check bot has chat:write permission ‚úÖ Re-authenticate Slack credential ‚úÖ Verify channel ID is correct ‚úÖ Check Slack workspace isn't on free plan limits ‚úÖ Test with different channel Debugging Tips Enable Debug Mode Settings ‚Üí Executions ‚Üí Save execution progress Watch each node execute step-by-step Check Execution Logs Click on failed node View ""Input"" and ""Output"" tabs Check error messages Test Nodes Individually Click ""Execute Node"" on each node Verify output before proceeding Use Browser Console Open Developer Tools (F12) Check for JavaScript errors Monitor network requests Enable Verbose Logging For self-hosted n8n N8N_LOG_LEVEL=debug npm start üìÑ License This template is licensed under the MIT License - see the LICENSE file for details.",1,2025-10-11 09:28:26.496000+00:00,True,5
9580,Real-time Registration Form Tracking with Google Sheets and Gmail Notifications,"Title: Gracewell Automated Registration Tracker Suite version: 1.0 tags: [education, automation, google-sheets, gmail, dashboard, form-tracking, workflow-suite] category: Academic Automation Description: Automate the real-time tracking of Google Form submissions using Google Sheets and n8n. This 3-workflow suite includes a live dashboard with submission statistics and two companion workflows To send acknowledgement and reminder emails to students ‚Äî all seamlessly connected through n8n webhooks. üöÄ Gracewell Automated Registration Tracker Suite A fully automated, real-time monitoring system for student form submissions ‚Äî built with Google Sheets, Gmail, and n8n. The suite provides a live dashboard, auto-generated summary analytics, and instant messaging (email or WhatsApp) capabilities. üß© Workflow Suite Overview | Workflow | Purpose | Webhook | Output | |-----------|----------|----------|----------| | 1Ô∏è‚É£ Live Dashboard | Displays summary of submissions and includes trigger buttons | /live-tracking | HTML Dashboard | | 2Ô∏è‚É£ Send Acknowledgements | Sends thank-you emails to completed respondents | /send-acknowledgements | HTML Confirmation Page | | 3Ô∏è‚É£ Send Reminders | Sends personalized reminders to pending students | /send-reminders | HTML Confirmation Page | üß† Who Is This For Educators managing student registration or feedback Department coordinators tracking submission completion EdTech and training institutions handling digital enrolments Universities automating exam registration or attendance forms ‚öôÔ∏è Problem It Solves Manual form tracking is slow, repetitive, and prone to error. This workflow suite provides: ‚úÖ Automated reconciliation of student vs. response data ‚úÖ Instant calculation of pending and completed submissions ‚úÖ One-click acknowledgement and reminder dispatch ‚úÖ Web-based dashboard with summary visualization üìä Workflow 1: Live Tracking Dashboard Steps Google Sheets ‚Äì Student List ‚Üí Reads master student sheet Google Sheets ‚Äì Form Responses ‚Üí Reads live form entries Code Node ‚Üí Compares Register Nos. and generates HTML dashboard Respond to Webhook ‚Üí Displays HTML with summary and control buttons Output A clean web dashboard showing: Total, Submitted, Pending, Completion % ‚úÖ Completed list and ‚ö†Ô∏è Pending list Two buttons: Send Acknowledgements Send Reminders üíå Workflow 2: Send Acknowledgements Steps Webhook Trigger ‚Üí /send-acknowledgements Google Sheets ‚Äì Student List & Form Responses Merge Node ‚Üí Combines matching rows Code Node ‚Üí Generates personalized thank-you emails Gmail Node ‚Üí Sends messages Respond Node ‚Üí Displays confirmation message Email Template &gt; Subject: ‚úÖ Thank You for Your Submission &gt; Body: &gt; Dear {{name}}, &gt; Thank you for completing your form (Reg No: {{reg}}). &gt; We appreciate your prompt response. &gt; ‚Äì Gracewell ‚ö†Ô∏è Workflow 3: Send Reminders Steps Webhook Trigger ‚Üí /send-reminders Google Sheets ‚Äì Student List & Form Responses Merge Node ‚Üí Aligns both data sets Code Node ‚Üí Identifies pending students Gmail Node ‚Üí Sends customized reminder Respond Node ‚Üí Confirms completion Email Template &gt; Subject: ‚ö†Ô∏è Reminder: Please Complete Your Form &gt; Body: &gt; Dear {{name}}, &gt; You haven‚Äôt yet completed your form (Reg No: {{reg}}). &gt; Please complete it at the earliest. &gt; ‚Äì Gracewell üõ†Ô∏è Setup Instructions Step 1 ‚Äì Connect Accounts Connect Google Sheets (OAuth2) Connect Gmail for sending messages Step 2 ‚Äì Prepare Sheets Student List Sheet:** Columns ‚Üí Register No., Student Name, Contact, Email Form Response Sheet:** Columns ‚Üí Timestamp, Reg No, Email Address Step 3 ‚Äì Deploy URLs | Workflow | Webhook Endpoint | |-----------|------------------| | Dashboard | /live-tracking | | Acknowledgements | /send-acknowledgements | | Reminders | /send-reminders | üé® Optional Customizations Add institution name/logo to dashboard HTML Include charts (Chart.js) for submission visualization Auto-refresh every 60 seconds Integrate WhatsApp messaging via Twilio or Gupshup Log emails into a ‚ÄúMail Logs‚Äù sheet üß© Version Summary | Workflow | Version | Description | |-----------|----------|-------------| | Live Dashboard | v3 | Interactive HTML dashboard with controls | | Send Acknowledgements | v1 | Personalized thank-you email sender | | Send Reminders | v1 | Automated reminder email sender | Need help customizing? ‚úâÔ∏è Contact Me üíº LinkedIn ‚ú® Credits Developed by Dr. J. Jeffin Gracewell Empowering education through intelligent automation üí° We are open for and Customizing development",1,2025-10-13 18:10:27.410000+00:00,False,3
9611,"AI-Powered NDA Review & Instant Alert System - Jotform, Gemini, Telegram","This workflow automates the process of analyzing a contract submitted via a web form. It extracts the text from an uploaded PDF, uses AI to identify potential red flags, and sends a summary report to a Telegram chat. Prerequisites Before you can use this workflow, you'll need a few things set up. 1. JotForm Form You need to create a form in JotForm with at least two specific fields: Email Address**: A standard field to collect the user's email. File Upload**: This field will be used to upload the contract or NDA. Make sure to configure it to allow .pdf files. 2. API Keys and IDs JotForm API Key**: You can generate this from your JotForm account settings under the ""API"" section. Gemini API Key**: You'll need an API key from Google AI Studio to use the Gemini model. Telegram Bot Token**: Create a new bot by talking to the @BotFather on Telegram. It will give you a unique token. Telegram Chat ID**: This is the ID of the user, group, or channel you want the bot to send messages to. You can get this by using a bot like @userinfobot. Node-by-Node Explanation Here is a breakdown of what each node in the workflow does, in the order they execute. 1. JotForm Trigger What it does**: This node kicks off the entire workflow. It actively listens for new submissions on the specific JotForm you select. How it works**: When someone fills out your form and hits ""Submit,"" JotForm sends the submission data (including the email and a link to the uploaded file) to this node. 2. Grab Attachment Details (HTTP Request) What it does**: The initial data from JotForm doesn't contain a direct download link for the file. This node takes the submissionID from the trigger and makes a request to the JotForm API to get the full details of that submission. How it works**: It constructs a URL using the submissionID and your JotForm API key to fetch the submission data, which includes the proper download URL for the uploaded contract. 3. Grab the Attached Contract (HTTP Request) What it does**: Now that it has the direct download link, this node fetches the actual PDF file. How it works**: It uses the file URL obtained from the previous node to download the contract. The node is set to expect a ""file"" as the response, so it saves the PDF data in binary format for the next step. 4. Extract Text from PDF File What it does**: This node takes the binary PDF data from the previous step and extracts all the readable text from it. How it works**: It processes the PDF and outputs plain text, stripping away any formatting or images. This raw text is now ready to be analyzed by the AI. 5. AI Agent (with Google Gemini Chat Model) What it does**: This is the core analysis engine of the workflow. It takes the extracted text from the PDF and uses a powerful prompt to analyze it. The ""Google Gemini Chat Model"" node is connected as its ""brain."" How it works**: It sends the contract text to the Gemini model. The prompt instructs Gemini to act as an expert contract analyst. It specifically asks the AI to identify major red flags and hidden/unfair clauses. It also tells the AI to format the output as a clean report using Telegram's MarkdownV2 style and to keep the response under 1500 characters. 6. Send a text message (Telegram) What it does**: This is the final step. It takes the formatted analysis report generated by the AI Agent and sends it to your specified Telegram chat. How it works**: It connects to your Telegram bot using your Bot Token and sends the AI's output ($json.output) to the Chat ID you've provided. Because the AI was instructed to format the text in MarkdownV2, the message will appear well-structured in Telegram with bolding and bullet points.",1,2025-10-14 10:06:35.784000+00:00,True,4
9769,Auto-Curate & Post LinkedIn Company Page using RSS + Gemini AI + Templated.io,"This workflow automates your LinkedIn content curation and posting using AI. Every week, it pulls the latest marketing insights from an RSS feed (like HubSpot‚Äôs), analyzes them, and turns the best article into a crisp, human-style LinkedIn post ‚Äî complete with branded visuals and headline text using the Templated API. You can fully customize it for your own brand‚Äôs voice, niche, and platform. ‚öôÔ∏è What It Does üì∞ Fetches articles from any RSS feed you choose üß© Uses AI (Gemini) to: Pick the most relevant article for your target audience Rewrite it into a short, save-worthy post (&lt;200 words) Optimize tone, clarity, and structure for readability üñºÔ∏è Uses Templated API to: Generate branded visuals with your own layout or assets Auto-inject headline text into image templates Maintain consistent post design across your feed ü§ñ Auto-posts to LinkedIn (only if AI confidence ‚â•7/10) ü™Ñ Setup Instructions Replace the RSS URL in the ‚ÄúRSS Read‚Äù node with your preferred feed. Connect your own credentials: Google Gemini (PaLM) API key via Credentials LinkedIn OAuth2 credentials Templated API credentials (optional, for image/headline generation) Review and tweak AI prompts in: ‚ÄúBest Article Finder‚Äù ‚ÄúContent Creator‚Äù ‚ÄúPost Optimizer‚Äù Adjust your posting frequency, tone, or feed to suit your brand. Activate and let it run automatically. üí° Ideal For Solo marketers or consultants managing content calendars Startup marketing teams automating top-of-funnel content Founders building personal brands on LinkedIn Creators who want branded visuals with consistent design",1,2025-10-16 09:41:48.972000+00:00,True,6
9788,Compare Flight Prices Across Multiple Booking Platforms with Email Reports,"This workflow automates flight price comparison across multiple booking platforms (Kayak, Skyscanner, Expedia, Google Flights). It accepts natural language queries, extracts flight details using NLP, scrapes prices in parallel, identifies the best deals, and sends professional email reports with comprehensive price breakdowns and booking links. üì¶ What You'll Get A fully functional, production-ready n8n workflow that: ‚úÖ Compares flight prices across 4 major platforms (Kayak, Skyscanner, Expedia, Google Flights) ‚úÖ Accepts natural language requests (""Flight from NYC to London on March 25"") ‚úÖ Sends beautiful email reports with best deals ‚úÖ Returns real-time JSON responses for web apps ‚úÖ Handles errors gracefully with helpful messages ‚úÖ Includes detailed documentation with sticky notes üöÄ Quick Setup (3 Steps) Step 1: Import Workflow to n8n Copy the JSON from the first artifact (workflow file) Open n8n ‚Üí Go to Workflows Click ""Import from File"" ‚Üí Paste JSON ‚Üí Click Import ‚úÖ Workflow imported successfully! Step 2: Setup Python Scraper On your server (where n8n SSH nodes will connect): Navigate to your scripts directory cd /home/oneclick-server2/ Create the scraper file nano flight_scraper.py Copy the entire Python script from the second artifact Save with Ctrl+X, then Y, then Enter Make it executable chmod +x flight_scraper.py Install required packages pip3 install selenium Install Chrome and ChromeDriver sudo apt update sudo apt install -y chromium-browser chromium-chromedriver Test the scraper python3 flight_scraper.py JFK LHR 2025-03-25 2025-03-30 round-trip 1 economy kayak Expected Output: Delta|$450|7h 30m|0|10:00 AM|6:30 PM|https://kayak.com/... British Airways|$485|7h 45m|0|11:30 AM|8:15 PM|https://kayak.com/... ... Step 3: Configure n8n Credentials A. Setup SMTP (for sending emails): In n8n: Credentials ‚Üí Add Credential ‚Üí SMTP Fill in details: Host: smtp.gmail.com Port: 587 User: your-email@gmail.com Password: [Your App Password] For Gmail Users: Enable 2FA: https://myaccount.google.com/security Create App Password: https://myaccount.google.com/apppasswords Use the 16-character password in n8n B. Setup SSH (already configured if you used existing credentials): In workflow, SSH nodes use: ilPh8oO4GfSlc0Qy Verify credential exists and points to correct server Update path if needed: /home/oneclick-server2/ C. Activate Workflow: Click the workflow toggle ‚Üí Active ‚úÖ Webhook is now live! üéØ How to Use Method 1: Direct Webhook Call curl -X POST https://your-n8n-domain.com/webhook/flight-price-compare \ -H ""Content-Type: application/json"" \ -d '{ ""message"": ""Flight from Mumbai to Dubai on 15th March, round-trip returning 20th March"", ""email"": ""user@example.com"", ""name"": ""John Doe"" }' Response: { ""success"": true, ""message"": ""Flight comparison sent to user@example.com"", ""route"": ""BOM ‚Üí DXB"", ""bestPrice"": 450, ""airline"": ""Emirates"", ""totalResults"": 18 } Method 2: Natural Language Queries The workflow understands various formats: ‚úÖ All these work: ""Flight from New York to London on 25th March, one-way"" ""NYC to LHR March 25 round-trip return March 30"" ""I need a flight from Mumbai to Dubai departing 15th March"" ""JFK LHR 2025-03-25 2025-03-30 round-trip"" Supported cities (auto-converts to airport codes): New York ‚Üí JFK London ‚Üí LHR Mumbai ‚Üí BOM Dubai ‚Üí DXB Singapore ‚Üí SIN And 20+ more cities Method 3: Structured JSON { ""from"": ""JFK"", ""to"": ""LHR"", ""departure_date"": ""2025-03-25"", ""return_date"": ""2025-03-30"", ""trip_type"": ""round-trip"", ""passengers"": 1, ""class"": ""economy"", ""email"": ""user@example.com"", ""name"": ""John"" } üìß Email Report Example Users receive an email like this: FLIGHT PRICE COMPARISON Route: JFK ‚Üí LHR Departure: 25 Mar 2025 Return: 30 Mar 2025 Trip Type: round-trip Passengers: 1 üèÜ BEST DEAL British Airways Price: $450 Duration: 7h 30m Stops: Non-stop Platform: Kayak üí∞ Save $85 vs highest price! üìä ALL RESULTS (Top 10) British Airways - $450 (Non-stop) - Kayak Delta - $475 (Non-stop) - Google Flights American Airlines - $485 (Non-stop) - Expedia Virgin Atlantic - $495 (Non-stop) - Skyscanner United - $520 (1 stop) - Kayak ... Average Price: $495 Total Results: 23 Prices subject to availability. Happy travels! ‚úàÔ∏è üîß Customization Options Change Scraping Platforms Add more platforms: Duplicate an SSH scraping node Change platform parameter: kayak ‚Üí new-platform Add scraping logic in flight_scraper.py Connect to ""Aggregate & Analyze Prices"" node Remove platforms: Delete unwanted SSH node Workflow continues with remaining platforms Modify Email Format Edit the ""Format Email Report"" node: // Change to HTML format const html = ` &lt;!DOCTYPE html&gt; &lt;html&gt; &lt;body&gt; Flight Deals Best price: ${bestDeal.currency}${bestDeal.price} &lt;/body&gt; &lt;/html&gt; `; return [{ json: { subject: ""..."", html: html, // Instead of text ...data } }]; Then update ""Send Email Report"" node: Change emailFormat to html Use {{$json.html}} instead of {{$json.text}} Add More Cities/Airports Edit ""Parse & Validate Flight Request"" node: const airportCodes = { ...existing codes..., 'berlin': 'BER', 'rome': 'FCO', 'barcelona': 'BCN', // Add your cities here }; Change Timeout Settings In each SSH node, add: ""timeout"": 30000 // 30 seconds üêõ Troubleshooting Issue: ""No flights found"" Possible causes: Scraper script not working Website structure changed Dates in past Invalid airport codes Solutions: Test scraper manually cd /home/oneclick-server2/ python3 flight_scraper.py JFK LHR 2025-03-25 """" one-way 1 economy kayak Check if output shows flights If no output, check Chrome/ChromeDriver installation Issue: ""Connection refused"" (SSH) Solutions: Verify SSH credentials in n8n Check server is accessible: ssh user@your-server Verify path exists: /home/oneclick-server2/ Check Python installed: which python3 Issue: ""Email not sending"" Solutions: Verify SMTP credentials Check email in spam folder For Gmail: Confirm App Password is used (not regular password) Test SMTP connection: telnet smtp.gmail.com 587 Issue: ""Webhook not responding"" Solutions: Ensure workflow is Active (toggle on) Check webhook path: /webhook/flight-price-compare Test with curl command (see ""How to Use"" section) Check n8n logs: Settings ‚Üí Log Streaming Issue: ""Scraper timing out"" Solutions: In flight_scraper.py, increase wait times time.sleep(10) # Instead of time.sleep(5) Or increase WebDriverWait timeout WebDriverWait(driver, 30) # Instead of 20 üìä Understanding the Workflow Node-by-Node Explanation 1. Webhook - Receive Flight Request Entry point for all requests Accepts POST requests Path: /webhook/flight-price-compare 2. Parse & Validate Flight Request Extracts flight details from natural language Converts city names to airport codes Validates required fields Returns helpful errors if data missing 3. Check If Request Valid Routes to scraping if valid Routes to error response if invalid 4-7. Scrape [Platform] (4 nodes) Run in parallel for speed Each calls Python script with platform parameter Continue on failure (don't break workflow) Return pipe-delimited flight data 8. Aggregate & Analyze Prices Collects all scraper results Parses flight data Finds best overall deal Finds best non-stop flight Calculates statistics Sorts by price 9. Format Email Report Creates readable text report Includes route details Highlights best deal Lists top 10 results Shows statistics 10. Send Email Report Sends formatted email to user Uses SMTP credentials 11. Webhook Response (Success) Returns JSON response immediately Includes best price summary Confirms email sent 12. Webhook Response (Error) Returns helpful error message Guides user on what's missing üé® Workflow Features ‚úÖ Included Features Natural Language Processing**: Understands flexible input formats Multi-Platform Comparison**: 4 major booking sites Parallel Scraping**: All platforms scraped simultaneously Error Handling**: Graceful failures, helpful messages Email Reports**: Professional format with all details Real-Time Responses**: Instant webhook feedback Sticky Notes**: Detailed documentation in workflow Airport Code Mapping**: Auto-converts 20+ cities üöß Not Included (Easy to Add) Price Alerts**: Monitor price drops (add Google Sheets) Analytics Dashboard**: Track searches (add Google Sheets) SMS Notifications**: Send via Twilio Slack Integration**: Post to channels Database Logging**: Store searches in PostgreSQL Multi-Currency**: Show prices in the user's currency üí° Pro Tips Tip 1: Speed Up Scraping Use faster scraping service (like ScraperAPI): // Replace SSH nodes with HTTP Request nodes { ""url"": ""http://api.scraperapi.com"", ""qs"": { ""api_key"": ""YOUR_KEY"", ""url"": ""https://kayak.com/flights/..."" } } Tip 2: Cache Results Add caching to avoid duplicate scraping: // In Parse node, check cache first const cacheKey = ${origin}-${dest}-${departureDate}; const cached = await $cache.get(cacheKey); if (cached && Date.now() - cached.time &lt; 3600000) { return cached.data; // Use 1-hour cache } Tip 3: Add More Platforms Easy to add Momondo, CheapOair, etc.: Add function in flight_scraper.py Add SSH node in workflow Connect to aggregator Tip 4: Improve Date Parsing Handle more formats: // Add to Parse node const formats = [ 'DD/MM/YYYY', 'MM-DD-YYYY', 'YYYY.MM.DD', // Add your formats ];",1,2025-10-16 15:15:14.462000+00:00,False,2
9826,"Automate Incident Response with Jira, Slack, Google Sheets & Drive","üìò Description: This workflow automates the incident response lifecycle ‚Äî from creation to communication and archival. It instantly creates Jira tickets for new incidents, alerts the on-call Slack team, generates timeline reports, logs the status in Google Sheets, and archives documentation to Google Drive ‚Äî all automatically. It helps engineering and DevOps teams respond faster, maintain audit trails, and ensure no incident details are lost, even after Slack or Jira history expires. ‚öôÔ∏è What This Workflow Does (Step-by-Step) üü¢ Manual Trigger ‚Äì Start the incident creation and alerting process manually on demand. üè∑Ô∏è Define Incident Metadata ‚Äì Sets up standardized incident data (Service, Severity, Description) used across Jira, Slack, and Sheets for consistent processing. üé´ Create Jira Incident Ticket ‚Äì Automatically creates a Jira task with service, severity, and description fields. Returns a unique Jira key and link for tracking. ‚úÖ Validate Jira Ticket Creation Success ‚Äì Confirms the Jira ticket was successfully created before continuing. True Path: Proceeds to Slack alerts and documentation flow. False Path: Logs the failure details to Google Sheets for debugging. üö® Log Jira Creation Failures to Error Sheet ‚Äì Records any Jira API errors, permission issues, or timeouts to an error log sheet for reliability monitoring. üîó Combine Incident & Jira Data ‚Äì Merges incident context with Jira ticket data to ensure all details are unified for downstream notifications. üí¨ Format Incident Alert for Slack ‚Äì Generates a rich Slack message containing Jira key, service, severity, and description with clickable Jira links. üì¢ Alert On-Call Team in Slack ‚Äì Posts the formatted message directly to the #oncall Slack channel to instantly notify engineers. üìã Generate Incident Timeline Report ‚Äì Parses Slack message content to create a detailed incident timeline including timestamps, service, severity, and placeholders for postmortem tracking. üìÑ Convert Timeline to Text File ‚Äì Converts the generated timeline into a structured .txt file for archival and compliance. ‚òÅÔ∏è Archive Incident Timeline to Drive ‚Äì Uploads the finalized incident report to Google Drive (‚ÄúIncident Reports‚Äù folder) with timestamped filenames for traceability. üìä Log Incident to Status Tracking Sheet ‚Äì Appends Jira key, service, severity, and timestamp to the ‚Äústatus update‚Äù Google Sheet to build a live incident dashboard and enable SLA tracking. üß© Prerequisites Jira account with API access Google Sheets for ‚Äústatus update‚Äù and ‚Äúerror log‚Äù tracking Slack workspace connected via API credentials Google Drive access for archival üí° Key Benefits ‚úÖ Instant Slack alerts for new incidents ‚úÖ Centralized Jira ticketing and tracking ‚úÖ Automated timeline documentation for audits ‚úÖ Seamless Google Drive archival and status logging ‚úÖ Reduced MTTR through faster communication üë• Perfect For DevOps and SRE teams managing production incidents Engineering managers overseeing uptime and reliability Organizations needing automated post-incident documentation Teams focused on SLA adherence and compliance reporting",1,2025-10-17 12:13:34.878000+00:00,False,5
9843,Create YouTube Videos Daily from Google Sheets using MagicHour + Gemini,"ü™Ñ Prompt To Video (MagicHour API) with Music & YouTube Automate AI video creation, background music, YouTube uploads, and result logging ‚Äî all from a single text prompt. ‚ö° Overview This n8n template turns a text prompt into a complete AI-generated video using the MagicHour API, adds background music, generates YouTube metadata, uploads to YouTube, and logs results in Google Sheets ‚Äî all in one flow. Perfect for creators, marketers, and startups producing YouTube content at scale ‚Äî from daily AI Shorts to explainers or marketing clips. üß© Use Cases üé• Daily AI-generated Shorts üß† Product explainers üöÄ Marketing & brand automation üîÅ Repurpose blog posts into videos üí° AI storytelling or creative projects ‚öôÔ∏è How It Works Trigger when a new row is added to Google Sheets or via Chat input. Gemini parses and normalizes the text prompt. MagicHour API generates the AI video. Poll until the render completes. (Optional) Mix background audio using MediaFX. Gemini generates YouTube title, description, and tags. Upload the video to YouTube with metadata. Log YouTube URL, metadata, and download link back to Google Sheets. üß∞ Requirements Service Purpose MagicHour API Key Text-to-video generation Gemini API Key Prompt parsing & metadata YouTube OAuth2 Video uploads Google Sheets OAuth2 Trigger & logging (Optional) MediaFX Node Audio mixing üóÇÔ∏è Google Sheets Setup Column Description Prompt Text prompt for video Background Music URL (Optional) Royalty-free track Status Tracks flow progress YouTube URL Auto-filled after upload Metadata Title, tags, and description JSON Date Generated (Optional) Auto-filled with video creation date üìÖ 100 Daily Prompts Automation You can scale this workflow to generate one video per day from a batch of 100 prompts in Google Sheets. Setup Steps Add 100 prompts to your Google Sheet ‚Äî one per row. Set the Status column for each to Pending. Use a Cron Trigger in n8n to run the workflow once daily (e.g., at 9 AM). Each run picks one Pending prompt, generates a video, uploads to YouTube, then marks it as Done. Continues daily until all 100 prompts are processed. Example Cron Expression 0 9 * * * ‚Üí Runs the automation every day at 9:00 AM. Node Sequence [Schedule Trigger (Daily)] ‚Üí [Get Pending Prompt from Sheets] ‚Üí [Gemini Prompt Parser] ‚Üí [MagicHour Video Generation] ‚Üí [Optional: MediaFX Audio Mix] ‚Üí [Gemini Metadata Generator] ‚Üí [YouTube Upload] ‚Üí [Update Row in Sheets] üí° Optional Enhancements: Add a notification node (Slack, Discord, or Email) after each upload. Add a counter check to stop after 100 videos. Add a ‚ÄúPaused‚Äù column to skip specific rows. üß† Gemini Integration Gemini handles: JSON parsing for MagicHour requests Metadata generation (title, description, tags) Optional creative rewriting of prompts üéß Audio Mixing (Optional) Install MediaFX Community Node ‚Üí Settings ‚Üí Community Nodes ‚Üí n8n-nodes-mediafx Use it to blend background music automatically into videos. ü™∂ Error Handling Avoid ‚ÄúContinue on Fail‚Äù in key nodes Use IF branches for MagicHour API errors Add retry/timeout logic for polling steps üß± Node Naming Tips Rename generic nodes for clarity: Merge ‚Üí Merge Video & Audio If ‚Üí Check Video Completion HTTP Request ‚Üí MagicHour API Request üöÄ How to Use Add MagicHour, Gemini, YouTube, and Sheets credentials Replace background music with your own track Use Google Sheets trigger or daily cron for automation Videos are created, uploaded, and logged ‚Äî hands-free ‚ö†Ô∏è Disclaimer This template uses community nodes (MediaFX). Install and enable them manually. MagicHour API usage may incur costs based on video duration and quality. üåê SEO Keywords MagicHour API, n8n workflow, AI video generator, automated YouTube upload, Gemini metadata, AI Shorts, MediaFX, Google Sheets automation, AI marketing, content automation.",1,2025-10-17 14:50:58.327000+00:00,True,7
9945,Scrape Detailed GitHub Profiles to Google Sheets Using BrowserAct,"Scrape Detailed GitHub Profiles to Google Sheets Using BrowserAct This template is a sophisticated data enrichment and reporting tool that scrapes detailed GitHub user profiles and organizes the information into dedicated, structured reports within a Google Sheet. This workflow is essential for technical recruiters, talent acquisition teams, and business intelligence analysts who need to dive deep into a pre-qualified list of developers to understand their recent activity, repositories, and technical footprint. Self-Hosted Only This Workflow uses a community contribution and is designed and tested for self-hosted n8n instances only. How it works The workflow is triggered manually but can be started by a Schedule Trigger or by integrating directly with a candidate sourcing workflow (like the ""Source Top GitHub Contributors"" template). A Google Sheets node reads a list of target GitHub user profile URLs from a master candidate sheet. The Loop Over Items node processes each user one by one. A Slack notification is sent at the beginning of the loop to announce that the scraping process has started for the user. A BrowserAct node visits the user's GitHub profile URL and scrapes all available data, including profile info, repositories, and social links. A custom Code node (labeled ""Code in JavaScript"") performs a critical task: it cleans, fixes, and consolidates the complex, raw scraped data into a single, clean JSON object. The workflow then dynamically manages your output. It creates a new sheet dedicated to the user (named after them) and clears it to ensure a fresh report every time. The consolidated data is separated into three paths: main profile data, links, and repositories. Three final Google Sheets nodes then append the structured data to the user's dedicated sheet, creating a clear, multi-section report (User Data, User Links, User Repositories). Requirements BrowserAct** API account for web scraping BrowserAct* ""Scraping GitHub Users Activity & Data*"" Template BrowserAct* ""* Source Top GitHub Contributors by Language & Location**"" Template Output BrowserAct** n8n Community Node -&gt; (n8n Nodes BrowserAct) Google Sheets** credentials for input (candidate list) and structured output (individual user sheets) Slack** credentials for sending notifications Need Help? How to Find Your BrowseAct API Key & Workflow ID How to Connect n8n to Browseract How to Use & Customize BrowserAct Templates How to Use the BrowserAct N8N Community Node Workflow Guidance and Showcase GitHub Data Mining: Extracting User Profiles & Repositories with N8N",1,2025-10-20 15:34:04.771000+00:00,False,3
9946,Scrape & Import Shoe Products to Shopify with BrowserAct (with Variants & Images),"Scrape & Import Products to Shopify from Any Site (with Variants & Images)-(Optimized for shoes) This advanced n8n template automates e-commerce operations by scraping product data (including variants and images) from any URL and creating fully detailed products in your Shopify store. This workflow is essential for dropshippers, e-commerce store owners, and anyone looking to quickly import product catalogs from specific websites into their Shopify store. Self-Hosted Only This Workflow uses a community contribution and is designed and tested for self-hosted n8n instances only. How it works The workflow reads a list of product page URLs from a Google Sheet. Your sheet, with its columns for Product Name and Product Link, acts as a database for your workflow. The Loop Over Items node processes products one URL at a time. Two BrowserAct nodes run sequentially to scrape all product details, including the Name, price, description, sizes, and image links. A custom Code node transforms the raw scraped data (where fields like sizes might be a single string) into a structured JSON format with clean lists for sizes and images. The Shopify node creates the base product entry using the main details. The workflow then uses a series of nodes (Set Option and Add Option via HTTP Request) to dynamically add product options (e.g., ""Shoe Size"") to the new product. The workflow intelligently uses HTTP Request nodes to perform two crucial bulk tasks: Create a unique variant for each available size, including a custom SKU. Upload all associated product images from their external URLs to the product. A final Slack notification confirms the batch has been processed. Requirements BrowserAct** API account for web scraping BrowserAct* ""Bulk Product Scraping From (URLs) and uploading to Shopify (Optimized for shoe - NIKE -&gt; Shopify)*"" Template BrowserAct** n8n Community Node -&gt; (n8n Nodes BrowserAct) Google Sheets** credentials for the input list Shopify** credentials (API Access Token) to create and update products, variants, and images Slack** credentials (optional) for notifications Need Help? How to Find Your BrowseAct API Key & Workflow ID How to Connect n8n to Browseract How to Use & Customize BrowserAct Templates How to Use the BrowserAct N8N Community Node Workflow Guidance and Showcase Automate Shoe Scraping to Shopify Using n8n, BrowserAct & Google Sheets",1,2025-10-20 15:42:24.449000+00:00,False,5
10002,Track Competitor SEO Keywords with Decodo + GPT-4.1-mini + Google Sheets,"This workflow automates competitor keyword research using OpenAI LLM and Decodo for intelligent web scraping. Who this is for SEO specialists, content strategists, and growth marketers who want to automate keyword research and competitive intelligence. Marketing analysts managing multiple clients or websites who need consistent SEO tracking without manual data pulls. Agencies or automation engineers using Google Sheets as an SEO data dashboard for keyword monitoring and reporting. What problem this workflow solves Tracking competitor keywords manually is slow and inconsistent. Most SEO tools provide limited API access or lack contextual keyword analysis. This workflow solves that by: Automatically scraping any competitor‚Äôs webpage with Decodo. Using OpenAI GPT-4.1-mini to interpret keyword intent, density, and semantic focus. Storing structured keyword insights directly in Google Sheets for ongoing tracking and trend analysis. What this workflow does Trigger ‚Äî Manually start the workflow or schedule it to run periodically. Input Setup ‚Äî Define the website URL and target country (e.g., https://dev.to, france). Data Scraping (Decodo) ‚Äî Fetch competitor web content and metadata. Keyword Analysis (OpenAI GPT-4.1-mini) Extract primary and secondary keywords. Identify focus topics and semantic entities. Generate a keyword density summary and SEO strength score. Recommend optimization and internal linking opportunities. Data Structuring ‚Äî Clean and convert GPT output into JSON format. Data Storage (Google Sheets) ‚Äî Append structured keyword data to a Google Sheet for long-term tracking. Setup Prerequisites n8n account with workflow editor access Decodo API credentials OpenAI API key Google Sheets account connected via OAuth2 Make sure to install the Decodo Community node. Create a Google Sheet Add columns for: primary_keywords, seo_strength_score, keyword_density_summary, etc. Share with your n8n Google account. Connect Credentials Add credentials for: Decodo API credentials - You need to register, login and obtain the Basic Authentication Token via Decodo Dashboard OpenAI API (for GPT-4o-mini) Google Sheets OAuth2 Configure Input Fields Edit the ‚ÄúSet Input Fields‚Äù node to set your target site and region. Run the Workflow Click Execute Workflow in n8n. View structured results in your connected Google Sheet. How to customize this workflow Track Multiple Competitors** ‚Üí Use a Google Sheet or CSV list of URLs; loop through them using the Split In Batches node. Add Language Detection** ‚Üí Add a Gemini or GPT node before keyword analysis to detect content language and adjust prompts. Enhance the SEO Report** ‚Üí Expand the GPT prompt to include backlink insights, metadata optimization, or readability checks. Integrate Visualization** ‚Üí Connect your Google Sheet to Looker Studio for SEO performance dashboards. Schedule Auto-Runs** ‚Üí Use the Cron Node to run weekly or monthly for competitor keyword refreshes. Summary This workflow automates competitor keyword research using: Decodo** for intelligent web scraping OpenAI GPT-4.1-mini** for keyword and SEO analysis Google Sheets** for live tracking and reporting It‚Äôs a complete AI-powered SEO intelligence pipeline ideal for teams that want actionable insights on keyword gaps, optimization opportunities, and content focus trends, without relying on expensive SEO SaaS tools.",1,2025-10-21 23:04:45.122000+00:00,True,4
10076,Automate Dutch Public Procurement Data Collection with TenderNed,"TenderNed Public Procurement What This Workflow Does This workflow automates the collection of public procurement data from TenderNed (the official Dutch tender platform). It: Fetches the latest tender publications from the TenderNed API Retrieves detailed information in both XML and JSON formats for each tender Parses and extracts key information like organization names, titles, descriptions, and reference numbers Filters results based on your custom criteria Stores the data in a database for easy querying and analysis Setup Instructions This template comes with sticky notes providing step-by-step instructions in Dutch and various query options you can customize. Prerequisites TenderNed API Access - Register at TenderNed for API credentials Configuration Steps Set up TenderNed credentials: Add HTTP Basic Auth credentials with your TenderNed API username and password Apply these credentials to the three HTTP Request nodes: ""Tenderned Publicaties"" ""Haal XML Details"" ""Haal JSON Details"" Customize filters: Modify the ""Filter op ..."" node to match your specific requirements Examples: specific organizations, contract values, regions, etc. How It Works Step 1: Trigger The workflow can be triggered either manually for testing or automatically on a daily schedule. Step 2: Fetch Publications Makes an API call to TenderNed to retrieve a list of recent publications (up to 100 per request). Step 3: Process & Split Extracts the tender array from the response and splits it into individual items for processing. Step 4: Fetch Details For each tender, the workflow makes two parallel API calls: XML endpoint** - Retrieves the complete tender documentation in XML format JSON endpoint** - Fetches metadata including reference numbers and keywords Step 5: Parse & Merge Parses the XML data and merges it with the JSON metadata and batch information into a single data structure. Step 6: Extract Fields Maps the raw API data to clean, structured fields including: Publication ID and date Organization name Tender title and description Reference numbers (kenmerk, TED number) Step 7: Filter Applies your custom filter criteria to focus on relevant tenders only. Step 8: Store Inserts the processed data into your database for storage and future analysis. Customization Tips Modify API Parameters In the ""Tenderned Publicaties"" node, you can adjust: offset: Starting position for pagination size: Number of results per request (max 100) Add query parameters for date ranges, status filters, etc. Add More Fields Extend the ""Splits Alle Velden"" node to extract additional fields from the XML/JSON data, such as: Contract value estimates Deadline dates CPV codes (procurement classification) Contact information Integrate Notifications Add a Slack, Email, or Discord node after the filter to get notified about new matching tenders. Incremental Updates Modify the workflow to only fetch new tenders by: Storing the last execution timestamp Adding date filters to the API query Only processing publications newer than the last run Troubleshooting No data returned? Verify your TenderNed API credentials are correct Check that you have setup youre filter proper Need help setting this up or interested in a complete tender analysis solution? Get in touch üîó LinkedIn ‚Äì Wessel Bulte",1,2025-10-23 06:05:53.523000+00:00,False,2
10089,Synchronizing WooCommerce Inventory and Creating Products with Google Gemini AI and BrowserAct,"Synchronize WooCommerce Inventory & Create Products with Gemini AI & BrowserAct This sophisticated n8n template automates WooCommerce inventory management by scraping supplier data, updating existing products, and intelligently creating new ones with AI-formatted descriptions. This workflow is essential for e-commerce operators, dropshippers, and inventory managers who need to ensure their product pricing and stock levels are synchronized with multiple third-party suppliers, minimizing overselling and maximizing profit. Self-Hosted Only This Workflow uses a community contribution and is designed and tested for self-hosted n8n instances only. How it works The workflow is typically run by a Schedule Trigger (though a Manual Trigger is also shown) to check stock automatically. It reads a list of suppliers and their inventory page URLs from a central Google Sheet. The workflow loops through each supplier: A BrowserAct node scrapes the current stock and price data from the supplier's inventory page. A Code node parses this bulk data into individual product items. It then loops through each individual product found. The workflow checks WooCommerce to see if the product already exists based on its name. If the product exists: It proceeds to update the existing product's price and stock quantity. If the product DOES NOT exist: An If node checks if the missing product's category matches a predefined type (optional filtering). If it passes the filter, a second BrowserAct workflow scrapes detailed product attributes from a dedicated product page (e.g., DigiKey). An AI Agent (Gemini) transforms these attributes into a specific, styled HTML table for the product description. Finally, the product is created in WooCommerce with all scraped details and the AI-generated description. Error Handling:* Multiple *Slack** nodes are configured to alert your team immediately if any scraping task fails or if the product update/creation process encounters an issue. Note: This workflow does not support image uploads for new products. To enable this functionality, you must modify both the n8n and BrowserAct workflows. Requirements BrowserAct** API account for web scraping BrowserAct** n8n Community Node -&gt; (n8n Nodes BrowserAct) BrowserAct* templates named *‚ÄúWooCommerce Inventory & Stock Synchronization‚Äù* and *‚ÄúWooCommerce Product Data Reconciliation‚Äù** Google Sheets** credentials for the supplier list WooCommerce** credentials for product management Google Gemini** account for the AI Agent Slack** credentials for error alerts Need Help? How to Find Your BrowseAct API Key & Workflow ID How to Connect n8n to Browseract How to Use & Customize BrowserAct Templates How to Use the BrowserAct N8N Community Node Workflow Guidance and Showcase STOP Overselling! Auto-Sync WooCommerce Inventory from ANY Supplier",1,2025-10-23 17:35:38.832000+00:00,True,6
10143,"Create Personalized Email Outreach with AI, Telegram Bot & Website Scraping","Demo Personalized Email This n8n workflow is built for AI and automation agencies to promote their workflows through an interactive demo that prospects can try themselves. The featured system is a deep personalized email demo. üîÑ How It Works Prospect Interaction A prospect starts the demo via Telegram. The Telegram bot (created with BotFather) connects directly to your n8n instance. Demo Guidance The RAG agent and instructor guide the user step-by-step through the demo. Instructions and responses are dynamically generated based on user input. Workflow Execution When the user triggers an action (e.g., testing the email demo), n8n runs the workflow. The workflow collects website data using Crawl4AI or standard HTTP requests. Email Demo The system personalizes and sends a demo email through SparkPost, showing the automation‚Äôs capability. Logging and Control Each user interaction is logged in your database using their name and id. The workflow checks limits to prevent misuse or spam. Error Handling If a low-CPU scraping method fails, the workflow automatically escalates to a higher-CPU method. ‚öôÔ∏è Requirements Before setting up, make sure you have the following: n8n ‚Äî Automation platform to run the workflow Docker ‚Äî Required to run Crawl4AI Crawl4AI ‚Äî For intelligent website crawling Telegram Account ‚Äî To create your Telegram bot via BotFather SparkPost Account ‚Äî To send personalized demo emails A database (e.g., PostgreSQL, MySQL, or SQLite) ‚Äî To store log data such as user name and ID üöÄ Features Telegram interface** using the BotFather API Instructor and RAG agent** to guide prospects through the demo Flow generation limits per user ID** to prevent abuse Low-cost yet powerful web scraping**, escalating from low- to high-CPU flows if earlier ones fail üí° Development Ideas Replace the RAG logic with your own query-answering and guidance method Remove the flow limit if you‚Äôre confident the demo can‚Äôt be misused Swap the personalized email demo with any other workflow you want to showcase üß† Technical Notes Telegram bot** created with BotFather Website crawl process:** Extract sub-links via /sitemap.xml, sitemap_index.xml, or standard HTTP requests Fall back to Crawl4AI if normal requests fail Fetch sub-link content via HTTPS or Crawl4AI as backup SparkPost** used for sending demo emails ‚öôÔ∏è Setup Instructions 1. Create a Telegram Bot Use BotFather on Telegram to create your bot and get the API token. This token will be used to connect your n8n workflow to Telegram. 2. Create a Log Data Table In your database, create a table to store user logs. The table must include at least the following columns: name ‚Äî to store the user‚Äôs name or Telegram username. id ‚Äî to store the user‚Äôs unique identifier. 3. Install Crawl4AI with Docker Follow the installation guide from the official repository: üëâ https://github.com/unclecode/crawl4ai Crawl4AI** will handle website crawling and content extraction in your workflow. üì¶ Notes This setup is optimized for low cost, easy scalability, and real-time interaction with prospects. You can customize each component ‚Äî Telegram bot behavior, RAG logic, scraping strategy, and email workflow ‚Äî to fit your agency‚Äôs demo needs. üëâ You can try the live demo here: @email_demo_bot",1,2025-10-25 06:41:25.093000+00:00,True,10
10416,"Collect & Analyze Reviews with Decodo Scraping, GPT-4 Sentiment Analysis & Reports","This workflow contains community nodes that are only compatible with the self-hosted version of n8n. Automated Review Intelligence System Transform customer feedback into actionable intelligence with this Automated Review Intelligence System! This workflow collects reviews from platforms like Trustpilot using advanced web scraping, analyzes sentiment and patterns with AI, and generates comprehensive business intelligence reports. Perfect for customer experience teams monitoring brand reputation and customer satisfaction across review platforms. What This Template Does Triggers manually to start review collection from specified sources. Validates URL format to ensure proper review source configuration. Uses AI agent with Decodo scraper to extract review data from platforms. Parses and structures review data (ratings, comments, dates, locations). Enriches review data with metadata and quality metrics. Stores all review data in Google Sheets for historical tracking. Reads aggregated reviews for comprehensive analysis. Generates AI-powered summaries and key insights from review patterns. Sends email reports with actionable business intelligence. Provides error alerts for processing issues and invalid URLs. Key Benefits Automated collection of customer reviews from multiple platforms AI-powered sentiment analysis and pattern recognition Historical tracking of review trends and customer satisfaction Actionable business intelligence from customer feedback Real-time alerting for review processing issues Centralized review database for team visibility Features Manual trigger for on-demand review intelligence URL validation and error handling AI-powered review collection and analysis Decodo web scraping for reliable data extraction Structured data parsing for consistent formatting Google Sheets integration for data centralization Automated summary generation with key insights Email reporting for stakeholder communication Multi-platform review source support Historical trend analysis capabilities Requirements Decodo API credentials for web scraping OpenAI API credentials for AI analysis Google Sheets OAuth2 credentials with edit access Gmail OAuth2 credentials for email reports Environment variables for configuration settings Review source URLs (Trustpilot, etc.) Target Audience Customer experience and success teams Product management and development teams Marketing and brand reputation managers Business intelligence and analytics teams Customer support operations teams E-commerce and retail businesses Step-by-Step Setup Instructions Connect Decodo API credentials for review scraping functionality Set up OpenAI credentials for AI analysis and summary generation Configure Google Sheets with required review data headers Add Gmail credentials for report delivery and error notifications Set your target review source URLs (Trustpilot, etc.) Test with sample review pages to verify data extraction Customize summary reports for your business intelligence needs Define alert recipients for error notifications and reports Run manually to generate your first review intelligence report Pro Tip: Use coupon code ""YARON"" for free Decodo credits to enhance your review intelligence capabilities! This workflow ensures you stay informed about customer sentiment with automated review collection, intelligent analysis, and actionable business insights!",1,2025-11-01 17:27:57.306000+00:00,True,5
10491,Automate Pharmaceutical COA Verification & Vendor Scoring with AI Document Analysis,"This enterprise-grade n8n workflow automates the Pharmaceutical Raw Material COA Verification & Vendor Quality Scoring System ‚Äî from upload to final reporting ‚Äî using AI-powered document extraction, specification matching, and dynamic vendor scoring. It processes Certificates of Analysis (COAs) to validate compliance, assign quality scores, generate approvals or CAPA requests, and notify stakeholders, ensuring regulatory adherence and vendor accountability with full audit trails and zero manual data entry. Key Features Webhook-triggered COA Upload** for seamless integration with file-sharing systems AI Document Extraction** to parse test results and data from uploaded COAs Automated Specification Analysis** matching against predefined quality standards Weighted Vendor Scoring** based on compliance metrics and historical performance Compliance Decision Engine** with approve/reject branching and CAPA flagging Dynamic Certificate Generation** for approved materials, including digital signatures Vendor Database Synchronization** to update scores and records in real-time Targeted Email Notifications** for QA, production, and executive teams Executive Reporting Dashboard** with summaries, scores, and verification logs Audit-Ready Logging** for all steps, deviations, and decisions Workflow Process | Step | Node | Description | | ---- | ----------------------------------- | -------------------------------------------------------- | | 1 | START: Upload COA | Webhook trigger receives uploaded COA file for verification process | | 2 | EXTRACT: Parse COA | Extracts test results and data from the COA document using AI parsing | | 3 | ANALYZE: Vendor Compliance | Compares extracted data against specifications and flags deviations | | 4 | SCORE: Vendor Quality Rating | Calculates weighted compliance score based on test results and history | | 5 | DECISION: Compliance Route | Evaluates score/status: Branches to approve (green) or reject (red) path | | 6 | APPROVED: Generate Approval Cert (Approved Path) | Creates digital approval certificate for compliant materials | | 7 | Update Vendor Database | Saves verification record, score, and status to vendor database | | 8 | NOTIFY: Email Alert | Sends detailed notifications to QA/production teams | | 9 | REPORT: Final Report | Generates executive summary with COA scores and verifications | | 10 | REJECT: Generate Rejection Report (Reject Path) | Produces rejection report with deviation details | | 11 | Request CAPA | Initiates Corrective and Preventive Action (CAPA) process | | 12 | PATH REJECTED | Terminates rejected branch with audit log entry | Setup Instructions 1. Import Workflow Open n8n ‚Üí Workflows ‚Üí Import from Clipboard Paste the JSON workflow 2. Configure Credentials | Integration | Details | | ----------------- | -------------------------------------------------- | | File Storage (e.g., Google Drive/AWS S3) | API key or OAuth for COA upload handling | | AI Extraction (e.g., Claude or OCR Tool) | API key for document parsing (e.g., claude-3-5-sonnet-20241022) | | Database (e.g., PostgreSQL/Airtable) | Connection string for vendor records and specs | | Email (SMTP/Gmail) | SMTP credentials or OAuth for notifications | 3. Update Database/Sheet IDs Ensure your database or Google Sheets include: VendorDatabase for scores and history Specifications for quality standards 4. Set Triggers Webhook:** /coa-verification (for real-time file uploads) Manual/Scheduled:** For batch processing if needed 5. Run a Test Use manual execution to confirm: COA extraction and analysis Score calculation and branching Email notifications and report generation (use sample COA file) Database/Sheets Structure VendorDatabase | vendorId | coaId | score | complianceStatus | lastVerified | deviations | capaRequested | |--------------|-------------|----------|--------------------|--------------------|--------|--------------------|---------------| | VEND-123456 | COA-789012 | 92.5 | Approved | 2025-11-04T14:30:00Z | None | No | Specifications | materialType | testParam | specMin | specMax | weight | |--------------|-------------|----------|--------------------|--------------------|--------|--------------------|---------------|----------| | API Excipient | Purity (%) | 98.0 | 102.0 | 0.4 | System Requirements | Requirement | Version/Access | | --------------------- | ---------------------------------------------- | | n8n | v1.50+ (AI and database integrations supported) | | AI Parsing API | claude-3-5-sonnet-20241022 or equivalent OCR | | Database API | SQL connection or Google Sheets API | | Email API | https://www.googleapis.com/auth/gmail or SMTP | | File Storage | AWS S3 or Google Drive API access | Optional Enhancements Integrate ERP Systems (e.g., SAP) for direct material release Add Regulatory Export to PDF/CSV for FDA audits Implement Historical Trend Analysis for vendor performance dashboards Use Multi-Language Support for global COA extraction Connect Slack/Teams for real-time alerts beyond email Enable Batch Processing for high-volume uploads Add AI Anomaly Detection for predictive non-compliance flagging Build Custom Scoring Models via integrated ML tools Result: A fully automated quality assurance pipeline that verifies COAs, scores vendors, and drives compliance decisions ‚Äî ensuring pharmaceutical safety and efficiency with AI precision and complete traceability. Explore More AI Workflows: Get in touch with us for custom n8n automation!",1,2025-11-04 10:05:26.819000+00:00,True,3
10672,Generate Twitter Content in Personal Style with OpenAI & Supabase RAG,"üéØ Self-Learning X Content Engine (Creator RAG Booster) Learn your voice. Generate posts that sound like you ‚Äî not AI. üß© Overview This n8n workflow builds a personal RAG (Retrieval-Augmented Generation) system for creators. It learns from your own past posts and generates new tweets, replies, and image prompts in your tone. ‚öôÔ∏è How it works Step 1 ‚Äî Ingest Use the ‚ÄúAdd to KB‚Äù Form to upload your past posts or notes. Text + metadata (topic, style) are stored in Supabase as vectors. Step 2 ‚Äî Generate Use the ‚ÄúGenerate Posts‚Äù Form to create new post ideas. The Agent fetches the most relevant style snippets (via Supabase VectorStore) Output includes: üìù post üí¨ quote üí≠ reply üé® image_prompt üîß Setup (3‚Äì5 min) Connect Supabase (URL + Key) Make sure the table name is documents Enable vector extension (pgvector) Connect OpenAI API Key Activate both Forms and open the URLs to test. Optionally replace Forms with Webhooks. üí° Tip: RLS enabled? Ensure your API key allows insert/select for documents. üß† Tech Stack n8n (self-hosted) Supabase (Vector Store) OpenAI (gpt-4.1-mini) HTML-based completion form ü™Ñ Credits Built by Yusuke | @yskautomation License: MITView on GitHub",1,2025-11-10 07:15:58.898000+00:00,True,6
10813,Create SEO-Optimized Blog Articles with GPT-4o Mini's Multi-Stage Generation,"Overview This workflow converts a single topic into a full blog article through a structured multi-step process. Instead of generating everything in one pass, it breaks the task into clear stages to produce cleaner structure, better SEO consistency, and more predictable output quality. How this workflow differs from asking ChatGPT directly It does not produce an article in one step. It separates the process into two focused stages: outline generation and paragraph expansion. This approach gives you more control over tone, SEO, structure, and keyword placement. How it works 1. Generate outline The workflow sends your topic to an AI Agent. It returns a structured outline based on the topic, desired depth, language, and keyword focus. 2. Expand each subtopic The workflow loops through each outline item. Every subtopic is expanded into a detailed, SEO-friendly paragraph. Output is consistent and optimized for readability. 3. Produce final outputs Combines all expanded sections into: A clean JSON object A Markdown version ready for blogs or CMS The JSON includes: Title HTML content Markdown content You can send this directly to REST APIs such as WordPress, Notion, or documentation platforms. Content is validated for readability and typically scores well in tools like Yoast SEO. Uses GPT-4o Mini by default, with average token usage between 2000 and 3000 depending on outline size. Use cases Auto-generate long-form articles for blogs or content marketing. Turn Instagram or short-form scripts into complete SEO articles. Create documentation or educational content using consistent templates. Setup steps 1. Prepare credentials Add your OpenAI API Key inside n8n‚Äôs credential manager. 2. Adjust input parameters Topic or main idea Number of outline items Language Primary keyword Tone or writing style (optional) 3. Customize the workflow Switch the model if you want higher quality or lower token usage. Modify the prompt for the outline or paragraph generator to match your writing style. Add additional nodes if you want to auto-upload the final article to WordPress, Notion, or any API. 4. Run the workflow Enter your topic Execute the workflow Retrieve both JSON and Markdown outputs for immediate publishing If you need help expanding this into a full content pipeline or want to integrate it with other automation systems, feel free to customize further.",1,2025-11-14 07:30:24.528000+00:00,True,5
10849,"Extract & Process Q&A from URLs with Airtop, OpenRouter AI & Safety Guardrails","Transform your Telegram bot into a secure content analyzer: send any URL, and get safe, structured Q&A extractions with AI-powered safety checks and web search capabilities. üìã What This Template Does This workflow activates when a user sends a valid URL to your Telegram bot. It extracts questions and answers from the webpage using Airtop, applies NSFW and PII guardrails to ensure safe content, then uses an OpenRouter AI agent (with optional Tavily search) to generate and send a concise response. If guardrails fail, it alerts the user instead. Filters for valid URLs only to prevent unnecessary processing Extracts structured Q&A from documents or forms Enforces safety checks for harmful or private content Supports web searches for enhanced responses when needed üîß Prerequisites A Telegram bot created via @BotFather Accounts with Airtop, OpenRouter, and Tavily üîë Required Credentials Telegram API Setup Open Telegram ‚Üí Search @BotFather ‚Üí Use /newbot command Follow prompts to create bot and obtain API token Add to n8n as Telegram API credential type Airtop API Setup Visit https://airtop.ai ‚Üí Sign up or log in ‚Üí Navigate to Dashboard ‚Üí API Keys Generate a new API key with extraction permissions Add to n8n as Airtop API credential type OpenRouter API Setup Go to https://openrouter.ai ‚Üí Sign up or log in ‚Üí Navigate to API Keys section Generate and copy your API key (free tier sufficient for basic use) Add to n8n as OpenRouter API credential type Tavily API Setup Visit https://app.tavily.com ‚Üí Sign up or log in ‚Üí Go to API Keys Generate and copy your API key Add to n8n as Tavily API credential type ‚öôÔ∏è Configuration Steps Import the workflow JSON into n8n Assign your Telegram, Airtop, OpenRouter, and Tavily credentials to the respective nodes Activate the workflow to register the Telegram trigger Test by sending a plain URL (no extra text) to your bot in Telegram Monitor the first execution and adjust guardrail thresholds if needed üéØ Use Cases Researchers summarizing academic papers or reports while ensuring no sensitive data leaks Support teams extracting info from customer-submitted docs/forms with automatic safety filtering Content creators pulling Q&A from articles for bots, blocking inappropriate responses Educators analyzing educational resources safely for student-facing chat tools ‚ö†Ô∏è Troubleshooting No response from bot: Verify the message contains only a valid URL; adjust regex in Filter Only URLs node if needed Guardrail failures: Lower NSFW threshold (e.g., from 0.7 to 0.5) or disable PII checks in Apply Safety Guardrails node Extraction errors: Test with public, text-heavy URLs; some JS-heavy sites may require alternative extractors Rate limits hit: Check OpenRouter/Tavily dashboards for usage; upgrade to paid tiers for heavy traffic",1,2025-11-14 18:29:25.162000+00:00,True,5
10877,"Automate Reddit Monitoring with GPT-5-Mini, Notion, and Gmail","Instantly Track Reddit Discussions and Generate Insights Manually monitoring Reddit for relevant discussions can be overwhelming. This automation does all the heavy lifting by automatically searching for keywords across selected subreddits or the entire Reddit, analyzing each post with GPT-5-Mini, and saving structured insights in your Notion database. Optionally, receive daily email notifications summarizing new discussions without lifting a finger. Key Features Daily Automated Reddit Monitoring** Checks Reddit once a day for new posts matching your keywords in selected subreddits or across all of Reddit. AI-Powered Analysis with Custom Prompts** Uses GPT-5-Mini to: Assign a relevance score to each post based on your defined priorities. Generate a suggested comment tailored to your audience or engagement strategy. Allow custom prompts so you can tweak AI behavior for lead generation, competitor monitoring, or brand mentions. Notion Integration** Saves all posts, relevance scores, and AI-generated comments directly into your Notion database for easy tracking and collaboration. Optional Email Alerts** Sends a clean, HTML-formatted email summarizing new discussions and insights. No API Keys Required for Reddit** Works with the official Reddit API without requiring users to create credentials. Customizable Inputs** Users define keywords, subreddits, relevance criteria, custom prompts, and notification preferences with minimal setup. Setup Instructions 1. Configure Keywords and Subreddits Open the Define Keywords and Subreddits node. Enter your target keywords and choose specific subreddits, or enable search across all Reddit. Map the inputs to the workflow‚Äôs search nodes. 2. Connect OpenAI (GPT-5-Mini) Create an OpenAI account and obtain your API key. Open the AI Analysis Node in the workflow. Paste your OpenAI API key into the credentials field. Customize your prompts to define how GPT-5-Mini: Scores relevance based on your specific criteria. Generates suggested comments tailored to your objectives (e.g., lead generation, brand engagement, competitor analysis). Test the node to ensure relevance scoring and suggested comments are generated correctly. 3. Connect Notion Duplicate the provided Notion template into your workspace. Obtain your Notion integration secret from Notion settings. Paste the secret into the Add Each Post to Notion Database node. Test the connection to ensure posts are saved correctly. 4. Optional Email Notifications Setup Google API credentials in the Google Cloud Console. Authenticate Gmail in the workflow. Map the recipient email and customize the notification format. Test sending an email to confirm delivery. 5. Configure Workflow Trigger Import the workflow into n8n. Ensure the daily trigger is enabled. Optionally adjust schedule or notification settings. How It Works Daily Trigger: Starts the workflow once per day. Reddit Search: Queries new posts based on user-defined keywords. AI Analysis: GPT-5-Mini evaluates relevance score and generates suggested comments based on your custom prompts. Save & Notify: Posts are saved to Notion and optionally summarized via email. Example Use Cases Generate leads by identifying potential prospects discussing your industry or product. Monitor brand mentions to stay on top of conversations around your company. Track competitor activity across relevant subreddits. Keep a daily digest of trending posts for market research or content ideas. Automate engagement by posting AI-generated suggested comments. Requirements | Tool | Purpose | | -------------- | ------------------------------------------------- | | Notion Account | Store posts, relevance scores, and comments | | Gmail Account | Optional daily email notifications | | n8n | Run, schedule, and manage the workflow | | OpenAI API Key | Access GPT-5-Mini for relevance scoring and AI-generated comments |",1,2025-11-16 09:17:07.416000+00:00,True,5
10899,Order Processing with Google Sheets and Slack: Inventory Checks and Alerts,"Overview A cornerstone of your Order Management System, this workflow ensures seamless inventory control through fully automated stock checks, leading to a direct reduction in operational costs. It provides real-time alerts to the responsible personnel, enabling proactive issue detection and resolution to eliminate the financial damages associated with unexpected stock-outs. How it works Order Webhook Receives orders from external sources (e.g., website, form, or app) via API. Check Order Request Checks the validity of the order (e.g., complete product, valid customer details) Check Inventory Retrieve inventory information and compare it with the order request. Notifications Generate a notification to Slack for the manager indicating a successful order or an out-of-stock situation. Logging Log the process details to a Google Sheet for tracking. Set up steps Webhook Create a JSON request with the following format to call the Webhook Url { ""id"": ""ORDER1001"", ""customer"": { ""name"": ""Customer"", ""email"": ""customer@example.com"" }, ""items"": [ { ""sku"": ""SKU001"", ""quantity"": 2, ""name"": ""Product A"", ""price"": 5000 }, { ""sku"": ""SKU002"", ""quantity"": 2, ""name"": ""Product C"", ""price"": 10000 } ], ""total"": 30000 } Define the greater than or less than conditions on the inventory level to enter the corresponding branches. Google Sheet Clone the file to your Google Drive. (WMS Data Demo) Replace your credentials and connect. Access permission must be granted to n8n. Slack Replace your credentials and connect. A channel named ""warehouse"" needs to be prepared to receive notifications (if using a different name, you must update the Slack node).",1,2025-11-17 03:34:41.960000+00:00,False,2
10942,"Automate Security Vulnerability Remediation with Port, OpenAI and Slack","Remediate security vulnerabilities with Port This workflow automatically enriches security vulnerability alerts with organizational context from Port and routes them to the appropriate teams or AI agents for remediation. How it works n8n receives a new vulnerability from your security scanner (e.g., Snyk) via webhook The workflow calls Port's context lake to enrich the alert with service, ownership, environment, and dependency data. With this context, the workflow either sends targeted Slack notifications to the owning team or triggers an AI remediation flow to propose and apply a fix. Setup steps [ ] Sign up with a free account on port.io. [ ] Create the required AI agent in Port. [ ] Configure credentials in n8n for Port, Slack, and OpenAI/Claude. [ ] Trigger the webhook with a sample vulnerability to verify context retrieval, Slack notifications, and AI remediation."",",1,2025-11-17 19:07:13.802000+00:00,True,3
10949,Compare Hotel Prices Across Booking Platforms with Scrape.do and Google Sheets,"üè® Hotel Price Comparison Workflow with Scrape.do This template requires a self-hosted n8n instance to run. A complete n8n automation that extracts hotel prices from multiple booking platforms (Booking.com, Hotels.com, Expedia, etc.) using Scrape.do API, compares prices across platforms, and saves structured results into Google Sheets for price monitoring and decision-making. üìã Overview This workflow provides an automated hotel price comparison solution that monitors hotel rates across different booking platforms for specific destinations and dates. Ideal for travel agencies, price comparison websites, travelers, and hospitality analysts who need real-time pricing insights without manual searching. Who is this for? Travel agencies automating price comparisons Price comparison website operators Budget-conscious travelers tracking best deals Hospitality analysts monitoring market pricing Hotel managers tracking competitor rates Travel bloggers researching accommodation options What problem does this workflow solve? Eliminates manual price checking across multiple sites Processes multiple hotels and date ranges automatically Extracts structured pricing data (price, rating, amenities) Identifies the cheapest option across platforms Automates saving results into Google Sheets Ensures consistent and repeatable price monitoring ‚öôÔ∏è What this workflow does Manual Trigger ‚Üí Starts the workflow manually or on schedule Get Search Parameters from Sheet ‚Üí Reads hotel names, destinations, check-in/check-out dates from Google Sheet URL Encode Parameters ‚Üí Converts search parameters into URL-safe format Process Hotels in Batches ‚Üí Handles multiple searches sequentially to avoid rate limits Fetch Hotel Data from Multiple Platforms ‚Üí Calls Scrape.do API to retrieve pricing from Booking.com, Hotels.com, and Expedia Extract and Structure Price Data ‚Üí Parses HTML into structured hotel data (name, price, rating, amenities) Compare Prices Across Platforms ‚Üí Identifies best price and calculates savings Append Results to Sheet ‚Üí Writes comparison results into Google Sheet üìä Output Data Points | Field | Description | Example | |-------|-------------|---------| | Hotel Name | Name of the hotel | Hilton Garden Inn Downtown | | Destination | City or location | New York, NY | | Check-in Date | Arrival date | 2025-12-15 | | Check-out Date | Departure date | 2025-12-18 | | Nights | Number of nights | 3 | | Booking.com Price | Price from Booking.com | $450 | | Hotels.com Price | Price from Hotels.com | $425 | | Expedia Price | Price from Expedia | $440 | | Best Price | Lowest price found | $425 | | Best Platform | Platform with lowest price | Hotels.com | | Savings | Difference from highest price | $25 | | Average Rating | Average customer rating | 8.5/10 | | Total Reviews | Number of reviews | 1,247 | | Free Cancellation | Cancellation policy | Yes | | Breakfast Included | Breakfast availability | No | ‚öôÔ∏è Setup Prerequisites n8n instance (self-hosted) Google account with Sheets access Scrape.do account with API token Google Sheet Structure This workflow uses one Google Sheet with two tabs: Input Tab: ""Search Parameters"" | Column | Type | Description | Example | |--------|------|-------------|---------| | Hotel Name | Text | Name of hotel (optional) | Hilton Garden Inn | | Destination | Text | City or location | New York, NY | | Check-in Date | Date | Arrival date (YYYY-MM-DD) | 2025-12-15 | | Check-out Date | Date | Departure date (YYYY-MM-DD) | 2025-12-18 | | Guests | Number | Number of guests | 2 | | Rooms | Number | Number of rooms | 1 | Output Tab: ""Price Comparison"" | Column | Type | Description | Example | |--------|------|-------------|---------| | Search Date | Timestamp | When search was performed | 2025-11-17 10:30:00 | | Hotel Name | Text | Name of the hotel | Hilton Garden Inn Downtown | | Destination | Text | City/location | New York, NY | | Check-in | Date | Arrival date | 2025-12-15 | | Check-out | Date | Departure date | 2025-12-18 | | Nights | Number | Number of nights | 3 | | Booking.com Price | Currency | Price from Booking.com | $450 | | Hotels.com Price | Currency | Price from Hotels.com | $425 | | Expedia Price | Currency | Price from Expedia | $440 | | Best Price | Currency | Lowest price | $425 | | Best Platform | Text | Cheapest platform | Hotels.com | | Savings | Currency | Potential savings | $25 | | Rating | Number | Average rating | 8.5 | | Reviews | Number | Total reviews | 1,247 | üõ† Step-by-Step Setup Import Workflow: Copy the JSON ‚Üí n8n ‚Üí Workflows ‚Üí + Add ‚Üí Import from JSON Configure Scrape.do API: Endpoint: https://api.scrape.do/ Parameter: token=YOUR_SCRAPEDO_TOKEN Add render=true for JavaScript-heavy booking sites Add country=US (or target country) for localized results Configure Google Sheets: Create a sheet with two tabs: Search Parameters (input), Price Comparison (output) Set up Google Sheets OAuth2 credentials in n8n Replace placeholders: YOUR_GOOGLE_SHEET_ID and YOUR_GOOGLE_SHEETS_CREDENTIAL_ID Configure Platform URLs: Update base URLs for Booking.com, Hotels.com, Expedia in HTTP Request nodes Customize search parameters based on platform URL structure Run & Test: Add test data in Search Parameters tab Execute workflow ‚Üí Check results in Price Comparison tab üß∞ How to Customize Add more platforms**: Include Agoda, Trivago, or direct hotel websites by adding new HTTP Request nodes Price alerts**: Add conditional logic to send email/Slack notification when price drops below threshold Historical tracking**: Store daily snapshots to track price trends over time Filtering**: Add filters for amenities (pool, gym, parking) or star ratings Batch Size**: Adjust ""Process Hotels in Batches"" based on API rate limits Rate Limiting**: Insert Wait nodes (20‚Äì30 seconds) between platform requests Currency conversion**: Add currency API integration for multi-currency comparison Scheduling**: Add Schedule Trigger to run automatically (daily/weekly) üìä Use Cases Travel Planning**: Find the best hotel deals for upcoming trips Price Monitoring**: Track price changes for specific hotels over time Agency Operations**: Automate price research for client bookings Market Analysis**: Monitor competitor pricing in hospitality market Deal Alerts**: Get notified when prices drop below target threshold Budget Planning**: Compare costs across multiple destinations üìà Performance & Limits Single Hotel (3 platforms)**: ~30‚Äì45 seconds (depends on Scrape.do response) Batch of 10 hotels**: 8‚Äì12 minutes typical Large Sets (50+ hotels)**: 45‚Äì90 minutes depending on API credits & batching API Calls**: 3 Scrape.do requests per hotel (one per platform) Reliability**: 90%+ extraction success, 95%+ price accuracy üß© Troubleshooting API error** ‚Üí Check YOUR_SCRAPEDO_TOKEN and API credits on Scrape.do dashboard No hotels loaded* ‚Üí Verify Google Sheet ID & tab name = *Search Parameters** Permission denied** ‚Üí Re-authenticate Google Sheets OAuth2 in n8n Empty prices** ‚Üí Check if Scrape.do rendered JavaScript (render=true) Parsing errors** ‚Üí Booking sites change HTML structure; update CSS selectors in Extract nodes Workflow timeout** ‚Üí Reduce batch size or add more Wait nodes between requests Wrong currency** ‚Üí Add country parameter to Scrape.do request for localized pricing ü§ù Support & Community n8n Forum**: https://community.n8n.io n8n Docs**: https://docs.n8n.io Scrape.do Dashboard**: https://dashboard.scrape.do Scrape.do Documentation**: https://scrape.do/docs üéØ Final Notes This workflow provides a powerful foundation for automated hotel price comparison across multiple booking platforms using Scrape.do and Google Sheets. You can extend it with: Real-time price alerts via email/Slack Historical price tracking and trend analysis Integration with travel planning dashboards Automated booking when price threshold is met Multi-destination comparison for trip planning Pro Tip: Schedule this workflow to run daily to catch early-bird discounts and flash sales automatically!",1,2025-11-18 07:09:43.848000+00:00,False,3
11010,Automated GA4 Analytics Data Backfill to BigQuery with Telegram Alerts,"This workflow automates the daily backfill of Google Analytics 4 (GA4) data into BigQuery. It fetches 13 essential pre-processed reports (including User Acquisition, Traffic, and E-commerce) and uploads them to automatically created tables in BigQuery, and then send an alert in telegram. How it works Configuration:** You define your Project ID, Dataset, and Date Range in a central ""Config"" node. Parallel Fetching:** The workflow runs 13 parallel API calls to GA4 to retrieve key reports (e.g., ga4_traffic_sources, ga4_ecommerce_items). Dynamic Tables:** It automatically checks if the target BigQuery table exists and creates it with the correct schema if it's missing. Telegram Alerts:** After execution, it sends a summary message to Telegram indicating success or failure for the day's run. Set up steps Google Credentials (OAuth): This workflow uses n8n's built-in ""Google OAuth2 API"" credential. You do not need a Service Account key. Connect your Google account and ensure you grant scopes for Google Analytics API and BigQuery API. Config Node: Open the ""Backfill Config"" node and fill in: GA4 Property ID Google Cloud Project ID BigQuery Dataset ID Telegram Setup (Optional): If you want alerts, configure the Telegram node with your Bot Token and Chat ID. If not, you can disable/remove this node. Schedule: By default, this is set to run daily. It is recommended to use a date expression (e.g., Today - 2 Days) to allow GA4 time to process data fully before fetching.",1,2025-11-19 11:49:52.598000+00:00,False,4
11027,"Extract & Analyze Amazon Reviews with Apify, Gemini AI & Save to Google Sheets","Template Description üìù Template Title Analyze Amazon product reviews with Gemini and save to Google Sheets üìÑ Description This workflow automates the process of analyzing customer feedback on Amazon products. Instead of manually reading through hundreds of reviews, this template scrapes reviews (specifically targeting negative feedback), uses Google Gemini (AI) to analyze the root causes of dissatisfaction, and generates specific improvement suggestions. The results are automatically logged into a Google Sheet for easy tracking, and a Slack notification is sent to keep your team updated. This tool is essential for understanding ""Voice of Customer"" data efficiently without manual data entry. üßç Who is this for Product Managers** looking for product improvement ideas. E-commerce Sellers (Amazon FBA, D2C)** monitoring brand reputation. Market Researchers** analyzing competitor weaknesses. Customer Support Teams** identifying recurring issues. ‚öôÔ∏è How it works Data Collection: The workflow triggers the Apify actor (junglee/amazon-reviews-scraper) to fetch reviews from a specified Amazon product URL. It is currently configured to filter for 1 and 2-star reviews to focus on complaints. AI Analysis: It loops through each review and sends the content to Google Gemini. The AI determines a sentiment score (1-5), categorizes the issue (Quality, Design, Shipping, etc.), summarizes the complaint, and proposes a concrete improvement plan. Formatting: A Code node parses the AI's response to ensure it is in a clean JSON format. Storage: The structured data is appended as a new row in a Google Sheet. Notification: A Slack message is sent to your specified channel to confirm the batch analysis is complete. üõ†Ô∏è Requirements n8n** (Self-hosted or Cloud) Apify Account:** You need to rent the junglee/amazon-reviews-scraper actor. Google Cloud Account:** For accessing the Gemini (PaLM) API and Google Sheets API. Slack Account:** For receiving notifications. üöÄ How to set up Apify Config: Enter your Apify API token in the credentials. In the ""Run an Actor"" node, update the startUrls to the Amazon product page you want to analyze. Google Sheets: Create a new Google Sheet with the following header columns: sentiment_score, category, summary, improvement. Copy the Spreadsheet ID into the Google Sheets node. AI Prompt: The ""Message a model"" node contains the prompt. It is currently set to output results in Japanese. If you need English output, simply translate the prompt text inside this node. Slack: Select the channel where you want to receive notifications in the Slack node.",1,2025-11-20 01:52:24.801000+00:00,True,4
11062,Track Amazon Prices & Monitor Competitors with Apify and Google Sheets,"Amazon Price Tracker & Competitor Monitoring Workflow (Apify + Google Sheets) This n8n workflow automates Amazon price tracking and competitor monitoring by scraping product pricing via Apify and updating your Google Sheet every day. It removes manual price checks, keeps your pricing data always fresh, and helps Amazon sellers stay ahead in competitive pricing, Buy Box preparation, and daily audits. üí° Use Cases Automatically track prices of your Amazon products Monitor competitor seller prices across multiple URLs Maintain a daily pricing database for reporting and insights Catch sudden competitor undercutting or pricing changes Support Buy Box analysis by comparing seller prices Scale from 10 to 1000+ product URLs without manual effort üß† How It Works Scheduled Trigger** runs the workflow every morning Google Sheets node** loads all product rows with seller URLs Loop node** processes each item one-by-one Apify Actor node** triggers the Amazon scraper HTTP Request node** fetches the scraped result from Apify JavaScript node** extracts, cleans, and formats price data Update Sheet node** writes the fresh prices back to the right row Supports additional price columns for more sellers or metrics ‚ûï Adding New Competitor Columns (Step-by-Step) 1. Add a new column in Google Sheets Add two new columns: competitor_url_3 price_comp_3 2. Update the Apify Actor (inside n8n) In the Apify Actor node, pass the new competitor URL: ""competitor_url_3"": {{$json.competitor_url_3}} This ensures Apify scrapes the additional competitor product page. 3. Update your Code (JavaScript) node Inside the Code node, extract the new competitor‚Äôs price from the Apify JSON and attach it to the output: const price_comp_3 = item?.offers?.[2]?.price || null; item.price_comp_3 = price_comp_3; return item; (Adjust the index [2] based on the Apify output structure.) Update the Google Sheet ‚ÄúUpdate Row‚Äù node To save the new values into your Sheet: Open your Google Sheets Update Row node Scroll to Field Mapping Map Columns with New Data Hit the ""Save & Execute"" Button.üöÄ ‚ö° Requirements Apify account (free tier is enough) Apify ""Amazon Product Scraper"" API (Costs $40/month - 14-day free trial) Google Sheet containing product URLs Basic credentials setup inside n8n üôå Want me to set it up for you? I‚Äôll configure the full automation ‚Äî Apify scraper, n8n workflow, Sheets mapping, and error handling. Email me at: imarunavadas@gmail.com Automate the boring work and focus on smarter selling. üöÄ",1,2025-11-20 21:28:33.119000+00:00,False,3
11063,AI-Powered Price Watchdog: Competitor Monitoring & Alerting (Decodo & Gemini),"Never miss a competitor price change again. This advanced workflow automates the most difficult aspect of market monitoring: intelligently extracting structured pricing data from complex, dynamic competitor websites and comparing it against your historical baseline. It sends instant, conditional alerts only when a significant price shift is detected. The workflow uses Decodo for dynamic scraping, Gemini for reliable data parsing, and Google Sheets for robust historical state management. ‚ú® Key Features AI-Powered Extraction:* Uses *Gemini 2.5 Flash** to analyze raw, noisy website HTML and output a clean JSON array of plan names, prices, and features, bypassing brittle CSS selectors. Historical Comparison:** Automatically retrieves the price from the previous workflow run and calculates the percentage difference (diff) for every single plan item. Edge Case Handling:* Includes specific code logic to prevent errors and flag crucial events like a *""Free-to-Paid""** plan transition (division by zero). Conditional Alerting:** Sends immediate Slack notifications only when the price change exceeds your predefined percentage threshold. State Management:* Uses Google Sheets to automatically *shift data** (Current Price $\rightarrow$ Old Price) to maintain the historical baseline for the next scheduled execution. ‚öôÔ∏è How it Works (The Monitoring Loop) Setup & Sourcing: The workflow is executed on a schedule, defining the global alert threshold and retrieving the list of target URLs from a Google Sheet. Scraping (Dynamic): Decodo runs with JavaScript rendering ON to fetch the complete, dynamic HTML of the pricing page. AI Structuring: Gemini receives the raw HTML and uses a strict System Prompt to extract a clean JSON array of all pricing plans. Comparison & Calculation: A Code Node parses the current plan list and the list from the previous run (stored in Sheets). It calculates the percentage change for every matching plan. Alert Decision: An If Node checks the calculated change against the threshold. If the condition is met, the filtered alert proceeds to Slack. Data Shift & Log: The final Sheets Update node shifts the current plan data to the ""Last Plans"" column and moves the previous ""Last Plans"" to the ""Old Plans"" column, setting the new baseline for the next scheduled check. üì• Decodo Node Installation The Decodo node is used three times in this workflow for precision scraping and searching. Find the Node: Click the + button in your n8n canvas. Search: Search for the Decodo node and select it. Credentials: When configuring the first Decodo node, use your API key (obtained with the 80% discount coupon). üéÅ Exclusive Deal for n8n Users To run this workflow, you require a robust scraping provider. We have secured a massive discount for Decodo users: Get 80% OFF the 23k Advanced Scraping API plan. Coupon Code: ATTAN8N Sign Up Here: Claim 80% Discount on Decodo üõ†Ô∏è Setup Instructions Credentials: Obtain API keys for Decodo (using the coupon below), Google Sheets, and Slack. Google Sheets Setup: Create a sheet with the following required columns for tracking (one row per URL): Name URL Old Plans (JSON String) Last Plans (JSON String) Updated At (Date) Global Configuration: Open the Config: Alert Parameters node to set your alert_threshold (e.g., 10). I understand. To complete the final template description for your Competitor Price Monitoring workflow, here is the dedicated How to Adapt the Template section, focusing on functional changes a user can make for advanced monitoring. ‚ûï How to Adapt the Template The workflow is currently configured for maximum efficiency and stability. To expand its functionality or change its dependencies, you can implement the following adaptations: Change Database for Storage:* Replace the *Google Sheets* nodes with *Airtable* or *Notion** nodes for historical storage. Since your comparison logic relies on the JSON string being saved and retrieved, you will only need to change the read/write operations (the Code logic remains the same). Change Alert Channel:* Easily swap the *Slack* node with a *Gmail, **Discord, or Pushover node to deliver critical price alerts to a different team or application. Dynamic Thresholds:* Modify the *Config: Alert Parameters* to include separate fields for price *increases (e.g., alert_increase_threshold) and price decreases (e.g., alert_decrease_threshold), allowing you to track competitor sales differently than price hikes. Advanced Price Filtering:* Adjust the code logic in *Code: Isolate Pricing Section** to target specific currency symbols (e.g., ‚Ç¨, ¬£) or to filter out prices that appear to be marked as promotional (e.g., text containing ""SALE"" or ""Discount""). Add Advanced Alert Reporting:* Instead of sending a simple Slack message, use the full list of price_diffs (which contains all plans) to generate a consolidated daily *CSV report* or a professional *HTML email** summarizing all movements, even those below the alert threshold.",1,2025-11-20 22:32:05.069000+00:00,True,6
11080,Track Daily Brand Mentions from Hacker News to Slack with GPT-4o-mini Sentiment Analysis,"üìä Description Monitor daily brand visibility and reputation with an automated AI-powered mention tracker. üîçü§ñ This workflow checks Hacker News every morning for new stories matching your brand keyword, classifies each mention‚Äôs sentiment and urgency using GPT-4o-mini, and delivers a clean daily summary to Slack. If no mentions are found, the workflow sends a simple ‚Äúno mentions today‚Äù update instead‚Äîensuring your team is always informed without manual monitoring. Perfect for reputation tracking, competitive intelligence, and early warning alerts. üìàüí¨ üîÅ What This Template Does 1Ô∏è‚É£ Triggers every morning at 09:00 to begin the analysis. ‚è∞ 2Ô∏è‚É£ Loads brand name + keyword filters from configuration. üè∑Ô∏è 3Ô∏è‚É£ Fetches relevant mentions from Hacker News using the Algolia API. üåê 4Ô∏è‚É£ Normalizes raw API data into clean fields (title, URL, snippet, author, points). üìÑ 5Ô∏è‚É£ Classifies each mention‚Äôs sentiment, stance, topic, and urgency using GPT-4o-mini. ü§ñ 6Ô∏è‚É£ Builds a ranked daily summary including top 10 mentions and sentiment totals. üìä 7Ô∏è‚É£ Sends the report to Slack, formatted cleanly and ready for team consumption. üí¨ 8Ô∏è‚É£ If no mentions exist, sends a simple ‚Äúno new mentions today‚Äù message. üö´ 9Ô∏è‚É£ Includes an error handler that notifies Slack of any workflow failures. ‚ö†Ô∏è ‚≠ê Key Benefits ‚úÖ Automatically tracks brand presence without manual searching ‚úÖ AI-powered sentiment & urgency analysis for deeper insights ‚úÖ Clean Slack summaries keep teams aligned and aware ‚úÖ Early detection of negative or high-urgency mentions ‚úÖ Zero manual monitoring ‚Äî runs fully on schedule ‚úÖ Suitable for brand monitoring, PR, marketing, and leadership teams üß© Features Daily schedule trigger Hacker News API (Algolia) integration Structured data normalization GPT-4o-mini classification (sentiment, stance, topic, urgency) Slack notifications (detailed report or no-mention message) Error-handling pipeline with Slack alerts Fully configurable brand keywords üîê Requirements Slack API credentials OpenAI API key (GPT-4o-mini) No authentication required for Hacker News API n8n with LangChain nodes enabled üéØ Target Audience Brand monitoring & PR teams AI companies tracking public sentiment Founders monitoring mentions of their product Marketing teams watching trends & community feedback",1,2025-11-21 06:38:23.420000+00:00,True,7
11086,Scrape Business Leads from Google Maps and Extract Decision-Maker Info with Olostep,"Olostep Google Maps Lead Generation Automation This n8n template automates lead generation by scraping Google Maps using the Olostep API. It extracts business names, locations, websites, phone numbers, and decision-maker names (CEO, Founder, etc.) directly from the business website ‚Äî and saves everything into a Google Sheet. Who‚Äôs it for Marketers and agencies doing local business outreach. SaaS founders looking for prospects. Freelancers and growth hackers scraping Google Maps leads. Anyone who wants automated business research without manual data entry. How it works / What it does Form Trigger: User submits a form with: City + Business Type (e.g., ""Dentist in Miami""). Google Maps Scraping: The workflow sends the query to the Olostep scraping API. Extracts: Business name Location Website Phone number Clean the Data: Parsed JSON is split into items. A Remove Duplicates node ensures only unique leads continue. Loop Through Each Business: For every business, the workflow triggers a second Olostep scrape ‚Äî this time on the business‚Äôs website. It extracts: First name of decision-maker Last name of decision-maker (Optional) general contact email found on the website Store the Lead: The final combined lead is appended to a Google Sheet with these fields: Business Name Location Website Phone Number Decision-Maker Name Contact Email (if found) Loop & Wait: A wait step ensures you stay within rate limits while scraping multiple websites. This produces a clean, enriched list of leads ready for outreach or CRM import. How to set up Import the template into your n8n workspace. Add your Olostep API key. Connect Google Sheets for output storage. Publish your form to collect search requests. Run the workflow ‚Äî leads will appear automatically in your sheet. Requirements Olostep API key. Google Sheets account. n8n account or self-hosted instance. How to customize the workflow Add CRM destinations (HubSpot, Airtable, Notion). Expand LLM-extraction to capture: social links, descriptions, ratings, etc. Add validation rules before saving a lead. Enable notification steps (Telegram, Slack) when batches finish. Add additional enrichment steps (e.g., scrape About pages, contact pages, multiple URLs). üëâ This workflow gives you a complete lead generation system from Google Maps + business website analysis ‚Äî with no manual scraping needed.",1,2025-11-21 06:53:06.721000+00:00,True,2
11098,"Track Website Changes with Firecrawl, GPT-5-Mini, Notion, and Gmail","Stay Updated on Website Changes Automatically Manually checking websites for updates or competitor changes can be tedious. This workflow automates the process by scraping target pages, capturing screenshots, and analyzing content changes using Firecrawl and GPT-5-mini. All updates are saved in Notion, and you can optionally receive email alerts ‚Äî keeping you informed without lifting a finger. Key Features Automated Website Monitoring** Tracks changes on multiple target URLs on a daily schedule. Visual & Text-Based Updates** Captures screenshots and generates concise text summaries for each change. Notion Integration** Stores snapshots, comparisons, and summaries directly in Notion for easy reference. GPT-Powered Comparison** Highlights meaningful changes between snapshots using GPT-5-mini. Change Criteria Input** Define what type of updates matter to you ‚Äî focus on specific content changes or ignore minor differences. Email Alerts via Gmail** Sends notifications when updates occur, so you never miss important changes. Reusable Notion Page Template** Includes a structured template for consistent tracking and reporting: View Template Email Updates: Notion Page Template: Notion Update Example: Setup Instructions 1. Add Target URLs Paste one or more websites you want to monitor. The workflow accepts multiple URLs in JSON format for scalable tracking. 2. Setup Firecrawl Credential Use Firecrawl to scrape website data without dealing with complex API setups. Steps: Go to firecrawl.dev/app and sign up. Copy your API Key from the dashboard. In n8n, open the Firecrawl node, add a new credential, and paste the key. Test the node to ensure the connection works. 3. Setup OpenAI API Key The workflow uses GPT-5-mini to analyze and summarize website changes. Steps: Sign up at OpenAI if you haven‚Äôt already. Generate a new API Key in your OpenAI dashboard. In n8n, open the GPT node, add a new credential, and paste the API key. Test the node to ensure it can successfully call the API. 4. Setup Gmail for Email Alerts In n8n, open the Gmail node. Click Add New Credentials and select OAuth2. Follow the Google OAuth setup to allow n8n to send emails on your behalf. Choose the recipient and customize the alert formatting to include snapshots and summaries. 5. Configure the Workflow Import the workflow into n8n. Add your target URLs in JSON format. Paste your Firecrawl API Key into the Firecrawl node. Add your OpenAI API key to the GPT node. Authenticate Gmail if email notifications are desired. Set your preferred change criteria to filter meaningful updates. Adjust the schedule for daily execution or your preferred frequency. How It Works Schedule Trigger: Runs daily or on-demand. Fetch Pages: Scrapes target websites using Firecrawl. Capture & Compare: Screenshots and text are stored and compared using GPT-5-mini. Apply Change Criteria: Only updates that meet your defined criteria are logged. Save Updates: Changes are stored in Notion with visual and text summaries. Optional Alerts: Sends Gmail notifications with updates. Example Use Cases Monitor competitor websites for product, pricing, or design changes. Track updates on blogs, documentation, or landing pages. Automatically log changes for research, reporting, or team awareness. Get notified instantly when key pages are updated. Requirements | Tool | Purpose | |------|----------| | Firecrawl API Key | Scrape website content | | OpenAI API Key | Generate summaries and compare changes with GPT-5-mini | | n8n | Run and schedule the automation | | Notion | Store snapshots, summaries, and comparisons | | Gmail (OAuth2) | Send alert emails | Have questions about this template? Feel free to reach out. Our DMs are always open! Email: hello@scoutnow.app X (Twitter): @ScoutNowApp",1,2025-11-21 11:42:13.148000+00:00,True,5
11103,Aggregate Crypto and Stock Market News Feed from Multiple Sources,"This n8n template helps in making informed decisions for Crypto and Stocks Trading by helping you keep track of breaking changes in the market. How it works Collects crypto and/or stock market headlines from multiple sources: CoinDesk, CoinTelegraph, Google News, and X (via an RSS proxy). Normalizes all items into a consistent structure with fields like source, kind, title, url, publishedAt, matchedKeywords, media[], and topic. Uses topic-specific keyword lists to keep relevant items and a small spam blacklist to drop giveaways / airdrops / obvious noise. Deduplicates items across runs by tracking previously seen links. Bundles everything into a compact { topic, items[] } JSON payload and sends it to your own backend or UI via HTTP. Set up steps Import the template JSON into a new n8n workflow. In the Init RunConfig code node, choose a default topic (crypto or stocks), which platforms to use (CoinDesk / CoinTelegraph / Google News / X), and optionally adjust tickers and keywords. Open the final HTTP Request - Send to your backend node and: Replace the example URL with your own API endpoint. Either set a x-webhook-secret header that your backend validates, or remove this header entirely if you don‚Äôt use it. Create any required credentials (for the webhook header auth and for HTTP requests, if needed) ‚Äì the template does not contain real API keys. Enable either the Schedule Trigger for periodic runs or the Webhook trigger for on-demand runs from your app. Expect roughly 10‚Äì20 minutes to import, configure, and test the first run; detailed per-node notes are available as sticky notes inside the workflow canvas.",1,2025-11-21 11:53:23.978000+00:00,False,2
11121,Auto Create & Publish X-Threads (Twitter) with GPT via Telegram & Approval Loop,"AI X (twitter) Threads Bot with Approval Loop This n8n workflow transforms your Telegram messenger into a personal assistant for creating and publishing X-Threads. You can simply send an idea as a text or voice message, collaboratively edit the AI‚Äôs suggestion in a chat, and then publish the finished thread directly to X just by saying ‚ÄúOkay.‚Äù What You‚Äôll Need to Get Started Before you can use this workflow, you‚Äôll need a few prerequisites set up. This workflow connects three different services, so you will need API credentials for each: Telegram Bot API Key*: You can get this by talking to the ‚ÄúBotFather‚Äù on Telegram. It will guide you through creating your new bot and provide you with the API token. New Chat with Telegram BotFather OpenAI API Key*: This is required for the ‚ÄúSpeech to Text‚Äù and ‚ÄúAI Agent‚Äù nodes. You‚Äôll need an account with OpenAI to generate this key. OpenAI API Platform Blotato API Key: This service is used to publish the final post to X. You‚Äôll need a Blotato account and to connect your X profile there to get the key. Blotato platform for social media publishing Once you have these keys, you can add them to the corresponding credentials in your n8n instance. How the Workflow Operates, Step-by-Step Here is a detailed breakdown of how the workflow processes your request and handles the publishing. 1. Input & Initial Processing This phase captures your idea and converts it into usable text. Node Name Role in Workflow Start: Telegram Message This Telegram Trigger node initiates the entire process upon receiving any message from you in the bot. Prepare Input Consolidates the message content, ensuring the AI receives only one clean text input. Check: ist it a Voice? Checks the incoming message for text. If text is empty, it proceeds to voice handling. Get Voice File If a voice note is detected, this node downloads the raw audio file from Telegram. Speech to Text This node uses the OpenAI Whisper API to convert the downloaded audio file into a text string. 2. AI Core & Iteration Loop This is the central dialogue system where the AI drafts the content and engages in the feedback loop. AI: Draft & Revise Post The main logic agent. It analyzes your request, applies the ‚ÄúSystem Prompt‚Äù rules, drafts the post, and handles revisions based on your feedback. OpenAI Chat Model Defines the large language model (LLM) used for generating and revising the post. Window Buffer Memory A memory buffer that stores the last turns of the conversation, allowing the AI to maintain context when you request changes (e.g., ‚ÄúMake it shorter‚Äù). Check if Approved This crucial node detects the specific JSON structure the AI outputs only when you provide an approval keyword (like ‚Äúok‚Äù or ‚Äúapproved‚Äù). Post Suggestion Or Ask For Approval Sends the AI‚Äôs post draft back to your Telegram chat for review and feedback. AI Agent System Prompt (Internal Instructions - English) The agent operates under a strict prompt that dictates its behavior and formatting (found within the AI: Draft & Revise Post node. 3. Publishing & Status Check Once approved, the workflow handles the publication and monitors the post‚Äôs status in real-time. Node Name Role in Workflow Approval: Extract Final Thread Posts Parses the incoming JSON, extracting only the clean text ready for publishing. Create post with Blotato Uses the Blotato API to upload the finalized content to your connected X account. Give Blotat 5s :) A brief pause to allow the publishing service to start processing the request. Check post status Checks back with Blotato to determine if the post is published, in progress, or failed. Published? Checks if the status is ‚Äúpublished‚Äù to send the success message. In Progress? Checks if the post is still being processed. If so, it loops back to the next wait period. Give Blotat other 5s :) Pauses the workflow before re-checking the post status, preventing unnecessary API calls. Final Notification Node Name Role in Workflow Send a confirmation message Sends a confirmation message and the direct link to the published X post. Send an error message Sends a notification if the post failed to upload or encountered an error during processing. üõ†Ô∏è Personalizing Your Content Bot The true power of this n8n workflow lies in its flexibility. You can easily modify key components to match your unique brand voice and technical preferences. 1. Tweak the Content Creator Prompt The personality, tone, and formatting rules for your X content are all defined in the System Prompt. Where to find it: Inside the AI: Draft & Revise Post node, under the System Message setting. What to personalize: Adjust the tone, change the formatting rules (e.g., number of hashtags, required emojis), or insert specific details about your industry or target audience. 2. Switch the AI Model or Provider You can easily swap the language model used for generation. Where to find it: The OpenAI Chat Model node. What to personalize: Model: Swap out the default model for a more powerful or faster alternative (e.g., gpt-4 family, or models from other providers if you change the node). Provider: You can replace the entire Langchain block (including the AI Model and Window Buffer Memory nodes) with an equivalent block using a different provider‚Äôs Chat/LLM node (e.g., Anthropic, Cohere, or Google Gemini), provided you set up the corresponding credentials and context flow. 3. Modify Publishing Behavior (Schedule vs. Post) The final step is currently set to publish immediately, but you might prefer to schedule posts. Where to find it: The Create post with Blotato node. What to personalize: Consult the Blotato documentation for alternative operations. Instead of choosing the ‚ÄúCreate Post‚Äù operation (which often posts immediately), you can typically select a ‚ÄúSchedule Post‚Äù or ‚ÄúAdd to Queue‚Äù operation within the Blotato node. If scheduling, you will need to add a step (e.g., a Set node or another agent prompt) before publishing to calculate and pass a Scheduled Time parameter to the Blotato node.",1,2025-11-21 19:07:44.882000+00:00,True,6
11131,Scrape and Analyze Amazon Product Info with Decodo + OpenAI,"The Scrape and Analyze Amazon Product Info with Decodo + OpenAI workflow automates the process of extracting product information from an Amazon product page and transforming it into meaningful insights. The workflow then uses OpenAI to generate descriptive summaries, competitive positioning insights, and structured analytical output based on the extracted information. Disclaimer Please note - This workflow is only available on n8n self-hosted as it‚Äôs making use of the community node for the Decodo Web Scraping Who this is for? This workflow is ideal for: E-commerce product researchers Marketplace sellers (Amazon, Flipkart, Shopify, etc.) Competitive intelligence teams Product comparison bloggers and reviewers Pricing and product analytics engineers Automation builders needing AI-powered product insights What problem is this workflow solving? Manually extracting Amazon product details, ads, pricing, reviews, and competitive signals is: Time-consuming Requires switching across tools Difficult to analyze at scale Not structured for reporting Hard to compare products objectively This workflow automates: Web scraping of Amazon product pages Extraction of product features and ad listings AI-generated product summaries Competitive positioning analysis Generation of structured product insight output Export to Google Sheets for tracking and reporting What this workflow does This workflow performs an end-to-end product intelligence pipeline, including: Data Collection Scrapes an Amazon product page using Decodo Retrieves product details and advertisement placements Data Extraction Extracts: Product specs Key feature descriptions Ads data Supplemental metadata AI-Driven Analysis Generates: Descriptive product summary Competitive positioning insights Structured product insight schema Data Consolidation Merges descriptive, analytical, and structured outputs Export & Persistence Aggregates results Writes final dataset to Google Sheets for: tracking comparison reporting product research archives Setup Prerequisites n8n instance** Decodo API credentials** OpenAI API credentials** Make sure to install the Decodo Community Node. Required Credentials Decodo API Go to Credentials Add Decodo API Enter API key Save as: Decodo Credentials account OpenAI API Go to Credentials Select OpenAI Enter API key Save as: OpenAi account Google Sheets Add Google Sheets OAuth Authorize via Google Save as desired account Inputs to configure Modify in Set the Input Fields node: product_url = https://www.amazon.in/Sony-DualSense-Controller-Grey-PlayStation/dp/B0BQXZ11B8 How to customize this workflow to your needs You can easily adapt this workflow for various use cases. Change the product being analyzed Modify: product_url Change AI model In OpenAI nodes: Replace gpt-4.1-mini Use Gemini, Claude, Mistral, Groq (if supported) Customize the insight schema Edit Product Insights node to include: sustainability markers sentiment extraction pricing bands safety compliance brand comparisons Expand data extraction You may extract: product reviews FAQs Q&A seller information delivery and logistics signals Change output destination Replace Google Sheets with: PostgreSQL MySQL Notion Slack Airtable Webhook delivery CSV export Turn it into a batch processor Loop over: multiple ASINs category listings search results pages Summary This workflow provides a complete automated product intelligence engine, combining Decodo‚Äôs scraping capabilities with OpenAI‚Äôs analytical reasoning to transform Amazon product pages into structured insights, competitive analysis, and summarized evaluations automatically stored for reporting and comparison.",1,2025-11-22 11:03:41.327000+00:00,True,6
11135,"WordPress Blog Automation: AI SEO Content, Images, Scheduling & Email Alerts","Description: This workflow fully automates your blog publishing process using n8n, AI, and WordPress. It pulls blog data from Google Sheets, generates SEO-optimized content with AI, creates feature images, sets meta titles & descriptions, uploads everything to WordPress, schedules posts, and sends email notifications‚Äîall without manual work. Ideal for digital marketers, content teams, and agencies who want: üîπ High-volume blog publishing üîπ SEO-friendly AI-generated content üîπ Automated WordPress post creation with featured images üîπ Optimized Google meta titles, descriptions, and URL slugs üîπ Structured chapters, FAQs, and internal/external linking üîπ Email notifications to alert team members or clients üåê What This Template Does Pulls Blog Data from Google Sheets üîπ Fetches blog post info from your Sheet: title, keywords, scheduled date, service areas, word count, style, CTA, logos, internal/external links, and more. Compares Scheduled Date with Today üîπ Ensures posts are published only on their intended date, preventing mistakes and saving time. Generates AI SEO Content üîπ Title & subtitle üîπ Introduction (~60 words) üîπ Multi-chapter body with logical flow üîπ Conclusions (~60 words) üîπ FAQ section (4‚Äì6 Q&A) üîπ SEO-focused URL slug & meta information Chapters integrate internal links, external links, service areas, and CTAs naturally. Creates Feature Images üîπ Generates eye-catching blog feature images using dynamic text wrapping and custom fonts, ready for WordPress upload. Uploads Everything to WordPress üîπ Automates post creation, uploads images, sets featured media, and applies Yoast SEO meta info for Google and social sharing. Sends Email Notifications üîπ Automatically notifies team members or clients when a blog post is published, including post title, link, and summary. Fully Marketing-Optimized Workflow üîπ SEO-friendly structure üîπ Brand-consistent imagery üîπ Engaging content with clear CTAs üîπ Automation that saves hours per post Optional HTML Design for Blog Pages üîπ Generates HTML-ready posts with clean formatting perfect for Elementor or any WordPress theme. üõ†Ô∏è Use Cases üîπ High-volume content marketing üîπ AI-assisted blog writing for agencies üîπ Automated SEO optimization üîπ Streamlined WordPress publishing üîπ Marketing campaigns with internal/external link integration üîπ Team/client notification workflow üöÄ Features üîπ fully automated üîπ Works with Google Sheets + WordPress üîπ AI-generated SEO content üîπ Custom feature images üîπ Meta title & description optimized for Google üîπ Chapter-based content structure with FAQs üîπ Marketing-ready calls to action üîπ Automated email notifications üìù Tags #n8n #automation #wordpress #blogautomation #seo #contentmarketing #ai #featureimage #metaoptimization #digitalmarketing #aiwriting #workflow #emailnotifications Sample of spreadsheet:",1,2025-11-22 15:01:23.973000+00:00,True,8
11141,Complete AI Safety Suite: Test 9 Guardrail Layers with Groq LLM,"Who's It For AI developers, automation engineers, and teams building chatbots, AI agents, or workflows that process user input. Perfect for those concerned about security, compliance, and content safety. What It Does This workflow demonstrates all 9 guardrail types available in n8n's Guardrails node through real-world test cases. It provides a comprehensive safety testing suite that validates: Keyword blocking for profanity and banned terms Jailbreak detection to prevent prompt injection attacks NSFW content filtering for inappropriate material PII detection and sanitization for emails, phone numbers, and credit cards Secret key detection to catch leaked API keys and tokens Topical alignment to keep conversations on-topic URL whitelisting to block malicious domains Credential URL blocking to prevent URLs with embedded passwords Custom regex patterns for organization-specific rules (employee IDs, order numbers) Each test case flows through its corresponding guardrail node, with results formatted into clear pass/fail reports showing violations and sanitized text. How to Set Up Add your Groq API credentials (free tier works fine) Import the workflow Click ""Test workflow"" to run all 9 cases Review the formatted results to understand each guardrail's behavior Requirements n8n version 1.119.1 or later (for Guardrails node) Groq API account (free tier sufficient) Self-hosted instance (some guardrails use LLM-based detection) How to Customize Modify test cases in the ""Test Cases Data"" node to match your specific scenarios Adjust threshold values (0.0-1.0) for AI-based guardrails to fine-tune sensitivity Add or remove guardrails based on your security requirements Integrate individual guardrail nodes into your production workflows Use the sticky notes as reference documentation for implementation This is a plug-and-play educational template that serves as both a testing suite and implementation reference for building production-ready AI safety layers.",1,2025-11-22 20:33:07.889000+00:00,True,2
11148,"Send Pre-Meeting Attendee Context with OpenAI, Google Calendar, and Slack","Overiew This workflow builds an AI meeting assistant who sends information-dense pre-meeting notifications for a user's upcoming meetings. How It Works A scheduled trigger fires hourly and checks for upcoming meetings within the hour. When found, a search for last correspondence + recent activity is performed for each attendee. Using available correspondance, an AI/LLM is used to summarize this information and generate a short notification message which should help the user prepare for the meeting. The notification is finally sent to the user's Slack. Set up Steps Google Cloud Create the credentials and replace them in the workflow. Please enable the following APIs: Gmail API Google Calendar API OpenAI Create the credentials as instructed Replace your credentials and connect. Slack Create the credentials as instructed Replace your credentials and connect.",1,2025-11-23 06:07:19.546000+00:00,True,6
11159,Analyze food ingredients from Telegram photos using Gemini and Airtable,"Analyze food ingredients from Telegram photos using Gemini and Airtable üõ°Ô∏è Personal Ingredient Bodyguard Turn your Telegram bot into an intelligent food safety scanner. This workflow analyzes photos of ingredient labels sent via Telegram, extracts the text using AI, and cross-references it against your personal database of ""Good"" and ""Bad"" ingredients in Airtable. It solves the problem of manually reading tiny, complex labels for allergies or dietary restrictions. Whether you are Vegan, Halal, allergic to nuts, or just avoiding specific additives, this workflow acts as a strict, personalized bodyguard for your diet. It even features a customizable ""Persona"" (like a Sarcastic Bodyguard) to make safety checks fun. üéØ Who is it for? People with specific dietary restrictions (Vegan, Gluten-free, Keto). Individuals with food allergies (Nuts, Dairy, Shellfish). Special dietary observers (Halal, Kosher). Health-conscious shoppers avoiding specific additives (e.g., E120, Aspartame). üöÄ How it works Trigger: You send a photo of a product label to your Telegram Bot. Fetch Rules: The workflow retrieves your active ""Watchlist"" (Ingredients to avoid/prefer) and ""Persona"" settings from Airtable. Vision & Logic: It uses an AI Vision model to extract text from the image (OCR) and Google Gemini to analyze the text against your strict veto rules (e.g., ""Safe"" only if ZERO bad items are found). Response: The bot replies instantly on Telegram with a Safe/Unsafe verdict, highlighting detected ingredients using HTML formatting. Log: The result is saved back to Airtable for your records. ‚öôÔ∏è How to set up This workflow relies on a specific Airtable structure to function as the ""Brain."" Set up Airtable Sign up for Airtable: Click here Copy the required Base: Click here to copy the ""Ingredients Brain"" base Connect Airtable to n8n (5-min guide): Watch Tutorial Set up Telegram Message @BotFather on Telegram to create a new bot and get your API Token. Add your Telegram credentials in n8n. Configure AI Add your Google Gemini API credentials. Note on OCR: This template is configured to use a local LLM for OCR to save costs (via the OpenAI-compatible node). If you do not have a local model running, simply swap the ""OpenAI Chat Model"" node for a standard GPT-4o or Gemini Vision node. üìã Requirements n8n** (Cloud or Self-hosted) Airtable** account (Free tier works) Telegram** account Google Gemini** API Key Local LLM* (Optional, for free OCR) OR *OpenAI/Gemini** Key (for standard Cloud Vision) üé® How to customize Change the Persona:** Go to the ""Preferences"" table in Airtable to change the bot's personality (e.g., ""Helpful Nutritionist"") and output language. Update Ingredients:** Add or remove items in the ""Watchlist"" table. Mark them as ""Good Stuff"" or ""Bad Stuff"" and set Status to ""Active"". Adjust Sensitivity:** The AI prompt in the ""AI Agent"" node is set to strict ""Veto"" mode (Bad overrides Good). You can modify the system prompt to change this logic. ‚ö†Ô∏è Disclaimer This tool is for informational purposes only. Not Medical Advice: Do not rely on this for life-threatening allergies. AI Limitations: OCR can misread text, and AI can hallucinate. Verify: Always double-check the physical product label. Use at your own risk.",1,2025-11-23 14:23:28.735000+00:00,True,6
11200,"Auto-Schedule Demos for High-Intent Leads with Clearbit, Slack, and Calendly","This workflow automatically captures, enriches, scores, and routes website leads in real-time, scheduling high-intent prospects for demos within minutes instead of hours‚Äîdramatically improving conversion rates by eliminating response delays. What Makes This Different: Real-Time Lead Processing** - Captures and processes leads instantly from website forms with zero delay Intelligent Fit Scoring** - Automatically scores leads 0-100 based on company size, seniority, and revenue Dual-Track Routing** - High-intent leads (60+) get fast-track treatment, others follow standard nurture Live Calendar Integration** - Shows actual available Calendly slots, not fake placeholders Automated Sales Alerts** - Posts rich lead details to Slack with booking links instantly Smart Follow-Up** - Sends fallback email if sales team doesn't respond within 10 minutes Complete CRM Automation** - Creates HubSpot contacts and deals automatically with enriched data Full Audit Trail** - Logs everything to Google Sheets for analytics and reporting Key Benefits of Instant Lead Response: Speed** - Minutes from form submission to scheduled demo, not hours or days Conversion** - Respond while leads are hot, dramatically improving booking rates Automation** - Zero manual work‚Äîenrichment, scoring, routing, and follow-up all automatic Intelligence** - Data-driven scoring ensures sales focuses on best-fit prospects Accountability** - Complete logging shows response times and follow-up actions Scalability** - Handles unlimited lead volume without adding sales admin work Who's it for This template is designed for B2B SaaS companies, sales teams, and revenue operations professionals who need to convert website leads faster. It's perfect for organizations that lose deals due to slow response times, want to prioritize high-intent prospects, need to automate CRM data entry, or want to ensure no hot lead falls through the cracks while sales is busy. How it works / What it does This workflow creates an end-to-end lead-to-meeting pipeline that automatically processes inbound leads and schedules high-intent prospects for demos. The system: Receives lead submissions via webhook from website forms or chat widgets Normalizes data from different form providers into a standard format Enriches contact information using Clearbit to get company size, revenue, job title, and industry Calculates fit score (0-100) based on company metrics: size (40 pts), seniority (30 pts), revenue (30 pts) Routes intelligently - High-intent leads (60+) ‚Üí fast track | Standard leads ‚Üí nurture channel Creates CRM records - Automatically creates/updates HubSpot contact and deal with enriched data Fetches real availability - Gets actual available Calendly demo slots via API (next 7 days) Alerts sales team - Posts formatted message to Slack with lead details and booking links Monitors response - Waits 10 minutes and checks if sales replied in Slack thread Sends fallback email - Automatically emails lead with self-service booking link if no response Logs everything - Records all data to Google Sheets for reporting and analytics Key Innovation: Smart Follow-Up Automation - Unlike basic lead capture workflows, this system ensures accountability by automatically following up with leads if the sales team is unavailable, preventing lost opportunities while maintaining a professional response time. How to set up 1. Configure API Credentials Add the following credentials in n8n: Clearbit (Lead Enrichment) Create account at clearbit.com Generate API key from Settings ‚Üí API Add as ""Clearbit API"" credential in n8n HubSpot (CRM Integration) Create private app in HubSpot Settings ‚Üí Integrations ‚Üí Private Apps Grant scopes: crm.objects.contacts.write, crm.objects.deals.write Copy app token Add as ""HubSpot App Token"" credential in n8n Calendly (Calendar Availability) Create OAuth app at calendly.com/integrations/api_webhooks Configure OAuth2 credentials in n8n Set environment variable: CALENDLY_USER_URI with your user URI Get this from: https://api.calendly.com/users/me (returns your user URI) Slack (Team Notifications) Create Slack app at api.slack.com/apps Add Bot Token Scopes: channels:read, chat:write, channels:history Install app to workspace and copy Bot User OAuth Token Add as ""Slack API"" credential in n8n Update channel names in nodes: change ""hot-leads"" and ""leads"" to your actual channel names SendGrid (Email Fallback) Create account at sendgrid.com Generate API key from Settings ‚Üí API Keys Verify sender email address Add as ""SendGrid API"" credential in n8n Update ""from"" email in ""Send Fallback Email"" node Google Sheets (Activity Logging) Create Google Cloud project and enable Sheets API Configure OAuth2 credentials in n8n Create a Google Sheet with columns matching the workflow Replace YOUR_GOOGLE_SHEET_ID in ""Log to Google Sheets"" node with your actual sheet ID 2. Customize Fit Scoring Logic Edit the ""Calculate Fit Score"" node to match your ideal customer profile: Default Scoring: Company size 50-5,000 employees = 40 points Executive/Director seniority = 30 points Annual revenue ‚â• $1M = 30 points Total possible:* 100 points | *High-intent threshold:** 60+ points To Customize: Adjust company size ranges based on your target market Change seniority requirements (C-level, VP, Manager, etc.) Modify revenue thresholds Update the 60-point threshold for high-intent routing 3. Set Up Webhook Endpoint Get Webhook URL: Activate the workflow Copy webhook URL from ""Lead Form Webhook"" node URL format: https://your-n8n-instance.com/webhook/demo-request Configure Form Provider: Point your website form POST request to the webhook URL Send JSON body with fields: email, name, company, phone, utm_source, utm_campaign, page_url, message Or map your existing form fields to these names in ""Normalize Lead Data"" node Example Form Integration: // HTML Form fetch('https://your-n8n.com/webhook/demo-request', { method: 'POST', headers: {'Content-Type': 'application/json'}, body: JSON.stringify({ email: 'lead@company.com', name: 'John Doe', company: 'Acme Inc', phone: '+1234567890', utm_source: 'google', page_url: window.location.href }) }) 4. Test the Workflow Initial Test: Activate the workflow Submit test lead via webhook (use Postman or curl) Verify Clearbit enrichment returns data Check HubSpot for created contact and deal Confirm Slack notification appears in correct channel Verify Google Sheet receives log entry Response Test: Wait 10 minutes after Slack notification Check if ""Check Slack Replies"" detects no response Verify fallback email sends via SendGrid Confirm lead receives booking email with Calendly link Calendly Test: Verify ""Get Calendly Event Types"" finds your Demo event Check ""Get Available Demo Slots"" returns actual time slots Confirm booking URLs work and pre-fill time selection 5. Monitor and Optimize Key Metrics to Track: Time from form submission to Slack notification (target: &lt;30 seconds) Sales response rate within 10 minutes Fallback email send rate (lower is better) High-intent lead conversion rate (booked ‚Üí closed) Average fit score of closed deals Optimization Tips: Adjust fit score weights based on actual conversion data Tune the 60-point threshold for high-intent routing Customize Slack message format for your team's workflow Modify wait time (10 minutes) based on team availability Add custom fields to Google Sheet for additional tracking Requirements n8n Instance: n8n Cloud or self-hosted (v1.0+) Code node execution enabled Webhook functionality active External Services: Clearbit** - Enrichment API (paid service, free trial available) HubSpot** - CRM with API access (free tier available) Calendly** - Scheduling platform with API access (paid plans) Slack** - Workspace with bot integration capability SendGrid** - Email API (free tier: 100 emails/day) Google Sheets** - Google account with Sheets API enabled Technical Requirements: Public webhook endpoint (HTTPS) Environment variable support for sensitive data OAuth2 authentication capability Minimum 256MB RAM for code node execution Data Privacy: Ensure GDPR/CCPA compliance for lead data storage Review data retention policies for all connected services Configure appropriate data handling in Google Sheets Add privacy policy link to email templates Tips and best practices Fit Scoring: Start with default scoring, then optimize based on actual conversion data Review monthly: which scores convert best? Adjust weights accordingly Consider adding industry filters for vertical-specific targeting Test different thresholds (50, 60, 70) to find optimal balance Lead Response: Keep 10-minute wait time during business hours Consider longer wait for after-hours leads (use schedule trigger) Customize Slack urgency based on fit score (üî• for 80+, ‚ö° for 60-79) Add @mentions in Slack for specific team members based on lead attributes Calendar Management: Use dedicated ""Demo"" event type in Calendly for consistent detection Ensure event name includes ""demo"" (case-insensitive) for workflow to find it Set appropriate buffer times between meetings in Calendly settings Review availability regularly to maintain high slot count Error Handling: All critical nodes have onError: continueRegularOutput to prevent workflow stops Monitor execution logs daily for failed enrichments or CRM errors Set up n8n error workflow to alert on consistent failures Keep fallback booking URL updated in case Calendly API fails Performance: Webhook responds immediately (within 2 seconds) even while processing continues Clearbit enrichment can take 3-5 seconds‚Äîthis is expected Consider batching Google Sheets updates if processing &gt;100 leads/day Monitor n8n resource usage; Code nodes can be memory-intensive Privacy and Compliance: Add unsubscribe link to fallback emails Include data handling disclosure in form Set Google Sheet permissions appropriately (team only) Review Clearbit's data sources for compliance requirements Configure data retention in HubSpot to match your policy Customization Ideas: Add SMS notification for ultra-high fit scores (90+) Integrate with territory routing (route to specific sales rep by region) Add lead source scoring (paid &gt; organic &gt; referral) Create separate tracks for different product lines Build competitor mention detection in form messages Add qualification questions that influence fit score",1,2025-11-24 19:20:25.091000+00:00,False,7
11244,Instant Infographic Generator (LINE + Nano Banana Pro),"About This Template This workflow turns complex data or topics sent via LINE into beautiful, easy-to-understand Infographics. It combines Gemini (to analyze data and structure the visual layout) and Nano Banana Pro (accessed via Kie.ai API) to generate high-quality, data-rich graphics (Charts, timelines, processes). How It Works Input: User sends a topic or data points via LINE (e.g., ""Japan's Energy Mix: 20% Solar, 10% Wind...""). Data Visualization Logic: Gemini acts as an Information Designer, deciding the best chart type (Pie, Bar, Flow) and layout for the data. Render: Nano Banana generates a professional 3:4 Vertical Infographic. Smart Polling: The workflow uses a loop to check the API status every 5 seconds, ensuring it waits exactly as long as needed. Delivery: Uploads to S3 and sends the visual report back to LINE. Who It‚Äôs For Social Media Managers needing quick visual content. Educators and presenters summarizing data. Consultants creating quick visual reports on the go. Requirements n8n** (Cloud or Self-hosted). Kie.ai API Key** (Nano Banana Pro). Google Gemini API Key**. AWS S3 Bucket** (Public access). LINE Official Account**. Setup Steps Credentials: Configure Header Auth for Kie.ai and your other service credentials. Webhook: Add the production URL to LINE Developers console.",1,2025-11-26 07:09:40.330000+00:00,False,4
11254,Create a Calendly Availability API Endpoint for Real-time Scheduling Data,"This n8n template implements a Calendly Availability Checker that provides real-time availability information for your Calendly event types via a simple API endpoint Who's it for This template is designed for developers, businesses, and service providers who need to programmatically check Calendly availability. It's perfect for: Web developers** building custom booking interfaces that need real-time availability data Chatbot developers** who want to suggest available times to users Website builders** who want to display available slots on their site Integration developers** who need to check availability before creating bookings Businesses** that want to build custom scheduling experiences on top of Calendly Service providers** who need availability data for their own applications or dashboards How it works / What it does This workflow creates a RESTful API endpoint that returns real-time availability information from your Calendly account. The system: Accepts webhook requests via POST with optional parameters: event_type_uri (optional) - Specific event type to check days_ahead (optional, default: 7) - Number of days to check ahead Authenticates with Calendly API using OAuth2 credentials to access your account Retrieves user information to get your Calendly user URI and account details Lists all active event types from your Calendly account Selects the target event type: Uses the event_type_uri from the request if provided Otherwise defaults to the first active event type Fetches available time slots from Calendly's availability API for the specified date range Formats the availability data into a structured response including: Total number of available slots Next available slot (formatted and ISO timestamp) Array of all available slots with formatted times and booking URLs Slots grouped by day for easy consumption Complete list of all event types Returns a comprehensive JSON response with all availability information ready for integration How to set up 1. Configure Calendly OAuth2 Credentials Go to calendly.com/integrations Click ""API & Webhooks"" Create an OAuth2 application or use Personal Access Token In n8n, create a new credential: Type: ""Calendly OAuth2 API"" Follow the OAuth flow to connect your Calendly account The credential will be used by all HTTP Request nodes in the workflow 2. Activate the Workflow Open the workflow in n8n Ensure the Calendly OAuth2 credential is properly configured Activate the workflow to make the webhook endpoint available 3. Test the Workflow Use the ""Test workflow"" button in n8n to manually trigger it Or send a POST request to the webhook URL: { ""event_type_uri"": ""optional-event-uri"", ""days_ahead"": 7 } Verify the response contains availability data Check that the ""Get Current User"" node successfully retrieves your account info 4. Customize (Optional) Adjust the default days_ahead value in the ""Set Configuration"" node (currently 7 days) Modify the date range calculation in ""Get Available Times"" node Customize the response format in ""Respond with Availability"" node Add filtering logic to select specific event types Add caching to reduce API calls Requirements Calendly account** with at least one active event type n8n instance** (self-hosted or cloud) Calendly OAuth2 API credentials** configured in n8n Webhook access** (if using n8n cloud, webhooks are automatically available) How to customize the workflow Modify Date Range Edit the ""Set Configuration"" node to change the default days_ahead value Update the start_time and end_time calculations in ""Get Available Times"" node Currently checks from tomorrow (1 day ahead) to 7 days ahead by default Adjust the date calculation expressions as needed Filter Specific Event Types Modify the ""Select Event Type"" node to add filtering logic Add an IF node to check event type names or slugs Filter by duration, active status, or custom criteria Return multiple event types if needed Customize Response Format Edit the ""Respond with Availability"" node to change the JSON structure Add or remove fields from the response Format dates/times differently Include additional metadata from event types Add pagination for large slot lists Add Caching Insert a Code node before ""Get Available Times"" to check cache Store availability data temporarily to reduce API calls Set appropriate cache expiration times Consider using n8n's built-in cache or external storage Add Error Handling Enhance error handling in HTTP Request nodes Add validation for request parameters Return meaningful error messages in the response Handle cases where no event types exist Handle cases where no availability exists Integrate with Other Services Add nodes to log availability checks to a database Send availability data to analytics platforms Trigger notifications when availability changes Sync availability with external calendars Build availability dashboards Key Features RESTful API endpoint** - Simple POST endpoint for checking availability Real-time availability** - Fetches current availability directly from Calendly API Flexible event type selection** - Supports specific event type or auto-selects first available Configurable date range** - Customizable number of days to check ahead Comprehensive response format** - Returns formatted and raw availability data Multiple data views** - Provides slots as array, grouped by day, and summary statistics Event type information** - Includes details about all available event types Human-readable formatting** - Formats dates and times for easy display Booking URLs included** - Each slot includes direct booking URL Error resilience** - Nodes configured with continueRegularOutput to handle API errors gracefully Use Cases Custom booking widgets** - Display available slots on your website without embedding Calendly Chatbot integration** - Let AI assistants suggest available times to users Mobile app integration** - Check availability before showing booking options in mobile apps Multi-calendar systems** - Aggregate availability from multiple Calendly accounts Availability dashboards** - Build internal dashboards showing team availability Smart scheduling** - Check availability before sending meeting invitations Booking confirmation flows** - Verify availability before processing bookings Calendar sync verification** - Ensure Calendly availability matches other calendar systems Analytics and reporting** - Track availability patterns and booking trends Custom scheduling UIs** - Build completely custom scheduling interfaces using availability data Data Fields Returned User Information User name, email, scheduling URL User URI and organization URI Event Type Information Event type name, duration (minutes), URI Complete list of all active event types with details Availability Summary has_slots - Boolean indicating if any slots are available total_slots - Total number of available slots next_available - Human-readable formatted string of next available slot next_available_iso - ISO 8601 timestamp of next available slot Available Slots Array Each slot includes: start_time - ISO 8601 timestamp formatted - Human-readable date/time string booking_url - Direct URL to book this specific slot Slots by Day Grouped object with days as keys Each day contains array of time slots with formatted times and booking URLs Format: { ""Monday, Dec 2"": [{ time: ""10:00 AM"", url: ""..."" }] } Metadata checked_at - ISO timestamp of when availability was checked success - Boolean indicating successful execution Workflow Architecture The workflow uses a linear processing pattern with data transformation at each step: Webhook Trigger ‚Üí Receives POST requests with optional parameters Set Configuration ‚Üí Extracts and sets default values for event_type_uri and days_ahead Get Current User ‚Üí Authenticates and retrieves Calendly user information Extract User Info ‚Üí Parses user data to extract URIs and account details Get Event Types ‚Üí Fetches all active event types for the user Select Event Type ‚Üí Chooses target event type (from request or first available) Get Available Times ‚Üí Queries Calendly API for available time slots Format Availability ‚Üí Transforms raw API data into structured, formatted response Respond with Availability ‚Üí Returns comprehensive JSON response to caller Example Scenarios Scenario 1: Check Default Availability Developer sends POST request to webhook endpoint with empty body Workflow uses default 7-day lookahead period Workflow selects first active event type automatically Returns availability for next 7 days with all slots formatted Developer displays slots in custom booking interface Scenario 2: Check Specific Event Type Developer sends POST request with specific event_type_uri Workflow uses provided event type instead of default Checks availability for that specific event type only Returns slots grouped by day for easy calendar display Developer shows availability in day-by-day calendar view Scenario 3: Extended Date Range Developer sends POST request with days_ahead: 30 Workflow checks availability for next 30 days Returns comprehensive list of all available slots Developer uses data to show monthly availability view User can see all available times for the next month Scenario 4: Chatbot Integration User asks chatbot ""When are you available?"" Chatbot calls webhook endpoint to get availability Workflow returns next available slot and total count Chatbot responds: ""I have 15 slots available. Next available: Monday, Dec 2 at 10:00 AM"" Chatbot offers to book the next available slot This template provides a powerful API endpoint for checking Calendly availability, enabling developers to build custom scheduling experiences while leveraging Calendly's robust scheduling infrastructure.",1,2025-11-26 16:16:53.437000+00:00,True,1
11268,Generate Hollywood-Style Video Ads from Images with GPT-5 Mini and Fal.ai Sora-2,"üé¨ Hollywood-Style Ads in Seconds (Fal.ai Sora-2 Version) Turn a single product image + short text description into a cinematic 8-second ad using Fal.ai‚Äôs Sora-2 Image-to-Video model. Perfect for ad agencies, marketing teams, and UGC creators who want to produce high-quality video ads instantly without editors or camera crews. üß© Problem It Solves Producing ad videos usually requires writers, editors, equipment, and several review cycles. This workflow replaces that process with a fully automated pipeline that generates studio-quality ads on demand. üè¢ Real Use Cases 1. Ad Agencies Deliver Hollywood-style ads to clients instantly from a simple image upload. 2. UGC Creators Create multiple ad variations in minutes instead of spending hours filming. 3. SMB Marketing Generate product promo videos for websites, social media, and email campaigns. ‚öôÔ∏è How It Works Your frontend sends image + text to the workflow webhook. Image is resized to 1280√ó720. GPT-5 Mini writes a cinematic Sora-2 compatible prompt. Fal.ai Sora-2 generates a realistic 8-second lifestyle ad. Workflow polls status and retrieves the final video. Sends the video URL back to your frontend. Optional: Sends a Telegram notification. üß™ Requirements Fal.ai API Key Cloudinary Account (optional, for image storage) n8n (latest recommended version) Frontend (Lovable / AI Studio / Bubble / React) üé® Template Metadata Template Author: Sandeep Patharkar Category: Content Generation / Video Ads Difficulty: Intermediate Setup Time: 10‚Äì12 minutes üì¨ Connect With Me LinkedIn ‚Üí www.linkedin.com/in/sandeeppatharkar YouTube ‚Üí https://www.youtube.com/@fasttrackaimastery Website ‚Üí https://fasttrackaimastery.com Skool Community ‚Üí https://www.skool.com/aic-plus",1,2025-11-27 00:09:30.986000+00:00,True,5
159,Send RSS feed data to webhook,"Filters articles based on keywords, checks against MongoDB for unique links, then sends results to different webhooks",0,,False,0
160,Convert XML to JSON,Transforms XML data to JSON honoring XML attributes by putting them in a separate key:,0,,False,0
175,Backs up n8n Workflows to NextCloud,Temporary solution using the undocumented REST API for backups with file versioning (Nextcloud),0,,False,0
179,Convert Typeform data into Spreadsheet,"Trigger on new Typeform form submission Get existing spreadsheet-file from NextCloud Read data from file into flow Append a new row to flow-data Convert flow-data to a spreadsheet-file Save updated spreadsheet-file to NextCloud Assumptions Spreadsheet file Named: Problems.xls in folder ""examples"". Columns Names: Name Email Severity Problem Typeform Typeform formular with questions named exactly like the columns of the Google Sheet.",0,,False,0
199,RSS Telegram bot,rss-telegramBot 1min refreash choose send message or one photo 0.1 beater code 2019/11/30 0.2 bug fix add instagram rss,0,,False,0
216,API queries data from GraphQL,Simpe API which queries the received country code via GraphQL and returns it. Example URL: https://n8n.exampl.ecom/webhook/1/webhook/webhook?code=DE Receives country code from an incoming HTTP Request Reads data via GraphQL Converts the data to JSON Constructs return string,0,,False,0
225,"Send trending ""Show HN"" to email","Triggers every day at 1pm Gets the current content from Hacker News Gets all the different submission items Extracts the rank, title and url Checks if it is a ""Show HN"" submission Combines the items into a simple email text Sends an email with the email text",0,,False,1
226,Receive Google Sheet data via REST API,"Simple workflow which allows to receive data from a Google Sheet via ""REST"" endpoint. Wait for Webhook Call Get data from Google Sheet Return data Example Sheet: https://docs.google.com/spreadsheets/d/17fzSFl1BZ1njldTfp5lvh8HtS0-pNXH66b7qGZIiGRU",0,,False,0
342,Send daily affirmations to Telegram,A workflow which allows you to receive daily affirmations via Telegram by querying a REST API triggered by a Cron node. I used the affirmations.dev API,0,,False,1
346,Get Weather Forecast via Telegram,A workflow to receive weather updates on demand using a Telegram bot. The workflow is triggered using the Telegram Trigger node by issuing a message to the Telegram bot. The OpenWeatherMap node queries the weather API and returns the result using the Telegram node.,0,,False,1
351,Webhooks with Mattermost,Does pretty much what this workflow does but is triggered by a slash command using a Webhook node.,0,,False,0
353,Manage custom incident response in PagerDuty and Jira,"This workflow automatically follows the steps in a custom incident response playbook and manages incidents in PagerDuty, Jira tickets, and notifies the on-call team in Mattermost. This workflow consists of three sub-workflows, each automating specific steps in the playbook. Read more about this use case and learn how to set up the workflows step-by-step in the blog tutorial How to automate every step of an incident response workflow. Prerequisites A PagerDuty account and credentials A Mattermost account and credentials A Jira account and credentials Nodes Webhook nodes trigger the workflows when an incident is created in PagerDuty, and when the incidedent is acknowledged and resolved. Mattermost nodes create an auxiliary channel for the on-call team to discuss the incident with buttons to acknowledge the incident and mark it as resolved. PagerDuty nodes update the status of the incident. Jira nodes create an issue about the incident and update its status when it's resolved.",0,,False,0
354,Incident Response Workflow - Part 2,This workflow is the second of three. You can find the other workflkows here: Incident Response Workflow - Part 1 Incident Response Workflow - Part 2 Incident Response Workflow - Part 3 We have the following nodes in the workflow: Webhook node: This trigger node listens to the event when the Acknowledge button is clicked. PagerDuty node: This node changes the status of the incident report from 'Triggered' to 'Acknowledged' in PagerDuty. Mattermost node: This node publishes a message in the auxiliary channel saying that the status of the incident report has been changed to Acknowledged.,0,,False,0
355,Incident Response Workflow - Part 3,This workflow is the third of three. You can find the other workflkows here: Incident Response Workflow - Part 1 Incident Response Workflow - Part 2 Incident Response Workflow - Part 3 We have the following nodes in the workflow: Webhook node: This trigger node listens to the event when the Resolve button is clicked. PagerDuty node: This node changes the status of the incident report from Acknowledged to Resolved in PagerDuty. Jira Software node: This node moves the incident issue to Done. Mattermost node: This node publishes a message in the auxiliary channel mentioning that the incident has been marked as resolved in PagerDuty and Jira. Mattermost node: This node publishes a message in the specified Incidents channel that the incident has been resolved by the on-call team.,0,,False,0
356,Generate and insert data into a Postgres database,"This is Workflow 1 in the blog tutorial Database activity monitoring and alerting. Prerequisites A Postgres database set up and credentials. Basic knowledge of JavaScript and SQL. Nodes Cron node starts the workflow every minute. Function node generates sensor data (sensor id (preset), a randomly generated value, timestamp, and notification (preset as false) ) Postgres node inserts the data into a Postgres database. You can create the database for this workflow with the following SQL statement: CREATE TABLE n8n (id SERIAL, sensor_id VARCHAR, value INT, time_stamp TIMESTAMP, notification BOOLEAN);",0,,False,0
357,Send SMS alerts based on database queries (Twilio and Postgres),"This workflow automatically queries a Postgres database to find outlier readings for which SMS notifications have not been sent. This is Workflow 2 in the blog tutorial Database activity monitoring and alerting. Prerequisites A Postgres database set up and credentials A Twilio account and credentials Nodes Cron node triggers the workflow every minute, so the database is queried at regular intervals. Postgres nodes extract values from, and update values in the database. Twilio node sends an alert SMS about the outlier reading to a specified phone number. Set node sets the notification value to true.",0,,False,1
359,Sample error workflow,A sample error workflow which when triggered sends a notification to the specified Mattermost channel as well as an SMS to the specified mobile number.,0,,False,0
365,Render custom text over images,This workflow gets triggered every Friday at 6 PM with the help of a Cron node. It pulls in data about a random cocktail via the HTTP Request Node and sends the data to a Bannerbear node to create an image based on a template. The image is then finally shared on a specified Rocket.Chat channel.,0,,False,0
368,Create ticket on specific customer messages in Telegram,"This is a workflow where a support channel on Telegram is being used to gather customer feedback. Depending on certain keywords in the customer's message, this workflow creates a ticket with a tag in your Freshdesk instance. The customer is then sent a message on Telegram and an item is created on Monday.com for tracking.",0,,False,0
371,Notify a team channel about new software releases via Slack and GitHub,"This workflow automatically notifies the team in a Slack channel when code in a GitHub repository gets a new release. Prerequisites A GitHub account and credentials A Slack account and credentials Nodes GitHub Trigger node triggers the workflow when a release event takes place in the specified repository. Slack node posts a message in a specified channel with the text ""New release is available in {repository name}"", along with further details and a link to the release.",0,,False,0
378,Tiny tiny Rss (aka tt-rss) Feed to Mastodon,Every 10 minutes look at your published news in your Tiny tiny RSS public feed and make a toot on your mastodon. You'll need: Your mastondon URL instance Your mastondon access token Your Tiny Tiny RSS public published feed URL,0,,False,0
384,Save Typeform survey results to Airtable,"With this workflow, you can collect the data from Typeform with the Typeform Trigger node every time someone submits a response and save it to Airtable with the Airtable node.",0,,False,0
385,Send Airtable data as tasks to Trello,"In this workflow, we'll automate the export of all the submissions which have a total score greater than 15 for a final review on Trello. The workflow will also generate social media assets for the organizers and add them to the Trello card.",0,,False,0
398,Create an invoice in Google Sheets based on Typeform submission,This form takes data from a Typeform submission and creates an invoice on Google Sheets,0,,False,1
401,Send an SMS/Whatsapp message with Twilio,,0,,False,0
404,Notify about Google Sheet changes in Slack and email,To notify by Slack and mail all modification on a Google Sheet,0,,False,1
409,Get daily SMS updates about weather,Get daily SMS updates to tell you if you should wear a sweater,0,,False,1
410,Send a message with a Discord Bot,,0,,False,0
411,"Create, update, and get an incident on PagerDuty",,0,,False,0
412,Create a contact in ActiveCampaign,,0,,False,0
413,Create a new member on Mailchimp,,0,,False,0
418,Cross-post your blog posts,This workflow uses Strapi as a CMS and then cross posts new blog posts to Medium and Dev.to.,0,,False,0
426,Set alert on the website changes,I used this to check for a page that had Out Of Stock not found when an item came back in stock. Set the URL for the HTTP Request node and your Webhook URL and Messages for the discord nodes.,0,,False,1
427,Add a event to Google Calendar automatically,Add an event to Google Calendar,0,,False,0
428,Add a task to Google Tasks,,0,,False,0
429,Gather leads in Google Sheet and Mailchimp,"Gather leads into Mailchimp, automate marketing, and sales process.",0,,False,0
434,Extract post titles from a blog,This workflow uses n8n to extract the names of all the posts from the Hackernoon homepage.,0,,False,0
435,Create a new DigitalOcean Droplet,Uses a DigitalOcean Personal Access Token to create a new Droplet. Add your personal access token and change the parameters to create droplets of different sizes. You can also specify more options; refer the API docs,0,,False,0
437,Perform speech-to-text on recorded audio clips using Wit.ai,"This workflow performs speech-to-text on recorded audio clips using Wit.ai. To get started, replace your Wit.ai Server Access Token in the Authorization header.",0,,False,0
438,Create a PayPal batch payout,,0,,False,0
440,Add a song to your Spotify queue,,0,,False,0
441,Create an alert on SIGNL4,Companion workflow for SIGNL4 node docs. .,0,,False,0
442,Create a URL on Bitly,,0,,False,0
445,Send a tweet to Twitter,Companion workflow for Twitter node docs.,0,,False,0
448,Create a new Freshdesk ticket,Companion workflow for Freshdesk node documentation,0,,False,0
450,Get the community profile of a repository,,0,,False,0
453,Create a meeting in Zoom automatically,,0,,False,0
454,Get a pipeline in CircleCI,,0,,False,0
455,Sending an SMS with MessageBird,,0,,False,0
458,Run a SQL query on Postgres,Companion Workflow for Postgres node docs,0,,False,0
459,Create a new issue in Jira,,0,,False,0
460,Get the current weather data for a city,,0,,False,0
461,Create a new card in Trello,,0,,False,0
462,Post a message to a channel in RocketChat,,0,,False,0
464,Create a new user in Intercom,,0,,False,0
465,Get details of a GitLab repository,,0,,False,0
466,Get all contacts of Hubspot account,,0,,False,0
467,Send sales data from Webhook to Mautic,"This workflow goes through the teachable webhook request types and adds a user, updates him and tags him with #unsubscribe or removes the #unsubscribe tag. It also tags the user with the tag of the name of the course. Enjoy!",0,,False,0
469,Sending an SMS using sms77,,0,,False,0
471,Send Github notifications to Discord Webhook,This will send your Github notifications to a discord webhook. Since Github doesn't send push notifications to mobile devices other then @mention this is a great workaround to receive notifications on Discord with this. Using a github trigger was not a good option as there is no trigger for notifications only events (which don't work on org repos). Using http request on notifications api is way better. ++TAGGING USER IN MESSATGE:++ Change ** with your discord Id to get tagged when sending notifications. To find your own id type in any channel backslash followed by your username with the 4 digit hash code You can copy this by clicking on your username next to your profile picture Example: \@username#9999 Enjoy!,0,,False,1
474,Create a new contact in Agile CRM,,0,,False,0
476,Create an organization in Affinity,,0,,False,0
478,Create a new task in Asana,,0,,False,0
479,Execute an SQL query in Microsoft SQL,,0,,False,0
481,Create a new task in Todoist,,0,,False,0
482,Insert data into a new row for a table in Coda,,0,,False,0
483,Create a new customer in Chargebee,,0,,False,0
484,Look up a person using their email in Clearbit,,0,,False,0
485,Create a task in ClickUp,,0,,False,0
486,Receive updates for events in Chargebee,,0,,False,0
487,Receive updates for events in ClickUp,,0,,False,0
488,Receive updates when a new account is added by an admin in ActiveCampaign,,0,,False,0
489,Create a deal in Pipedrive,,0,,False,0
490,Receive updates for all changes in Pipedrive,,0,,False,0
491,Receive updates for changes in the specified list in Trello,,0,,False,0
493,Get details of a forum in Disqus,,0,,False,0
494,Create a client in Harvest,,0,,False,0
495,Track an event in Segment,,0,,False,0
496,Create a ticket in Zendesk,,0,,False,0
497,Create a contact in Drift,,0,,False,0
498,Send a private message on Zulip,,0,,False,0
499,Create a user profile in Vero,,0,,False,0
500,Create a company in Salesmate,,0,,False,0
501,Send a message via AWS SNS,Companion workflow for AWS SNS node docs,0,,False,0
503,Insert a document in MongoDB,Companion workflow for MongoDB node docs,0,,False,0
504,Get information about a company with UpLead,,0,,False,0
506,Get all the tasks in Flow,,0,,False,0
507,Send an email using AWS SES,Companion workflow for AWS SES node docs,0,,False,0
508,Receive updates for specified tasks in Flow,,0,,False,0
509,Receive updates for AWS SNS Events,Companion workflow for AWS SNS Trigger node docs,0,,False,0
510,Invoke an AWS Lambda function,Companion workflow for AWS Lambda docs,0,,False,0
511,Send an SMS using MSG91,Sending an SMS using MSG91,0,,False,0
513,Receive messages for an ActiveMQ queue,,0,,False,0
514,Get first and last names from Facebook Graph API,Companion workflow for Facebook node docs,0,,False,0
515,Download a file from Google Drive,Companion workflow for Google Drive node docs,0,,False,0
516,Catch MailChimp subscribe events,Companion workflow for Mailchimp Trigger node docs,0,,False,0
517,Get new time entries from Toggl,,0,,False,0
518,Get entries from a Cockpit collection,Companion workflow for Cockpit node docs,0,,False,0
519,Verify email deliverability with Hunter,Companion workflow for Hunter node docs,0,,False,0
520,Send an email using Mailjet,Companion workflow for Mailjet node docs,0,,False,0
521,Receive updates on emails sent via Mailjet,Companion workflow for Mailjet Trigger node docs,0,,False,0
522,Send an email using Mailgun,Companion workflow for Mailgun node docs,0,,False,0
524,Get today's date and day using the Function node,,0,,False,0
525,Get articles from Hacker News,Companion workflow for Hacker News node docs,0,,False,0
526,Assign values to variables using the Set node,,0,,False,0
527,Receive updates for GitHub events,Companion workflow for Github Trigger node docs,0,,False,0
528,Receive updates for GitLab events,Companion workflow for GitLab Trigger docs,0,,False,0
529,Receive updates for Bitbucket events,Companion workflow for Bitbucket Trigger node docs,0,,False,0
533,Receive updates on appointments via Acuity Scheduling,Companion workflow for Acuity Scheduling Trigger node docs,0,,False,0
534,Get multiple clients' data from Invoice Ninja,Companion workflow for Invoice Ninja node docs,0,,False,0
535,Receive updates on a new invoice via Invoice Ninja,Companion workflow for Invoice Ninja Trigger node docs,0,,False,0
536,Receive updates for new events in Clockify,Companion workflow for Clockify Trigger node docs,0,,False,0
537,Receive updates on a new project created in Copper,Companion workflow for Copper Trigger node docs,0,,False,0
538,Receive updates for Eventbrite events,Companion workflow for Eventbrite Trigger node docs,0,,False,0
539,Execute a job on Rundeck,Companion workflow for Rundeck node docs,0,,False,0
540,Receive updates for Calendly events,Companion workflow for Calendly Trigger node docs,0,,False,0
541,Receive updates on events in JotForm,Companion workflow for JotForm Trigger node docs,0,,False,0
543,Get invoices from Xero,Companion workflow for Xero node docs,0,,False,0
544,Create an image procedurally using Bannerbear,Companion workflow for Bannerbear node docs,0,,False,0
545,Receive updates for Stripe events,Companion workflow for Stripe Trigger node docs,0,,False,0
546,Get all posts from Wordpress,Companion workflow for Wordpress node docs,0,,False,0
547,Receive updates on an order created in Shopify,Companion workflow for Shopify Trigger node docs,0,,False,0
548,Get all orders in Shopify,Companion workflow for Shopify node docs,0,,False,0
549,Get all contacts from Mautic,Companion workflow for Mautic node docs,0,,False,0
551,Get updates when a response is created in SurveyMonkey,Companion workflow for SurveyMonkey node docs,0,,False,0
552,Get all leads from Zoho CRM,Companion workflow for Zoho CRM node docs,0,,False,0
553,Get all contacts from Keap,Companion workflow for Keap node docs,0,,False,0
554,Receive updates when a new contact is added in Keap,Companion workflow for Keap Trigger node docs,0,,False,0
556,Get a board from monday.com,Companion workflow for monday.com node docs,0,,False,0
557,Get the value of a key from Redis,Companion workflow for Redis node docs,0,,False,0
558,Get the last five SpaceX launches from the spacex.land API using GraphQL,Companion workflow for GraphQL node docs,0,,False,0
559,Create a new folder in Box,Companion workflow for Box node docs,0,,False,0
560,Receive updates for events in Box,Companion workflow for Box Trigger node docs,0,,False,0
563,Get contributors information from GitHub in Slack,Get your contributors GitHub information with a slash command in your Slack Workspace.,0,,False,0
565,Create a folder in Onedrive,Companion workflow for Onedrive node docs,0,,False,0
566,Get all Excel workbooks,Companion workflow for Excel node docs,0,,False,0
567,Get all mailboxes from Help Scout,Companion workflow for Help Scout node docs,0,,False,0
569,Receive updates for events in Jira,Companion workflow for Jira Trigger node docs,0,,False,0
571,Send an email template using Mandrill,Companion workflow for Mandrill node docs,0,,False,0
574,Encrypt some data using the crypto node,Companion workflow for Crypto node docs,0,,False,0
575,Convert a date from one format to another,Companion workflow for Date & Time node docs,0,,False,0
576,Get information of an image,Companion workflow for Edit Image node docs,0,,False,1
577,Read a file from disk,Companion workflow for Read Binary File node docs,0,,False,0
578,Read multiple files from disk,Companion workflow for Read Binary Files node docs,0,,False,0
581,Execute Set node based on Function output,Companion workflow for IF node docs,0,,False,0
582,Rename a key in n8n,Companion workflow for Rename Keys node docs,0,,False,0
583,Read an RSS Feed,Companion workflow for RSS Feed Read node docs,0,,False,0
584,Send an email,Companion workflow for Send Email node docs,0,,False,0
585,Extract text from a PDF file,Companion workflow for Read PDF node docs,0,,False,0
586,Read a spreadsheet file,Companion workflow for Spreadsheet File node docs,0,,False,0
587,Receive email updates via IMAP,Companion workflow for IMAP Email node docs,0,,False,0
588,Execute another workflow,Companion workflow for Execute Workflow node docs,0,,False,0
590,Write a file to the host machine,,0,,False,0
591,Add text to a downloaded image,,0,,False,1
592,Create a table in Quest DB and insert data,,0,,False,0
594,Publish a post to a publication on Medium,,0,,False,0
595,Send a message to a channel on Twake,,0,,False,0
596,Sending SMS to users,,0,,False,1
597,Create a table in CrateDB and insert data,Companion workflow for CrateDB node docs,0,,False,0
598,Create a table in MySQL and insert data,Companion workflow for MySQL node docs,0,,False,0
599,Create a table in Postgres and insert data,Companion workflow for Postgres node docs,0,,False,0
600,Insert and read data from Google Sheets,Companion workflow for Google Sheets node docs,0,,False,0
602,Manage users automatically in reqres.in,Companion workflow for HTTP Request node docs,0,,False,0
608,Generate and queue factory sensor data in AMQP,"This workflow generates sensor data, which is used in another workflow for managing factory incident reports. Read more about this use case and how to build both workflows with step-by-step instructions in the blog post How to automate your factory‚Äôs incident reporting. Prerequisites AMQP, an ActiveMQ connection, and credentials Nodes Interval node triggers the workflow every second. Set node set the necessary values for the items that are addeed to the queue. AMQP Sender node sends a raw message to add to the queue.",0,,False,0
609,Manage incident reporting in PagerDuty and CrateDB,"This workflow automatically monitors the functionality of a factory. The workflow logs machine data coming from factory sensors in a CrateDB database, generates an incident report in PagerDuty, and notifies the responsible staff members when the temperature of a machine crosses the threshold value. This workflow builds on a workflow that generates factory data. Read more about this use case and how to build both workflows with step-by-step instructions in the blog post How to automate your factory's incident reporting. Prerequisites A PagerDuty account and credentials AMQP, an ActiveMQ connection, and credentials A CrateDB instance running locally or on a server, and credentials. Nodes AMQP Trigger node starts the workflow. IF node filters sensor values higher than 50¬∞C. PagerDuty node creates an incident in the account. Set nodes set the required incident information and sensor data, respectively. CrateDB nodes ingest the information data and machine sensor data, respectively. Function node converts degrees from Celsius to Fahrenheit.",0,,False,0
613,Start a Whereby video call from Mattermost,Companion workflow for blog post,0,,False,0
615,Manage folders automatically in Dropbox,Companion workflow for Dropbox node docs,0,,False,1
617,Post latest Twitter mentions to Slack,"This workflow will allow you to get the latest twitter mentions and send those mentions to Rocket.Chat. To ensure that we don't resend the same tweets as before, we use the Function Node and getWorkflowStaticData() to persist the ids of the tweets which have already been sent and filter those out. This leaves us with only the newest tweets.",0,,False,1
620,Manage folders in Nextcloud,Companion workflow for enhanced Nextcloud node docs,0,,False,0
621,Set automated labels in Gmail,,0,,False,0
628,Receive updates from HubSpot on a new contact creation,Companion workflow for HubSpot Trigger node docs,0,,False,0
629,Enviar Miembros del CMS Ghost hacia Newsletter Sendy,"Ghost + Sendy Integration Est√° es una integraci√≥n del CMS Ghost hacia Sendy Sendy ( www.sendy.co ) Ghost ( www.ghost.org ) Con esta integraci√≥n podr√°s importar los miembros del CMS Ghost en su nueva versi√≥n que incluye la parte de Membres√≠a hac√≠a el Software de newsletter sendy. Est√° integraci√≥n adem√°s nos avisa si se ha registrado un nuevo miembro via telegram. Para realizar esta customizaci√≥n es necesaria la creaci√≥n de una custom integration en Ghost. Para ello desde el panel de Administraci√≥n vamos a CUSTOM INTEGRATIONS / + Add custom Integration Una vez all√≠ nos solicitar√° un nombre le ponemos el que queramos y a√±adimos un nuevo Hook: En Target URL debe ir La url que nos genera nuestro webhook dentro de n8n: Pegamos la URL y acamos de rellenar los datos del HTTP REQUEST1 con los datos de nuestra lista rellenando los campos. api_key list Que encontaras en t√∫ instalaci√≥n de Sendy Por √∫ltimo faltara a√±adir las credenciales de Telegram de Nuestro BOT ( https://docs.n8n.io/credentials/telegram/ ) e indicar el grupo o usuario donde queremos que notifique. Saludos,",0,,False,1
632,Nathan: Your n8n Personal Assistant,Nathan is a proof of concept framework for creating a personal assistant who can handle various day to day functions for you.,0,,False,1
635,Export WordPress posts to Spreadsheet,Export WordPress Posts to Spreadsheet and download .csv to your local machine.,0,,False,0
636,Add Google Sheets data to the Mautic contact,Automatically import the name and email in Google Sheet to the mautic contact each time the lead is filled to the sheet.,0,,False,0
857,"Create screenshots with uProc, save to Dropbox and send by email","Do you want to create a website screenshot without browser extensions? This workflow creates screenshots of any website using the uProc Get Screenshot by URL tool and sends an email with the screenshots. You need to add your credentials (Email and API Key - real -) located at Integration section to n8n. Node ""Create Web + Email Item"" can be replaced by any other supported service returning Website and Email values, like Google Sheets, Mailchimp, MySQL, or Typeform. Every ""uProc"" node returns an image URL of the captured website. This generated URL will remain only 24 hours in our server. You can set up the uProc node with several parameters: width: you can choose one of the predefined values to generate the screenshot, or you can set up a custom width you want. full-page: the tool will return a screenshot of the website from top to bottom with the defined width. In our workflow, we generate two screenshots: 1) One screenshot of 640 pixels width. 2) One full-page screenshot of 640 pixels width. Screenshots are downloaded by ""Get File"" nodes and saved to the screenshots folder in Dropbox. Finally, we use the Amazon SES node to send an HTML email with both screenshots to the specified email. We will receive the next email:",0,,False,2
858,Create a website screenshot and send via Telegram Channel,This workflow allows you to create a screenshot of a website and send it to a telegram channel.,0,,False,0
859,Tracking your crypto portfolio in Airtable,"If you have made some investments in cryptocurrency, this workflow will allow you to create an Airtable base that will update the value of your portfolio every hour. You can then track how well your investments are doing. You can check out my Airtable base to see how it works or even copy my base so that you can customize this workflow for yourself. To implement this workflow, you will need to update the Airtable nodes with your own credentials and make sure that they are pointing to your Airtable",0,,False,0
860,Discover company data by name with uProc,"Do you want to discover company-related information to enrich a signup process? This workflow enriches any company by name using the uProc Get Company by Name tool. This tool combines Google Maps and emails research on the internet to return results. You get no results if the company has no presence on Google Maps. You need to add your credentials (Email and API Key - real -) located at Integration section to n8n. You can replace node ""Create Company Item"" with any other supported service returning Company names and countries, like Hubspot, Google Sheets, MySQL, or Typeform. You can set up the uProc node with several parameters: country: the country name you want to use. name: the name of the company you need to locate. Every ""uProc"" node returns the next fields per every located company: name: Contains the company's given name. email: Contains the company's given email. cif: Contains company's cif number. address: Contains company's formatted address. city: Contains the city location of the company. state: Contains province location of the company. county: Contains state location of the company country: Contains country location of the company zipcode: Contains zipcode code of the company phone: Contains phone number of the company website: Contains website of the company latitude: Contains latitude of the company longitude: Contains longitude of the company Next, you can save results to a CRM or Google Sheets, and prepare returned email or phone to launch an email or telemarketing campaign.",0,,False,0
861,Monitor SSL certificate of any domain with uProc,"Do you want to check the SSL certificate expiration dates of your customers or servers? This workflow gets information of an SSL certificate using the uProc Get Certificate by domain tool. You can use this workflow to query SSL certificates in bulk and send alarms when any certificate has expired. You need to add your credentials (Email and API Key - real -) located at Integration section to n8n. You can replace ""Create Domain Item"" with any integration containing a domain, like Google Sheets, MySQL, or Zabbix server. Every ""uProc"" node returns the next fields per every analyzed SSL certificate: issuer: Contains the issuer. provider: Contains the provider. valid_from: Contains the start date. valid_to: Contains the end date. serial_number: Contains the serial number. type: Contains if supports one or multiple domains. protocol: Contains the protocol. valid: Contains its validity. domains: Contains all domains and subdomains supported. An ""IF"" node detects if the certificate is valid or not. Finally, the workflow sends an alarm to a Telegram channel to know if the certificate has expired.",0,,False,0
862,Get DNS entries of any domain with uProc,"Do you want to control the DNS domain entries of your customers or servers? This workflow gets DNS information of any domain using the uProc Get Domain DNS records tool. You can use this workflow to check existing DNS records in real-time to ensure that any domain setup is correct. You need to add your credentials (Email and API Key - real -) located at Integration section to n8n. You can replace ""Create Domain Item"" with any integration containing a domain, like Google Sheets, MySQL, or Zabbix server. Every ""uProc"" node returns multiple items with the next fields per every item: type: Contains the DNS record type (A, ALIAS, AAAA, CERT, CNAME, MX, NAPTR, NS, PTR, SOA, SRV, TXT, URL). values: Contains the DNS record values.",0,,False,0
863,Verify a phone number with uProc,"Do you want to avoid communication problems when launching phone calls? This workflow verifies landline and mobile phone numbers using the uProc Get Parsed and validated phone tool with worldwide coverage. You need to add your credentials (Email and API Key - real -) located at Integration section to n8n. Node ""Create Phone Item"" can be replaced by any other supported service with phone values, like databases (MySQL, Postgres), or Typeform. The ""uProc"" node returns the next fields per every parsed and validated phone number: country_prefix: contains the international country phone prefix number. country_code: contains the 2-digit ISO country code of the phone number. local_number: contains the phone number without international prefix. formatted: contains a formatted version of the phone number, according to country detected. valid: detects if the phone number has a valid format and prefix. type: the phone number type (mobile, landline, or something else). ""If"" node checks if the phone number is valid. You can use the result to mark invalid phone numbers in your database or discard them from future telemarketing campaigns.",0,,False,0
864,Monitor changes in Google Sheets every 45 mins,"Based on your use case, you might want to trigger a workflow if new data gets added to your database. This workflow allows you to send a message to Mattermost when new data gets added in Google Sheets. The Interval node triggers the workflow every 45 minutes. You can modify the timing based on your use case. You can even use the Cron node to trigger the workflow. If you wish to fetch new Tweets from Twitter, replace the Google Sheet node with the respective node. Update the Function node accordingly.",0,,False,0
865,Gathering tasks in Typeform and send to ClickUp,"Using Typeform to push task requests to an n8n webhook that then categorizes the request and assigns it in ClickUp accordingly. In order to get his workflow working for yourself, you will need: ClickUp account Typeform account Credentials for these services ClickUp configured with appropriate lists Typeform setup with options that correspond with ClickUp lists. You may modify this workflow to meet your specific needs and configuration. This is a very simple version of this workflow and you can make it as complicated as you wish to meet your requirements.",0,,False,0
867,"Create, add an attachment, and send a draft using Microsoft Outlook","This workflow allows you to create, add an attachment, and send a draft using the Microsoft Outlook node. Microsoft Outlook node: This node creates a draft message with HTML content. You can either set the content as Text or HTML. You can also add the recipients to the draft in this node. HTTP Request node: This node fetches the logo of n8n from a URL and returns the binary data. You might want to fetch files from your machine or another email or a database. You can replace this node with the relevant node. Microsoft Outlook1 node: This node adds the attachment that we receive from the previous node to the draft message that we created. Microsoft Outlook2 node: This node sends the draft message to a recipient. Since we didn't mention the recipient in the Microsoft Outlook node, we add the recipient in this node. You can also enter multiple recipients.",0,,False,1
869,Find a new book recommendations,Receive a new book recommendation each Friday from Open Library. You can select the subject from thousands of options and have a new recommendation delivered to you automatically.,0,,False,1
870,Send an SMS to a number whenever you go out,"This workflow allows you to send an SMS to a number whenever you go out. Pushcut is an app for iOS that lets you create smart notifications to kick off shortcuts, URLs, and online automation. You can have multiple actions for a notification. You can use the IF node to check which action was selected and build the workflow accordingly. Pushcut Trigger node: This node triggers the workflow when an action is selected by the user. Twilio node: The Twilio node sends an SMS with the input given by the user. Based on your use-case, you might want to do something else, for example, send a Tweet, dim or turn off your Philips Hue lights, add activity to Strava or play music on Spotify. Replace the Twilio node with these nodes to customize the workflow as per your needs.",0,,False,0
871,N8N Espa√±ol - Tratamiento de textos,"Este workflow es para trabajar con tratamiento de texto usando n8n y poder iniciarte en como funciona. How To, Paso a Paso: https://comunidad-n8n.com/tratamiento-de-textos/ Comunidad de telegram: https://t.me/comunidadn8n",0,,False,0
872,N8N Espa√±ol - Telegram Welcome Bot,"Este workflow te permite crear un Bot para dar la Bienvenida y despedida en tu grupo de telegram. How To, Paso a Paso: https://comunidad-n8n.com/construye-tu-bot-con-n8n Comunidad de telegram: https://t.me/comunidadn8n",0,,False,0
875,Send tweets every minute to Mattermost,"This workflow executes every minute and fetches the recent tweets from Twitter with the search query n8n_io. The workflow is built on the concept of polling. Cron node: The Cron node triggers the workflow every minute. Based on your use-case you can configure the time. You can even use the Interval node to trigger the workflow at a certain time interval. Twitter node: The Twitter node searches for the tweets that contain n8n_io and returns the most recent tweets. You can specify a different search query based on your use-case. Set node: The Set node sets the data that we pass on to the next nodes in the workflow. You can set only the values that you require in your workflow. Function node: All the magic happens in this node. The Twitter node returns all the recent tweets, including the ones which were returned earlier. The Function node, using the getWorkflowStaticData() method, only returns the tweets that are new, i.e., the tweets that were not returned in the previous workflow. Mattermost node: The Mattermost node sends the tweets from the Function node to the Twitter notifications channel. If you don't use Mattermost and want to share this data on a different platform, replace this node with the appropriate node.",0,,False,1
876,Monitor Strava and send email updates,Allow your friends to get updates when you do not meet your workout goals so that they can help you get to where you need to be! Monitors your Strava account and sends an email to three accountability partners each day if you have not had enough activity so that they can reach out to you with some encouragement. Make sure that you configure the Accountability Settings to meet your needs along with the Strava and Send Email credentials.,0,,False,0
878,N8N Espa√±ol - Bot Multi Idioma NoCode,Tutorial: https://comunidad-n8n.com/bot-multi-idioma-no-code/ Comunidad de telegram: https://t.me/comunidadn8n BOT: https://t.me/NocodeTranslateBot,0,,False,1
879,Access data from bubble application,This is a proof of concept workflow showing how you would connect n8n to a Bubble data collection.,0,,False,0
880,Receive updates of the position of the ISS every minute,"This workflow demonstrates the use of static data in n8n. The workflow is built on the concept of polling. Cron node: The Cron node triggers the workflow every minute. You can configure the time based on your use-case. HTTP Request node: This node makes an HTTP Request to an API that returns the position of the ISS. Set node: In the Set node we set the information that we need in the workflow. Since we only need the timestamp, latitude, and longitude we set this in the node. If you need other information, you can set them in this node. Function node: The Function node, checks if the incoming data is similar to the data that was returned in the previous execution or not. If the data is different the Function node returns this new node, otherwise, it returns a message 'No New Items'. The data is also stored as static data with the workflow. Based on your use-case, you can build the workflow further. For example, you can use it send updates to Mattermost or Slack",0,,False,0
882,Store the output of a phantom in Airtable,"This workflow allows you to store the output of a phantom in Airtable. This workflow uses the LinkedIn Profile Scraper phantom. Configure and launch this phantom from your Phantombuster account before executing the workflow. The workflow uses the following node: Phantombuster node: The Phantombuster node gets the output of the LinkedIn Profile Scraper phantom that ran earlier. You can select a different phantom from the Agent dropdown list, but make sure to configure the workflow accordingly. Set node: Using the Set node we are setting the data for the workflow. The data that we set in this node will be used by the next nodes in the workflow. Based on your use-case, you can modify the node. Airtable node: The Airtable node allows us to append the data in an Airtable. Based on your use-case you can replace this node with any other node. Instead of storing the data in Airtable, you can store the data in a database or Google Sheet, or send it as an email using the Send Email node, Gmail node, or Microsoft Outlook node.",0,,False,0
888,Get data from Hacker News and send to Airtable or via SMS,"This n8n workflow automates sending out SMS notifications via Vonage which includes new tech-related vocabulary everyday. To build this handy vocabulary improver, you‚Äôll need the following: n8n ‚Äì You can find details on how to install n8n on the Quickstart page. LingvaNex account ‚Äì You can create a free account here. Up to 200,000 characters are included in the free plan when you generate your API key. Airtable account ‚Äì You can register for free. Vonage account ‚Äì You can sign up free of charge if you aren‚Äôt already.",0,,False,1
890,Read in an Excel spreadsheet file,How to take the path of a local Excel file and read its contents into n8n.,0,,False,0
892,Transfer Google Analytics data to Airtable database,"This workflow allows you to get analytics of a website and store it Airtable. In this workflow, we get the analytics for the sessions grouped by the country. Based on your use-case, you can select different Dimensions and set different Metrics. You can use the Cron node or the Interval node to trigger the workflow on a particular interval and fetch the analytics data regularly. Based on your use-case, you might want to store the data returned by Google Analytics to a database or a Google Sheet. Replace the Airtable node with the appropriate node.",0,,False,0
898,Create new member in Mailchimp from Airtable,This workflow allows you to automatically create new members in MailChimp based on entries in an Airtable base.,0,,False,0
900,Add a datapoint to Beeminder on Strava activity update,"This workflow allows you to add a datapoint to Beeminder when a new activity is added to Strava. You can use this workflow to keep a track of your fitness activities and connect Strava with Beeminder. If you want to keep a track of different activities like the number of hours worked in a week or the number of tasks completed, you can use the relevant node. For example, you can use the Clockify Trigger node or the Toggl Trigger node.",0,,False,0
901,Add contacts to SendGrid automatically,"This workflow allows you to create, update and get a contact using the SendGrid node. Based on your use-case, you may want to add contacts to SendGrid from a different email service or from your CRM. Fetch the information of the contact using the node of that service. Connect the node with the SendGrid node and reference the value from the node.",0,,False,0
908,Compress binary files to zip format,"This workflow allows you to compress binary files to zip format. HTTP Request node: The workflow uses the HTTP Request node to fetch files from the internet. If you want to fetch files from your local machine, replace it with the Read Binary File or Read Binary Files node. Compression node: The Compression node compresses the file into a zip. If you want to compress the files to gzip, then select the gzip format instead. Based on your use-case, you may want to write the files to your disk or upload it to Google Drive or Box. If you want to write the compressed file to your disk, replace the Dropbox node with the Write Binary File node, or if you want to upload the file to a different service, use the respective node.",0,,False,1
913,Execute multiple Command Lines based on Text File Inputs,This workflow takes a text file as input. It pulls the information from the text file and used it as a parameter to execute a command for each text line. This workflow references a file /home/n8n/filelist.txt in the Read Binary File node which will need to be changed to work properly. You can also edit the Execute Command node to modify what happens for each of these lines of text. Note: This workflow requires the Execute Command node which is only available on the on-premise version of n8n.,0,,False,0
916,Store responses from Typeform into Airtable,This workflow stores responses form responses of Typeform in Airtable. The workflow also sends the response to a channel on Slack. You will have to configure the Set node if your form uses different fields.,0,,False,1
917,Receive updates of the position of the ISS and add it to a table in TimescaleDB,"This workflow allows you to receive updates about the positiong of the ISS and add it to a table in TimescaleDB. Cron node: The Cron node triggers the workflow every minute. You can configure the time based on your use-case. HTTP Request node: This node makes an HTTP Request to an API that returns the position of the ISS. Based on your use-case you may want to fetch data from a different URL. Enter the URL in the URL field. Set node: In the Set node we set the information that we need in the workflow. Since we only need the timestamp, latitude, and longitude we set this in the node. If you need other information, you can set them in this node. TimescaleDB node: This node stores the information in a table named iss. You can use a different table as well.",0,,False,0
920,"Generate, retrieve and download a report using the SecurityScorecard","This workflow allows you to generate, retrieve and download a report using the SecurityScorecard node. SecurityScorecard node: This node generates a full scorecard report. Based on your use-case, you can generate other type of report. SecurityScorecard1 node: This node fetches the latest report from SecurirtScoredcard. Toggle Return All to true to return all the reports. SecurityScorecard2 node: This node downloads the report that got fetched from the previous node. Based on your use-case, you can either store this report in Dropbox, Google Drive etc. or email it using the Gmail node, Send Email node or the Microsoft Outlook node. You can replace the Strat node with the Cron node to trigger the workflow on a regurlar interval.",0,,False,0
928,Create a post with comments in Reddit automatically,"This workflow allows you to create a post a Reddit and add a comment on the post. Reddit node: This node creates a post in the subreddit n8n. If you want to create a post in a different subreddit, enter the name of that subreddit instead. Reddit1 node: This node gets the post that we created previously. It returns the data like the post ID, title of the post, content of the post, etc. Reddit2 node: This node adds a comment on the post that we created earlier.",0,,False,0
930,"Create, update and get a post via Discourse","This workflow allows you to create, update and get a post using the Discourse node. Discourse node: This node creates a new post under a category. Based on your use-case, you can select a different category. Discourse1 node: This node updates the content of the post. Discourse2 node: This node fetches the node that we created using the Discourse node. Based on your use-case, you can add or remove nodes to connect Discourse to different services.",0,,False,0
933,Add subscribed customers to Airtable automatically,"This workflow allows you to receive updates when a customer is subscribed to a list in GetResponse and add them to a base in Airtable. GetResponse Trigger node: This node triggers the workflow when a customer is added to a list. Based on your use-case, you can select a different event. Set node: The Set node is uded here to ensure that only the data that we set in this node gets passed on to the next nodes in the workflow. For this workflow, we set the name and email of the customer. Airtable node: The data from the Set node is added to a table in Airtable. Based on your use-case, you may want to add the infromation about the customer to a CRM instead of a table in Airtable. Replace the Airtable node with the node of the CRM where you want to add the data.",0,,False,0
934,Insert and retrieve data from a table in Stackby,"This workflow allows you to insert and retrieve data from a table in Stackby. Set node: The Set node is used to set the values for the name and id fields for a new record. You might want to add data from an external source, for example an API or a CRM. Based on your use-case, add the respective node before the Set node and configure your Set node accordingly. Stackby node: This node appends data from the previous node to a table in Stackby. Based on the values you want add to your table, enter the column names in the Column field. Stackby1 node: This node fetches all the data that is stored in the table in Stackby.",0,,False,0
935,Check for preview for a link,"This workflow allows you to check for preview for a link and return the preview if it exists. Peekalink node: This node checks if a preview is available for a URL or not. If a preivew is available the node returns true, otherwise false. IF node: The IF node checks the output from the previous node. If the condition is true the node connected to the true branch is executed. If the condition is false the node connected to the false branch is executed. Peekalink1 node: This node will fetch the preview of the URL. Based on your use-case, you can connect the Slack node, Mattermost node etc. to get the response on these platforms. NoOp node: Adding this node here is optional, as the absence of this node won't make a difference to the functioning of the workflow. We've added this as it can sometimes help others with a better understanding of the workflow, visually.",0,,False,0
936,Add the affiliate to a program automatically,"This workflow allows you to create an affiliate, add metadata, and add the affiliate to a program. Tapfiliate node: This node allows you to create a new affiliate in Tapfiliate. Tapfiliate1 node: This node allows you add metadata to the affiliate that you created previously. Based on your use-case, you may or may not require this node. Tapfiliate2 node: This node allows you to add the affiliate that you created previously to a program. Based on your use-case, you might want to replace the Start node with a trigger node that gets you the information of an affiliate.",0,,False,0
947,Register users to an event on Demio via Typeform,"This workflow allows you to register your audience to an event on Demio via a Typeform submission. Typeform Trigger node: This node will trigger the workflow when a form response is submitted. Based on your use-case, you may use a different platform. Replace the Typeform Trigger node with a node of that platform. Demio node: This node registers a user for an event. It gets the details of the users from the Typeform response.",0,,False,0
949,Create a customer and send the invoice automatically,This workflows allows you to create a customer and an invoice and send the invoice to the customer. QuickBooks node: This node will create a new customer in QuickBooks. QuickBooks1 node: This node will create an invoice for the customer that we created in the previous node. QuickBooks2 node: This node will send the invoice that we created in the previous node.,0,,False,0
959,"Create a collection and create, update, and get a bookmark in Raindrop","This workflow allows you to create a collection and create, update, and get a bookmark in Raindrop. Raindrop node: This node will create a new collection in Raindrop. If you already have a collection, you can skip this node. Raindrop1 node: This node will create a new bookmark and add it to a collection. Raindrop2 node: This node will update the bookmark that we created in the previous node. Raindrop3 node: This node will return the information about the bookmark that we created earlier.",0,,False,0
960,"Create, update, and get a webinar in GoToWebinar","This workflow allows you to create, update, and get a webinar in GoToWebinar. GoToWebinar node: This node will create a new webinar in GoToWebinar. GoToWebinar1 node: This node will update the description of the webinar that we created in the previous node. GoToWebinar2 node: This node will get the information about the webinar that we created earlier.",0,,False,0
961,"Create a campaign, add a contact, and get the campaign from Emelia","This workflow allows you to create a campaign, add a contact, and get the campaign from Emelia. Emelia node: This node will create a new campaign in Emelia. Emelia1 node: This node will add a contact to the campaign that we created in the previous node. Based on your use-case, you can add a Google Sheets node or an Airtable node to get the email address of the contact. Emelia2 node: This node will get the information about the campaign that we created earlier.",0,,False,0
965,Analyze feedback using AWS Comprehend and send it to a Mattermost channel,"This workflow analyzes the sentiments of the feedback provided by users and sends them to a Mattermost channel. Typeform Trigger node: Whenever a user submits a response to the Typeform, the Typeform Trigger node will trigger the workflow. The node returns the response that the user has submitted in the form. AWS Comprehend node: This node analyses the sentiment of the response the user has provided and gives a score. IF node: The IF node uses the data provided by the AWS Comprehend node and checks if the sentiment is negative. If the sentiment is negative we get the result as true, otherwise false. Mattermost node: If the score is negative, the IF node returns true and the true branch of the IF node is executed. We connect the Mattermost node with the true branch of the IF node. Whenever the score of the sentiment analysis is negative, the node gets executed and a message is posted on a channel in Mattermost. NoOp: This node here is optional, as the absence of this node won't make a difference to the functioning of the workflow. This workflow can be used by Product Managers to analyze the feedback of the product. The workflow can also be used by HR to analyze employee feedback. You can even use this node for sentiment analysis of Tweets. To perform a sentiment analysis of Tweets, replace the Typeform Trigger node with the Twitter node. Note: You will need a Trigger node or Start node to start the workflow. Instead of posting a message on Mattermost, you can save the results in a database or a Google Sheet, or Airtable. Replace the Mattermost node with (or add after the Mattermost node) the node of your choice to add the result to your database.",0,,False,0
966,Send weather alerts to your mobile phone with OpenWeatherMap and SIGNL4,"Get weather alerts on your mobile phone via push, SMS or voice call. This flow gets weather information every morning and sends out an alert to your SIGNL4 on-call team. For example you can send out weather alerts in case of freezing temperatures, snow, rain, hail storms, hot weather, etc. The flow also supports automatic alert resolution. So, for example if the temperature goes up again the alert is closed automatically in the app. User cases: Dispatch snow removal teams Inform car dealers to protect the cars outside in case of hail storms Set sails if there are high winds And much more ... Can be adapted easily to other weather warnings, like rain, hail storm, etc.",0,,False,0
967,Monitor a file for changes and send an alert,"This flow monitors a file for changes of its content. If changed, an alert is sent out and you receive it as push, SMS or voice call on SIGNL4. User cases: Log-file monitoring Monitoring of production data Integration with third-party systems via file interface Etc. Sample file ""alert-data.json"": { ""Body"": ""Alert in building A2."", ""Done"": false, ""eventId"": ""2518088743722201372_4ee5617b-2731-4d38-8e16-e4148b8fb8a0"" } Body: The alert text to be sent. Done: If false this is a new alert. If true this indicated the alert has been closed. eventId: Last SIGNL4 event ID written by SIGNL4. This flow can be easily adapted for database monitoring as well.",0,,False,0
968,Create an event in PostHog when a request is made to a webhook URL,This workflow automatically creates an event in PostHog when a request is made to a webhook URL. Prerequisites A PostHog account and credentials Nodes Webhook node triggers the workflow when a URL is accessed. PostHog node creates a new event in PostHog.,0,,False,0
975,Get daily poems in Telegram,This workflow posts a poem translated into English every day in a Telegram chat. Cron node: triggers the workflow every day at 10:00. You can change the time and interval based on your use case. HTTP Request node: makes an HTTP request to the Poemist API that returns a random poem. LingvaNex node: translates the returned poems into English. Telegram node: takes in the translated poem and posts it in the chat.,0,,False,1
980,Load data into spreadsheet or database,"This workflow is a generic example of how to load data from your workflow into a destination that stores tabular data. For example, a Google Sheets or Airtable sheet, a .CSV file, or any relational database like MySQL. Generally, you need to ensure that you send well-formatted data into the Spreadsheet or Database node. You can use the Set or Function node to transform data into the correct format for your destination. Key concepts Spreadsheets and databases have columns, like ""Name"" and ""Email"". The data you send into a Spreadsheet/ Database node needs to match these column names for each row of data that you want to insert. Data points need to be represented as key-value pairs. Specifically, each item of data needs to have a JSON key for each column in the sheet. For a spreadsheet with ""Name"" and ""Email"" columns, it would look like: {""Name"" : ""Karla"", ""Email"" : ""karla@email.com""} Before appending or inserting data to a spreadsheet or database, you might need to transform it into the correct format. You can preprocess the data with a Set or Function node. The Set node allows you to perform simple transforms when the data you want to load into spreadsheet rows is already represented as items. Use the Function node when you need to map nested data (like arrays) inside a single item to their own top-level items (Example in community forums). Spreadsheet and database nodes in n8n perform their configured action (like Append, Create Row, Write to File) on each item of input data. Workflow walkthrough",0,,False,0
982,Gender inclusive language bot for Mattermost,"This workflow ensures gender inclusive language in Mattermost channels. If someone addresses the group with ‚Äúguys‚Äù or ‚Äúgals‚Äù, a bot promptly replies with: ""May I suggest ‚Äúfolks‚Äù or ‚Äúy'all‚Äù? We use gender inclusive language here. üòÑ"". Webhook node**: triggers the workflow when a new message is posted in Mattermost. IF node**: verifies if the message includes the words ""guys"" or ""gals"". If false, it does not take any action. If true, it triggers the Mattermost node. Mattermost node**: posts the language warning message in the Mattermost channel.",0,,False,0
983,Add new leads in Lemlist from Airtable,"This workflow allows you to add new leads in Lemlist from Airtable. Airtable node: This node lists all the emails that are stored in your Table. You may have the email addresses stored in a Google Sheet, CRM, or database. Replace the Airtable node with the respective node to get the list of the email addresses. Lemlist node: This node creates new leads for a campaign in Lemlist taking the information from the previous node. Lemlist1 node: This node returns the information of a lead from Lemlist.",0,,False,0
984,Send a message on Mattermost when a lead replies to your Lemlist email,"This workflow allows you to send a message on Mattermost when a lead replies to your email. Lemlist Trigger: The Lemlist Trigger node will trigger the workflow when a lead sends a reply to a campaign. Mattermost node: This node will send a message to the Leads channel in Mattermost with the information about the reply. Based on your use-case, you may want to send the message to a different channel. You may even want to use a different service. Replace the node with the service where you want to send a message.",0,,False,0
986,Weather check workflow for bash-dash,"This workflow returns the current weather at a predefined or given city and returns it so that it can be displayed with bash-dash. By default does it return the weather in Berlin if no city got defined. That default can be changed in the ""Set City"" node. Example usage: \- weather london Example bash-dash config: commands[weather]=""http://localhost:5678/webhook/weather""",0,,False,0
987,Create Asana Ticket from Terminal Bash-dash,"This workflow allows creating a new Asana task via bash-dash Example usage: \- asana My new task Example bash-dash config: commands[asana]=""http://localhost:5678/webhook/asana""",0,,False,0
988,Telegram messaging for bash-dash,"This workflow allows you to send a message in a Telegram chat via bash-dash. Example usage: - telegram I'll be late If you want to send a predefined message without typing it in the command line, you can replace the Text Expression in the Telegram node with a specific message. In this case, the dash command - telegram will send the predefined message to the chat. Example bash-dash config: commands[telegram]=""http://localhost:5678/webhook/telegram""",0,,False,0
989,Create an invoice based on the Typeform submission,"This workflow allows you to create an invoice with the information received via Typeform submission. Typeform node: This node triggers the workflow. Whenever the form is submitted, the node triggers the workflow. We will use the information received in this node to generate the invoice. APITemplate.io node: This node generates the invoice using the information from the previous node.",0,,False,0
990,Manage contacts via Autopilot,"This workflow allows you to create a new list, add a new contact to that list, update the contact, and get all contacts in the list using the Autopilot node. Autopilot node: This node will create a new list called n8n-docs in Autopilot. Autopilot1 node: This node creates a new contact and adds it to the list created in the previous node. Autopilot2 node: This node updates the information of the contact that we created in the previous node. Autopilot3 node: This node returns all the contacts of the n8n-docs list that we created using the Autopilot node.",0,,False,0
991,Add new contacts from Autopilot to Airtable,This workflow allows you to receive updates when a new contact is added in Autopilot and add them to a base in Airtable. Autopilot Trigger node: The Autopilot Trigger node will trigger the workflow when a new contact is added in Autopilot. Set node: We use the Set node to ensure that only the data that we set in this node gets passed on to the next nodes in the workflow. Airtable node: This node will store the data coming from the previous node in a table in Airtable.,0,,False,0
992,Manage transfers automatically in Wise,"This workflow allows you to create a quote and a transfer, execute the transfer and get the information of the transfer using the Wise node. Wise node: This node will create a new quote in Wise. Wise1 node: This node will create a new transfer for the quote that we created in the previous node. Wise2 node: This node will execute the transfer that we created in the previous node. Wise3 node: This node will return the information of the transfer that we executed in the previous node.",0,,False,0
993,Receive transfer updates from Wise and add to Airtable,"This workflow allows you to receive updates from Wise and add information of a transfer to a base in Airtable. Wise Trigger node: This node will trigger the workflow when the status of your transfer changes. Wise node: This node will get the information about the transfer. Set node: We use the Set node to ensure that only the data that we set in this node gets passed on to the next nodes in the workflow. We set the value of Transfer ID, Date, Reference, and Amount in this node. Airtable node: This node will append the data that we set in the previous node to a table.",0,,False,1
995,Split In Batches node noItemsLeft example,"This workflow demonstrates how to use noItemsLeft to check if there are items left to be processed by the SplitInBatches node. Function node: This node generates mock data for the workflow. Replace it with the node whose data you want to split into batches. SplitInBatches node: This node splits the data with the batch size equal to 1. Based on your use-case, set the value of the Batch Size. IF node: This node check if all the data by the SplitInBatches are not processed or not. It uses the expression {{$node[""SplitInBatches""].context[""noItemsLeft""]}} which returns a boolean value. If there is data yet to be processed, the expression will return false, otherwise true. Set node: This node prints a message No Items Left. Based on your use-case, connect the false output of the IF node to the input of the node you want to execute, after the data is processed by the SplitInBatches node.",0,,False,0
996,Split In Batches node currentRunIndex example,"This workflow demonstrates how to use currentRunIndex to get the running index. Function node: This node generates mock data for the workflow. Replace it with the node whose data you want to split into batches. SplitInBatches node: This node splits the data with the batch size equal to 1. Based on your use-case, set the value of the Batch Size. IF node: This node checks the running index. If the running index equals 5 the node returns true and breaks the loop. The node uses the expression {{$node[""SplitInBatches""].context[""currentRunIndex""];}}, which returns the running index. Set node: This node prints a message Loop Ended. Based on your use-case, connect the false output of the IF node to the input of the node you want to execute if the condition is false.",0,,False,0
998,Translate cocktail instructions using DeepL,"This workflow allows you to translate cocktail instructions using DeepL. HTTP Request node: This node will make a GET request to the API https://www.thecocktaildb.com/api/json/v1/1/random.php to fetch a random cocktail. This information gets passed on to the next node in the workflow. Based on your use case, replace the node with the node from where you might receive the data. DeepL node: This node will translate the cocktail instructions that we got from the previous node to French. To translate the instructions in your language, select your language instead.",0,,False,1
1001,Manage group members in Bitwarden automatically,"This workflow allows you to create a group, add members to the group, and get the members of the group. Bitwarden node: This node will create a new group called documentation in Bitwarden. Bitwarden1 node: This node will get all the members from Bitwarden. Bitwarden2 node: This node will update all the members in the group that we created earlier. Bitwarden3 node: This node will get all the members in the group that we created earlier.",0,,False,0
1005,Send daily weather updates to a phone number via Plivo,"This workflow allows you to send daily weather updates via an SMS message using the Plivo node. Cron node: The Cron node will trigger the workflow daily at 9 AM. OpenWeatherMap node: This node will return data about the current weather in Berlin. To get the weather updates for your city, you can enter the name of your city instead. Plivo node: This node will send an SMS with the weather update, which was sent by the previous node.",0,,False,0
1021,"Create, update, and get a person from Copper","This workflow allows you to create, update, and get a person from Copper. Copper node: This node will create a new person in Copper. Copper1 node: This node will update the information of the person that we created using the previous node. Copper2 node: This node will retrieve the information of the person that we created earlier.",0,,False,0
1028,Manage newsletter signups in Baserow,"This is the workflow that I presented at the April 9, 2021 n8n Meetup. This workflow uses Baserow.io to store registration information collected using n8n as both the web server and the data processor. To get this workflow working properly, you will either need to run it on n8n.cloud or using the on premise version with people having the ability to connect to n8n externally. You will need an account on Baserow.io to store your subscriptions with a table with the following fields: GUID First Name Last Name Email Confirmed",0,,False,1
1035,Get all the slides from a presentation and get thumbnails of pages,"This workflow allows you to get all the slides from a presentation and get thumbnails of pages. Google Slides node: This Google Slides node will get all the slides from a presentation. Google Slides1 node: This node will return thumbnails of the pages that were returned by the previous node. Based on your use case, to upload the thumbnails to Dropbox, Google Drive, etc, you can use the respective nodes.",0,,False,0
1039,Send a message on Mattermost when you get a reply in Emelia,"This workflow allows you to send a message on Mattermost when a lead replies to your email. Emelia Trigger node: The Emelia Trigger node will trigger the workflow when a lead sends a reply to a campaign Mattermost node: This node will send a message to the Leads channel in Mattermost with the information about the reply. Based on your use case, you may want to send the message to a different channel. You may even want to use a different service. Replace the node with the service where you want to send a message.",0,,False,0
1041,"Create, update, and get an object from Bubble","This workflow allows you to create, update, and get an object from Bubble. Bubble node: This node will create a new object of the type Doc in Bubble. If you want to create an object with a different type, use that type instead. Bubble1 node: This node will update the object that we created using the previous node. Bubble2 node: This node will retrieve the information of the object that we created earlier.",0,,False,0
1045,ETL pipeline for text processing,"This workflow allows you to collect tweets, store them in MongoDB, analyse their sentiment, insert them into a Postgres database, and post positive tweets in a Slack channel. Cron node: Schedule the workflow to run every day Twitter node: Collect tweets MongoDB node: Insert the collected tweets in MongoDB Google Cloud Natural Language node: Analyse the sentiment of the collected tweets Set node: Extract the sentiment score and magnitude Postgres node: Insert the tweets and their sentiment score and magnitude in a Posgres database IF node: Filter tweets with positive and negative sentiment scores Slack node: Post tweets with a positive sentiment score in a Slack channel NoOp node: Ignore tweets with a negative sentiment score",0,,False,2
1047,Send location updates of the ISS every minute to a queue in AWS SQS,"This workflow allows you to send position updates of the ISS every minute to a queue using the AWS SQS node. Cron node: The Cron node will trigger the workflow every minute. HTTP Request node: This node will make a GET request to the API https://api.wheretheiss.at/v1/satellites/25544/positions to fetch the position of the ISS. This information gets passed on to the next node in the workflow. Set node: We will use the Set node to ensure that only the data that we set in this node gets passed on to the next nodes in the workflow. AWS SQS: This node will send the data from the previous node to the iss-position queue. If you have created a queue with a different one, you can use that queue instead.",0,,False,0
1048,"Create, update, and get an item from Webflow","This workflow allows you to create, update, and get an item from Webflow. Webflow node: This node will create a new collection of the type Team Members in Webflow. If you want to create a collection with a different type, use that type instead. Webflow1 node: This node will update the item that we created using the previous node. Webflow2 node: This node will retrieve the information of the object that we created earlier.",0,,False,0
1049,Send location updates of the ISS every minute to a table in Google BigQuery,"This workflow allows you to send position updates of the ISS every minute to a table in Google BigQuery. Cron node: The Cron node will trigger the workflow every minute. HTTP Request node: This node will make a GET request to the API https://api.wheretheiss.at/v1/satellites/25544/positions to fetch the position of the ISS. This information gets passed on to the next node in the workflow. Set node: We will use the Set node to ensure that only the data that we set in this node gets passed on to the next nodes in the workflow. Google BigQuery: This node will send the data from the previous node to the position table in Google BigQuery. If you have created a table with a different name, use that table instead.",0,,False,0
1053,Git backup of workflows and credentials,"This creates a git backup of the workflows and credentials. It uses the n8n export command with git diff, so you can run as many times as you want, but only when there are changes they will create a commit. Setup You need some access to the server. Create a repository in some remote place to host your project, like Github, Gitlab, or your favorite private repo. Clone the repository in the server in a place that the n8n has access. In the example, it's the ., and the repository name is repo. Change it in the commands and in the workflow commands (you can set it as a variable in the wokflow). Checkout to another branch if you won't use the master one. cd . git clone repository Or you could git init and then add the remote (git remote add origin YOUR_REPO_URL), whatever pleases you more. As the server, check if everything is ok for beeing able to commit. Very likely you'll need to setup the user email and name. Try to create a commit, and push it to upstream, and everything you need (like config a user to comit) will appear in way. I strong suggest testing with exporting the commands to garantee it will work too. cd ./repo git commit -c ""Initial commmit"" --allow-empty -u is the same as --set-upstream git push -u origin master Testing to push to upstream with the first exported data npx n8n export:workflow --backup --output ./repo/workflows/ npx n8n export:credentials --backup --output repo/credentials/ cd ./repo git add . git commit -c ""manual backup: first export"" git push After that, if everything is ok, the workflow should work just fine. Adjustments Adjust the path in used in the workflow. See the the git -C PATH command is the same as cd PATH; git .... Also, adjust the cron to run as you need. As I said in the beginning, you can run it even for every minute, but it will create commits only when there are changes. Credentials encryption The default for exporting the credentials is to do them encrypted. You can add the flag --decrypted to the n8n export:credentials command if you need to save them in plain. But as general rule, it's better to save the encryption key, that you only need to do that once, and them export it safely encrypted.",0,,False,0
1055,Validate emails in a table using Mailcheck,"This workflow allows you to validate emails stored in a table using the Mailcheck node. Airtable node: This node will list all the records from a table. Based on your use case, you might want to replace this node. Mailcheck node: This node will check the emails that got returned by the previous node. Set node: We will use the Set node to ensure that only the data that we set in this node gets passed on to the next nodes in the workflow. Airtable1 node: This node will update the Valid field in the table. Based on your use case, you might want to replace this node.",0,,False,0
1058,Send message on Mattermost when your n8n instance starts,"This workflow allows you to receive a message on Mattermost when your n8n instance starts. n8n Trigger node: The n8n Trigger node will trigger the workflow whenever the instance starts. Mattermost node: This node will send a message on Mattermost, notifying you when n8n starts.",0,,False,0
1059,Send a message on Mattermost when a workflow is updated,"This workflow allows you to send a message on Mattermost when a workflow is updated. Workflow Trigger node: The Workflow Trigger node will trigger the workflow when the workflow gets updated. Mattermost node: This node will send a message on Mattermost, notifying you about the update.",0,,False,0
1068,"Create, update, and retrieve a record from FileMaker","This workflow allows you to create, update, and retrieve a record from FileMaker. FileMaker node: This node will create a new record in FileMaker. FileMaker1 node: This node will add a new field to the record that we created in the previous node. FileMaker2 node: This node will get the information about the record that we created earlier.",0,,False,0
1069,Send location updates of the ISS to a topic in MQTT,"This workflow allows you to send position updates of the ISS every minute to a topic in MQTT using the MQTT node. Cron node: The Cron node will trigger the workflow every minute. HTTP Request node: This node will make a GET request to the API https://api.wheretheiss.at/v1/satellites/25544/positions to fetch the position of the ISS. This information gets passed on to the next node in the workflow. Set node: We will use the Set node to ensure that only the data that we set in this node gets passed on to the next nodes in the workflow. AWS SQS: This node will send the data from the previous node to the iss-position topic. If you have created a topic with a different one, you can use that topic instead.",0,,False,0
1073,Scrape and store data from multiple website pages,"This workflow allows extracting data from multiple pages website. The workflow: 1) Starts in a country list at https://www.theswiftcodes.com/browse-by-country/. 2) Loads every country page (https://www.theswiftcodes.com/albania/) 3) Paginates every page in the country page. 4) Extracts data from the country page. 5) Saves data to MongoDB. 6) Paginates through all pages in all countries. It uses getWorkflowStaticData('global') method to recover the next page (saved from the previous page), and it goes ahead with all the pages. There is a first section where the countries list is recovered and extracted. Later, I try to read if a local cache page is available and I recover the cached page from the disk. Finally, I save data to MongoDB, and we paginate all the pages in the country and for all the countries. I have applied a cache system to save a visited page to n8n local disk. If I relaunch workflow, we check if a cache file exists to discard non-required requests to the webpage. If the data present in the website changes, you can apply a Cron node to check the website once per week. Finally, before inserting data in MongoDB, the best way to avoid duplicates is to check that swift_code (the primary value of the collection) doesn't exist. I recommend using a proxy for all requests to avoid IP blocks. A good solution for proxy plus IP rotation is scrapoxy.io. This workflow is perfect for small data requirements. If you need to scrape dynamic data, you can use a Headless browser or any other service. If you want to scrape huge lists of URIs, I recommend using Scrapy + Scrapoxy.",0,,False,0
1074,Add liked songs to a Spotify monthly playlist,"üé∂ Add liked songs to a monthly playlist &gt; This Workflow is a port of Add saved songs to a monthly playlist from IFTTT. When you like a song, the workflow will save this song in a monthly playlist. E.g.: It's June 2024, I liked a song. The workflow will save this song in a playlist called June '24. If this playlist does not exist, the workflow will create it for me. ‚öô How it works Each 5 minutes, the workflow will start automatically. He will do 3 things : Get the last 10 songs you saved in the ""Liked song"" playlist (by clicking on the heart in the app) and save them in a NocoDB table (of course, the workflow avoid to create duplicates). Check if the monthly playlist is already created. Otherwise, the playlist is created. The created playlist is also saved in NocoDB to avoid any problems. Check if the monthly playlist contains all the song liked this month by getting them from NocoDB. If they are not present, add them one by one in the playlist. You may have a question regarding the need of NocoDB. Over the last few weeks/months, I've had duplication problems in my playlists and some playlists have been created twice because Spotify wasn't returning all the information but only partial information. Having the database means I don't have to rely on Spotify's data but on my own, which is accurate and represents reality. üìù Prerequisites You need to have : Spotify API keys, which you can obtain by creating a Spotify application here: https://developer.spotify.com/dashboard. Create a NocoDB API token üìö Instructions Follow the instructions below Create your Spotify API credential Create your NocoDB credential Populate all Spotify nodes with your credentials Populate all Spotify nodes with your credentials Enjoy ! If you need help, feel free to ping me on the N8N Discord server or send me a DM at ""LucasAlt"" Show your support Share your workflow on X and mention @LucasCtrlAlt Consider buying me a coffee üòâ",0,,False,0
1075,Filter the feedback from Typeform and store in Google Sheets,This workflow allows you to filter positive and negative feedback received from a Typeform and insert the data into Google Sheets. Typeform Trigger node: Start the workflow when a new form is submitted via Typeform Set node: Extract the information submitted in typeform IF node: Filter positive and negative reviews (i.e. ratings above or below 3 out of 5). Google Sheets node: Store the positive and negative reviews and ratings in two different sheets for each case.,0,,False,0
1076,Transfer data from website to Google Sheets,Take data from website form via webhook and save data into Google Sheets document!,0,,False,0
1078,Get an mp4 attachment from Gmail to Google Drive,This simple workflow allows you to get an mp4 attachment from gmail email and then upload it to Google Drive to get attachment webview link.,0,,False,1
1083,Create an event file and send it as an email attachment,This workflow allows you to create an event file and send it as an attachment via email. iCalendar node: This node will create an event file. Send Email: This node will send the event file as an attachment.,0,,False,1
1088,Add a new user to Notion database on Calendly invite creation,This workflow allows you to add a new user to your Notion database when an invite gets created via Calendly. Calendly Trigger node: The Calendly node will trigger the workflow when an invite gets created. Notion node: This node will create a new record using the information received from the previous node.,0,,False,0
1089,Receive a Mattermost message when new record gets added to Notion,"This workflow allows you to receive a Mattermost message when meeting notes get added to the Notion. Prerequisites Create a table in Notion similar to this: Meeting Notes Follow the steps mentioned in the documentation, to create credentials for the Notion Trigger node. Create create credentials for Mattermost. Notion Trigger: The Notion Trigger node will trigger the workflow when new data gets added to Notion. IF node: This node will check if the notes belong to the team Marketing. If the team is Marketing the node will true, otherwise false. Mattermost node: This node will send a message about the new data in the channel 'Marketing' in Mattermost. If you have a different channel, use that instead. You can even replace the Mattermost node with nodes of other messaging platforms, like Slack, Telegram, Discord, etc. NoOp node: Adding this node here is optional, as the absence of this node won't make a difference to the functioning of the workflow.",0,,False,0
1093,Build a self-hosted URL shortener with a dashboard,"This workflow creates an automatic self-hosted URL shortener. It consists of three sub-workflows: Short URL creation for extracting the provided long URL, generating an ID, and saving the record in the database. It returns a short link as a result. Redirection for extracting the ID value, validating the existence of its correspondent record in the database, and returning a redirection page after updating the visits (click) count. Dashboard for calculating simple statistics about the saved record and displaying them on a dashboard. Read more about this use case and how to set up the workflow in the blog post How to build a low-code, self-hosted URL shortener in 3 steps. Prerequisites A local proxy set up that redirects the n8n.ly domain to your n8n instance An Airtable account and credentials Basic knowledge of JavaScript, HTML, and CSS Nodes Webhook nodes trigger the sub-workflows on calls to a specified link. IF nodes route the workflows based on specified query parameters. Set nodes set the required values returned by the previous nodes (id, longUrl, and shortUrl). Airtable nodes retrieve records (values) from or append records to the database. Function node calculates statistics on link clicks to be displayed on the dashboard, as well as its design. Crypto node generates a SHA256 hash.",0,,False,1
1105,Check To Do on Notion and send message on Slack,"This workflow allows you to check the To-Do list on Notion and send a message on Slack. Prerequisites Create a Notion page similar to this page. Create credentials for Notion by following the instructions mentioned in the documentation. Follow the steps mentioned in the documentation to create credentials for Slack. Cron node: This node triggers the workflow every day. Notion node: This node fetches all the tasks from Notion. IF node: This node checks if the task is assigned to a particular user. Create a Direct Message: This node will create a direct message channel with the user. Send a Direct Message: This node will send the to-do lists in the direct message. NoOp: This node is connected to the false output of the IF node. If the condition is false, no further action will be taken.",0,,False,0
1107,Enrich and manage candidates data in Notion,This workflow allows you to add candidates‚Äô profile assessments to Notion before an interview. Prerequisites Add an input field on your Calendly Invite page where the candidate can enter their LinkedIn URL. Create credentials for your Calendly account. Follow the steps mentioned in the documentation to learn how to do that. Create credentials for Humantic AI following the steps mentioned here. Create a page on Notion similar to this page. Create credentials for the Notion node by following the steps in the documentation. Calendly Trigger node: This node will trigger the workflow when an interview gets scheduled. Make sure to add a field to collect the candidates' LinkedIn URL on your invite page. Humantic AI: This node uses the LinkedIn URL received by the previous node to create a candidate profile in Humantic AI. Humantic AI1: This node will analyze the candidates' profile. Notion node: This node will create a new page in Notion using the information from the previous node.,0,,True,0
1109,Add positive feedback messages to a table in Notion,"This workflow allows you to add positive feedback messages to a table in Notion. Prerequisites Create a Typeform that contains Long Text filed question type to accepts feedback from users. Get your Typeform credentials by following the steps mentioned in the documentation. Follow the steps mentioned in the documentation to create credentials for Google Cloud Natural Language. Create a page on Notion similar to this page. Create credentials for the Notion node by following the steps in the documentation. Follow the steps mentioned in the documentation to create credentials for Slack. Follow the steps mentioned in the documentation to create credentials for Trello. Typeform Trigger node: Whenever a user submits a response to the Typeform, the Typeform Trigger node will trigger the workflow. The node returns the response that the user has submitted in the form. Google Cloud Natural Language node: This node analyses the sentiment of the response the user has provided and gives a score. IF node: The IF node uses the score provided by the Google Cloud Natural Language node and checks if the score is positive (larger than 0). If the score is positive we get the result as True, otherwise False. Notion node: This node gets connected to the true branch of the IF node. It adds the positive feedback shared by the user in a table in Notion. Slack node: This node will share the positive feedback along with the score and username to a channel in Slack. Trello node: If the score is negative, the Trello node is executed. This node will create a card on Trello with the feedback from the user.",0,,False,0
1110,Add articles to a Notion list by accessing a Discord slash command,"This workflow allows you to add articles to a Notion reading list by accessing a Discord slash command. Prerequisites A Notion account and credentials, and a reading list similar to this template. A Discord account and credentials, and Discord Slash Command connected to n8n. Nodes Webhook node triggers the workflow whenever the Discord Slash command is issued. IF node checks the type returned by Discord. If the type is not equal to 1, it will return true, otherwise false. HTTP Request node makes an HTTP call to the link and gets the HTML of the webpage. HTML Extract node extracts the title from the HTML which we will use in the next node. Notion node adds the link to your Notion reading list. Set nodes set the reply values for Discord and register the Interaction Endpoint URL.",0,,False,0
1111,Create transcription jobs using AWS Transcribe,This workflow allows you to create transcription jobs for all your audio and video files stored in AWS S3. AWS S3: This node will retrieve all the files from an S3 bucket you specify. AWS Transcribe: This node will create a transcription job for the files that get returned by the previous node.,0,,False,0
1112,"Create, update, and get a monitor using UptimeRobot","This workflow allows you to create, update, and get a monitor using the UptimeRobot node. UptimeRobot node: This node creates a new monitor of the type HTTP(S). UptimeRobot1 node: This node will update the monitor that we created in the previous node. UptimeRobot2 node: This node will get the information of the monitor that we created in the previous node.",0,,False,0
1114,"Create, update and get a task in Microsoft To Do","This workflow allows you to create, update and get a task in Microsoft To Do. Microsoft To Do node: This node will create a task with the importance High in the Tasks list. You can select a different list as well as the importance level. Microsoft To Do1 node: This node will update the status of the task that we created in the previous node. Microsoft To Do2 node: This node will get the task that we created earlier.",0,,False,0
1115,Manage changes using the Git node,"This workflow allows you to add, commit, and push changes to a git repository. Git node: This node will add the README.md file to the staging area. If you want to add a different file, enter the path of that file instead. Git1 node: This node will commit all the changes that were added to the staging area by the previous node. Git2 node: This node will return the commit logs of your repository. Git3 node: This node will push the changes to a cloud repository.",0,,False,0
1118,Sync Google Calendar tasks to Trello every day,"This workflow will allow you at the beginning of each day to copy your google calendar events into Trello so you can take notes, label, or automate your tasks. When deploying this, don't forget to change: Label ID for meeting type under ""Create Trello Cards"". You should be able to find instructions Here on how to find the label ID. Description for Trello cards under ""Create Trello Cards"". I currently pull in notes but it should be simple to change to pull the Gcal description instead. You can change the trigger time to fire at a different time.",0,,False,0
1122,Database alerts with Notion and SIGNL4,"Objective In industry and production sometimes machine data is available in databases. That might be sensor data like temperature or pressure or just binary information. In this sample flow reads machine data and sends an alert to your SIGNL4 team when the machine is down. When the machine is up again the alert in SIGNL4 will get closed automatically. Setup We simulate the machine data using a Notion table. When we un-check the Up box we simulate a machine-down event. In certain intervals n8n checks the database for down items. If such an item has been found an alert is send using SIGNL4 and the item in Notion is updates (in order not to read it again). Status updates from SIGNL4 (acknowledgement, close, annotation, escalation, etc.) are received via webhook and we update the Notion item accordingly. This is how the alert looks like in the SIGNL4 app. The flow can be easily adapted to other database monitoring scenarios.",0,,False,0
1130,Add a check condition for a loop in n8n,This workflow demonstrates the use of $runIndex expression. It demonstrates how the expression can be used to avoid an infinite loop. The workflow will create 5 Tweets with the content 'Hello from n8n!'. You can use this workflow by replacing the Twitter node with any other node(s) and updating the condition in the IF node.,0,,False,0
1132,Trigger a build in Travis CI when code changes are push to a GitHub repo,"This workflow allows you to trigger a build in Travis CI when code changes are pushed to a GitHub repo or a pull request gets opened. GitHub Trigger node: This node will trigger the workflow when changes are pushed or when a pull request is created, updated, or deleted. IF node: This node checks for the action type. We want to trigger a build when code changes are pushed or when a pull request is opened. We don't want to build the project when a PR is closed or updated. TravisCI node: This node will trigger the build in Travis CI. If you're using CircleCI in your pipeline, replace the node with the CircleCI node. NoOp node: Adding this node is optional.",0,,False,0
1134,Release a new version via Telegram bot command,"This workflow allows you to release a new version via a Telegram bot command. This workflow can be used in your Continous Delivery pipeline. Telegram Trigger node: This node will trigger the workflow when a message is sent to the bot. If you want to trigger the workflow via a different messaging platform or a service, replace the Telegram Trigger node with the Trigger node of that service. IF node The IF node checks for the incoming command. If the command is not deploy, the IF node will return false, otherwise true. Set node: This node extracts the value of the version from the Telegram message and sets the value. This value is used later in the workflow. GitHub node: This node creates a new version release. It uses the version from the Set node to create the tag. NoOp node: Adding this node is optional.",0,,False,0
1150,Backup n8n workflows to Google Drive,"Temporary solution using the undocumented REST API for backups using Google drive. Please note that there are issues with this workflow. It does not support versioning, so please know that it will create multiple copies of the workflows so if you run this daily it will make the folder grow quickly. Once I figure out how to version in Gdrive I'll update it here.",0,,False,0
1153,Clean out unwanted emails from Gmail,Workflow to clean out unwanted email from Gmail. Further information here.,0,,False,0
1160,Merge data for multiple executions,"This workflow demonstrates how to merge data for different executions. The Merge Data Function node fetches the data from different executions of the RSS Feed Read node and merges them under a single object. Note: If you want to process the items that get merged, you will have to convert the single item into n8n understandable multiple items.",0,,False,0
1169,Handle pagination in HTTP Requests,"This example workflow demonstrates how to handle pagination. This example assumes that the API you are making the request to has pagination, and returns a cursor (something that points to the next page). This example workflow makes a request to the HubSpot API to fetch contacts. You will have to modify the parameters based on your API. Config URL node: This node sets the URL that the HTTP Request node calls. HTTP Request node: This node makes the API call and returns the data from the API. Based on your API, you will have to modify the parameters of the node. NoOp node and Wait node: These nodes help me avoiding any rate limits. If you're API has rate limits, make sure you configure the correct time in the Wait node. Check if pagination: This IF node checks if the API returns any cursor. If the API doesn't return any cursor, it means that there is no data to be fetched, and the node returns false. If the API returns a cursor, it means that there is still some data that needs to be fetched. In this case, the node returns true. Set next URL: This Set node is used to set the URL. In the next cycle, the HTTP Request node makes a call to this URL. Combine all data: This node combines all the data that gets returned by the API calls from the HTTP Request node.",0,,False,0
1205,Promote new Shopify products on Twitter and Telegram,"This workflow automatically promotes your new Shopify products on Twitter and Telegram. This workflow is also featured in the blog post 6 e-commerce workflows to power up your Shopify store. Prerequisites A Shopify account and credentials A Twitter account and credentials A Telegram account and credentials for the channel you want to send messages to. Nodes Shopify Trigger node triggers the workflow when you create a new product in Shopify. Twitter node posts a tweet with the text ""Hey there, my design is now on a new product! Visit my {shop name} to get this cool {product title} (and check out more {product type})"". Telegram node posts a message with the same text as above in a Telegram channel.",0,,False,1
1206,Process Shopify new orders with Zoho CRM and Harvest,"This workflow is triggered when a new order is created in Shopify. Then: the order information is stored in Zoho CRM, an invoice is created in Harvest and stored in Trello, if the order value is above 50, an email with a discount coupon is sent to the customer and they are added to a MailChimp campaign for high-value customers; otherwise, only a ""thank you"" email is sent to the customer. Note that you need to replace the List ID in the Trello node with your own ID (see instructions in our docs). Same goes for the Account ID in the Harvest node (see instructions here).",0,,False,1
1207,Run weekly inventories on Shopify sales,"This workflow is scheduled to run every week, when it gets all your Shopify orders, calculates their sales value, stores the data in Google Sheets, and sends a notification message to a Slack channel.",0,,False,1
1216,Detect toxic language in Telegram messages,"This workflow detects toxic language (such as profanity, insults, threats) in messages sent via Telegram. This blog tutorial explains how to configure the workflow nodes step-by-step. Telegram Trigger: triggers the workflow when a new message is sent in a Telegram chat. Google Perspective: analyzes the text of the message and returns a probability value between 0 and 1 of how likely it is that the content is toxic. IF: filters messages with a toxic probability value above 0.7. Telegram: sends a message in the chat with the text ""I don't tolerate toxic language"" if the probability value is above 0.7. NoOp: takes no action if the probability value is below 0.7.",0,,False,1
1217,Fetch a YouTube playlist and send new items Raindrop,"This simple workflow will fetch a YouTube playlist every n minutes and send the new items s to a collection in Raindrop. You can connect any application at the end of the flow. Make sure to authenticate to YouTube using Google Auth, and to Raindrop using an API. Update the Playlist ID and the Raindrop collection.",0,,False,0
1221,Create a Pipedrive activity on Calendly event scheduled,"This workflow is triggered when a meeting is scheduled via Calendly. Then, an activity is automatically created in Pipedrive and 15 minutes after the end of the meeting, a message is sent to the interviewer in Slack, reminding them to write down their notes and insights from the meeting.",0,,False,1
1222,Backup workflows to GitHub,"Note: This workflow uses the internal API which is not official. This workflow might break in the future. The workflow executes every night at 23:59. You can configure a different time bin the Cron node. Configure the GitHub nodes with your username, repo name, and the file path. In the HTTP Request nodes (making a request to localhost:5678), create Basic Auth credentials with your n8n instance username and password.",0,,False,1
1223,Capture leads in HubSpot from Typeform,"This workflow is triggered when a typeform is submitted, then it saves the sender's information into HubSpot as a new contact. Typeform Trigger: triggers the workflow when a typeform is submitted. Set: sets the fields for the values from Typeform. HubSpot 1: creates a new contact with information from Typeform. IF: filters contacts who expressed their interest in business services. HubSpot 2: updates the contact's stage to opportunity. Gmail: sends an email to the opportunity contacts with informational material. NoOp: takes no action for contacts who are not interested.",0,,False,1
1225,Export new deals from HubSpot to Slack and Airtable,"This workflow is triggered when a new deal is created in HubSpot. Then, it processes the deal based on its value and stage. The first branching follows three cases: If the deal is closed and won, a message is sent in a Slack channel, so that the whole team can celebrate the success. If a presentation has been scheduled for the deal, then a Google Slides presentation template is created. If the deal is closed and lost, the deal‚Äôs details are added to an Airtable table. From here, you can analyze the data to get insights into what and why certain deals don‚Äôt get closed. The second branching follows two cases: If the deal is for a new business and has a value above 500, a high-priority ticket assigned to an experienced team member is created in HubSpot If the deal is for an existing business and has a value below 500, a low-priority ticket is created.",0,,False,2
1236,Use Redis to rate-limit your low-code API,"This workflow demonstrates how to can use Redis to implement rate limits to your API. The workflow uses the incoming API key to uniquely identify the user and use it as a key in Redis. Every time a request is made, the value is incremented by one, and we check for the threshold using the IF node. Duplicate the following Airtable to try out the workflow: https://airtable.com/shraudfG9XAvqkBpF",0,,False,0
1243,Avoid rate limiting by batching HTTP requests,"This workflow demonstrates the use of the Split In Batches node and the Wait node to avoid API rate limits. Customer Datastore node: The workflow fetches data from the Customer Datastore node. Based on your use case, replace it with a relevant node. Split In Batches node: This node splits the items into a single item. Based on the API limit, you can configure the Batch Size. HTTP Request node: This node makes API calls to a placeholder URL. If the Split In Batches node returns 5 items, the HTTP Request node will make 5 different API calls. Wait node: This node will pause the workflow for the time you specify. On resume, the Split In Batches node gets executed node, and the next batch is processed. Replace Me (NoOp node): This node is optional. If you want to continue your workflow and process the items, replace this node with the corresponding node(s).",0,,False,0
1250,Parse Ycombinator news page,"Extract data from a webpage (Ycombinator news page) and create a nice list using itemList node. It seems that current version in n8n (0.141.1) requires to extract each variable one by one. Hopefully in a futute it will be possible to create the table using just one itemList node. Another nice feature of the workflow is an automatically generated file name with the resulting table. Check out the ""fileName"" option of the Spreadsheet File node: ""Ycombinator_news_{{new Date().toISOString().split('T', 1)[0]}}.{{$parameter[\""fileFormat\""]}}"" The resulting table is saved as .xls file and delivered via email",0,,False,1
1253,Add Netlify Form submissions to Airtable,"This workflow demonstrates how to use the Netlify Trigger node to capture form submissions and add it Airtable. You can reuse the workflow to add the data to another similar database by replacing the Airtable node with the corresponding node. Netlify Trigger node: This node triggers the workflow when a new form is submitted. Select your site from the Site Name/ID dropdown list and the form from the Form ID dropdown list. Set node: This node extract the required data from the Netlify Trigger node. In this example, we only want to add the Name, Email, and Role of the user. Airtable node: This node appends the data to Airtable. If you want the data to Google Sheets or a database, replace this node with the corresponding node.",0,,False,0
1254,Deploy site when new content gets added,"This workflow demonstrates how to create a new deployment when new content gets added to the database. This example workflow can be used when building a JAMstack site. Webhook node: This node triggers the workflow when new content gets added. For this example, we have configured the webhook in GraphCMS. Netlify node: This node will start the build process and deploy the website. You will have to select your site from the Site ID dropdown list. To identify the deployment, we are passing a title.",0,,False,0
1255,Send notification when deployment fails,"This workflow sends a message on Slack when site deployment fails. Netlify Trigger node: This node triggers the workflow when the site deployment fails. Slack node: This node sends a message on Slack alerting the team about the failed deployment. If you want to send a message to a different platform, replace the Slack node with the node of the respective platform.",0,,False,0
1265,Post unassigned Zendesk tickets to Slack,"&gt; This has been updated to support the Query feature added to the Zendesk node in 0.144.0 This workflow will post all New and Open tickets without an agent assigned to a Slack channel on a schedule. The function node is used in this example to merge multiple inputs into one output message which is then used as the Slack message. The output in Slack will be similar to the below message, The ""TICKET_ID"" will be a link to the ticket. &gt; Unassigned Tickets TICKET_ID [STATUS] - TICKET_SUBJECT Usage Update the Cron schedule, The default value is 16:30 daily. Update the Credentials in the Zendesk nodes Update the Credentials and Channel in the Slack Node Grab a coffee and enjoy! Zendesk Query In the Zendesk node we are using the query assignee:none status&lt;pending this returns all New and Open tickets with no assignee allowing us to remove the extra nodes.",0,,False,0
1274,Assign issues to interested contributors,"This workflow handles the incoming issues and issues comments for your open-source project. If a contributor is interested, the workflow will assign them the issue. Note: For organizations, you will have to use the Webhook node to trigger the workflow. You will also have to use the HTTP Request node instead of the regular GitHub node. You can learn more about this workflow by reading the blog on https://n8n.io/blog.",0,,False,0
2500,Create an automated workitem(incident/bug/userstory) in azure devops,Who is this template for? This template can be used by any automator who wants to create a workitem(incident/user story/bugs) in azure devops whenever an alert raised by systems. How it works Each time an alert raised in system( for ex: Elastic raises an alert for missing host or domain). Workflow reads an alert and creates a workitem in azure devops Workflow can be customized to send any required information as possible in azure devops Setup Instructions Azure DevOps Organization and Project:** Make sure you have access to an Azure DevOps organization and a project where the work item will be created. Personal Access Token (PAT):** You need a Personal Access Token with permissions to create work items. You can generate a PAT from the Azure DevOps user settings.,0,2024-10-26 08:18:46.232000+00:00,False,2
