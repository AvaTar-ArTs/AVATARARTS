<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Python Code Gallery - Modern Card View</title>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-tomorrow.min.css" rel="stylesheet">
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: 'Inter', -apple-system, BlinkMacSystemFont, sans-serif;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            min-height: 100vh;
            color: #333;
        }

        .app {
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }

        /* Header */
        .header {
            background: rgba(255, 255, 255, 0.95);
            backdrop-filter: blur(20px);
            padding: 3rem 0;
            text-align: center;
            box-shadow: 0 8px 32px rgba(0, 0, 0, 0.1);
            border-bottom: 1px solid rgba(255, 255, 255, 0.2);
        }

        .header-content h1 {
            font-size: 4rem;
            font-weight: 800;
            background: linear-gradient(135deg, #667eea, #764ba2, #f093fb);
            -webkit-background-clip: text;
            -webkit-text-fill-color: transparent;
            margin-bottom: 1rem;
            letter-spacing: -0.02em;
        }

        .header-content p {
            font-size: 1.4rem;
            color: #666;
            font-weight: 400;
            max-width: 600px;
            margin: 0 auto;
        }

        /* Controls */
        .controls {
            background: rgba(255, 255, 255, 0.9);
            backdrop-filter: blur(20px);
            padding: 2rem;
            display: flex;
            gap: 1.5rem;
            align-items: center;
            justify-content: center;
            flex-wrap: wrap;
            box-shadow: 0 4px 20px rgba(0, 0, 0, 0.1);
            border-bottom: 1px solid rgba(255, 255, 255, 0.2);
        }

        .search-container {
            position: relative;
            flex: 1;
            max-width: 500px;
        }

        #searchInput {
            width: 100%;
            padding: 1rem 1.5rem 1rem 3.5rem;
            border: 2px solid #e1e5e9;
            border-radius: 50px;
            font-size: 1.1rem;
            outline: none;
            transition: all 0.3s ease;
            background: rgba(255, 255, 255, 0.8);
        }

        #searchInput:focus {
            border-color: #667eea;
            box-shadow: 0 0 0 4px rgba(102, 126, 234, 0.1);
            background: white;
        }

        .search-icon {
            position: absolute;
            left: 1.5rem;
            top: 50%;
            transform: translateY(-50%);
            color: #999;
            font-size: 1.4rem;
        }

        .filter-container {
            display: flex;
            gap: 1rem;
            align-items: center;
        }

        select, .sort-btn {
            padding: 1rem 1.5rem;
            border: 2px solid #e1e5e9;
            border-radius: 12px;
            font-size: 1rem;
            background: white;
            cursor: pointer;
            transition: all 0.3s ease;
            font-weight: 500;
        }

        select:focus, .sort-btn:focus {
            outline: none;
            border-color: #667eea;
        }

        .sort-btn {
            background: linear-gradient(135deg, #667eea, #764ba2);
            color: white;
            border: none;
            font-weight: 600;
        }

        .sort-btn:hover {
            transform: translateY(-2px);
            box-shadow: 0 8px 25px rgba(102, 126, 234, 0.3);
        }

        /* Stats */
        .stats {
            background: rgba(255, 255, 255, 0.9);
            backdrop-filter: blur(20px);
            padding: 1.5rem 2rem;
            display: flex;
            justify-content: center;
            gap: 4rem;
            box-shadow: 0 4px 20px rgba(0, 0, 0, 0.1);
        }

        .stat {
            text-align: center;
        }

        .stat-number {
            display: block;
            font-size: 2.5rem;
            font-weight: 800;
            background: linear-gradient(135deg, #667eea, #764ba2);
            -webkit-background-clip: text;
            -webkit-text-fill-color: transparent;
        }

        .stat-label {
            font-size: 1rem;
            color: #666;
            text-transform: uppercase;
            letter-spacing: 1px;
            font-weight: 600;
        }

        /* Code Grid */
        .code-grid {
            flex: 1;
            padding: 3rem;
            display: grid;
            grid-template-columns: repeat(auto-fill, minmax(450px, 1fr));
            gap: 2rem;
            max-width: 1600px;
            margin: 0 auto;
            width: 100%;
        }

        /* Code Cards */
        .code-card {
            background: rgba(255, 255, 255, 0.95);
            backdrop-filter: blur(20px);
            border-radius: 20px;
            padding: 2rem;
            box-shadow: 0 10px 40px rgba(0, 0, 0, 0.1);
            transition: all 0.4s cubic-bezier(0.175, 0.885, 0.32, 1.275);
            border: 1px solid rgba(255, 255, 255, 0.3);
            cursor: pointer;
            position: relative;
            overflow: hidden;
        }

        .code-card::before {
            content: '';
            position: absolute;
            top: 0;
            left: 0;
            right: 0;
            height: 4px;
            background: linear-gradient(135deg, #667eea, #764ba2, #f093fb);
        }

        .code-card:hover {
            transform: translateY(-12px) scale(1.02);
            box-shadow: 0 20px 60px rgba(0, 0, 0, 0.15);
        }

        .card-header {
            display: flex;
            align-items: flex-start;
            gap: 1.5rem;
            margin-bottom: 1.5rem;
        }

        .file-icon {
            font-size: 3rem;
            flex-shrink: 0;
            filter: drop-shadow(0 4px 8px rgba(0, 0, 0, 0.1));
        }

        .file-info {
            flex: 1;
            min-width: 0;
        }

        .file-name {
            font-size: 1.5rem;
            font-weight: 700;
            color: #333;
            margin-bottom: 0.5rem;
            white-space: nowrap;
            overflow: hidden;
            text-overflow: ellipsis;
        }

        .file-path {
            font-size: 0.9rem;
            color: #666;
            font-family: 'Monaco', 'Menlo', monospace;
            white-space: nowrap;
            overflow: hidden;
            text-overflow: ellipsis;
            background: rgba(102, 126, 234, 0.1);
            padding: 0.25rem 0.5rem;
            border-radius: 6px;
            display: inline-block;
        }

        .file-stats {
            display: flex;
            flex-direction: column;
            align-items: flex-end;
            gap: 0.5rem;
            font-size: 0.85rem;
            color: #999;
        }

        .stat-badge {
            background: linear-gradient(135deg, #667eea, #764ba2);
            color: white;
            padding: 0.25rem 0.75rem;
            border-radius: 12px;
            font-weight: 600;
            font-size: 0.8rem;
        }

        .card-body {
            margin-bottom: 1.5rem;
        }

        .file-description {
            font-size: 1rem;
            color: #555;
            line-height: 1.6;
            margin-bottom: 1.5rem;
            display: -webkit-box;
            -webkit-line-clamp: 3;
            -webkit-box-orient: vertical;
            overflow: hidden;
        }

        .keywords {
            display: flex;
            flex-wrap: wrap;
            gap: 0.75rem;
            margin-bottom: 1.5rem;
        }

        .keyword {
            background: linear-gradient(135deg, #f0f2ff, #e8ecff);
            color: #667eea;
            padding: 0.5rem 1rem;
            border-radius: 20px;
            font-size: 0.8rem;
            font-weight: 600;
            border: 1px solid rgba(102, 126, 234, 0.2);
        }

        .functions {
            font-size: 0.9rem;
            color: #666;
            font-family: 'Monaco', 'Menlo', monospace;
            background: #f8f9fa;
            padding: 1rem;
            border-radius: 12px;
            white-space: nowrap;
            overflow: hidden;
            text-overflow: ellipsis;
            border-left: 4px solid #667eea;
        }

        .card-footer {
            border-top: 1px solid #e1e5e9;
            padding-top: 1.5rem;
        }

        .preview-code {
            background: #1e1e1e;
            border-radius: 12px;
            padding: 1.5rem;
            margin-bottom: 1.5rem;
            overflow: hidden;
            max-height: 200px;
            border: 1px solid #333;
        }

        .preview-code pre {
            margin: 0;
            font-size: 0.85rem;
            line-height: 1.5;
        }

        .preview-code code {
            color: #d4d4d4;
            font-family: 'Monaco', 'Menlo', monospace;
        }

        .view-code-btn {
            width: 100%;
            background: linear-gradient(135deg, #667eea, #764ba2);
            color: white;
            border: none;
            padding: 1rem 1.5rem;
            border-radius: 12px;
            font-size: 1rem;
            font-weight: 600;
            cursor: pointer;
            transition: all 0.3s ease;
            text-transform: uppercase;
            letter-spacing: 0.5px;
        }

        .view-code-btn:hover {
            transform: translateY(-3px);
            box-shadow: 0 8px 25px rgba(102, 126, 234, 0.4);
        }

        /* Modal */
        .modal {
            display: none;
            position: fixed;
            top: 0;
            left: 0;
            width: 100%;
            height: 100%;
            background: rgba(0, 0, 0, 0.9);
            backdrop-filter: blur(10px);
            z-index: 1000;
            overflow-y: auto;
        }

        .modal-content {
            background: white;
            margin: 2rem auto;
            max-width: 95%;
            width: 1200px;
            border-radius: 20px;
            box-shadow: 0 25px 80px rgba(0, 0, 0, 0.3);
            overflow: hidden;
        }

        .modal-header {
            background: linear-gradient(135deg, #667eea, #764ba2);
            color: white;
            padding: 2rem;
            display: flex;
            justify-content: space-between;
            align-items: center;
        }

        .modal-header h2 {
            font-size: 2rem;
            font-weight: 700;
        }

        .close-btn {
            background: none;
            border: none;
            color: white;
            font-size: 2.5rem;
            cursor: pointer;
            padding: 0;
            width: 50px;
            height: 50px;
            display: flex;
            align-items: center;
            justify-content: center;
            border-radius: 50%;
            transition: all 0.3s ease;
        }

        .close-btn:hover {
            background: rgba(255, 255, 255, 0.2);
            transform: rotate(90deg);
        }

        .modal-body {
            padding: 2rem;
        }

        .file-info {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));
            gap: 1.5rem;
            margin-bottom: 2rem;
            padding: 1.5rem;
            background: #f8f9fa;
            border-radius: 12px;
        }

        .info-item {
            display: flex;
            flex-direction: column;
            gap: 0.5rem;
        }

        .info-label {
            font-size: 0.9rem;
            color: #666;
            text-transform: uppercase;
            letter-spacing: 1px;
            font-weight: 600;
        }

        .info-value {
            font-size: 1rem;
            color: #333;
            font-family: 'Monaco', 'Menlo', monospace;
            font-weight: 500;
        }

        .code-preview {
            background: #1e1e1e;
            border-radius: 12px;
            overflow: hidden;
            max-height: 600px;
            overflow-y: auto;
            border: 1px solid #333;
        }

        .code-preview pre {
            margin: 0;
            padding: 2rem;
        }

        .code-preview code {
            color: #d4d4d4;
            font-family: 'Monaco', 'Menlo', monospace;
            font-size: 0.9rem;
            line-height: 1.6;
        }

        /* Responsive Design */
        @media (max-width: 768px) {
            .header-content h1 {
                font-size: 2.5rem;
            }
            
            .controls {
                flex-direction: column;
                align-items: stretch;
            }
            
            .search-container {
                max-width: none;
            }
            
            .filter-container {
                justify-content: center;
            }
            
            .stats {
                gap: 2rem;
            }
            
            .code-grid {
                grid-template-columns: 1fr;
                padding: 2rem 1rem;
            }
            
            .modal-content {
                margin: 1rem;
                max-width: calc(100% - 2rem);
            }
            
            .modal-body {
                padding: 1rem;
            }
        }

        /* Animation for cards */
        @keyframes fadeInUp {
            from {
                opacity: 0;
                transform: translateY(40px);
            }
            to {
                opacity: 1;
                transform: translateY(0);
            }
        }

        .code-card {
            animation: fadeInUp 0.8s ease-out;
        }

        /* Hidden class for filtering */
        .hidden {
            display: none !important;
        }

        /* Loading animation */
        .loading {
            display: flex;
            justify-content: center;
            align-items: center;
            height: 200px;
            font-size: 1.2rem;
            color: #666;
        }

        .spinner {
            width: 40px;
            height: 40px;
            border: 4px solid #f3f3f3;
            border-top: 4px solid #667eea;
            border-radius: 50%;
            animation: spin 1s linear infinite;
            margin-right: 1rem;
        }

        @keyframes spin {
            0% { transform: rotate(0deg); }
            100% { transform: rotate(360deg); }
        }
    </style>
</head>
<body>
    <div class="app">
        <!-- Header -->
        <div class="header">
            <div class="header-content">
                <h1>Python Code Gallery</h1>
                <p>Explore your organized Python projects with beautiful card-based interface</p>
            </div>
        </div>

        <!-- Controls -->
        <div class="controls">
            <div class="search-container">
                <span class="search-icon">üîç</span>
                <input type="text" id="searchInput" placeholder="Search files, functions, or keywords...">
            </div>
            <div class="filter-container">
                <select id="categoryFilter">
                    <option value="">All Categories</option>
                    <option value="07_experimental">Experimental (12)</option>
<option value="00_shared_libraries">Shared Libraries (4)</option>
<option value="01_core_ai_analysis">AI Analysis & Transcription (1152)</option>
<option value="02_media_processing">Media Processing (1048)</option>
<option value="06_development_tools">Development Tools (213)</option>
<option value="03_automation_platforms">Platform Automation (262)</option>
<option value="05_data_management">Data Management (86)</option>

                </select>
                <select id="typeFilter">
                    <option value="">All Types</option>
                    <option value="transcription">Transcription</option>
                    <option value="analysis">Analysis</option>
                    <option value="youtube">YouTube</option>
                    <option value="image_processing">Image Processing</option>
                    <option value="video_processing">Video Processing</option>
                    <option value="audio_processing">Audio Processing</option>
                    <option value="web_tools">Web Tools</option>
                    <option value="data_processing">Data Processing</option>
                    <option value="testing">Testing</option>
                    <option value="utility">Utility</option>
                </select>
                <button class="sort-btn" id="sortBtn">Sort A-Z</button>
            </div>
        </div>

        <!-- Stats -->
        <div class="stats">
            <div class="stat">
                <span class="stat-number" id="totalCount">2777</span>
                <span class="stat-label">Total Files</span>
            </div>
            <div class="stat">
                <span class="stat-number" id="visibleCount">2777</span>
                <span class="stat-label">Visible</span>
            </div>
            <div class="stat">
                <span class="stat-number" id="categoriesCount">7</span>
                <span class="stat-label">Categories</span>
            </div>
        </div>

        <!-- Code Grid -->
        <div class="code-grid" id="codeGrid">
            <!-- Cards will be generated by JavaScript -->
        </div>
    </div>

    <!-- Modal -->
    <div class="modal" id="codeModal">
        <div class="modal-content">
            <div class="modal-header">
                <h2 id="modalTitle">File Name</h2>
                <button class="close-btn" id="closeModal">&times;</button>
            </div>
            <div class="modal-body">
                <div class="file-info">
                    <div class="info-item">
                        <span class="info-label">Path</span>
                        <span class="info-value" id="modalPath">/path/to/file.py</span>
                    </div>
                    <div class="info-item">
                        <span class="info-label">Lines</span>
                        <span class="info-value" id="modalLines">0</span>
                    </div>
                    <div class="info-item">
                        <span class="info-label">Size</span>
                        <span class="info-value" id="modalSize">0 KB</span>
                    </div>
                    <div class="info-item">
                        <span class="info-label">Type</span>
                        <span class="info-value" id="modalType">Python</span>
                    </div>
                </div>
                <div class="code-preview">
                    <pre><code class="language-python" id="modalCode"># Code will be displayed here</code></pre>
                </div>
            </div>
        </div>
    </div>

    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-core.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/plugins/autoloader/prism-autoloader.min.js"></script>
    <script>
        // Data from Python analysis
        const filesData = [
  {
    "id": "1",
    "name": "generate_code_gallery.py",
    "path": "generate_code_gallery.py",
    "category": "07_experimental",
    "type": "analysis",
    "lines": 1382,
    "size": 49754,
    "docstring": "Python Code Gallery Generator\n============================\n\nThis script scans your organized Python directory and generates a beautiful\ncard-based gallery showing all your Python projects with metadata, code previews,\nand interactive features.\n\nUsage:\n    python generate_code_gallery.py [--port 8000] [--host localhost]\n\nFeatures:\n    - Scans all Python files in organized directory structure\n    - Extracts metadata (functions, docstrings, keywords)\n    - Generates beautiful card-based HTML gallery\n    - Provides web server for interactive browsing\n    - Real-time search and filtering",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "main",
      "__init__",
      "analyze_python_file",
      "basic_file_analysis",
      "determine_file_type",
      "get_category_from_path",
      "scan_directory",
      "generate_categories",
      "get_category_display_name",
      "generate_html"
    ],
    "classes": [
      "PythonCodeAnalyzer",
      "CodeGalleryServer"
    ],
    "imports": [
      "os",
      "json",
      "ast",
      "argparse",
      "pathlib",
      "datetime",
      "typing",
      "http.server",
      "socketserver",
      "webbrowser"
    ],
    "preview": "#!/usr/bin/env python3\n\"\"\"\nPython Code Gallery Generator\n============================\n\nThis script scans your organized Python directory and generates a beautiful\ncard-based gallery showing all your Python projects with metadata, code previews,\nand interactive features.\n\nUsage:\n    python generate_code_gallery.py [--port 8000] [--host localhost]\n\nFeatures:\n    - Scans all Python files in organized directory structure\n    - Extracts metadata (functions, docstrings, keywords)\n    - Generates beautiful card-based HTML gallery\n    - Provides web server for interactive browsing\n    - Real-time search and filtering\n\"\"\"\n",
    "last_modified": "2025-10-09T07:02:09.598622"
  },
  {
    "id": "2",
    "name": "setup.py",
    "path": "github_repo/setup.py",
    "category": "07_experimental",
    "type": "setup",
    "lines": 36,
    "size": 999,
    "docstring": "Setup script for Python Projects Collection",
    "keywords": [],
    "functions": [
      "main"
    ],
    "classes": [],
    "imports": [
      "subprocess",
      "sys",
      "pathlib"
    ],
    "preview": "#!/usr/bin/env python3\n\"\"\"\nSetup script for Python Projects Collection\n\"\"\"\n\nimport subprocess\nimport sys\nfrom pathlib import Path\n\ndef main():\n    print(\"\ud83d\udc0d Setting up Python Projects Collection...\")\n    \n    # Check Python version\n    if sys.version_info < (3, 8):\n        print(\"\u274c Python 3.8+ required\")\n        sys.exit(1)\n    \n    print(\"\u2705 Python version OK\")\n    \n    # Install optional dependencies",
    "last_modified": "2025-10-09T06:38:42.091566"
  },
  {
    "id": "3",
    "name": "common_imports.py",
    "path": "00_shared_libraries/common_imports.py",
    "category": "00_shared_libraries",
    "type": "utility",
    "lines": 24,
    "size": 640,
    "docstring": "Common imports used across projects",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [],
    "preview": "\"\"\"Common imports used across projects\"\"\"\n\n# Most frequently used imports\n# os (used in 843 files)\n# sys (used in 252 files)\n# csv (used in 219 files)\n# json (used in 202 files)\n# requests (used in 177 files)\n# time (used in 154 files)\n# load_dotenv (used in 144 files)\n# logging (used in 142 files)\n# Path (used in 130 files)\n# datetime (used in 130 files)\n# OpenAI (used in 127 files)\n# re (used in 120 files)\n# Image (used in 115 files)\n# subprocess (used in 114 files)\n# shutil (used in 100 files)\n# tqdm (used in 100 files)\n# pandas (used in 99 files)",
    "last_modified": "2025-10-09T06:17:16.281716"
  },
  {
    "id": "4",
    "name": "utility_functions.py",
    "path": "00_shared_libraries/utility_functions.py",
    "category": "00_shared_libraries",
    "type": "utility",
    "lines": 24,
    "size": 809,
    "docstring": "Common utility functions used across projects",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [],
    "preview": "\"\"\"Common utility functions used across projects\"\"\"\n\n# Most frequently used functions\n# __init__ (used in 234 files)\n# main (used in 182 files)\n# format_timestamp (used in 62 files)\n# analyze_text_for_section (used in 56 files)\n# transcribe_audio (used in 50 files)\n# upscale_image (used in 34 files)\n# get_authenticated_service (used in 31 files)\n# upload_video (used in 28 files)\n# process_audio_directory (used in 26 files)\n# run (used in 25 files)\n# organize_files (used in 25 files)\n# process_directory (used in 23 files)\n# process_images (used in 23 files)\n# __str__ (used in 23 files)\n# convert_and_upscale_images (used in 23 files)\n# parse_args (used in 21 files)\n# get_creation_date (used in 20 files)",
    "last_modified": "2025-10-09T06:17:16.282631"
  },
  {
    "id": "5",
    "name": "cli.py_02.py",
    "path": "01_core_ai_analysis/cli.py_consolidated/cli.py_02.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 32,
    "size": 856,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "cli",
      "process_image"
    ],
    "classes": [],
    "imports": [
      "click",
      "chaos_scheduler",
      "quantum_media_processor"
    ],
    "preview": "import click\n\nfrom .chaos_scheduler import ChaosScheduler\nfrom .quantum_media_processor import QuantumMediaProcessor\n\n\n@click.group()\ndef cli():\n    \"\"\"QuantumForge CLI - Where Order Meets Chaos\"\"\"\n    pass\n\n\n@cli.command()\n@click.option(\"--chaos\", default=0.07, help=\"Chaos factor (0.0-1.0)\")\n@click.argument(\"input_path\")\n@click.argument(\"output_path\")\ndef process_image(chaos, input_path, output_path):\n    \"\"\"Quantum-inspired image processing\"\"\"\n    qmp = QuantumMediaProcessor(chaos_factor=chaos)\n    scheduler = ChaosScheduler()",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "6",
    "name": "cli.py.py",
    "path": "01_core_ai_analysis/cli.py_consolidated/cli.py.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 39,
    "size": 1197,
    "docstring": "",
    "keywords": [],
    "functions": [
      "main"
    ],
    "classes": [],
    "imports": [
      "__future__",
      "argparse",
      "json",
      "os",
      "sys",
      "pipeline"
    ],
    "preview": "from __future__ import annotations\n\nimport argparse\nimport json\nimport os\nimport sys\n\nfrom .pipeline import OpusClonePipeline\n\n\ndef main():\n    p = argparse.ArgumentParser(\n        description=\"Opus-style local video-to-shorts pipeline (open-source clone)\"\n    )\n    p.add_argument(\"--video\", required=True, help=\"Path to input video (mp4/mov/etc.)\")\n    p.add_argument(\"--out\", required=True, help=\"Output directory for clips\")\n    p.add_argument(\"--brand\", required=True, help=\"Path to brand.json template\")\n    p.add_argument(\"--k\", type=int, default=5, help=\"Number of clips to export\")\n    p.add_argument(\n        \"--whisper\",",
    "last_modified": "2025-09-11T13:27:06.797107"
  },
  {
    "id": "7",
    "name": "audio_to_text.py",
    "path": "01_core_ai_analysis/transcription/audio_to_text.py",
    "category": "01_core_ai_analysis",
    "type": "transcription",
    "lines": 753,
    "size": 28563,
    "docstring": "AudioToText.ipynb",
    "keywords": [
      "openai",
      "transcription"
    ],
    "functions": [
      "get_audio",
      "add_chunk",
      "raw_split",
      "write_result",
      "write_result"
    ],
    "classes": [
      "WriteText(WriteTXT)"
    ],
    "imports": [
      "subprocess",
      "sys",
      "platform",
      "io",
      "base64",
      "os.path",
      "ffmpeg",
      "numpy",
      "google.colab.output",
      "IPython.display"
    ],
    "preview": "# -*- coding: utf-8 -*-\n\"\"\"AudioToText.ipynb\n\nAutomatically generated by Colab.\n\nOriginal file is located at\n    https://colab.research.google.com/github/Carleslc/AudioToText/blob/master/AudioToText.ipynb\n\n# \ud83d\udde3\ufe0f [**AudioToText**](https://github.com/Carleslc/AudioToText)\n\n[![Donate](https://www.ko-fi.com/img/githubbutton_sm.svg)](https://ko-fi.com/carleslc)\n\n### \ud83d\udee0 [Whisper by OpenAI](https://github.com/openai/whisper)\n\n## [Step 1] \u2699\ufe0f Install the required libraries\n\nClick \u25b6\ufe0f button below to install the dependencies for this notebook.\n\"\"\"\n\n#@title { display-mode: \"form\" }",
    "last_modified": "2025-09-11T13:26:54.712610"
  },
  {
    "id": "8",
    "name": "quiz-.py",
    "path": "01_core_ai_analysis/transcription/quiz-.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 59,
    "size": 2128,
    "docstring": "",
    "keywords": [
      "image_processing",
      "youtube",
      "analysis",
      "web_tools",
      "openai"
    ],
    "functions": [
      "upscale_image",
      "generate_youtube_content",
      "analyze_and_generate"
    ],
    "classes": [],
    "imports": [
      "openai",
      "csv",
      "io",
      "requests",
      "PIL"
    ],
    "preview": "from openai import OpenAI\n\nclient = OpenAI(api_key='sk-acw35nWnLFMd23JUzcQoQ7QBhg4y5wLxoQJpR64ITBWHqBT7')\nimport csv\nfrom io import BytesIO\n\nimport requests\nfrom PIL import Image\n\n# Set your OpenAI API key here\n\ndef upscale_image(image_url):\n    # Fetch the image\n    response = requests.get(image_url)\n    image = Image.open(BytesIO(response.content))\n\n    # Calculate the new size, doubling the width and height\n    new_size = (image.width * 2, image.height * 2)\n\n    # Resize the image to the new size",
    "last_modified": "2025-05-04T22:47:13.043882"
  },
  {
    "id": "9",
    "name": "tts-doc.py",
    "path": "01_core_ai_analysis/transcription/tts-doc.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 220,
    "size": 7712,
    "docstring": "",
    "keywords": [
      "openai"
    ],
    "functions": [
      "load_home_env",
      "read_docx_text",
      "make_heading_regex",
      "slugify",
      "normalize_body",
      "extract_chapters",
      "ensure_voice",
      "openai_client",
      "synth_mp3_bytes",
      "write_jsonl"
    ],
    "classes": [],
    "imports": [
      "os",
      "re",
      "json",
      "argparse",
      "pathlib",
      "typing",
      "dotenv",
      "docx",
      "openai"
    ],
    "preview": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\nDOCX -> per-chapter MP3s using OpenAI TTS.\n- Keys from ~/.env  (OPENAI_API_KEY=sk-...)\n- Chapter detection via docx heading styles OR fallback regex for known titles\n- Voices: alloy, ash, ballad, coral, echo, fable, nova, onyx, sage, shimmer\n- Alias: cove -> sage\n\"\"\"\n\nimport os\nimport re\nimport json\nimport argparse\nfrom pathlib import Path\nfrom typing import List, Tuple, Iterable, Dict\n\n# -------- env loader (quiet) --------\ndef load_home_env():\n    try:",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "10",
    "name": "generate_screen_shot.py",
    "path": "01_core_ai_analysis/transcription/generate_screen_shot.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 120,
    "size": 4242,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "logging",
      "os",
      "shutil",
      "time",
      "pyrogram",
      "translation",
      "helper_funcs.chat_base",
      "helper_funcs.display_progress",
      "helper_funcs.help_Nekmo_ffmpeg",
      "sample_config"
    ],
    "preview": "import logging\n\nlogging.basicConfig(\n    level=logging.DEBUG, format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\n)\nlogger = logging.getLogger(__name__)\n\nimport os\nimport shutil\nimport time\n\n# the secret configuration specific things\nif bool(os.environ.get(\"WEBHOOK\", False)):\n    from sample_config import Config\nelse:\n    from config import Config\n\n# the Strings used for this \"thing\"\nimport pyrogram\nfrom translation import Translation",
    "last_modified": "2025-09-13T05:53:44.156301"
  },
  {
    "id": "11",
    "name": "ocr_gpt_renamer.py",
    "path": "01_core_ai_analysis/transcription/ocr_gpt_renamer.py",
    "category": "01_core_ai_analysis",
    "type": "youtube",
    "lines": 422,
    "size": 13808,
    "docstring": "Recursive OCR + GPT-4o Renamer & Prompt Generator (Noise-Resistant)\n\n- Images: .jpg .jpeg .png .webp .tiff\n- Videos: .mp4 .webm .mov .mkv\n- Loads OPENAI_API_KEY from ~/env\n- Preview mode by default (no changes). Use --apply / -a to rename.\n- CSV (TAB-delimited): Old Path | New Path | Detected Title | File Type | Prompt\n\nmacOS setup:\n  brew install tesseract\n  pip install pillow pytesseract opencv-python python-dotenv openai\n\nRun (preview):\n  python ocr_gpt_renamer.py /path/to/folder\n\nApply:\n  python ocr_gpt_renamer.py /path/to/folder --apply",
    "keywords": [
      "openai",
      "opencv",
      "analysis"
    ],
    "functions": [
      "load_api_key_from_home_env",
      "_preprocess_for_ocr",
      "_ocr_data",
      "_clean_token",
      "_good_token",
      "_titlecase_join",
      "ocr_detect_title",
      "best_video_frame",
      "gpt_refine_and_describe",
      "process_media"
    ],
    "classes": [],
    "imports": [
      "argparse",
      "csv",
      "os",
      "re",
      "sys",
      "pathlib",
      "typing",
      "cv2",
      "openai",
      "pytesseract"
    ],
    "preview": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n\"\"\"\nRecursive OCR + GPT-4o Renamer & Prompt Generator (Noise-Resistant)\n\n- Images: .jpg .jpeg .png .webp .tiff\n- Videos: .mp4 .webm .mov .mkv\n- Loads OPENAI_API_KEY from ~/env\n- Preview mode by default (no changes). Use --apply / -a to rename.\n- CSV (TAB-delimited): Old Path | New Path | Detected Title | File Type | Prompt\n\nmacOS setup:\n  brew install tesseract\n  pip install pillow pytesseract opencv-python python-dotenv openai\n\nRun (preview):\n  python ocr_gpt_renamer.py /path/to/folder\n\nApply:",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "12",
    "name": "trek 2.py",
    "path": "01_core_ai_analysis/transcription/trek 2.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 58,
    "size": 2102,
    "docstring": "",
    "keywords": [
      "analysis",
      "web_tools"
    ],
    "functions": [
      "generate_speech",
      "process_csv_and_generate_speech"
    ],
    "classes": [],
    "imports": [
      "csv",
      "os",
      "requests"
    ],
    "preview": "import csv\nimport os\n\nimport requests  # Make sure to install this package\n\n\ndef generate_speech(input_text, output_path, api_key):\n    \"\"\"\n    Generates speech from text using an API (e.g., OpenAI's text-to-speech API).\n    \"\"\"\n    url = \"https://api.openai.com/v1/audio/speech\"\n    headers = {\n        \"Authorization\": f\"Bearer {api_key}\",\n        \"Content-Type\": \"application/json\",\n    }\n    data = {\n        \"model\": \"tts-1\",  # Update this according to the specific model you're using\n        \"input\": input_text,\n        \"voice\": \"shimmer\",  # Update the voice parameter as needed\n    }",
    "last_modified": "2025-09-13T05:55:27.236819"
  },
  {
    "id": "13",
    "name": "pythoncatorgize.py",
    "path": "01_core_ai_analysis/transcription/pythoncatorgize.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 60,
    "size": 1966,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis"
    ],
    "functions": [
      "get_openai_category",
      "create_directory",
      "save_to_category",
      "categorize_files"
    ],
    "classes": [],
    "imports": [
      "os",
      "openai"
    ],
    "preview": "import os\n\nfrom openai import OpenAI\n\nclient = OpenAI(api_key=api_key)\n\n# Load the API key from the environment variable\napi_key = os.getenv(\"OPENAI_API_KEY\")\nif not api_key:\n    raise ValueError(\"API key not found. Please ensure it is set in your environment variables.\")\n\n\ndef get_openai_category(script_content):\n    try:\n        response = client.chat.completions.create(\n            model=\"gpt-3.5-turbo\",\n            messages=[\n                {\"role\": \"system\", \"content\": \"You are an expert Python programmer.\"},\n                {\n                    \"role\": \"user\",",
    "last_modified": "2025-09-13T05:55:12.859828"
  },
  {
    "id": "14",
    "name": "song-process.py",
    "path": "01_core_ai_analysis/transcription/song-process.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 179,
    "size": 6943,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis",
      "transcription"
    ],
    "functions": [
      "format_timestamp",
      "transcribe_audio",
      "analyze_text_for_section",
      "process_audio_file",
      "process_audio_directory"
    ],
    "classes": [],
    "imports": [
      "csv",
      "os",
      "sys",
      "dotenv",
      "openai",
      "termcolor"
    ],
    "preview": "import csv\nimport os\nimport sys\n\nfrom dotenv import load_dotenv\nfrom openai import OpenAI\nfrom termcolor import colored\n\n# Load environment variables from .env file\nload_dotenv(dotenv_path=\"/Users/steven/.env\")\n\n# Get API key and validate it\napi_key = os.getenv(\"OPENAI_API_KEY\")\nif not api_key:\n    raise EnvironmentError(\"\u274c OpenAI API key not found. Please check your .env file.\")\n\n# Initialize OpenAI client\nclient = OpenAI(api_key=api_key)\n\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "15",
    "name": "import.py",
    "path": "01_core_ai_analysis/transcription/import.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 66,
    "size": 2512,
    "docstring": "",
    "keywords": [
      "data_processing",
      "openai",
      "analysis"
    ],
    "functions": [
      "generate_clickable_title",
      "generate_description",
      "generate_tags"
    ],
    "classes": [],
    "imports": [
      "pandas",
      "openai"
    ],
    "preview": "import pandas as pd\nfrom openai import OpenAI\n\nclient = OpenAI(api_key='your_openai_api_key')\n\n# Set your OpenAI API key\n\n Function to generate a clickable title\ndef generate_clickable_title(detail):\n    prompt = f\"Generate a catchy and clickable title for a Cork Back Coaster with the theme: '{detail}'. Maximum 50 characters. At the end of each title write Cork Back Coaster\"\n    response = client.chat.completions.create(model=\"gpt-3.5-turbo\", messages=[{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, {\"role\": \"user\", \"content\": prompt}])\n    clickable_title = response.choices[0].message.content.strip()\n    clickable_title = clickable_title.replace('\"', '')  # Remove double quotes\n    return clickable_title\n\n# Function to generate a description\ndef generate_description(detail):\n    prompt = f\"'{detail}'. Maximum 150 characters.\"\n    response = client.chat.completions.create(model=\"gpt-3.5-turbo\", messages=[{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, {\"role\": \"user\", \"content\": prompt}])\n    description = response.choices[0].message.content.strip()",
    "last_modified": "2025-05-04T22:47:11.601196"
  },
  {
    "id": "16",
    "name": "analyze-mp3-transcript-prompts.py",
    "path": "01_core_ai_analysis/transcription/analyze-mp3-transcript-prompts.py",
    "category": "01_core_ai_analysis",
    "type": "analysis",
    "lines": 110,
    "size": 4649,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis",
      "transcription"
    ],
    "functions": [
      "transcribe_audio",
      "format_timestamp",
      "analyze_text_for_section",
      "process_audio_directory"
    ],
    "classes": [],
    "imports": [
      "openai",
      "os",
      "dotenv"
    ],
    "preview": "from openai import OpenAI\n\nclient = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\nimport os\n\nfrom dotenv import load_dotenv\n\n# Load environment variables from .env (make sure your OPENAI_API_KEY is stored here)\nenv_path = \"/Users/steven/.env\"\nload_dotenv(dotenv_path=env_path)\n\n# Directory pathsp\nAUDIO_DIR = \"/Users/steven/Music/NocTurnE-meLoDieS/mp3\"  # Directory containing MP3 files\nTRANSCRIPT_DIR = \"/Users/steven/Music/NocTurnE-meLoDieS/song-video/analysis-transcript\"  # Directory to save transcripts\nANALYSIS_DIR = \"/Users/steven/Music/NocTurnE-meLoDieS/song-video/analysis-transcript\"  # Directory to save the analysis files\n\n# Create output directories if they don't exist\nos.makedirs(TRANSCRIPT_DIR, exist_ok=True)\nos.makedirs(ANALYSIS_DIR, exist_ok=True)\n",
    "last_modified": "2025-09-13T05:53:54.886407"
  },
  {
    "id": "17",
    "name": "analyze-prompt-1.py",
    "path": "01_core_ai_analysis/transcription/analyze-prompt-1.py",
    "category": "01_core_ai_analysis",
    "type": "analysis",
    "lines": 97,
    "size": 3941,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis",
      "transcription"
    ],
    "functions": [
      "transcribe_audio",
      "format_timestamp",
      "analyze_text_for_section",
      "process_audio_directory"
    ],
    "classes": [],
    "imports": [
      "openai",
      "os",
      "subprocess",
      "dotenv"
    ],
    "preview": "from openai import OpenAI\n\nclient = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\nimport os\nimport subprocess\n\nfrom dotenv import load_dotenv\n\n# Load environment variables from .env (make sure your OPENAI_API_KEY is stored here)\nenv_path = \"/Users/steven/.env\"\nload_dotenv(dotenv_path=env_path)\n\n# Directory paths\nAUDIO_DIR = \"/Users/steven/Music/shadow\"  # Directory containing MP3 files\nTRANSCRIPT_DIR = \"/Users/steven/Music/shadow/transcript\"  # Directory to save transcripts\nANALYSIS_DIR = \"/Users/steven/Music/shadow/analysis\"  # Directory to save the analysis files\n\n# Create output directories if they don't exist\nos.makedirs(TRANSCRIPT_DIR, exist_ok=True)\nos.makedirs(ANALYSIS_DIR, exist_ok=True)",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "18",
    "name": "docks.py",
    "path": "01_core_ai_analysis/transcription/docks.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 115,
    "size": 3160,
    "docstring": "",
    "keywords": [],
    "functions": [
      "is_excluded",
      "get_category",
      "scan_directory",
      "save_to_csv"
    ],
    "classes": [],
    "imports": [
      "csv",
      "os",
      "re",
      "datetime"
    ],
    "preview": "# docs.py\nimport csv\nimport os\nimport re\nfrom datetime import datetime\n\n# === CONFIGURATION ===\nBASE_DIRECTORIES = [\n    \"/Users/steven/Documents/Python_backup\",\n    \"/Users/steven/Documents/Python\",\n    \"/Users/steven/Music/nocTurneMeLoDieS/lyrics-keys-indo\",\n    \"/Users/steven/Music/nocTurneMeLoDieS/mp3-analyze-transcribe\",\n]\n\n# === EXCLUDED PATTERNS (regex) ===\nEXCLUDED_PATTERNS = [\n    r\"\\/.?venv\\/\",\n    r\"\\/lib\\/\",\n    r\"\\/my_global_venv\\/\",\n    r\"\\/simplegallery\\/\",",
    "last_modified": "2025-09-13T05:53:47.869781"
  },
  {
    "id": "19",
    "name": "batch_image_seo_pipeline_20250530220524.py",
    "path": "01_core_ai_analysis/transcription/batch_image_seo_pipeline_20250530220524.py",
    "category": "01_core_ai_analysis",
    "type": "image_processing",
    "lines": 409,
    "size": 16734,
    "docstring": "",
    "keywords": [
      "openai"
    ],
    "functions": [
      "load_openai_client",
      "retry_on_api_error",
      "call_gpt4o",
      "get_image_tech_meta",
      "build_source_tag",
      "discover_images",
      "build_gpt_messages",
      "parse_args",
      "main"
    ],
    "classes": [],
    "imports": [
      "argparse",
      "csv",
      "json",
      "logging",
      "os",
      "sys",
      "time",
      "datetime",
      "pathlib",
      "typing"
    ],
    "preview": "import argparse\nimport csv\nimport json\nimport logging\nimport os\nimport sys\nimport time\nfrom datetime import datetime\nfrom pathlib import Path\nfrom typing import Any, Dict, List, Optional\n\nimport backoff  # pip install backoff\nfrom dotenv import load_dotenv\nfrom openai import OpenAI, OpenAIAPIError\nfrom PIL import Image, UnidentifiedImageError\nfrom tqdm import tqdm\n\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n# CONFIGURATION\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500",
    "last_modified": "2025-09-13T05:53:26.777321"
  },
  {
    "id": "20",
    "name": "generate_speech 2.py",
    "path": "01_core_ai_analysis/transcription/generate_speech 2.py",
    "category": "01_core_ai_analysis",
    "type": "transcription",
    "lines": 46,
    "size": 1449,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis"
    ],
    "functions": [
      "generate_speech",
      "main"
    ],
    "classes": [],
    "imports": [
      "csv",
      "openai"
    ],
    "preview": "import csv\n\nfrom openai import OpenAI\n\nclient = OpenAI(api_key='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')\n\n# Replace 'your_api_key_here' with your actual OpenAI API key\n\n\ndef generate_speech(text, voice='shimmer', output_path='speech.mp3'):\n    response = client.audio.create(model=\"tts-1-hd\",  # Ensure this model supports the audio creation\n                                   input=text,\n                                   voice=voice,\n                                   format=\"mp3\")\n    with open(output_path, \"wb\") as file:\n        file.write(response.content)\n\n\ndef main():\n    # Update this path to where your CSV is located",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "21",
    "name": "political-analysist-prompter.py",
    "path": "01_core_ai_analysis/transcription/political-analysist-prompter.py",
    "category": "01_core_ai_analysis",
    "type": "analysis",
    "lines": 123,
    "size": 5917,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis",
      "transcription"
    ],
    "functions": [
      "format_timestamp",
      "transcribe_media",
      "analyze_text",
      "analyze_image",
      "process_media"
    ],
    "classes": [
      "Config"
    ],
    "imports": [
      "os",
      "argparse",
      "pathlib",
      "dotenv",
      "openai",
      "concurrent.futures",
      "logging",
      "base64",
      "subprocess"
    ],
    "preview": "import os\nimport argparse\nfrom pathlib import Path\nfrom dotenv import load_dotenv\nfrom openai import OpenAI\nfrom concurrent.futures import ThreadPoolExecutor\nimport logging\nimport base64\nimport subprocess\n\n# Config class with updated prompt for Christian Nationalism\nclass Config:\n    MODEL = \"gpt-4o\"\n    MAX_TOKENS = 1500\n    TEMPERATURE = 0.7\n    MAX_ATTEMPTS = 3\n    PROMPT_TYPE = \"christian_nationalism\"  # New prompt type\n\n# Setup logging\nlogging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")",
    "last_modified": "2025-10-09T02:38:03.660405"
  },
  {
    "id": "22",
    "name": "vid-mp3-transcribe-analyze.py",
    "path": "01_core_ai_analysis/transcription/vid-mp3-transcribe-analyze.py",
    "category": "01_core_ai_analysis",
    "type": "transcription",
    "lines": 137,
    "size": 6455,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis",
      "transcription"
    ],
    "functions": [
      "split_video_to_segments",
      "transcribe_video_segment",
      "format_timestamp",
      "analyze_text_for_section",
      "process_video_by_section"
    ],
    "classes": [],
    "imports": [
      "os",
      "subprocess",
      "openai",
      "dotenv",
      "sys"
    ],
    "preview": "import os\nimport subprocess\n\nimport openai\nfrom dotenv import load_dotenv\n\n# Load environment variables from .env (make sure your OPENAI_API_KEY is stored here)\nload_dotenv()\nopenai.api_key = os.getenv(\"OPENAI_API_KEY\")\n\n# Directory paths\nVIDEO_DIR = \"/Users/steven/Movies/poject2025/Media\"  # Directory containing MP4 files\nTRANSCRIPT_DIR = (\n    \"/Users/steven/Movies/poject2025/Media/Mp4/transcript\"  # Directory to save transcripts\n)\nANALYSIS_DIR = (\n    \"/Users/steven/Movies/poject2025/Media/Mp4/analysis\"  # Directory to save the analysis files\n)\n\n# Create output directories if they don't exist",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "23",
    "name": "mp4-transcript.py",
    "path": "01_core_ai_analysis/transcription/mp4-transcript.py",
    "category": "01_core_ai_analysis",
    "type": "video_processing",
    "lines": 134,
    "size": 4974,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis",
      "transcription"
    ],
    "functions": [
      "split_video_to_segments",
      "transcribe_video_segment",
      "format_timestamp",
      "analyze_text_for_section",
      "process_video_by_section"
    ],
    "classes": [],
    "imports": [
      "openai",
      "os",
      "subprocess",
      "dotenv",
      "sys"
    ],
    "preview": "from openai import OpenAI\n\nclient = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\nimport os\nimport subprocess\n\nfrom dotenv import load_dotenv\n\n# Load environment variables from .env (make sure your OPENAI_API_KEY is stored here)\n# Load environment variables from .env (make sure your OPENAI_API_KEY is stored here)\nenv_path = \"/Users/steven/.env\"\nload_dotenv(dotenv_path=env_path)\n\n# Directory paths\nVIDEO_DIR = \"/Users/steven/Movies/2025/mp4\"  # Directory containing MP4 files\nTRANSCRIPT_DIR = \"/Users/steven/Movies/2025/mp4/transcribe\"  # Directory to save transcripts\nANALYSIS_DIR = \"/Users/steven/Movies/2025/mp4/analysis\"  # Directory to save the analysis files\n\n# Create output directories if they don't exist\nos.makedirs(TRANSCRIPT_DIR, exist_ok=True)",
    "last_modified": "2025-09-13T05:53:55.342653"
  },
  {
    "id": "24",
    "name": "FFMpegRoBot.py",
    "path": "01_core_ai_analysis/transcription/FFMpegRoBot.py",
    "category": "01_core_ai_analysis",
    "type": "video_processing",
    "lines": 258,
    "size": 9667,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "logging",
      "os",
      "time",
      "pyrogram",
      "translation",
      "hachoir.metadata",
      "hachoir.parser",
      "helper_funcs.chat_base",
      "helper_funcs.display_progress",
      "helper_funcs.help_Nekmo_ffmpeg"
    ],
    "preview": "import logging\n\nlogging.basicConfig(\n    level=logging.DEBUG, format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\n)\nlogger = logging.getLogger(__name__)\n\nimport os\nimport time\n\n# the secret configuration specific things\nif bool(os.environ.get(\"WEBHOOK\", False)):\n    from sample_config import Config\nelse:\n    from config import Config\n\n# the Strings used for this \"thing\"\nimport pyrogram\nfrom translation import Translation\n",
    "last_modified": "2025-09-13T05:53:43.583647"
  },
  {
    "id": "25",
    "name": "prompt_analyzer.py",
    "path": "01_core_ai_analysis/transcription/prompt_analyzer.py",
    "category": "01_core_ai_analysis",
    "type": "analysis",
    "lines": 103,
    "size": 5483,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis",
      "transcription"
    ],
    "functions": [
      "transcribe_audio",
      "format_timestamp",
      "analyze_text_for_section",
      "process_audio_directory"
    ],
    "classes": [],
    "imports": [
      "openai",
      "os",
      "subprocess",
      "shared.config"
    ],
    "preview": "from openai import OpenAI\n\nclient = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\nimport os\nimport subprocess\n\nfrom shared.config import *\n\n# Load environment variables from .env (make sure your OPENAI_API_KEY is stored here)\nenv_path = \"/Users/steven/.env\"\nload_dotenv(dotenv_path=env_path)\n\n# Directory paths\nAUDIO_DIR = (\n    \"/Users/steven/Music/NocTurnE-meLoDieS/Discography/Audio\"  # Directory containing MP3 files\n)\nTRANSCRIPT_DIR = \"/Users/steven/Music/NocTurnE-meLoDieS/Discography/Audio/transcript\"  # Directory to save transcripts\nANALYSIS_DIR = \"/Users/steven/Music/NocTurnE-meLoDieS/Discography/Audio/analysis\"  # Directory to save the analysis files\n\n# Create output directories if they don't exist",
    "last_modified": "2025-10-09T05:27:15.575895"
  },
  {
    "id": "26",
    "name": "batch_image_seo_pipeline_20250530221037.py",
    "path": "01_core_ai_analysis/transcription/batch_image_seo_pipeline_20250530221037.py",
    "category": "01_core_ai_analysis",
    "type": "analysis",
    "lines": 428,
    "size": 17474,
    "docstring": "batch_image_seo_pipeline.py\n\n1) Discovers all image files under a given folder.\n2) Extracts technical metadata via Pillow.\n3) Calls GPT-4o Vision (multimodal) using a \u201csystem\u201d + \u201cuser\u201d prompt.\n4) Parses the JSON response into structured fields:\n   main_subject, style, color_palette, tags, orientation, suggested_products,\n   SEO_title, SEO_description, emotion, safety_rating, dominant_keyword.\n5) Builds a \u201csource\u201d tag: folder-subfolder-YYYYMMDD.\n6) Applies a \u201cTop 5% SEO Analytics\u201d framework to generate SEO columns:\n   \u2013 SEO Keywords, Traffic Source, CRO Tactic, Backlink Source, Engagement Rate.\n7) Adds niche-specific design prompts (Geeky, Dark Humor, Anime).\n8) Writes everything into a final CSV for bulk upload.\n\nDependencies:\n    pip install openai python-dotenv pillow tqdm backoff",
    "keywords": [
      "openai"
    ],
    "functions": [
      "load_openai_client",
      "retry_on_api_error",
      "call_gpt4o",
      "get_image_tech_meta",
      "build_source_tag",
      "discover_images",
      "build_gpt_messages",
      "parse_args",
      "main"
    ],
    "classes": [],
    "imports": [
      "argparse",
      "csv",
      "json",
      "logging",
      "os",
      "sys",
      "datetime",
      "pathlib",
      "typing",
      "backoff"
    ],
    "preview": "#!/usr/bin/env python3\n\"\"\"\nbatch_image_seo_pipeline.py\n\n1) Discovers all image files under a given folder.\n2) Extracts technical metadata via Pillow.\n3) Calls GPT-4o Vision (multimodal) using a \u201csystem\u201d + \u201cuser\u201d prompt.\n4) Parses the JSON response into structured fields:\n   main_subject, style, color_palette, tags, orientation, suggested_products,\n   SEO_title, SEO_description, emotion, safety_rating, dominant_keyword.\n5) Builds a \u201csource\u201d tag: folder-subfolder-YYYYMMDD.\n6) Applies a \u201cTop 5% SEO Analytics\u201d framework to generate SEO columns:\n   \u2013 SEO Keywords, Traffic Source, CRO Tactic, Backlink Source, Engagement Rate.\n7) Adds niche-specific design prompts (Geeky, Dark Humor, Anime).\n8) Writes everything into a final CSV for bulk upload.\n\nDependencies:\n    pip install openai python-dotenv pillow tqdm backoff\n\"\"\"\n",
    "last_modified": "2025-09-13T05:53:25.697217"
  },
  {
    "id": "27",
    "name": "analyze 1 1.py",
    "path": "01_core_ai_analysis/transcription/analyze 1 1.py",
    "category": "01_core_ai_analysis",
    "type": "analysis",
    "lines": 60,
    "size": 3440,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis"
    ],
    "functions": [
      "analyze_text"
    ],
    "classes": [],
    "imports": [
      "dotenv",
      "openai",
      "os"
    ],
    "preview": "from dotenv import load_dotenv\nfrom openai import OpenAI\n\nclient = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\nimport os\n\n# Load environment variables from .env\nenv_path = os.path.expanduser(\"~/.env\")\n# Update the path if needed\nload_dotenv(dotenv_path=env_path)\n\n# Error checking for openai API key\nif not openai.api_key:\n    raise EnvironmentError(\"openai API key not found. Please check your .env file.\")\n\n\ndef analyze_text(text):\n    response = client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        messages=[",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "28",
    "name": "batch_process.py",
    "path": "01_core_ai_analysis/transcription/batch_process.py",
    "category": "01_core_ai_analysis",
    "type": "transcription",
    "lines": 78,
    "size": 2242,
    "docstring": "Batch processing script for multiple audio/video files",
    "keywords": [],
    "functions": [
      "main"
    ],
    "classes": [],
    "imports": [
      "os",
      "sys",
      "glob",
      "pathlib",
      "transcription_analyzer",
      "dotenv",
      "os"
    ],
    "preview": "#!/usr/bin/env python3\n\"\"\"\nBatch processing script for multiple audio/video files\n\"\"\"\n\nimport os\nimport sys\nimport glob\nfrom pathlib import Path\nfrom transcription_analyzer import TranscriptionAnalyzer\nfrom dotenv import load_dotenv\nimport os\n\n# Load environment variables from ~/.env\nload_dotenv(os.path.expanduser(\"~/.env\"))\n\ndef main():\n    \"\"\"Process multiple files in a directory.\"\"\"\n    # Check for OpenAI API key\n    api_key = os.getenv('OPENAI_API_KEY')",
    "last_modified": "2025-10-09T05:14:19.003548"
  },
  {
    "id": "29",
    "name": "test_setup.py",
    "path": "01_core_ai_analysis/transcription/test_setup.py",
    "category": "01_core_ai_analysis",
    "type": "analysis",
    "lines": 141,
    "size": 4254,
    "docstring": "Test script to verify the transcription analyzer setup",
    "keywords": [
      "openai",
      "testing"
    ],
    "functions": [
      "test_imports",
      "test_environment",
      "test_ffmpeg",
      "test_whisper_model",
      "main"
    ],
    "classes": [],
    "imports": [
      "os",
      "sys",
      "pathlib",
      "dotenv",
      "subprocess",
      "whisper",
      "moviepy",
      "openai",
      "dotenv",
      "whisper"
    ],
    "preview": "#!/usr/bin/env python3\n\"\"\"\nTest script to verify the transcription analyzer setup\n\"\"\"\n\nimport os\nimport sys\nfrom pathlib import Path\n\ndef test_imports():\n    \"\"\"Test if all required modules can be imported.\"\"\"\n    print(\"Testing imports...\")\n    \n    try:\n        import whisper\n        print(\"\u2705 whisper imported successfully\")\n    except ImportError as e:\n        print(f\"\u274c whisper import failed: {e}\")\n        return False\n    ",
    "last_modified": "2025-10-09T05:14:25.392980"
  },
  {
    "id": "30",
    "name": "analyze-mp4s copy.py",
    "path": "01_core_ai_analysis/transcription/analyze-mp4s copy.py",
    "category": "01_core_ai_analysis",
    "type": "analysis",
    "lines": 114,
    "size": 5693,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis",
      "transcription"
    ],
    "functions": [
      "format_timestamp",
      "transcribe_audio",
      "analyze_text_for_section",
      "process_audio_directory"
    ],
    "classes": [],
    "imports": [
      "logging",
      "os",
      "openai",
      "dotenv",
      "termcolor",
      "tqdm"
    ],
    "preview": "import logging\nimport os\n\nimport openai\nfrom dotenv import load_dotenv\nfrom termcolor import colored\nfrom tqdm import tqdm\n\n# Load environment variables from .env\nenv_path = os.path.expanduser(\"~/.env\")\nload_dotenv(dotenv_path=env_path)\n\n# Set OpenAI API key\nopenai.api_key = os.getenv(\"OPENAI_API_KEY\")\nif not openai.api_key:\n    raise EnvironmentError(\"OpenAI API key not found. Please check your .env file.\")\n\n\ndef format_timestamp(seconds):\n    minutes = int(seconds // 60)",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "31",
    "name": "CreateVideo.py",
    "path": "01_core_ai_analysis/transcription/CreateVideo.py",
    "category": "01_core_ai_analysis",
    "type": "youtube",
    "lines": 55,
    "size": 1864,
    "docstring": "",
    "keywords": [],
    "functions": [
      "add_captions"
    ],
    "classes": [],
    "imports": [
      "os",
      "random",
      "moviepy.editor",
      "moviepy.audio.AudioClip",
      "moviepy.audio.io.AudioFileClip"
    ],
    "preview": "import os\nimport random\n\nimport moviepy.editor as mp\nfrom moviepy.audio.AudioClip import CompositeAudioClip, concatenate_audioclips\nfrom moviepy.audio.io.AudioFileClip import AudioFileClip\n\n# texts = [\"FACT 1 ABOUT PSICOLOGY\", \"FACT 2 ABOUT PSICOLOGY\", \"FACT 3 ABOUT PSICOLOGY\"]\ntexts = [\n    \"1. Psicology is the science of the mind and behavior, exploring how our experiences shape our thoughts, feelings, and behaviors.\",\n    \"2. It looks at how our biology, environment, and culture shape our mental and emotional states.\",\n    \"3. Psicology can help us understand ourselves and others\",\n]\n\n\ndef add_captions(texts):\n    video = mp.VideoFileClip(\"./background/test.mp4\")\n    height = int(video.w * 16 / 9)\n    top_crop = int((video.h - height) / 2)\n    bottom_crop = video.h - height - top_crop",
    "last_modified": "2025-05-04T22:47:11"
  },
  {
    "id": "32",
    "name": "cover.py",
    "path": "01_core_ai_analysis/transcription/cover.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 51,
    "size": 1566,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis",
      "web_tools"
    ],
    "functions": [
      "generate_cover_image_with_dalle",
      "convert_mp3_to_mp4",
      "process_directory"
    ],
    "classes": [],
    "imports": [
      "openai",
      "glob",
      "os",
      "io",
      "requests",
      "moviepy.editor",
      "PIL",
      "sys"
    ],
    "preview": "from openai import OpenAI\n\nclient = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\nimport glob\nimport os\nfrom io import BytesIO\n\nimport requests\nfrom moviepy.editor import AudioFileClip, ImageClip\nfrom PIL import Image\n\n\ndef generate_cover_image_with_dalle(file_name, output_path):\n    prompt = f\"lets create a series of typography cover image for '{file_name}' in the Font and style and contexts to tell the story'\"\n    response = client.images.generate(prompt=prompt, n=1, size=\"1024x1024\")\n    image_url = response.data[0].url\n    response = requests.get(image_url)\n    img = Image.open(BytesIO(response.content))\n    img.save(output_path)\n",
    "last_modified": "2025-05-04T22:47:12.619976"
  },
  {
    "id": "33",
    "name": "speak.py",
    "path": "01_core_ai_analysis/transcription/speak.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 46,
    "size": 1599,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis"
    ],
    "functions": [
      "generate_speech",
      "main"
    ],
    "classes": [],
    "imports": [
      "csv",
      "openai"
    ],
    "preview": "import csv\n\nimport openai\n\n# Initialize the OpenAI client\nclient = openai()\n\n\ndef generate_speech(text, pause_duration=\"5s\", voice=\"shimmer\", output_path=\"speech.mp3\"):\n    # Adding a pause using the SSML <break> tag\n    # Assuming that 'text' contains something like \"Option 3: <your text>\"\n    # and you want to insert a pause right after this before continuing with the answer\n    modified_text = text.replace(\"Option 3:\", f'Option 3:\"/>')\n\n    response = OpenAI.Audio.create(\n        model=\"tts-1-hd\",  # Ensure this model supports the audio creation\n        input=modified_text,\n        voice=voice,\n        format=\"mp3\",\n        ssml=True,  # Indicate that the input text contains SSML",
    "last_modified": "2025-09-13T05:53:51.117240"
  },
  {
    "id": "34",
    "name": "GenerateTexts.py",
    "path": "01_core_ai_analysis/transcription/GenerateTexts.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 21,
    "size": 548,
    "docstring": "",
    "keywords": [
      "openai"
    ],
    "functions": [
      "get_facts"
    ],
    "classes": [],
    "imports": [
      "openai"
    ],
    "preview": "from openai import OpenAI\n\nclient = OpenAI(api_key=\"[[Insert openai api key]]\")\n\n\ndef get_facts(topic):\n\n    response = client.completions.create(\n        model=\"text-davinci-003\",\n        prompt=\"Create three tik-tok like plain text for curious facts about \"\n        + topic\n        + \" separated by enters without text before or after\",\n        temperature=0.5,\n        max_tokens=150,\n        top_p=1.0,\n        frequency_penalty=0.0,\n        presence_penalty=0.0,\n    )\n    facts = response.choices[0].text.strip().split(\"\\n\")\n    return facts",
    "last_modified": "2025-05-04T22:47:11"
  },
  {
    "id": "35",
    "name": "speek.py",
    "path": "01_core_ai_analysis/transcription/speek.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 15,
    "size": 320,
    "docstring": "",
    "keywords": [
      "openai"
    ],
    "functions": [],
    "classes": [],
    "imports": [
      "pathlib",
      "openai"
    ],
    "preview": "from pathlib import Path\n\nfrom openai import OpenAI\n\nclient = OpenAI()\n\nspeech_file_path = Path(__file__).parent / \"speech.mp3\"\nresponse = client.audio.speech.create(\n    model=\"tts-1\",\n    voice=\"alloy\",\n    input=\"Today is a wonderful day to build something people love!\",\n)\n\nresponse.stream_to_file(speech_file_path)\n",
    "last_modified": "2025-05-04T22:47:13.052330"
  },
  {
    "id": "36",
    "name": "transcription_analyzer.py",
    "path": "01_core_ai_analysis/transcription/transcription_analyzer.py",
    "category": "01_core_ai_analysis",
    "type": "transcription",
    "lines": 391,
    "size": 17249,
    "docstring": "Audio/Video Transcription and Analysis Tool\nConverts MP4 to MP3, transcribes with timestamps, and provides GPT-4o analysis",
    "keywords": [
      "openai",
      "analysis",
      "transcription"
    ],
    "functions": [
      "main",
      "__init__",
      "convert_mp4_to_mp3",
      "transcribe_audio",
      "analyze_transcript",
      "_format_timestamp",
      "create_output_structure",
      "process_file",
      "_process_short_file",
      "_process_long_file"
    ],
    "classes": [
      "TranscriptionAnalyzer"
    ],
    "imports": [
      "os",
      "sys",
      "json",
      "logging",
      "pathlib",
      "datetime",
      "typing",
      "whisper",
      "moviepy.editor",
      "openai"
    ],
    "preview": "#!/usr/bin/env python3\n\"\"\"\nAudio/Video Transcription and Analysis Tool\nConverts MP4 to MP3, transcribes with timestamps, and provides GPT-4o analysis\n\"\"\"\n\nimport os\nimport sys\nimport json\nimport logging\nfrom pathlib import Path\nfrom datetime import datetime\nfrom typing import Optional, Dict, Any\n\nimport whisper\nfrom moviepy.editor import VideoFileClip\nfrom openai import OpenAI\nfrom dotenv import load_dotenv\nfrom audio_chunker import AudioChunker\nimport config",
    "last_modified": "2025-10-09T05:17:27.383917"
  },
  {
    "id": "37",
    "name": "Quiz22sec 2.py",
    "path": "01_core_ai_analysis/transcription/Quiz22sec 2.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 100,
    "size": 3342,
    "docstring": "",
    "keywords": [
      "analysis",
      "web_tools"
    ],
    "functions": [
      "generate_speech",
      "calculate_text_duration",
      "process_csv_and_generate_speech"
    ],
    "classes": [],
    "imports": [
      "csv",
      "os",
      "requests",
      "dotenv",
      "pydub"
    ],
    "preview": "import csv\nimport os\n\nimport requests  # Install using `pip install requests`\nfrom dotenv import load_dotenv\nfrom pydub import AudioSegment  # Install using `pip install pydub`\n\n\ndef generate_speech(input_text, output_path, api_key):\n    \"\"\"\n    Generates speech from text using OpenAI's text-to-speech API.\n    \"\"\"\n    url = \"https://api.openai.com/v1/audio/speech\"\n    headers = {\n        \"Authorization\": f\"Bearer {api_key}\",\n        \"Content-Type\": \"application/json\",\n    }\n    data = {\n        \"model\": \"tts-1\",\n        \"input\": input_text,",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "38",
    "name": "analyze_4.py",
    "path": "01_core_ai_analysis/transcription/analyze_4.py",
    "category": "01_core_ai_analysis",
    "type": "analysis",
    "lines": 110,
    "size": 5586,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis",
      "transcription"
    ],
    "functions": [
      "transcribe_audio",
      "format_timestamp",
      "analyze_text_for_section",
      "process_media_directory"
    ],
    "classes": [],
    "imports": [
      "os",
      "logging",
      "dotenv",
      "openai"
    ],
    "preview": "import os\nimport logging\nfrom dotenv import load_dotenv\nfrom openai import OpenAI, Audio  # Import the correct module for audio processing\n\n# Initialize OpenAI client\nclient = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\naudio_client = Audio(client)  # Initialize the audio client\n\n# Load environment variables\nenv_path = \"~/.env\"\nload_dotenv(dotenv_path=env_path)\n\n# Prompt for base directory\nbase_dir_input = input(\"Enter base directory for media files (leave blank for current): \").strip()\nMEDIA_DIR = base_dir_input or os.getcwd()\n\n# Output directories\nTRANSCRIPT_DIR = os.path.join(MEDIA_DIR, \"transcripts\")\nANALYSIS_DIR = os.path.join(MEDIA_DIR, \"analysis\")",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "39",
    "name": "curl.py",
    "path": "01_core_ai_analysis/transcription/curl.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 44,
    "size": 1382,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "generate_speech_with_curl",
      "main"
    ],
    "classes": [],
    "imports": [
      "csv",
      "os",
      "subprocess"
    ],
    "preview": "import csv\nimport os\nimport subprocess\n\n\ndef generate_speech_with_curl(question_text, question_number, api_key):\n    curl_command = [\n        \"curl\",\n        \"https://api.openai.com/v1/audio/speech\",\n        \"-H\",\n        f\"Authorization: Bearer {api_key}\",\n        \"-H\",\n        \"Content-Type: application/json\",\n        \"-d\",\n        f\"\"\"{{\n            \"model\": \"tts-1\",\n            \"input\": \"{question_text}\",\n            \"voice\": \"alloy\"\n        }}\"\"\",\n        \"--output\",",
    "last_modified": "2025-05-04T22:47:13.042596"
  },
  {
    "id": "40",
    "name": "analyze (1).py",
    "path": "01_core_ai_analysis/transcription/analyze (1).py",
    "category": "01_core_ai_analysis",
    "type": "analysis",
    "lines": 57,
    "size": 1994,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis",
      "transcription"
    ],
    "functions": [
      "transcribe_audio",
      "analyze_text"
    ],
    "classes": [],
    "imports": [
      "openai",
      "os",
      "dotenv",
      "sys"
    ],
    "preview": "from openai import OpenAI\n\nclient = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\nimport os\n\nfrom dotenv import load_dotenv\n\n# Load environment variables from .env\nload_dotenv()\n\n\n# Function to transcribe audio using OpenAI Whisper\ndef transcribe_audio(file_path):\n    with open(file_path, \"rb\") as audio_file:\n        transcript = client.audio.transcribe(\"whisper-1\", audio_file)\n        return transcript.text\n\n\n# Function to analyze the transcript using GPT\ndef analyze_text(text):",
    "last_modified": "2025-05-04T22:47:13.341006"
  },
  {
    "id": "41",
    "name": "Media-Analysis-Pipeline-gpt-claude.py",
    "path": "01_core_ai_analysis/transcription/Media-Analysis-Pipeline-gpt-claude.py",
    "category": "01_core_ai_analysis",
    "type": "analysis",
    "lines": 473,
    "size": 17763,
    "docstring": "ULTIMATE Media Analysis Pipeline - Simplified Edition\nUses only GPT-4o and Claude for reliability",
    "keywords": [
      "analysis",
      "openai",
      "transcription",
      "organization"
    ],
    "functions": [
      "main",
      "__init__",
      "extract_audio_from_video",
      "cleanup_temp_files",
      "__init__",
      "setup_directories",
      "setup_clients",
      "get_file_hash",
      "check_cache",
      "save_to_cache"
    ],
    "classes": [
      "LargeFileHandler",
      "UltimateMediaAnalyzer"
    ],
    "imports": [
      "os",
      "logging",
      "time",
      "json",
      "subprocess",
      "hashlib",
      "pathlib",
      "typing",
      "dotenv",
      "openai"
    ],
    "preview": "#!/usr/bin/env python3\n\"\"\"\nULTIMATE Media Analysis Pipeline - Simplified Edition\nUses only GPT-4o and Claude for reliability\n\"\"\"\n\nimport os\nimport logging\nimport time\nimport json\nimport subprocess\nimport hashlib\nfrom pathlib import Path\nfrom typing import Optional, Dict, Any, List\nfrom dotenv import load_dotenv\n\n# Load environment variables first\nload_dotenv()\n\n# Now import other packages",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "42",
    "name": "meida-trans-analyze.py",
    "path": "01_core_ai_analysis/transcription/meida-trans-analyze.py",
    "category": "01_core_ai_analysis",
    "type": "analysis",
    "lines": 124,
    "size": 5779,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis",
      "transcription"
    ],
    "functions": [
      "format_timestamp",
      "transcribe_audio",
      "analyze_text_for_section",
      "process_media_directory"
    ],
    "classes": [],
    "imports": [
      "logging",
      "os",
      "sys",
      "pathlib",
      "openai",
      "dotenv"
    ],
    "preview": "import logging\nimport os\nimport sys\nfrom pathlib import Path\nimport openai\nfrom dotenv import load_dotenv\n\n# Setup basic logging for information tracking\nlogging.basicConfig(level=logging.INFO, format=\"%(asctime)s [%(levelname)s] %(message)s\")\n\n# Load environment variables from the .env file (ensure your OPENAI_API_KEY is stored here)\nenv_path = Path(\"/Users/steven/.env\")\nload_dotenv(dotenv_path=env_path)\n\n# Set the OpenAI API key from environment variables\nopenai.api_key = os.getenv(\"OPENAI_API_KEY\")\n\n# Initialize configuration. This can be expanded as needed.\nconfig = {\"base_dir\": \"\"}  # Leave blank to prompt for base directory\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "43",
    "name": "mp3-transcript.py",
    "path": "01_core_ai_analysis/transcription/mp3-transcript.py",
    "category": "01_core_ai_analysis",
    "type": "audio_processing",
    "lines": 51,
    "size": 1454,
    "docstring": "",
    "keywords": [
      "analysis",
      "transcription"
    ],
    "functions": [
      "transcribe_audio",
      "save_transcription",
      "process_directory",
      "main"
    ],
    "classes": [],
    "imports": [
      "os",
      "whisper"
    ],
    "preview": "import os\n\nimport whisper\n\n\ndef transcribe_audio(file_path):\n    # Load the Whisper model\n    model = whisper.load_model(\"base\")\n\n    # Transcribe the audio file\n    result = model.transcribe(file_path)\n\n    return result[\"segments\"]\n\n\ndef save_transcription(segments, output_file):\n    with open(output_file, \"w\") as f:\n        for segment in segments:\n            start = segment[\"start\"]\n            end = segment[\"end\"]",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "44",
    "name": "mp3-4-transcribe.py",
    "path": "01_core_ai_analysis/transcription/mp3-4-transcribe.py",
    "category": "01_core_ai_analysis",
    "type": "transcription",
    "lines": 178,
    "size": 6449,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis",
      "transcription"
    ],
    "functions": [
      "format_timestamp",
      "convert_mp4_to_mp3",
      "split_audio",
      "transcribe_file",
      "analyze_text_for_section",
      "process_media_directory"
    ],
    "classes": [],
    "imports": [
      "os",
      "subprocess",
      "openai",
      "dotenv"
    ],
    "preview": "import os\nimport subprocess\n\nfrom openai import OpenAI\n\nclient = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\nfrom dotenv import load_dotenv\n\n# Load environment variables from .env\nenv_path = os.path.expanduser(\"~/.env\")\nload_dotenv(dotenv_path=env_path)\n\n# Initialize OpenAI API key\n\nif not openai.api_key:\n    raise EnvironmentError(\"OpenAI API key not found. Please check your .env file.\")\n\n\n# Helper to format timestamps\ndef format_timestamp(seconds):",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "45",
    "name": "shorts.py",
    "path": "01_core_ai_analysis/transcription/shorts.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 105,
    "size": 2815,
    "docstring": "",
    "keywords": [
      "openai"
    ],
    "functions": [],
    "classes": [],
    "imports": [
      "os",
      "random",
      "dotenv",
      "openai",
      "moviepy.video.fx.crop",
      "gtts",
      "moviepy.editor"
    ],
    "preview": "# Import everything\nimport os\nimport random\n\nfrom dotenv import load_dotenv\nfrom openai import OpenAI\n\nclient = OpenAI(api_key=os.environ[\"OPENAI_API\"])\nimport moviepy.video.fx.crop as crop_vid\nfrom gtts import gTTS\nfrom moviepy.editor import *\n\nload_dotenv()\n\n# Ask for video info\ntitle = input(\"\\nEnter the name of the video >  \")\noption = input(\"Do you want AI to generate content? (yes/no) >  \")\n\nif option == \"yes\":\n    # Generate content using OpenAI API",
    "last_modified": "2025-05-04T22:47:12"
  },
  {
    "id": "46",
    "name": "categorygpt.py",
    "path": "01_core_ai_analysis/transcription/categorygpt.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 61,
    "size": 1969,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis"
    ],
    "functions": [
      "get_openai_category",
      "create_directory",
      "save_to_category",
      "categorize_files"
    ],
    "classes": [],
    "imports": [
      "os",
      "openai"
    ],
    "preview": "import os\n\nfrom openai import OpenAI\n\nclient = OpenAI(api_key=api_key)\n\n# Load the API key from the environment variable\napi_key = os.getenv(\"OPENAI_API_KEY\")\nif not api_key:\n    raise ValueError(\"API key not found. Please ensure it is set in your environment variables.\")\n\n\ndef get_openai_category(script_content):\n    try:\n        response = client.chat.completions.create(\n            model=\"gpt-3.5-turbo\",\n            messages=[\n                {\"role\": \"system\", \"content\": \"You are an expert Python programmer.\"},\n                {\n                    \"role\": \"user\",",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "47",
    "name": "batch_image_seo_pipeline_20250530222550.py",
    "path": "01_core_ai_analysis/transcription/batch_image_seo_pipeline_20250530222550.py",
    "category": "01_core_ai_analysis",
    "type": "image_processing",
    "lines": 105,
    "size": 4746,
    "docstring": "",
    "keywords": [
      "openai"
    ],
    "functions": [
      "call_gpt4o_analysis_and_prompts",
      "batch_flexible_analysis"
    ],
    "classes": [],
    "imports": [
      "csv",
      "json",
      "logging",
      "os",
      "pathlib",
      "dotenv",
      "openai",
      "tqdm"
    ],
    "preview": "import csv\nimport json\nimport logging\nimport os\nfrom pathlib import Path\n\nfrom dotenv import load_dotenv\nfrom openai import OpenAI\nfrom tqdm import tqdm\n\n# CONFIG\nload_dotenv(os.path.expanduser(\"~/.env\"))\nOPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\nif not OPENAI_API_KEY:\n    raise EnvironmentError(\"OPENAI_API_KEY not found in ~/.env\")\nclient = OpenAI(api_key=OPENAI_API_KEY)\n\nLOG_FILE = \"avatararts_flexible_analysis.log\"\nlogging.basicConfig(\n    level=logging.INFO,",
    "last_modified": "2025-09-06T12:24:07.935983"
  },
  {
    "id": "48",
    "name": "quiz-talk.py",
    "path": "01_core_ai_analysis/transcription/quiz-talk.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 108,
    "size": 3424,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis",
      "web_tools"
    ],
    "functions": [
      "generate_base_prompt",
      "refine_prompt",
      "create_image",
      "upscale_image",
      "text_to_speech"
    ],
    "classes": [],
    "imports": [
      "csv",
      "io",
      "pathlib",
      "requests",
      "openai",
      "PIL"
    ],
    "preview": "import csv\nfrom io import BytesIO\nfrom pathlib import Path\n\nimport requests\nfrom openai import OpenAI\nfrom PIL import Image, ImageResampling\n\n# Initialize the OpenAI client\n\n\ndef generate_base_prompt(question, options):\n    # Combine question and options to generate a base prompt\n    return f\"Question: {question} Options: {', '.join(options)}\"\n\n\ndef refine_prompt(prompt):\n    # Refine the prompt using ChatGPT for more creativity\n    response = openai.Completion.create(\n        model=\"gpt-4\",",
    "last_modified": "2025-09-13T05:53:50.920880"
  },
  {
    "id": "49",
    "name": "enhanced_pipeline (1).py",
    "path": "01_core_ai_analysis/transcription/enhanced_pipeline (1).py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 461,
    "size": 15867,
    "docstring": "",
    "keywords": [
      "opencv",
      "transcription"
    ],
    "functions": [
      "__init__",
      "__init__",
      "transcribe",
      "_enhanced_hook_score",
      "_analyze_subjects",
      "_sentence_windows",
      "select_topk",
      "_decide_style",
      "_get_platform_specs",
      "export_clip"
    ],
    "classes": [
      "ClipCandidate",
      "PipelineOptions",
      "OpusClonePipeline"
    ],
    "imports": [
      "__future__",
      "json",
      "os",
      "re",
      "subprocess",
      "dataclasses",
      "typing",
      "cv2",
      "numpy",
      "whisper"
    ],
    "preview": "from __future__ import annotations\n\nimport json\nimport os\nimport re\nimport subprocess\nfrom dataclasses import dataclass\nfrom typing import Any, Dict, List, Optional, Tuple\n\nimport cv2\nimport numpy as np\n\nimport whisper\n\nfrom .brand import BrandTemplate\nfrom .captions import burn_subtitles, write_ass_from_words, write_srt\nfrom .overlays import OverlayEngine\nfrom .reframe import SubjectTracker, generate_smooth_path\nfrom .segments import temporal_nms\nfrom .style_engine import decide_style",
    "last_modified": "2025-09-13T05:55:10.022964"
  },
  {
    "id": "50",
    "name": "transcript_analyzer.py",
    "path": "01_core_ai_analysis/transcription/transcript_analyzer.py",
    "category": "01_core_ai_analysis",
    "type": "analysis",
    "lines": 278,
    "size": 10598,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis",
      "transcription"
    ],
    "functions": [
      "slugify",
      "retry_with_backoff",
      "format_timestamp",
      "parse_transcript",
      "transcribe_audio",
      "analyze_text_for_section",
      "link_timestamps_to_analysis",
      "process_audio_file",
      "check_conda_env",
      "load_progress_cache"
    ],
    "classes": [],
    "imports": [
      "argparse",
      "json",
      "logging",
      "logging.handlers",
      "os",
      "random",
      "re",
      "sys",
      "threading",
      "time"
    ],
    "preview": "#!/usr/bin/env python3\nimport argparse\nimport json\nimport logging\nimport logging.handlers\nimport os\nimport random\nimport re\nimport sys\nimport threading\nimport time\nfrom concurrent.futures import ThreadPoolExecutor\nfrom pathlib import Path\n\nfrom shared.config import *\nfrom openai import OpenAI\nfrom termcolor import colored\nfrom tqdm import tqdm\n\n# ---------- CONFIG & UTILITIES ----------",
    "last_modified": "2025-10-09T05:27:15.572659"
  },
  {
    "id": "51",
    "name": "pythoncat.py",
    "path": "01_core_ai_analysis/transcription/pythoncat.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 86,
    "size": 3021,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis"
    ],
    "functions": [
      "get_openai_category",
      "sanitize_category",
      "create_directory",
      "save_to_category",
      "categorize_files"
    ],
    "classes": [],
    "imports": [
      "os",
      "openai",
      "hashlib",
      "re",
      "shutil"
    ],
    "preview": "import os\n\nfrom openai import OpenAI\n\nclient = OpenAI(api_key=api_key)\nimport hashlib\nimport re\nimport shutil\n\n# Load the API key from the environment variable\napi_key = os.getenv(\"OPENAI_API_KEY\")\nif not api_key:\n    raise ValueError(\"API key not found. Please ensure it is set in your environment variables.\")\n\n\nSOURCE_DIR = \"/Users/steven/Documents/Python\"\nDEST_DIR = \"/Users/steven/Documents/Categorized\"\n\n\ndef get_openai_category(script_content):",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "52",
    "name": "conda_env_analyzer.py",
    "path": "01_core_ai_analysis/transcription/conda_env_analyzer.py",
    "category": "01_core_ai_analysis",
    "type": "analysis",
    "lines": 247,
    "size": 8029,
    "docstring": "Conda Environment Analyzer\nAnalyzes multiple conda environments to provide insights on packages, sizes, and dependencies.",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "get_dir_size",
      "format_size",
      "get_python_version",
      "get_installed_packages",
      "analyze_environment",
      "find_duplicate_packages",
      "print_summary",
      "save_detailed_report",
      "main"
    ],
    "classes": [],
    "imports": [
      "os",
      "json",
      "subprocess",
      "pathlib",
      "collections",
      "sys"
    ],
    "preview": "#!/usr/bin/env python3\n\"\"\"\nConda Environment Analyzer\nAnalyzes multiple conda environments to provide insights on packages, sizes, and dependencies.\n\"\"\"\n\nimport os\nimport json\nimport subprocess\nfrom pathlib import Path\nfrom collections import defaultdict\nimport sys\n\n# Environment paths to analyze\nENV_PATHS = [\n    \"/Users/steven/miniforge3/envs/analytics\",\n    \"/Users/steven/miniforge3/envs/analyze\",\n    \"/Users/steven/miniforge3/envs/audio-tools\",\n    \"/Users/steven/miniforge3/envs/comic8\",\n    \"/Users/steven/miniforge3/envs/gemini\",",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "53",
    "name": "Quiz-txt2speech.py",
    "path": "01_core_ai_analysis/transcription/Quiz-txt2speech.py",
    "category": "01_core_ai_analysis",
    "type": "transcription",
    "lines": 85,
    "size": 2501,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis",
      "web_tools"
    ],
    "functions": [
      "analyze_image_with_gpt4_vision",
      "upscale_image",
      "text_to_speech"
    ],
    "classes": [],
    "imports": [
      "csv",
      "io",
      "openai",
      "requests",
      "PIL"
    ],
    "preview": "import csv\nfrom io import BytesIO\n\nimport openai\nimport requests\nfrom PIL import Image\n\n# Initialize the OpenAI client\nclient = openai(api_key=\"sk-acw35nWnLFMd23JUzcQoQ7QBhg4y5wLxoQJpR64ITBWHqBT7\")\n\n\ndef analyze_image_with_gpt4_vision(image_url):\n    # Analyze the image using GPT-4 with Vision\n    response = client.chat.completions.create(\n        model=\"gpt-4-vision-preview\",\n        messages=[\n            {\n                \"role\": \"user\",\n                \"content\": [\n                    {\"type\": \"text\", \"text\": \"What\u2019s in this image?\"},",
    "last_modified": "2025-09-13T05:53:50.768630"
  },
  {
    "id": "54",
    "name": "pythoncatorize.py",
    "path": "01_core_ai_analysis/transcription/pythoncatorize.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 60,
    "size": 1966,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis"
    ],
    "functions": [
      "get_openai_category",
      "create_directory",
      "save_to_category",
      "categorize_files"
    ],
    "classes": [],
    "imports": [
      "os",
      "openai"
    ],
    "preview": "import os\n\nfrom openai import OpenAI\n\nclient = OpenAI(api_key=api_key)\n\n# Load the API key from the environment variable\napi_key = os.getenv(\"OPENAI_API_KEY\")\nif not api_key:\n    raise ValueError(\"API key not found. Please ensure it is set in your environment variables.\")\n\n\ndef get_openai_category(script_content):\n    try:\n        response = client.chat.completions.create(\n            model=\"gpt-3.5-turbo\",\n            messages=[\n                {\"role\": \"system\", \"content\": \"You are an expert Python programmer.\"},\n                {\n                    \"role\": \"user\",",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "55",
    "name": "analyze-prompt.py",
    "path": "01_core_ai_analysis/transcription/analyze-prompt.py",
    "category": "01_core_ai_analysis",
    "type": "analysis",
    "lines": 133,
    "size": 4640,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis",
      "transcription"
    ],
    "functions": [
      "split_video_to_segments",
      "transcribe_video_segment",
      "format_timestamp",
      "analyze_text_for_section",
      "process_video_by_section"
    ],
    "classes": [],
    "imports": [
      "os",
      "subprocess",
      "openai",
      "dotenv",
      "sys"
    ],
    "preview": "import os\nimport subprocess\n\nimport openai\nfrom dotenv import load_dotenv\n\n# Load environment variables from .env (make sure your OPENAI_API_KEY is stored here)\nload_dotenv()\nopenai.api_key = os.getenv(\"OPENAI_API_KEY\")\n\n# Directory paths\nVIDEO_DIR = \"/Users/steven/Movies/2025/mp4\"  # Directory containing MP4 files\nTRANSCRIPT_DIR = \"/Users/steven/Movies/2025/mp4/transcribe\"  # Directory to save transcripts\nANALYSIS_DIR = \"/Users/steven/Movies/2025/mp4/analysis\"  # Directory to save the analysis files\n\n# Create output directories if they don't exist\nos.makedirs(TRANSCRIPT_DIR, exist_ok=True)\nos.makedirs(ANALYSIS_DIR, exist_ok=True)\n\n",
    "last_modified": "2025-09-13T05:55:06.402437"
  },
  {
    "id": "56",
    "name": "trek 3.py",
    "path": "01_core_ai_analysis/transcription/trek 3.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 70,
    "size": 2498,
    "docstring": "",
    "keywords": [
      "analysis",
      "web_tools"
    ],
    "functions": [
      "generate_speech",
      "process_csv_and_generate_speech"
    ],
    "classes": [],
    "imports": [
      "csv",
      "os",
      "requests"
    ],
    "preview": "import csv\nimport os\n\nimport requests  # Make sure to install this package\n\n\ndef generate_speech(input_text, output_path, api_key):\n    \"\"\"\n    Generates speech from text using an API (e.g., OpenAI's text-to-speech API).\n    \"\"\"\n    url = \"https://api.openai.com/v1/audio/speech\"\n    headers = {\n        \"Authorization\": f\"Bearer {api_key}\",\n        \"Content-Type\": \"application/json\",\n    }\n    data = {\n        \"model\": \"tts-1\",  # Update this according to the specific model you're using\n        \"input\": input_text,\n        \"voice\": \"shimmer\",  # Update the voice parameter as needed\n    }",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "57",
    "name": "clips.py",
    "path": "01_core_ai_analysis/transcription/clips.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 136,
    "size": 4039,
    "docstring": "",
    "keywords": [
      "organization",
      "analysis",
      "transcription"
    ],
    "functions": [
      "gen_comment_image",
      "process_content",
      "add_newlines",
      "change_char",
      "gen_title_message_image",
      "gen_title_message_clip",
      "create_comment_clip",
      "randomStringDigits",
      "gen_audio_clip",
      "audio_concatenate"
    ],
    "classes": [],
    "imports": [
      "random",
      "string",
      "textwrap",
      "io",
      "pathlib",
      "numpy",
      "soundfile",
      "gtts",
      "moviepy.audio.fx.audio_fadein",
      "moviepy.audio.fx.audio_fadeout"
    ],
    "preview": "import random\nimport string\nimport textwrap\nfrom io import BytesIO\nfrom pathlib import Path\n\nimport numpy as np\nimport soundfile as sf\nfrom gtts import gTTS\nfrom moviepy.audio.fx.audio_fadein import audio_fadein\nfrom moviepy.audio.fx.audio_fadeout import audio_fadeout\nfrom moviepy.audio.fx.audio_left_right import audio_left_right\nfrom moviepy.audio.fx.audio_loop import audio_loop\nfrom moviepy.audio.fx.audio_normalize import audio_normalize\nfrom moviepy.audio.fx.volumex import volumex\nfrom moviepy.editor import *\nfrom PIL import Image, ImageDraw, ImageFont\nfrom pydub import AudioSegment\nfrom pydub.playback import play\n",
    "last_modified": "2025-05-04T23:28:22.812944"
  },
  {
    "id": "58",
    "name": "upcreatmod (1).py",
    "path": "01_core_ai_analysis/transcription/upcreatmod (1).py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 119,
    "size": 3808,
    "docstring": "",
    "keywords": [
      "data_processing",
      "openai",
      "analysis",
      "web_tools"
    ],
    "functions": [
      "generate_clickable_title",
      "generate_description",
      "generate_tags"
    ],
    "classes": [],
    "imports": [
      "os",
      "openai",
      "io",
      "pandas",
      "requests",
      "PIL",
      "tqdm"
    ],
    "preview": "import os\n\nfrom openai import OpenAI\n\nclient = OpenAI(api_key=\"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\")\nimport io\n\nimport pandas as pd\nimport requests\nfrom PIL import Image\nfrom tqdm import tqdm\n\n# Set OpenAI and Stability API keys\nstability_ai_key = \"sk-2O6mutk6X4HI9olxMeGKYv0MpXcNRzs6fVRPJMdEvgEzRRII\"\n\n# Function to generate a clickable title\n\n\ndef generate_clickable_title(detail):\n    prompt = f\"Generate a catchy and clickable title for a 3D Breakthrough Grinch Round Christmas ORnament with the theme: '{detail}'. Maximum 30 characters. At the end of each title write Cork Back Coaster\"",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "59",
    "name": "batch_image_seo_pipeline_20250530220434.py",
    "path": "01_core_ai_analysis/transcription/batch_image_seo_pipeline_20250530220434.py",
    "category": "01_core_ai_analysis",
    "type": "analysis",
    "lines": 429,
    "size": 17578,
    "docstring": "batch_image_seo_pipeline.py\n\n1) Discovers all image files under a given folder.\n2) Extracts technical metadata via Pillow.\n3) Calls GPT-4o Vision (multimodal) using a \u201csystem\u201d + \u201cuser\u201d prompt.\n4) Parses the JSON response into structured fields:\n   main_subject, style, color_palette, tags, orientation, suggested_products,\n   SEO_title, SEO_description, emotion, safety_rating, dominant_keyword.\n5) Builds a \u201csource\u201d tag: folder-subfolder-YYYYMMDD.\n6) Applies a \u201cTop 5% SEO Analytics\u201d framework to generate SEO columns:\n   \u2013 SEO Keywords, Traffic Source, CRO Tactic, Backlink Source, Engagement Rate.\n7) Adds niche-specific design prompts (Geeky, Dark Humor, Anime).\n8) Writes everything into a final CSV for bulk upload.\n\nDependencies:\n    pip install openai python-dotenv pillow tqdm backoff",
    "keywords": [
      "openai"
    ],
    "functions": [
      "load_openai_client",
      "retry_on_api_error",
      "call_gpt4o",
      "get_image_tech_meta",
      "build_source_tag",
      "discover_images",
      "build_gpt_messages",
      "parse_args",
      "main"
    ],
    "classes": [],
    "imports": [
      "argparse",
      "csv",
      "json",
      "logging",
      "os",
      "sys",
      "time",
      "datetime",
      "pathlib",
      "typing"
    ],
    "preview": "#!/usr/bin/env python3\n\"\"\"\nbatch_image_seo_pipeline.py\n\n1) Discovers all image files under a given folder.\n2) Extracts technical metadata via Pillow.\n3) Calls GPT-4o Vision (multimodal) using a \u201csystem\u201d + \u201cuser\u201d prompt.\n4) Parses the JSON response into structured fields:\n   main_subject, style, color_palette, tags, orientation, suggested_products,\n   SEO_title, SEO_description, emotion, safety_rating, dominant_keyword.\n5) Builds a \u201csource\u201d tag: folder-subfolder-YYYYMMDD.\n6) Applies a \u201cTop 5% SEO Analytics\u201d framework to generate SEO columns:\n   \u2013 SEO Keywords, Traffic Source, CRO Tactic, Backlink Source, Engagement Rate.\n7) Adds niche-specific design prompts (Geeky, Dark Humor, Anime).\n8) Writes everything into a final CSV for bulk upload.\n\nDependencies:\n    pip install openai python-dotenv pillow tqdm backoff\n\"\"\"\n",
    "last_modified": "2025-09-13T05:53:26.523803"
  },
  {
    "id": "60",
    "name": "fancier.py",
    "path": "01_core_ai_analysis/transcription/fancier.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 239,
    "size": 8122,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis"
    ],
    "functions": [
      "sanitize_filename",
      "resize_image",
      "process_batch",
      "process_images_and_generate_csv",
      "write_csv",
      "main"
    ],
    "classes": [],
    "imports": [
      "csv",
      "os",
      "time",
      "datetime",
      "dotenv",
      "openai",
      "PIL"
    ],
    "preview": "import csv\nimport os\nimport time\nfrom datetime import datetime\n\nfrom dotenv import load_dotenv\nfrom openai import OpenAI\nfrom PIL import Image, UnidentifiedImageError\n\n# Load environment variables\nenv_path = \"/Users/steven/.env\"\nload_dotenv(dotenv_path=env_path)\n\n# Initialize OpenAI API key\napi_key = os.getenv(\"OPENAI_API_KEY\")\n\n# Error checking for OpenAI API key\nif not api_key:\n    raise EnvironmentError(\"OpenAI API key not found. Please check your .env file.\")\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "61",
    "name": "split_audio.py",
    "path": "01_core_ai_analysis/transcription/split_audio.py",
    "category": "01_core_ai_analysis",
    "type": "transcription",
    "lines": 96,
    "size": 3975,
    "docstring": "Audio Splitter for Transcription (convenience version)",
    "keywords": [],
    "functions": [
      "human_bytes",
      "compute_chunk_ms_from_target_size",
      "split_file",
      "main"
    ],
    "classes": [],
    "imports": [
      "argparse",
      "csv",
      "pathlib",
      "typing",
      "pydub",
      "json"
    ],
    "preview": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"Audio Splitter for Transcription (convenience version)\"\"\"\nimport argparse, csv\nfrom pathlib import Path\nfrom typing import List, Optional\nfrom pydub import AudioSegment\n\ndef human_bytes(n: int) -> str:\n    units = ['B','KB','MB','GB','TB']\n    i = 0; x = float(n)\n    while x >= 1024 and i < len(units)-1:\n        x /= 1024.0; i += 1\n    return f\"{x:.2f} {units[i]}\"\n\ndef compute_chunk_ms_from_target_size(file_path: Path, audio: AudioSegment, target_size_mb: float,\n                                      min_minutes: int = 1, max_minutes: int = 15) -> int:\n    total_bytes = file_path.stat().st_size\n    total_ms = len(audio)\n    if total_ms == 0:",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "62",
    "name": "mp4-transcript copy.py",
    "path": "01_core_ai_analysis/transcription/mp4-transcript copy.py",
    "category": "01_core_ai_analysis",
    "type": "video_processing",
    "lines": 155,
    "size": 6892,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis",
      "transcription"
    ],
    "functions": [
      "split_video_to_segments",
      "transcribe_video_segment",
      "format_timestamp",
      "analyze_text_for_section",
      "process_video_by_section"
    ],
    "classes": [],
    "imports": [
      "openai",
      "os",
      "subprocess",
      "dotenv",
      "sys"
    ],
    "preview": "from openai import OpenAI\n\nclient = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\nimport os\nimport subprocess\n\nfrom dotenv import load_dotenv\n\n# Load environment variables from .env (make sure your OPENAI_API_KEY is stored here)\n# Load environment variables from .env (make sure your OPENAI_API_KEY is stored here)\nenv_path = \"/Users/steven/.env\"\nload_dotenv(dotenv_path=env_path)\n\n# Directory paths\nVIDEO_DIR = \"/Users/steven/AvaTarArTs/canva/Video\"  # Directory containing MP4 files\nTRANSCRIPT_DIR = \"/Users/steven/AvaTarArTs/canva/Video/transcribe\"  # Directory to save transcripts\nANALYSIS_DIR = (\n    \"/Users/steven/AvaTarArTs/canva/Video/analysis\"  # Directory to save the analysis files\n)\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "63",
    "name": "mp4-trans.py",
    "path": "01_core_ai_analysis/transcription/mp4-trans.py",
    "category": "01_core_ai_analysis",
    "type": "video_processing",
    "lines": 139,
    "size": 5119,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis",
      "transcription"
    ],
    "functions": [
      "split_video_to_segments",
      "transcribe_video_segment",
      "format_timestamp",
      "analyze_text_for_section",
      "process_video_by_section"
    ],
    "classes": [],
    "imports": [
      "openai",
      "os",
      "subprocess",
      "dotenv",
      "sys"
    ],
    "preview": "from openai import OpenAI\n\nclient = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\nimport os\nimport subprocess\n\nfrom dotenv import load_dotenv\n\n# Load environment variables from .env\nload_dotenv()\n\n# Directory paths\nVIDEO_DIR = \"/Users/steven/Movies/Kath/Katheria_and_Salome_The_Daughters_of_Destinay-30m_compressed_segments\"  # Directory containing MP4 files\nTRANSCRIPT_DIR = \"/Users/steven/Movies/Kath/Katheria_and_Salome_The_Daughters_of_Destinay-30m_compressed_segments/transcribe\"  # Directory to save transcripts\nANALYSIS_DIR = \"/Users/steven/Movies/Kath/analysis\"  # Directory to save the analysis files\n\n# Create output directories if they don't exist\nos.makedirs(TRANSCRIPT_DIR, exist_ok=True)\nos.makedirs(ANALYSIS_DIR, exist_ok=True)\n",
    "last_modified": "2025-09-13T05:55:06.473366"
  },
  {
    "id": "64",
    "name": "trek.py",
    "path": "01_core_ai_analysis/transcription/trek.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 69,
    "size": 2503,
    "docstring": "",
    "keywords": [
      "analysis",
      "web_tools"
    ],
    "functions": [
      "generate_speech",
      "process_csv_and_generate_speech"
    ],
    "classes": [],
    "imports": [
      "csv",
      "os",
      "requests"
    ],
    "preview": "import csv\nimport os\n\nimport requests  # Make sure to install this package\n\n\ndef generate_speech(input_text, output_path, api_key):\n    \"\"\"\n    Generates speech from text using an API (e.g., OpenAI's text-to-speech API).\n    \"\"\"\n    url = \"https://api.openai.com/v1/audio/speech\"\n    headers = {\n        \"Authorization\": f\"Bearer {api_key}\",\n        \"Content-Type\": \"application/json\",\n    }\n    data = {\n        \"model\": \"tts-1\",  # Update this according to the specific model you're using\n        \"input\": input_text,\n        \"voice\": \"shimmer\",  # Update the voice parameter as needed\n    }",
    "last_modified": "2025-09-13T05:53:51.076590"
  },
  {
    "id": "65",
    "name": "mp3-csv 1.py",
    "path": "01_core_ai_analysis/transcription/mp3-csv 1.py",
    "category": "01_core_ai_analysis",
    "type": "audio_processing",
    "lines": 107,
    "size": 4019,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis",
      "transcription"
    ],
    "functions": [
      "format_timestamp",
      "transcribe_audio",
      "analyze_text_for_section",
      "process_audio_file",
      "process_audio_directory"
    ],
    "classes": [],
    "imports": [
      "os",
      "openai",
      "dotenv"
    ],
    "preview": "import os\n\nimport openai\nfrom dotenv import load_dotenv\n\n# Load environment variables\nload_dotenv()\nopenai.api_key = os.getenv(\"OPENAI_API_KEY\")\nif not openai.api_key:\n    raise EnvironmentError(\"OpenAI API key not found. Please check your .env file.\")\n\n\ndef format_timestamp(seconds):\n    \"\"\"Format seconds into MM:SS timestamp.\"\"\"\n    minutes = int(seconds // 60)\n    seconds = seconds % 60\n    return f\"{minutes:02d}:{int(seconds):02d}\"\n\n\ndef transcribe_audio(file_path):",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "66",
    "name": "thinketh_tts.py",
    "path": "01_core_ai_analysis/transcription/thinketh_tts.py",
    "category": "01_core_ai_analysis",
    "type": "transcription",
    "lines": 155,
    "size": 4665,
    "docstring": "thinketh_tts.py\nText-to-Speech generator for 'As A Man Thinketh' (James Allen)\n\nFeatures:\n - Reads DOCX\n - Splits into chapters automatically\n - Handles token limits by chunking text\n - Merges all chunks into one MP3 per chapter\n - Skips already completed files",
    "keywords": [
      "openai"
    ],
    "functions": [
      "load_env",
      "read_docx_text",
      "normalize_text",
      "slugify",
      "extract_chapters",
      "synthesize_to_mp3",
      "main"
    ],
    "classes": [],
    "imports": [
      "os",
      "re",
      "pathlib",
      "docx",
      "dotenv",
      "openai",
      "pydub"
    ],
    "preview": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\nthinketh_tts.py\nText-to-Speech generator for 'As A Man Thinketh' (James Allen)\n\nFeatures:\n - Reads DOCX\n - Splits into chapters automatically\n - Handles token limits by chunking text\n - Merges all chunks into one MP3 per chapter\n - Skips already completed files\n\"\"\"\n\nimport os\nimport re\nfrom pathlib import Path\nfrom docx import Document\nfrom dotenv import load_dotenv\nfrom openai import OpenAI",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "67",
    "name": "main 3.py",
    "path": "01_core_ai_analysis/transcription/main 3.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 21,
    "size": 613,
    "docstring": "",
    "keywords": [],
    "functions": [
      "main"
    ],
    "classes": [],
    "imports": [
      "csv",
      "generate_speech"
    ],
    "preview": "import csv\n\nfrom generate_speech import generate_speech\n\n\ndef main():\n    csv_path = \"/Users/steven/Music/quiz-talk/Gtrivia - Sheet1.csv\"\n\n    with open(csv_path, newline=\"\", encoding=\"utf-8\") as csvfile:\n        reader = csv.DictReader(csvfile)\n        for i, row in enumerate(reader):\n            # Assuming 'Question' is the column name\n            question_text = row[\"Question\"]\n            output_path = f\"/Users/steven/Music/quiz-talk/speech/question_{\n                i + 1}.mp3\"\n            generate_speech(question_text, voice=\"shimmer\", output_path=output_path)\n\n\nif __name__ == \"__main__\":\n    main()",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "68",
    "name": "upcreatmod.py",
    "path": "01_core_ai_analysis/transcription/upcreatmod.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 113,
    "size": 3796,
    "docstring": "",
    "keywords": [
      "data_processing",
      "openai",
      "analysis",
      "web_tools"
    ],
    "functions": [
      "generate_clickable_title",
      "generate_description",
      "generate_tags"
    ],
    "classes": [],
    "imports": [
      "os",
      "openai",
      "io",
      "pandas",
      "requests",
      "PIL",
      "tqdm"
    ],
    "preview": "import os\n\nfrom openai import OpenAI\n\nclient = OpenAI(api_key=\"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\")\nimport io\n\nimport pandas as pd\nimport requests\nfrom PIL import Image\nfrom tqdm import tqdm\n\n# Set OpenAI and Stability API keys\nstability_ai_key = \"sk-2O6mutk6X4HI9olxMeGKYv0MpXcNRzs6fVRPJMdEvgEzRRII\"\n\n\n# Function to generate a clickable title\ndef generate_clickable_title(detail):\n    prompt = f\"Generate a catchy and clickable title for a 3D Breakthrough Grinch Round Christmas ORnament with the theme: '{detail}'. Maximum 30 characters. At the end of each title write Cork Back Coaster\"\n    response = client.chat.completions.create(",
    "last_modified": "2025-05-04T22:47:11.602837"
  },
  {
    "id": "69",
    "name": "segments.py",
    "path": "01_core_ai_analysis/transcription/segments.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 75,
    "size": 2011,
    "docstring": "",
    "keywords": [],
    "functions": [
      "detect_scenes",
      "detect_silence_segments",
      "ffprobe_duration",
      "merge_overlaps"
    ],
    "classes": [],
    "imports": [
      "__future__",
      "json",
      "math",
      "os",
      "re",
      "subprocess",
      "typing",
      "pydub",
      "scenedetect",
      "scenedetect.detectors"
    ],
    "preview": "from __future__ import annotations\n\nimport json\nimport math\nimport os\nimport re\nimport subprocess\nfrom typing import Any, Dict, List, Tuple\n\nfrom pydub import AudioSegment, silence\nfrom scenedetect import SceneManager, VideoManager\nfrom scenedetect.detectors import ContentDetector\n\n\ndef detect_scenes(video_path: str, threshold: float = 27.0) -> List[Tuple[float, float]]:\n    \"\"\"Return list of (start_sec, end_sec) scene ranges using PySceneDetect.\"\"\"\n    vm = VideoManager([video_path])\n    vm.set_downscale_factor()\n    sm = SceneManager()\n    sm.add_detector(ContentDetector(threshold=threshold))",
    "last_modified": "2025-09-13T05:55:09.639520"
  },
  {
    "id": "70",
    "name": "generate_speech 2 (1).py",
    "path": "01_core_ai_analysis/transcription/generate_speech 2 (1).py",
    "category": "01_core_ai_analysis",
    "type": "transcription",
    "lines": 44,
    "size": 1447,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis"
    ],
    "functions": [
      "generate_speech",
      "main"
    ],
    "classes": [],
    "imports": [
      "csv",
      "openai"
    ],
    "preview": "import csv\n\nfrom openai import OpenAI\n\nclient = OpenAI(api_key='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')\n\n# Replace 'your_api_key_here' with your actual OpenAI API key\n\n\ndef generate_speech(text, voice='shimmer', output_path='speech.mp3'):\n    response = client.audio.create(model=\"tts-1-hd\",  # Ensure this model supports the audio creation\n                                   input=text,\n                                   voice=voice,\n                                   format=\"mp3\")\n    with open(output_path, \"wb\") as file:\n        file.write(response.content)\n\n\ndef main():\n    # Update this path to where your CSV is located",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "71",
    "name": "analyze-prompts.py",
    "path": "01_core_ai_analysis/transcription/analyze-prompts.py",
    "category": "01_core_ai_analysis",
    "type": "analysis",
    "lines": 99,
    "size": 4036,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis",
      "transcription"
    ],
    "functions": [
      "transcribe_audio",
      "format_timestamp",
      "analyze_text_for_section",
      "process_audio_directory"
    ],
    "classes": [],
    "imports": [
      "openai",
      "os",
      "subprocess",
      "dotenv"
    ],
    "preview": "from openai import OpenAI\n\nclient = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\nimport os\nimport subprocess\n\nfrom dotenv import load_dotenv\n\n# Load environment variables from .env (make sure your OPENAI_API_KEY is stored here)\nenv_path = \"/Users/steven/.env\"\nload_dotenv(dotenv_path=env_path)\n\n# Directory paths\nAUDIO_DIR = (\n    \"/Users/steven/Music/NocTurnE-meLoDieS/Discography/Audio\"  # Directory containing MP3 files\n)\nTRANSCRIPT_DIR = \"/Users/steven/Music/NocTurnE-meLoDieS/Discography/Audio/transcript\"  # Directory to save transcripts\nANALYSIS_DIR = \"/Users/steven/Music/NocTurnE-meLoDieS/Discography/Audio/analysis\"  # Directory to save the analysis files\n\n# Create output directories if they don't exist",
    "last_modified": "2025-09-13T05:53:55.019781"
  },
  {
    "id": "72",
    "name": "curld.py",
    "path": "01_core_ai_analysis/transcription/curld.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 50,
    "size": 1786,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "generate_speech_curl",
      "main"
    ],
    "classes": [],
    "imports": [
      "csv",
      "json",
      "os",
      "subprocess"
    ],
    "preview": "import csv\nimport json\nimport os  # Import the os module\nimport subprocess\n\n\ndef generate_speech_curl(input_text, output_path, api_key):\n    data = {\"model\": \"tts-1\", \"input\": input_text, \"voice\": \"shimmer\"}\n    command = [\n        \"curl\",\n        \"https://api.openai.com/v1/audio/speech\",\n        \"-H\",\n        f\"Authorization: Bearer {api_key}\",\n        \"-H\",\n        \"Content-Type: application/json\",\n        \"-d\",\n        json.dumps(data),\n        \"--output\",\n        output_path,\n    ]",
    "last_modified": "2025-09-13T05:53:50.849222"
  },
  {
    "id": "73",
    "name": "python_repo_analyzer.py",
    "path": "01_core_ai_analysis/transcription/python_repo_analyzer.py",
    "category": "01_core_ai_analysis",
    "type": "analysis",
    "lines": 679,
    "size": 25234,
    "docstring": "Python Repository Analyzer & Portfolio Generator\nAnalyzes Python scripts, categorizes them with GPT-4, and generates portfolio-ready outputs",
    "keywords": [
      "data_processing",
      "openai",
      "analysis"
    ],
    "functions": [
      "__init__",
      "load_environment",
      "is_excluded",
      "extract_imports",
      "analyze_script_content",
      "categorize_script",
      "analyze_with_gpt",
      "scan_repositories",
      "generate_csv_report",
      "generate_html_portfolio"
    ],
    "classes": [
      "PythonRepoAnalyzer"
    ],
    "imports": [
      "csv",
      "hashlib",
      "json",
      "os",
      "re",
      "subprocess",
      "datetime",
      "pathlib",
      "typing",
      "pandas"
    ],
    "preview": "#!/usr/bin/env python3\n\"\"\"\nPython Repository Analyzer & Portfolio Generator\nAnalyzes Python scripts, categorizes them with GPT-4, and generates portfolio-ready outputs\n\"\"\"\n\nimport csv\nimport hashlib\nimport json\nimport os\nimport re\nimport subprocess\nfrom datetime import datetime\nfrom pathlib import Path\nfrom typing import Any, Dict, List, Optional\n\ntry:\n    import pandas as pd\n    from dotenv import load_dotenv\n    from openai import OpenAI",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "74",
    "name": "mp3_batch.py",
    "path": "01_core_ai_analysis/transcription/mp3_batch.py",
    "category": "01_core_ai_analysis",
    "type": "audio_processing",
    "lines": 46,
    "size": 1417,
    "docstring": "",
    "keywords": [],
    "functions": [
      "convert_to_mp3"
    ],
    "classes": [],
    "imports": [
      "os",
      "subprocess"
    ],
    "preview": "import os\nimport subprocess\n\n# Folder containing your source videos\nSOURCE_DIR = \"/Users/steven/Movies/PROJECt2025-DoMinIon\"\n# Bitrate and sample rate for output MP3\nBITRATE = \"32k\"  # change to \"64k\" if you want slightly better quality\nSAMPLERATE = \"16000\"  # Whisper-friendly\nCHANNELS = \"1\"  # mono to reduce size\n\n\ndef convert_to_mp3(input_path, output_path):\n    cmd = [\n        \"ffmpeg\",\n        \"-y\",  # overwrite without asking\n        \"-i\",\n        input_path,  # input file\n        \"-vn\",  # strip video\n        \"-ac\",\n        CHANNELS,",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "75",
    "name": "quantumforgelabs.py",
    "path": "01_core_ai_analysis/transcription/quantumforgelabs.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 349,
    "size": 15121,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "os",
      "zipfile",
      "shutil",
      "textwrap",
      "json",
      "pathlib",
      "PIL"
    ],
    "preview": "# Create a ready-to-deploy static site scaffold for QuantumForgeLabs\n# Files:\n# - index.html\n# - assets/style.css\n# - assets/qfl-logo.svg (simple torus + QFL monogram)\n# - assets/og-image.png (copy from previously generated image if available)\n# - README.md\n# - LICENSE (MIT)\n# Then zip everything for download.\n\nimport os, zipfile, shutil, textwrap, json, pathlib\n\nroot = \"/mnt/data/qfl_site\"\nassets = os.path.join(root, \"assets\")\nos.makedirs(assets, exist_ok=True)\n\n# Minimal SVG logo (monospace QFL with torus-ish ring)\nqfl_svg = \"\"\"<svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 640 640\" role=\"img\" aria-label=\"QuantumForgeLabs logo\">\n  <defs>\n    <linearGradient id=\"g\" x1=\"0\" x2=\"1\" y1=\"0\" y2=\"1\">",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "76",
    "name": "transcribe 1.py",
    "path": "01_core_ai_analysis/transcription/transcribe 1.py",
    "category": "01_core_ai_analysis",
    "type": "transcription",
    "lines": 76,
    "size": 2537,
    "docstring": "",
    "keywords": [
      "analysis",
      "transcription"
    ],
    "functions": [
      "convert_video_to_audio",
      "transcribe_audio",
      "save_transcription",
      "process_directory",
      "main"
    ],
    "classes": [],
    "imports": [
      "os",
      "sys",
      "ffmpeg",
      "whisper"
    ],
    "preview": "import os\nimport sys\n\nimport ffmpeg\n\nimport whisper\n\n\ndef convert_video_to_audio(video_file, output_dir):\n    try:\n        base_name = os.path.basename(video_file)\n        output_file = os.path.join(output_dir, f\"{os.path.splitext(base_name)[0]}.mp3\")\n\n        ffmpeg.input(video_file).output(output_file).run(overwrite_output=True)\n        print(f\"Converted {video_file} to {output_file}\")\n        return output_file\n    except Exception as e:\n        print(f\"An error occurred while converting {video_file}: {e}\")\n        return None\n",
    "last_modified": "2025-09-11T13:27:02.807489"
  },
  {
    "id": "77",
    "name": "mp3-csv 5.py",
    "path": "01_core_ai_analysis/transcription/mp3-csv 5.py",
    "category": "01_core_ai_analysis",
    "type": "audio_processing",
    "lines": 101,
    "size": 4057,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis",
      "transcription"
    ],
    "functions": [
      "format_timestamp",
      "transcribe_audio",
      "analyze_text_for_section",
      "process_audio_file",
      "process_audio_directory"
    ],
    "classes": [],
    "imports": [
      "os",
      "openai",
      "dotenv"
    ],
    "preview": "import os\n\nimport openai\nfrom dotenv import load_dotenv\n\n# Load environment variables from ~/.env\nload_dotenv()\nopenai.api_key = os.getenv(\"OPENAI_API_KEY\")\nif not openai.api_key:\n    raise EnvironmentError(\"OpenAI API key not found. Please check your .env file.\")\n\n\ndef format_timestamp(seconds):\n    \"\"\"Format seconds into MM:SS timestamp.\"\"\"\n    minutes, seconds = divmod(int(seconds), 60)  # Convert seconds to int\n    return f\"{minutes:02d}:{seconds:02d}\"\n\n\ndef transcribe_audio(file_path):\n    \"\"\"Transcribe audio file using OpenAI's Whisper API.\"\"\"",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "78",
    "name": "deep_analyze_split.py",
    "path": "01_core_ai_analysis/transcription/deep_analyze_split.py",
    "category": "01_core_ai_analysis",
    "type": "analysis",
    "lines": 133,
    "size": 6216,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis",
      "transcription"
    ],
    "functions": [
      "format_timestamp",
      "get_video_duration",
      "transcribe_audio",
      "analyze_text",
      "process_media_directory"
    ],
    "classes": [],
    "imports": [
      "logging",
      "os",
      "sys",
      "pathlib",
      "openai",
      "dotenv",
      "moviepy.editor"
    ],
    "preview": "import logging\nimport os\nimport sys\nfrom pathlib import Path\nimport openai\nfrom dotenv import load_dotenv\nfrom moviepy.editor import VideoFileClip\n\n# Setup logging\nlogging.basicConfig(level=logging.INFO, format=\"%(asctime)s [%(levelname)s] %(message)s\")\n\n# Load environment variables\nenv_path = Path(\"~/.env\")\nload_dotenv(dotenv_path=env_path)\nopenai.api_key = os.getenv(\"OPENAI_API_KEY\")\n\n# Base config\nconfig = {\"base_dir\": \"\"}\nif not config.get(\"base_dir\"):\n    base_dir_input = input(\"Enter base directory (leave blank for current): \").strip()",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "79",
    "name": "analyze-shorts-info (1).py",
    "path": "01_core_ai_analysis/transcription/analyze-shorts-info (1).py",
    "category": "01_core_ai_analysis",
    "type": "analysis",
    "lines": 122,
    "size": 4761,
    "docstring": "",
    "keywords": [
      "analysis",
      "transcription"
    ],
    "functions": [
      "transcribe_audio",
      "format_timestamp",
      "analyze_text_for_section",
      "process_audio_directory"
    ],
    "classes": [],
    "imports": [
      "logging",
      "os",
      "sys",
      "time",
      "concurrent.futures",
      "dotenv",
      "termcolor",
      "tqdm"
    ],
    "preview": "cimport openai\n\nimport logging\nimport os\nimport sys\nimport time\nfrom concurrent.futures import ThreadPoolExecutor\n\nfrom dotenv import load_dotenv\nfrom termcolor import colored\nfrom tqdm import tqdm\n\n# Load environment variables from .env (make sure your OPENAI_API_KEY is stored here)\nenv_path = '/Users/steven/.env'  # Update this path if necessary\nload_dotenv(dotenv_path=env_path)\n\n# Initialize OpenAI API key\nopenai.api_key = os.getenv('OPENAI_API_KEY')\n\n# Error checking for OpenAI API key",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "80",
    "name": "speech_recog_transformers.py",
    "path": "01_core_ai_analysis/transcription/speech_recog_transformers.py",
    "category": "01_core_ai_analysis",
    "type": "transcription",
    "lines": 36,
    "size": 1118,
    "docstring": "author: abhilasha_lodha",
    "keywords": [],
    "functions": [
      "speech2TextTransformer"
    ],
    "classes": [],
    "imports": [
      "os",
      "librosa",
      "numpy",
      "soundfile",
      "torch",
      "IPython.display",
      "scipy.io",
      "transformers"
    ],
    "preview": "\"\"\"\nauthor: abhilasha_lodha\n\"\"\"\n\nimport os\n\nimport librosa\nimport numpy as np\nimport soundfile as sf\nimport torch\nfrom IPython.display import Audio\nfrom scipy.io import wavfile\nfrom transformers import Wav2Vec2ForCTC, Wav2Vec2Tokenizer\n\ntokenizer = Wav2Vec2Tokenizer.from_pretrained(\"facebook/wav2vec2-base-960h\")\nmodel = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-base-960h\")\n\n\ndef speech2TextTransformer(folder_path):\n    for files in os.listdir(folder_path):",
    "last_modified": "2025-03-28T18:37:11"
  },
  {
    "id": "81",
    "name": "story-key-trans.py",
    "path": "01_core_ai_analysis/transcription/story-key-trans.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 137,
    "size": 6492,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis",
      "transcription"
    ],
    "functions": [
      "format_timestamp",
      "transcribe_file",
      "analyze_text_for_section",
      "process_media_directory"
    ],
    "classes": [],
    "imports": [
      "os",
      "subprocess",
      "openai",
      "dotenv"
    ],
    "preview": "import os\nimport subprocess\n\nimport openai\nfrom dotenv import load_dotenv\n\n# Load OpenAI API Key\nload_dotenv()\nopenai.api_key = os.getenv(\"OPENAI_API_KEY\")\n\n# Error checking for OpenAI API key\nif not openai.api_key:\n    raise EnvironmentError(\"OpenAI API key not found. Please check your .env file.\")\n\n\n# Helper to format timestamps\ndef format_timestamp(seconds):\n    \"\"\"Convert seconds into the format MM:SS.\"\"\"\n    minutes = int(seconds // 60)\n    seconds = seconds % 60",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "82",
    "name": "batch-info-python.py",
    "path": "01_core_ai_analysis/transcription/batch-info-python.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 193,
    "size": 7135,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis"
    ],
    "functions": [
      "categorize_script",
      "generate_pydocgen",
      "get_openai_batch_titles",
      "suggest_script_titles_batch",
      "process_directory_with_batching"
    ],
    "classes": [],
    "imports": [
      "os",
      "openai",
      "csv",
      "re",
      "subprocess",
      "collections",
      "dotenv"
    ],
    "preview": "import os\n\nfrom openai import OpenAI\n\nclient = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\nimport csv\nimport re\nimport subprocess\nfrom collections import Counter\n\nfrom dotenv import load_dotenv\n\n# Load environment variables from the specified .env file\nload_dotenv(\"/Users/steven/Documents/python/.env\")\n\n# Set OpenAI API key from the environment variable\n\n\n# Function to categorize scripts based on content\ndef categorize_script(content, file_name):",
    "last_modified": "2025-09-13T05:53:47.775497"
  },
  {
    "id": "83",
    "name": "migrate_projects.py",
    "path": "01_core_ai_analysis/transcription/migrate_projects.py",
    "category": "01_core_ai_analysis",
    "type": "organization",
    "lines": 497,
    "size": 18870,
    "docstring": "Automated Python Projects Reorganization Script\nSafely migrates your projects to the new organized structure",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "__init__",
      "log_action",
      "create_backup",
      "create_new_structure",
      "create_shared_libraries",
      "migrate_analysis_scripts",
      "migrate_youtube_projects",
      "migrate_ai_creative_tools",
      "migrate_web_scraping_tools",
      "migrate_audio_video_tools"
    ],
    "classes": [
      "ProjectMigrator"
    ],
    "imports": [
      "os",
      "shutil",
      "json",
      "pathlib",
      "datetime"
    ],
    "preview": "#!/usr/bin/env python3\n\"\"\"\nAutomated Python Projects Reorganization Script\nSafely migrates your projects to the new organized structure\n\"\"\"\n\nimport os\nimport shutil\nimport json\nfrom pathlib import Path\nfrom datetime import datetime\n\nclass ProjectMigrator:\n    def __init__(self, base_path=\"/Users/steven/Documents/python\"):\n        self.base_path = Path(base_path)\n        self.migration_log = []\n        self.backup_path = self.base_path / \"MIGRATION_BACKUP\"\n        \n    def log_action(self, action, source, destination=None, status=\"SUCCESS\"):\n        \"\"\"Log migration actions for rollback capability.\"\"\"",
    "last_modified": "2025-10-09T05:25:40.903623"
  },
  {
    "id": "84",
    "name": "speech_to_text.py",
    "path": "01_core_ai_analysis/transcription/speech_to_text.py",
    "category": "01_core_ai_analysis",
    "type": "transcription",
    "lines": 97,
    "size": 3323,
    "docstring": "",
    "keywords": [],
    "functions": [
      "mp3_to_wav_Conversion",
      "split_files_with_timestamp",
      "writeInFile_key_value"
    ],
    "classes": [],
    "imports": [
      "os",
      "subprocess",
      "time",
      "speech_recognition",
      "pydub",
      "pydub.utils"
    ],
    "preview": "import os\nimport subprocess\nimport time\n\nimport speech_recognition as sr\nfrom pydub import AudioSegment\nfrom pydub.utils import make_chunks\n\n\ndef mp3_to_wav_Conversion(mp3_src, wav_dst):\n    subprocess.call([\"ffmpeg\", \"-i\", mp3_src, wav_dst])\n    # test audio of the dst file\n    test_audio = AudioSegment.from_file(wav_dst, \"wav\")\n    return test_audio\n\n\ndef split_files_with_timestamp(test_audio):\n    chunk_length_ms = 10000\n    chunks = make_chunks(test_audio, chunk_length_ms)\n    return chunks",
    "last_modified": "2025-09-13T05:53:45.851465"
  },
  {
    "id": "85",
    "name": "config 2.py",
    "path": "01_core_ai_analysis/transcription/config 2.py",
    "category": "01_core_ai_analysis",
    "type": "setup",
    "lines": 22,
    "size": 762,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [],
    "preview": "# MODEL SETTINGS\nMODEL = \"gpt-4-1106-preview\"  # Updated model name\nAPI_PARAM = {\n    \"engine\": MODEL,\n    \"max_tokens\": 512,\n    \"temperature\": 0.77,\n    \"top_p\": 1,\n    \"frequency_penalty\": 0.28,\n    \"presence_penalty\": 0.13,\n}\n# VIDEO SETTINGS\nCHANNEL_NAME = \"iChoake\"\nDURATION = 8\nSIZE = (1080, 1920)\nFPS = 30\n# FOLDERS\nVIDEO = \"video\"\nMUSIC = \"music\"\n\nVID_TO_GENRATE = 12  # How many videos generate for each request",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "86",
    "name": "batch_image_seo_pipeline_20250530220659.py",
    "path": "01_core_ai_analysis/transcription/batch_image_seo_pipeline_20250530220659.py",
    "category": "01_core_ai_analysis",
    "type": "image_processing",
    "lines": 409,
    "size": 16734,
    "docstring": "",
    "keywords": [
      "openai"
    ],
    "functions": [
      "load_openai_client",
      "retry_on_api_error",
      "call_gpt4o",
      "get_image_tech_meta",
      "build_source_tag",
      "discover_images",
      "build_gpt_messages",
      "parse_args",
      "main"
    ],
    "classes": [],
    "imports": [
      "argparse",
      "csv",
      "json",
      "logging",
      "os",
      "sys",
      "time",
      "datetime",
      "pathlib",
      "typing"
    ],
    "preview": "import argparse\nimport csv\nimport json\nimport logging\nimport os\nimport sys\nimport time\nfrom datetime import datetime\nfrom pathlib import Path\nfrom typing import Any, Dict, List, Optional\n\nimport backoff  # pip install backoff\nfrom dotenv import load_dotenv\nfrom openai import OpenAI, OpenAIAPIError\nfrom PIL import Image, UnidentifiedImageError\nfrom tqdm import tqdm\n\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n# CONFIGURATION\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500",
    "last_modified": "2025-09-13T05:53:25.488247"
  },
  {
    "id": "87",
    "name": "mp3-csv 4.py",
    "path": "01_core_ai_analysis/transcription/mp3-csv 4.py",
    "category": "01_core_ai_analysis",
    "type": "audio_processing",
    "lines": 101,
    "size": 4057,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis",
      "transcription"
    ],
    "functions": [
      "format_timestamp",
      "transcribe_audio",
      "analyze_text_for_section",
      "process_audio_file",
      "process_audio_directory"
    ],
    "classes": [],
    "imports": [
      "os",
      "openai",
      "dotenv"
    ],
    "preview": "import os\n\nimport openai\nfrom dotenv import load_dotenv\n\n# Load environment variables from ~/.env\nload_dotenv()\nopenai.api_key = os.getenv(\"OPENAI_API_KEY\")\nif not openai.api_key:\n    raise EnvironmentError(\"OpenAI API key not found. Please check your .env file.\")\n\n\ndef format_timestamp(seconds):\n    \"\"\"Format seconds into MM:SS timestamp.\"\"\"\n    minutes, seconds = divmod(int(seconds), 60)  # Convert seconds to int\n    return f\"{minutes:02d}:{seconds:02d}\"\n\n\ndef transcribe_audio(file_path):\n    \"\"\"Transcribe audio file using OpenAI's Whisper API.\"\"\"",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "88",
    "name": "story-key-trans copy.py",
    "path": "01_core_ai_analysis/transcription/story-key-trans copy.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 142,
    "size": 6647,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis",
      "transcription"
    ],
    "functions": [
      "format_timestamp",
      "transcribe_file",
      "analyze_text_for_section",
      "process_media_directory"
    ],
    "classes": [],
    "imports": [
      "os",
      "subprocess",
      "openai",
      "dotenv"
    ],
    "preview": "import os\nimport subprocess\n\nfrom openai import OpenAI\n\nclient = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\nfrom dotenv import load_dotenv\n\n# Load environment variables from .env\nenv_path = os.path.expanduser(\"~/.env\")\n# Update the path if needed\nload_dotenv(dotenv_path=env_path)\n\n# Initialize OpenAI API key\n\n# Error checking for OpenAI API key\nif not openai.api_key:\n    raise EnvironmentError(\"OpenAI API key not found. Please check your .env file.\")\n\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "89",
    "name": "analyze-1.py",
    "path": "01_core_ai_analysis/transcription/analyze-1.py",
    "category": "01_core_ai_analysis",
    "type": "analysis",
    "lines": 60,
    "size": 3440,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis"
    ],
    "functions": [
      "analyze_text"
    ],
    "classes": [],
    "imports": [
      "dotenv",
      "openai",
      "os"
    ],
    "preview": "from dotenv import load_dotenv\nfrom openai import OpenAI\n\nclient = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\nimport os\n\n# Load environment variables from .env\nenv_path = os.path.expanduser(\"~/.env\")\n# Update the path if needed\nload_dotenv(dotenv_path=env_path)\n\n# Error checking for openai API key\nif not openai.api_key:\n    raise EnvironmentError(\"openai API key not found. Please check your .env file.\")\n\n\ndef analyze_text(text):\n    response = client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        messages=[",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "90",
    "name": "deep_content_analyzer.py",
    "path": "01_core_ai_analysis/transcription/deep_content_analyzer.py",
    "category": "01_core_ai_analysis",
    "type": "analysis",
    "lines": 590,
    "size": 25441,
    "docstring": "Deep Content Analysis and Reorganization Tool\nAnalyzes file content to determine actual functionality and reorganize accordingly",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "main",
      "__init__",
      "analyze_python_file",
      "extract_imports",
      "extract_functions",
      "extract_classes",
      "detect_apis",
      "extract_keywords",
      "determine_purpose",
      "identify_main_functionality"
    ],
    "classes": [
      "DeepContentAnalyzer"
    ],
    "imports": [
      "os",
      "re",
      "ast",
      "json",
      "pathlib",
      "collections",
      "typing"
    ],
    "preview": "#!/usr/bin/env python3\n\"\"\"\nDeep Content Analysis and Reorganization Tool\nAnalyzes file content to determine actual functionality and reorganize accordingly\n\"\"\"\n\nimport os\nimport re\nimport ast\nimport json\nfrom pathlib import Path\nfrom collections import defaultdict, Counter\nfrom typing import Dict, List, Set, Tuple\n\nclass DeepContentAnalyzer:\n    def __init__(self, base_path=\"/Users/steven/Documents/python\"):\n        self.base_path = Path(base_path)\n        self.analysis_results = {\n            \"file_analysis\": {},\n            \"function_categories\": defaultdict(list),",
    "last_modified": "2025-10-09T05:35:18.315372"
  },
  {
    "id": "91",
    "name": "avatar-download.py",
    "path": "01_core_ai_analysis/transcription/avatar-download.py",
    "category": "01_core_ai_analysis",
    "type": "youtube",
    "lines": 108,
    "size": 2360,
    "docstring": "",
    "keywords": [
      "youtube"
    ],
    "functions": [
      "list_remote_files",
      "list_local_files",
      "download_files"
    ],
    "classes": [],
    "imports": [
      "os",
      "paramiko"
    ],
    "preview": "import os\n\nimport paramiko\n\n# SSH Config\nhostname = \"access981577610.webspace-data.io\"\nusername = \"u114071855\"\npassword = \"A^p1yT@AHn*akbhs\"\nlocal_dir = \"/Users/steven/AvaTarArTs\"\nrremote_dirs = [\n    \"/2025\",\n    \"/Users\",\n    \"/all\",\n    \"/audio-texts\",\n    \"/blog\",\n    \"/build\",\n    \"/card\",\n    \"/city\",\n    \"/clickandbuilds\",\n    \"/convo\",",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "92",
    "name": "Whisper-Quiz-Voice.py",
    "path": "01_core_ai_analysis/transcription/Whisper-Quiz-Voice.py",
    "category": "01_core_ai_analysis",
    "type": "transcription",
    "lines": 90,
    "size": 3032,
    "docstring": "",
    "keywords": [
      "image_processing",
      "openai",
      "analysis",
      "web_tools"
    ],
    "functions": [
      "generate_base_prompt",
      "refine_prompt",
      "create_image",
      "upscale_image",
      "text_to_speech"
    ],
    "classes": [],
    "imports": [
      "csv",
      "io",
      "pathlib",
      "requests",
      "openai",
      "PIL"
    ],
    "preview": "import csv\nfrom io import BytesIO\nfrom pathlib import Path\n\nimport requests\nfrom openai import OpenAI\nfrom PIL import Image\n\n# Initialize the OpenAI client\nclient = OpenAI()\n\ndef generate_base_prompt(question, options):\n    # Combine question and options to generate a base prompt\n    return f\"Question: {question} Options: {', '.join(options)}\"\n\ndef refine_prompt(prompt):\n    # Refine the prompt using ChatGPT for more creativity\n    response = client.chat.completions.create(\n        model=\"gpt-4\",\n        messages=[{\"role\": \"system\", \"content\": \"You are a creative writer.\"},",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "93",
    "name": "info 1.py",
    "path": "01_core_ai_analysis/transcription/info 1.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 159,
    "size": 7063,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis",
      "transcription"
    ],
    "functions": [
      "split_media_to_segments",
      "transcribe_media_segment",
      "format_timestamp",
      "analyze_text_for_section",
      "process_media_by_section"
    ],
    "classes": [],
    "imports": [
      "os",
      "subprocess",
      "openai",
      "dotenv",
      "sys"
    ],
    "preview": "import os\nimport subprocess\n\nimport openai\nfrom dotenv import load_dotenv\n\n# Load environment variables from .env (make sure your OPENAI_API_KEY is stored here)\nload_dotenv(\"/Users/steven/.env\")\nopenai.api_key = os.getenv(\"OPENAI_API_KEY\")\n\n# Prompt for directory paths\nproject_root = input(\"Please enter the project root directory path: \").strip()\n\n# Directory paths based on user input\nMEDIA_DIR = os.path.join(project_root, \"Media\")  # Directory containing media files\nTRANSCRIPT_DIR = os.path.join(MEDIA_DIR, \"transcript\")  # Directory to save transcripts\nANALYSIS_DIR = os.path.join(MEDIA_DIR, \"analysis\")  # Directory to save the analysis files\n\n# Create output directories if they don't exist\nos.makedirs(TRANSCRIPT_DIR, exist_ok=True)",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "94",
    "name": "analyze_merged.py",
    "path": "01_core_ai_analysis/transcription/analyze_merged.py",
    "category": "01_core_ai_analysis",
    "type": "analysis",
    "lines": 293,
    "size": 14578,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis",
      "transcription"
    ],
    "functions": [
      "format_timestamp",
      "parse_transcript",
      "transcribe_audio",
      "analyze_text_for_section",
      "link_timestamps_to_analysis",
      "process_audio_file",
      "process_audio_directory",
      "get_openai_api_key",
      "get_directory_path",
      "get_pydocgen_paths"
    ],
    "classes": [
      "Analyze(base.Command)"
    ],
    "imports": [
      "logging",
      "os",
      "sys",
      "time",
      "concurrent.futures",
      "dotenv",
      "openai",
      "termcolor",
      "tqdm",
      "subprocess"
    ],
    "preview": "import logging\nimport os\nimport sys\nimport time\nfrom concurrent.futures import ThreadPoolExecutor\n\nfrom dotenv import load_dotenv\nfrom openai import OpenAI\nfrom termcolor import colored\nfrom tqdm import tqdm\nclient = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n# Load environment variables from .env (make sure your OPENAI_API_KEY is stored here)\nenv_path = os.path.expanduser(\"~/.env\")\nload_dotenv(dotenv_path=env_path)\n# Configure logging for error reporting\nlogging.basicConfig(\n    filename=\"transcription_analysis_errors.log\",\n    level=logging.ERROR,\n    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n)",
    "last_modified": "2025-10-08T06:39:59"
  },
  {
    "id": "95",
    "name": "all.py",
    "path": "01_core_ai_analysis/transcription/all.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 229,
    "size": 7174,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis"
    ],
    "functions": [
      "get_creation_date",
      "format_file_size",
      "format_duration",
      "write_csv",
      "get_unique_file_path",
      "process_audio_file",
      "process_image_file",
      "process_video_file",
      "process_files",
      "categorize_script"
    ],
    "classes": [],
    "imports": [
      "csv",
      "logging",
      "os",
      "re",
      "datetime",
      "typing",
      "dotenv",
      "mutagen.easyid3",
      "mutagen.mp3",
      "mutagen.mp4"
    ],
    "preview": "import csv\nimport logging\nimport os\nimport re\nfrom datetime import datetime\nfrom typing import Optional, Tuple\n\nfrom dotenv import load_dotenv\nfrom mutagen.easyid3 import EasyID3\nfrom mutagen.mp3 import MP3\nfrom mutagen.mp4 import MP4\nfrom openai import OpenAI\nfrom PIL import Image\n\n# Initialize OpenAI\nload_dotenv(\"/Users/steven/.env\")\nopenai = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n\n# Set up logging\nlogging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")",
    "last_modified": "2025-09-13T05:53:47.534883"
  },
  {
    "id": "96",
    "name": "WhisperTranscriber.py",
    "path": "01_core_ai_analysis/transcription/WhisperTranscriber.py",
    "category": "01_core_ai_analysis",
    "type": "transcription",
    "lines": 202,
    "size": 7564,
    "docstring": "",
    "keywords": [
      "transcription"
    ],
    "functions": [
      "load_config",
      "get_media_files",
      "transcribe_audio",
      "write_srt_with_word_timestamps",
      "write_srt_with_default_line_breaks",
      "format_timestamp"
    ],
    "classes": [],
    "imports": [
      "json",
      "pathlib",
      "the",
      "yaml",
      "whisper"
    ],
    "preview": "# WhisperTranscriber\n# src: https://github.com/VimWei/WhisperTranscriber\n# Implement Whisper's basic parameter configuration\n# Implement parameter control of srt output, so that verbatim srt can be achieved\n# max_line_width\uff0cmax_line_count\uff0cmax_words_per_line\n# Realize free switching of srt line breaking control: manual or automatic\n\nimport json\nfrom pathlib import Path\n\nimport the\nimport yaml\n\nimport whisper\n\n\ndef load_config(config_file=\"config.yaml\"):\n    \"\"\"Load configuration file\"\"\"\n    with open(config_file, \"r\", encoding=\"utf-8\") as f:\n        return yaml.safe_load(f)",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "97",
    "name": "docgen.py",
    "path": "01_core_ai_analysis/transcription/docgen.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 204,
    "size": 7635,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis"
    ],
    "functions": [
      "categorize_script",
      "generate_pydocgen",
      "get_openai_batch_titles",
      "suggest_script_titles_batch",
      "process_directory_with_batching"
    ],
    "classes": [],
    "imports": [
      "os",
      "openai",
      "csv",
      "subprocess",
      "dotenv"
    ],
    "preview": "import os\n\nfrom openai import OpenAI\n\nclient = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\nimport csv\nimport subprocess\n\nfrom dotenv import load_dotenv\n\n# Load environment variables from the specified .env file\nload_dotenv(\"/Users/steven/Documents/python/.env\")\n\n# Set OpenAI API key from the environment variable\n\n\n# Function to categorize scripts based on content\ndef categorize_script(content, file_name):\n    if \"image\" in content.lower() or \"convert\" in file_name.lower():\n        return \"Image Conversion and Upscaling Script\"",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "98",
    "name": "test.py",
    "path": "01_core_ai_analysis/transcription/test.py",
    "category": "01_core_ai_analysis",
    "type": "testing",
    "lines": 57,
    "size": 1994,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis",
      "transcription"
    ],
    "functions": [
      "transcribe_audio",
      "analyze_text"
    ],
    "classes": [],
    "imports": [
      "openai",
      "os",
      "dotenv",
      "sys"
    ],
    "preview": "from openai import OpenAI\n\nclient = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\nimport os\n\nfrom dotenv import load_dotenv\n\n# Load environment variables from .env\nload_dotenv()\n\n\n# Function to transcribe audio using OpenAI Whisper\ndef transcribe_audio(file_path):\n    with open(file_path, \"rb\") as audio_file:\n        transcript = client.audio.transcribe(\"whisper-1\", audio_file)\n        return transcript.text\n\n\n# Function to analyze the transcript using GPT\ndef analyze_text(text):",
    "last_modified": "2025-05-04T22:47:12.998801"
  },
  {
    "id": "99",
    "name": "transcribe_mp3.py",
    "path": "01_core_ai_analysis/transcription/transcribe_mp3.py",
    "category": "01_core_ai_analysis",
    "type": "transcription",
    "lines": 51,
    "size": 1454,
    "docstring": "",
    "keywords": [
      "analysis",
      "transcription"
    ],
    "functions": [
      "transcribe_audio",
      "save_transcription",
      "process_directory",
      "main"
    ],
    "classes": [],
    "imports": [
      "os",
      "whisper"
    ],
    "preview": "import os\n\nimport whisper\n\n\ndef transcribe_audio(file_path):\n    # Load the Whisper model\n    model = whisper.load_model(\"base\")\n\n    # Transcribe the audio file\n    result = model.transcribe(file_path)\n\n    return result[\"segments\"]\n\n\ndef save_transcription(segments, output_file):\n    with open(output_file, \"w\") as f:\n        for segment in segments:\n            start = segment[\"start\"]\n            end = segment[\"end\"]",
    "last_modified": "2025-09-13T05:55:25.704656"
  },
  {
    "id": "100",
    "name": "Quiz22s.py",
    "path": "01_core_ai_analysis/transcription/Quiz22s.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 118,
    "size": 4341,
    "docstring": "",
    "keywords": [
      "analysis",
      "web_tools"
    ],
    "functions": [
      "generate_speech",
      "calculate_text_duration",
      "process_csv_and_generate_speech"
    ],
    "classes": [],
    "imports": [
      "csv",
      "os",
      "requests",
      "dotenv",
      "pydub"
    ],
    "preview": "import csv\nimport os\n\nimport requests  # Make sure to install this package\nfrom dotenv import load_dotenv\nfrom pydub import AudioSegment  # Make sure to install pydub package\n\n\ndef generate_speech(input_text, output_path, api_key):\n    \"\"\"\n    Generates speech from text using an API (e.g., OpenAI's text-to-speech API).\n    \"\"\"\n    url = \"https://api.openai.com/v1/audio/speech\"\n    headers = {\n        \"Authorization\": f\"Bearer {api_key}\",\n        \"Content-Type\": \"application/json\",\n    }\n    data = {\n        \"model\": \"tts-1\",  # Update this according to the specific model you're using\n        \"input\": input_text,",
    "last_modified": "2025-09-13T05:53:54.737514"
  },
  {
    "id": "101",
    "name": "example_usage.py",
    "path": "01_core_ai_analysis/transcription/example_usage.py",
    "category": "01_core_ai_analysis",
    "type": "analysis",
    "lines": 98,
    "size": 3164,
    "docstring": "Example usage of the Transcription Analyzer",
    "keywords": [],
    "functions": [
      "example_single_file",
      "example_batch_processing",
      "show_output_structure",
      "main"
    ],
    "classes": [],
    "imports": [
      "os",
      "transcription_analyzer",
      "dotenv"
    ],
    "preview": "#!/usr/bin/env python3\n\"\"\"\nExample usage of the Transcription Analyzer\n\"\"\"\n\nimport os\nfrom transcription_analyzer import TranscriptionAnalyzer\nfrom dotenv import load_dotenv\n\n# Load environment variables from ~/.env\nload_dotenv(os.path.expanduser(\"~/.env\"))\n\ndef example_single_file():\n    \"\"\"Example of processing a single file.\"\"\"\n    print(\"Example: Processing a single file\")\n    print(\"-\" * 40)\n    \n    # Check for API key\n    api_key = os.getenv('OPENAI_API_KEY')\n    if not api_key:",
    "last_modified": "2025-10-09T05:14:50.275937"
  },
  {
    "id": "102",
    "name": "mp4-mp3-analyze2 copy.py",
    "path": "01_core_ai_analysis/transcription/mp4-mp3-analyze2 copy.py",
    "category": "01_core_ai_analysis",
    "type": "analysis",
    "lines": 189,
    "size": 6198,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis",
      "transcription"
    ],
    "functions": [
      "format_timestamp",
      "run_ffmpeg",
      "extract_small_audio",
      "transcribe_audio",
      "analyze_text_for_section",
      "process_media_directory",
      "attempt"
    ],
    "classes": [],
    "imports": [
      "logging",
      "os",
      "subprocess",
      "sys",
      "openai",
      "dotenv"
    ],
    "preview": "import logging\nimport os\nimport subprocess\nimport sys\n\nimport openai\nfrom dotenv import load_dotenv\n\nlogging.basicConfig(level=logging.INFO, format=\"%(asctime)s [%(levelname)s] %(message)s\")\n\nenv_path = \"/Users/steven/.env\"\nload_dotenv(dotenv_path=env_path)\nopenai.api_key = os.getenv(\"OPENAI_API_KEY\")\n\nMAX_SIZE = 25 * 1024 * 1024  # 25 MB hard limit\n\n\n# ----- helpers -----\ndef format_timestamp(seconds: float) -> str:\n    m = int(seconds // 60)",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "103",
    "name": "quiz-text-speech.py",
    "path": "01_core_ai_analysis/transcription/quiz-text-speech.py",
    "category": "01_core_ai_analysis",
    "type": "transcription",
    "lines": 88,
    "size": 2698,
    "docstring": "",
    "keywords": [
      "analysis",
      "web_tools"
    ],
    "functions": [
      "generate_description",
      "upscale_image",
      "text_to_speech"
    ],
    "classes": [],
    "imports": [
      "csv",
      "os",
      "io",
      "pathlib",
      "openAI",
      "requests",
      "PIL"
    ],
    "preview": "import csv\nimport os\nfrom io import BytesIO\nfrom pathlib import Path\n\nimport openAI\nimport requests\nfrom PIL import Image\n\n# Initialize the OpenAI client\nclient = openAI(api_key=\"your-api-key\")\n\n\ndef generate_description(image_url):\n    # Generate a description for the image using GPT-4 with Vision\n    response = client.chat.completions.create(\n        model=\"gpt-4-vision-preview\",\n        messages=[\n            {\n                \"role\": \"user\",",
    "last_modified": "2025-09-13T05:53:50.964576"
  },
  {
    "id": "104",
    "name": "dalle.py",
    "path": "01_core_ai_analysis/transcription/dalle.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 86,
    "size": 2510,
    "docstring": "",
    "keywords": [
      "data_processing",
      "openai",
      "analysis",
      "web_tools"
    ],
    "functions": [
      "generate_content"
    ],
    "classes": [],
    "imports": [
      "os",
      "openai",
      "io",
      "pandas",
      "requests",
      "PIL",
      "tqdm"
    ],
    "preview": "import os\n\nfrom openai import OpenAI\n\nclient = OpenAI(api_key=\"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\")\nimport io\n\nimport pandas as pd\nimport requests\nfrom PIL import Image\nfrom tqdm import tqdm\n\n# Set OpenAI and Stability API keys\nstability_ai_key = \"sk-2O6mutk6X4HI9olxMeGKYv0MpXcNRzs6fVRPJMdEvgEzRRII\"\n\n\n# Function to generate content based on a prompt\ndef generate_content(prompt):\n    response = client.chat.completions.create(\n        model=\"gpt-4\",",
    "last_modified": "2025-09-13T05:53:45.949415"
  },
  {
    "id": "105",
    "name": "main 2.py",
    "path": "01_core_ai_analysis/transcription/main 2.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 21,
    "size": 603,
    "docstring": "",
    "keywords": [],
    "functions": [
      "main"
    ],
    "classes": [],
    "imports": [
      "csv",
      "generate_speech"
    ],
    "preview": "import csv\n\nfrom generate_speech import generate_speech\n\n\ndef main():\n    csv_path = \"quiz329.csv\"\n\n    with open(csv_path, newline=\"\", encoding=\"utf-8\") as csvfile:\n        reader = csv.DictReader(csvfile)\n        for i, row in enumerate(reader):\n            # Assuming 'Question' is the column name\n            question_text = row[\"Question\"]\n            output_path = f\"/ Users / steven / Documents / quiz - talk / quiz329 / question / question_{\n                i + 1}.mp3\"\n            generate_speech(question_text, voice=\"shimmer\", output_path=output_path)\n\n\nif __name__ == \"__main__\":\n    main()",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "106",
    "name": "analyze 2.py",
    "path": "01_core_ai_analysis/transcription/analyze 2.py",
    "category": "01_core_ai_analysis",
    "type": "analysis",
    "lines": 120,
    "size": 4133,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis"
    ],
    "functions": [
      "get_openai_api_key",
      "get_directory_path",
      "get_pydocgen_paths",
      "generate_docs",
      "enhance_docs",
      "run_flake8"
    ],
    "classes": [],
    "imports": [
      "os",
      "subprocess",
      "openai"
    ],
    "preview": "import os\nimport subprocess\n\nimport openai\n\n\n# Prompt the user to input the OpenAI API key if it's not found in environment variables\ndef get_openai_api_key():\n    api_key = os.getenv(\"OPENAI_API_KEY\")\n    if not api_key:\n        api_key = input(\"Enter your OpenAI API key: \").strip()\n    return api_key\n\n\n# Prompt the user for directory paths if not provided\ndef get_directory_path(prompt_message, default_path):\n    directory = input(f\"{prompt_message} (default: {default_path}): \").strip()\n    return directory if directory else default_path\n\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "107",
    "name": "trek3.py",
    "path": "01_core_ai_analysis/transcription/trek3.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 73,
    "size": 2547,
    "docstring": "",
    "keywords": [
      "analysis",
      "web_tools"
    ],
    "functions": [
      "generate_speech",
      "process_csv_and_generate_speech"
    ],
    "classes": [],
    "imports": [
      "csv",
      "os",
      "requests"
    ],
    "preview": "import csv\nimport os\n\nimport requests  # Make sure to install this package\n\n\ndef generate_speech(input_text, output_path, api_key):\n    \"\"\"\n    Generates speech from text using an API (e.g., OpenAI's text-to-speech API).\n    \"\"\"\n    url = \"https://api.openai.com/v1/audio/speech\"\n    headers = {\n        \"Authorization\": f\"Bearer {api_key}\",\n        \"Content-Type\": \"application/json\",\n    }\n    data = {\n        \"model\": \"tts-1\",  # Update this according to the specific model you're using\n        \"input\": input_text,\n        \"voice\": \"shimmer\",  # Update the voice parameter as needed\n    }",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "108",
    "name": "analyze-shorts-info.py",
    "path": "01_core_ai_analysis/transcription/analyze-shorts-info.py",
    "category": "01_core_ai_analysis",
    "type": "analysis",
    "lines": 121,
    "size": 4716,
    "docstring": "",
    "keywords": [
      "analysis",
      "transcription"
    ],
    "functions": [
      "transcribe_audio",
      "format_timestamp",
      "analyze_text_for_section",
      "process_audio_directory"
    ],
    "classes": [],
    "imports": [
      "logging",
      "os",
      "sys",
      "time",
      "concurrent.futures",
      "dotenv",
      "termcolor",
      "tqdm"
    ],
    "preview": "cimport openai\n\nimport logging\nimport os\nimport sys\nimport time\nfrom concurrent.futures import ThreadPoolExecutor\n\nfrom dotenv import load_dotenv\nfrom termcolor import colored\nfrom tqdm import tqdm\n\n# Load environment variables from .env (make sure your OPENAI_API_KEY is stored here)\nenv_path = '/Users/steven/.env'  # Update this path if necessary\nload_dotenv(dotenv_path=env_path)\n\n# Initialize OpenAI API key\n\n# Error checking for OpenAI API key\nif not openai.api_key:",
    "last_modified": "2025-05-04T22:47:13.342876"
  },
  {
    "id": "109",
    "name": "analyze-promptr.py",
    "path": "01_core_ai_analysis/transcription/analyze-promptr.py",
    "category": "01_core_ai_analysis",
    "type": "analysis",
    "lines": 167,
    "size": 6796,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis",
      "transcription"
    ],
    "functions": [
      "split_video_to_segments",
      "transcribe_video_segment",
      "format_timestamp",
      "analyze_text_for_section",
      "process_video_by_section"
    ],
    "classes": [],
    "imports": [
      "os",
      "subprocess",
      "openai",
      "dotenv",
      "sys"
    ],
    "preview": "import os\nimport subprocess\n\nimport openai\nfrom dotenv import load_dotenv\n\n# Load environment variables from .env (make sure your OPENAI_API_KEY is stored here)\nload_dotenv()\nopenai.api_key = os.getenv(\"OPENAI_API_KEY\")\n\n# Directory paths\nVIDEO_DIR = \"/Users/steven/Music/nocTurneMeLoDieS/mp4\"  # Directory containing MP4 files\nTRANSCRIPT_DIR = (\n    \"/Users/steven/Music/nocTurneMeLoDieS/mp4/transcript\"  # Directory to save transcripts\n)\nANALYSIS_DIR = (\n    \"/Users/steven/Music/nocTurneMeLoDieS/mp4/analysis\"  # Directory to save the analysis files\n)\n\n# Create output directories if they don't exist",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "110",
    "name": "main 2 (1).py",
    "path": "01_core_ai_analysis/transcription/main 2 (1).py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 21,
    "size": 588,
    "docstring": "",
    "keywords": [],
    "functions": [
      "main"
    ],
    "classes": [],
    "imports": [
      "csv",
      "generate_speech"
    ],
    "preview": "import csv\n\nfrom generate_speech import generate_speech\n\n\ndef main():\n    csv_path = \"quiz329.csv\"\n\n    with open(csv_path, newline=\"\", encoding=\"utf-8\") as csvfile:\n        reader = csv.DictReader(csvfile)\n        for i, row in enumerate(reader):\n            # Assuming 'Question' is the column name\n            question_text = row[\"Question\"]\n            output_path = f\"/Users/steven/Documents/quiz-talk/quiz329/question/question_{\n                i + 1}.mp3\"\n            generate_speech(question_text, voice=\"shimmer\", output_path=output_path)\n\n\nif __name__ == \"__main__\":\n    main()",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "111",
    "name": "upscalecreateimages.py",
    "path": "01_core_ai_analysis/transcription/upscalecreateimages.py",
    "category": "01_core_ai_analysis",
    "type": "image_processing",
    "lines": 130,
    "size": 4449,
    "docstring": "",
    "keywords": [
      "data_processing",
      "openai",
      "analysis",
      "web_tools"
    ],
    "functions": [
      "generate_clickable_title",
      "generate_description",
      "generate_tags"
    ],
    "classes": [],
    "imports": [
      "os",
      "openai",
      "io",
      "pandas",
      "requests",
      "PIL",
      "tqdm"
    ],
    "preview": "import os\n\nfrom openai import OpenAI\n\nclient = OpenAI(api_key=\"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\")\nimport io\n\nimport pandas as pd\nimport requests\nfrom PIL import Image\nfrom tqdm import tqdm\n\n# Set OpenAI and Stability.ai API keys\nstability_ai_key = \"sk-2O6mutk6X4HI9olxMeGKYv0MpXcNRzs6fVRPJMdEvgEzRRII\"\n\n# Set Stability API key for image upscaling\napi_key = \"sk-2O6mutk6X4HI9olxMeGKYv0MpXcNRzs6fVRPJMdEvgEzRRII\"\nif api_key is None:\n    raise Exception(\"Missing Stability API key.\")\napi_host = os.getenv(\"API_HOST\", \"https://api.stability.ai\")",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "112",
    "name": "batch_image_seo_pipeline_20250530222518.py",
    "path": "01_core_ai_analysis/transcription/batch_image_seo_pipeline_20250530222518.py",
    "category": "01_core_ai_analysis",
    "type": "image_processing",
    "lines": 122,
    "size": 4833,
    "docstring": "",
    "keywords": [
      "openai"
    ],
    "functions": [
      "call_gpt4o_analysis_and_prompts",
      "batch_flexible_analysis"
    ],
    "classes": [],
    "imports": [
      "csv",
      "json",
      "logging",
      "os",
      "pathlib",
      "dotenv",
      "openai",
      "tqdm"
    ],
    "preview": "import csv\nimport json\nimport logging\nimport os\nfrom pathlib import Path\n\nfrom dotenv import load_dotenv\nfrom openai import OpenAI\nfrom tqdm import tqdm\n\n# CONFIG\nload_dotenv(os.path.expanduser(\"~/.env\"))\nOPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\nif not OPENAI_API_KEY:\n    raise EnvironmentError(\"OPENAI_API_KEY not found in ~/.env\")\nclient = OpenAI(api_key=OPENAI_API_KEY)\n\nLOG_FILE = \"avatararts_flexible_analysis.log\"\nlogging.basicConfig(\n    level=logging.INFO,",
    "last_modified": "2025-09-13T05:53:26.113479"
  },
  {
    "id": "113",
    "name": "mp4-mp3-analyze2.py",
    "path": "01_core_ai_analysis/transcription/mp4-mp3-analyze2.py",
    "category": "01_core_ai_analysis",
    "type": "analysis",
    "lines": 159,
    "size": 6244,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis",
      "transcription"
    ],
    "functions": [
      "format_timestamp",
      "transcribe_audio",
      "analyze_text_for_section",
      "process_media_directory"
    ],
    "classes": [],
    "imports": [
      "logging",
      "os",
      "subprocess",
      "sys",
      "openai",
      "dotenv"
    ],
    "preview": "import logging\nimport os\nimport subprocess\nimport sys\n\nimport openai\nfrom dotenv import load_dotenv\n\n# Setup basic logging\nlogging.basicConfig(level=logging.INFO, format=\"%(asctime)s [%(levelname)s] %(message)s\")\n\n# Load environment variables from .env (ensure your OPENAI_API_KEY is stored here)\nenv_path = \"/Users/steven/.env\"\nload_dotenv(dotenv_path=env_path)\nopenai.api_key = os.getenv(\"OPENAI_API_KEY\")\n\n# Maximum allowed file size (25 MB)\nMAX_SIZE = 25 * 1024 * 1024  # 25 MB in bytes\n\n# Determine base directory from command-line argument or prompt",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "114",
    "name": "main 5.py",
    "path": "01_core_ai_analysis/transcription/main 5.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 21,
    "size": 626,
    "docstring": "",
    "keywords": [],
    "functions": [
      "main"
    ],
    "classes": [],
    "imports": [
      "csv",
      "generate_speech"
    ],
    "preview": "import csv\n\nfrom generate_speech import generate_speech\n\n\ndef main():\n    csv_path = \"/Users/steven/Music/quiz-talk/Gtrivia - Sheet1.csv\"\n\n    with open(csv_path, newline=\"\", encoding=\"utf-8\") as csvfile:\n        reader = csv.DictReader(csvfile)\n        for i, row in enumerate(reader):\n            # Assuming 'Question' is the column name\n            question_text = row[\"Question\"]\n            output_path = f\"/ Users / steven / Music / quiz - talk / speech / question_{\n                i + 1}.mp3\"\n            generate_speech(question_text, voice=\"shimmer\", output_path=output_path)\n\n\nif __name__ == \"__main__\":\n    main()",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "115",
    "name": "story-section-gpt.py",
    "path": "01_core_ai_analysis/transcription/story-section-gpt.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 129,
    "size": 5945,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis",
      "transcription"
    ],
    "functions": [
      "format_timestamp",
      "transcribe_audio",
      "analyze_text_for_section",
      "process_media_directory"
    ],
    "classes": [],
    "imports": [
      "logging",
      "os",
      "sys",
      "dotenv",
      "openai"
    ],
    "preview": "import logging\nimport os\nimport sys\n\nfrom dotenv import load_dotenv\nfrom openai import OpenAI\n\n# Setup basic logging\nlogging.basicConfig(level=logging.INFO, format=\"%(asctime)s [%(levelname)s] %(message)s\")\n\n# Load environment variables from .env (ensure your OPENAI_API_KEY is stored here)\nenv_path = \"/Users/steven/.env\"\nload_dotenv(dotenv_path=env_path)\n\nclient = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n\n# Define a simple config dictionary. You can expand this as needed.\nconfig = {\"base_dir\": \"\"}  # Leave blank to prompt for base directory\n\n# Prompt for base directory if not set in config; default to current directory if input is empty",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "116",
    "name": "mp4-transcript2.py",
    "path": "01_core_ai_analysis/transcription/mp4-transcript2.py",
    "category": "01_core_ai_analysis",
    "type": "video_processing",
    "lines": 166,
    "size": 7158,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis",
      "transcription"
    ],
    "functions": [
      "split_video_to_segments",
      "transcribe_video_segment",
      "format_timestamp",
      "analyze_text_for_section",
      "process_video_by_section",
      "process_segment"
    ],
    "classes": [],
    "imports": [
      "logging",
      "os",
      "subprocess",
      "concurrent.futures",
      "dotenv",
      "openai",
      "sys"
    ],
    "preview": "import logging\nimport os\nimport subprocess\nfrom concurrent.futures import ThreadPoolExecutor\n\nfrom dotenv import load_dotenv\nfrom openai import OpenAI\n\n# Configure logging\nlogging.basicConfig(\n    filename=\"video_processing.log\",\n    level=logging.INFO,\n    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n)\n\n# Load environment variables\nload_dotenv(dotenv_path=\"/Users/steven/.env\")\n\n# Directory paths\nVIDEO_DIR = \"/Users/steven/Movies/project2025/Media/\"",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "117",
    "name": "polly.py",
    "path": "01_core_ai_analysis/transcription/polly.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 81,
    "size": 2928,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "__init__",
      "synthesize_speech",
      "generate_transcript"
    ],
    "classes": [
      "AudioProcessor"
    ],
    "imports": [
      "json",
      "logging",
      "re",
      "boto3",
      "utilities.const"
    ],
    "preview": "import json\nimport logging\nimport re\n\nimport boto3\nfrom utilities.const import (\n    AWS_ACCESS_KEY,\n    AWS_SEC_KEY,\n    LOG_PATH,\n    OUTPUT_AUDIO,\n    OUTPUT_TRANSCRIPT,\n    get_current_date,\n)\n\nlogging.basicConfig(level=logging.INFO, format=\"[%(levelname)s] %(message)s\", filename=LOG_PATH)\n\n\nclass AudioProcessor:\n    def __init__(self, _title, _description):\n        logging.info(f\"AudioProcessor class  {_title} , {_description}\")",
    "last_modified": "2025-09-13T05:53:28.818906"
  },
  {
    "id": "118",
    "name": "shorts_analyzer.py",
    "path": "01_core_ai_analysis/transcription/shorts_analyzer.py",
    "category": "01_core_ai_analysis",
    "type": "analysis",
    "lines": 105,
    "size": 4771,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis",
      "transcription"
    ],
    "functions": [
      "format_timestamp",
      "transcribe_audio",
      "analyze_text_for_section",
      "process_audio_directory"
    ],
    "classes": [],
    "imports": [
      "logging",
      "os",
      "openai",
      "shared.config",
      "termcolor",
      "tqdm",
      "shared.openai_client"
    ],
    "preview": "import logging\nimport os\n\nimport openai\nfrom shared.config import *\nfrom termcolor import colored\nfrom tqdm import tqdm\n\n# Load environment variables from .env\nenv_path = os.path.expanduser(\"~/.env\")\nload_dotenv(dotenv_path=env_path)\n\n# Set OpenAI API key\nfrom shared.openai_client import get_openai_client\nif not openai.api_key:\n    raise EnvironmentError(\"OpenAI API key not found. Please check your .env file.\")\n\n\ndef format_timestamp(seconds):\n    minutes = int(seconds // 60)",
    "last_modified": "2025-10-09T05:27:15.574791"
  },
  {
    "id": "119",
    "name": "analyze-mp3-transcript-prompts (1).py",
    "path": "01_core_ai_analysis/transcription/analyze-mp3-transcript-prompts (1).py",
    "category": "01_core_ai_analysis",
    "type": "analysis",
    "lines": 109,
    "size": 4632,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis",
      "transcription"
    ],
    "functions": [
      "transcribe_audio",
      "format_timestamp",
      "analyze_text_for_section",
      "process_audio_directory"
    ],
    "classes": [],
    "imports": [
      "os",
      "openai",
      "dotenv"
    ],
    "preview": "import os\n\nimport openai\nfrom dotenv import load_dotenv\n\n# Load environment variables from .env (make sure your OPENAI_API_KEY is stored here)\nenv_path = \"/Users/steven/.env\"\nload_dotenv(dotenv_path=env_path)\nopenai.api_key = os.getenv(\"OPENAI_API_KEY\")\n\n# Directory pathsp\nAUDIO_DIR = \"/Users/steven/Music/NocTurnE-meLoDieS/mp3\"  # Directory containing MP3 files\nTRANSCRIPT_DIR = \"/Users/steven/Music/NocTurnE-meLoDieS/song-video/analysis-transcript\"  # Directory to save transcripts\nANALYSIS_DIR = \"/Users/steven/Music/NocTurnE-meLoDieS/song-video/analysis-transcript\"  # Directory to save the analysis files\n\n# Create output directories if they don't exist\nos.makedirs(TRANSCRIPT_DIR, exist_ok=True)\nos.makedirs(ANALYSIS_DIR, exist_ok=True)\n\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "120",
    "name": "info 2.py",
    "path": "01_core_ai_analysis/transcription/info 2.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 159,
    "size": 7063,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis",
      "transcription"
    ],
    "functions": [
      "split_media_to_segments",
      "transcribe_media_segment",
      "format_timestamp",
      "analyze_text_for_section",
      "process_media_by_section"
    ],
    "classes": [],
    "imports": [
      "os",
      "subprocess",
      "openai",
      "dotenv",
      "sys"
    ],
    "preview": "import os\nimport subprocess\n\nimport openai\nfrom dotenv import load_dotenv\n\n# Load environment variables from .env (make sure your OPENAI_API_KEY is stored here)\nload_dotenv(\"/Users/steven/.env\")\nopenai.api_key = os.getenv(\"OPENAI_API_KEY\")\n\n# Prompt for directory paths\nproject_root = input(\"Please enter the project root directory path: \").strip()\n\n# Directory paths based on user input\nMEDIA_DIR = os.path.join(project_root, \"Media\")  # Directory containing media files\nTRANSCRIPT_DIR = os.path.join(MEDIA_DIR, \"transcript\")  # Directory to save transcripts\nANALYSIS_DIR = os.path.join(MEDIA_DIR, \"analysis\")  # Directory to save the analysis files\n\n# Create output directories if they don't exist\nos.makedirs(TRANSCRIPT_DIR, exist_ok=True)",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "121",
    "name": "fancyimg (1).py",
    "path": "01_core_ai_analysis/transcription/fancyimg (1).py",
    "category": "01_core_ai_analysis",
    "type": "image_processing",
    "lines": 84,
    "size": 3000,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis"
    ],
    "functions": [
      "generate_filename_with_gpt",
      "pair_and_rename_images"
    ],
    "classes": [],
    "imports": [
      "os",
      "openai",
      "csv",
      "dotenv"
    ],
    "preview": "import os\n\nfrom openai import OpenAI\n\nclient = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\nimport csv\n\nfrom dotenv import load_dotenv\n\n# Load API key from .env file\nload_dotenv(\"/Users/steven/.env\")\n\n\n# Function to generate a filename using GPT based on the prompt\ndef generate_filename_with_gpt(prompt):\n    response = client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        messages=[\n            {\n                \"role\": \"system\",",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "122",
    "name": "gpt-all.py",
    "path": "01_core_ai_analysis/transcription/gpt-all.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 88,
    "size": 2698,
    "docstring": "",
    "keywords": [
      "analysis",
      "web_tools"
    ],
    "functions": [
      "generate_description",
      "upscale_image",
      "text_to_speech"
    ],
    "classes": [],
    "imports": [
      "csv",
      "os",
      "io",
      "pathlib",
      "openAI",
      "requests",
      "PIL"
    ],
    "preview": "import csv\nimport os\nfrom io import BytesIO\nfrom pathlib import Path\n\nimport openAI\nimport requests\nfrom PIL import Image\n\n# Initialize the OpenAI client\nclient = openAI(api_key=\"your-api-key\")\n\n\ndef generate_description(image_url):\n    # Generate a description for the image using GPT-4 with Vision\n    response = client.chat.completions.create(\n        model=\"gpt-4-vision-preview\",\n        messages=[\n            {\n                \"role\": \"user\",",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "123",
    "name": "media-trans-analyze..py",
    "path": "01_core_ai_analysis/transcription/media-trans-analyze..py",
    "category": "01_core_ai_analysis",
    "type": "analysis",
    "lines": 130,
    "size": 5973,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis",
      "transcription"
    ],
    "functions": [
      "format_timestamp",
      "transcribe_audio",
      "analyze_text_for_section",
      "process_media_directory"
    ],
    "classes": [],
    "imports": [
      "logging",
      "os",
      "sys",
      "openai",
      "dotenv"
    ],
    "preview": "import logging\nimport os\nimport sys\nimport openai\nfrom dotenv import load_dotenv\n\n# Setup basic logging\nlogging.basicConfig(level=logging.INFO, format=\"%(asctime)s [%(levelname)s] %(message)s\")\n\n# Load environment variables from .env (ensure your OPENAI_API_KEY is stored here)\nenv_path = \"/Users/steven/.env\"\nload_dotenv(dotenv_path=env_path)\n\n# Set the OpenAI API key\nopenai.api_key = os.getenv(\"OPENAI_API_KEY\")\n\n# Define a simple config dictionary. You can expand this as needed.\nconfig = {\"base_dir\": \"\"}  # Leave blank to prompt for base directory\n\n# Prompt for base directory if not set in config; default to current directory if input is empty",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "124",
    "name": "media_processor2.py",
    "path": "01_core_ai_analysis/transcription/media_processor2.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 172,
    "size": 7477,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis",
      "transcription"
    ],
    "functions": [
      "format_timestamp",
      "transcribe_audio",
      "analyze_text_for_section",
      "process_media_directory"
    ],
    "classes": [],
    "imports": [
      "logging",
      "os",
      "subprocess",
      "sys",
      "openai",
      "dotenv"
    ],
    "preview": "import logging\nimport os\nimport subprocess  # Needed for running ffmpeg\nimport sys\n\nimport openai\nfrom dotenv import load_dotenv\n\n# Setup basic logging\nlogging.basicConfig(level=logging.INFO, format=\"%(asctime)s [%(levelname)s] %(message)s\")\n\n# Load environment variables from .env (ensure your OPENAI_API_KEY is stored here)\nenv_path = \"/Users/steven/.env\"\nload_dotenv(dotenv_path=env_path)\n\n# Set the OpenAI API key\nopenai.api_key = os.getenv(\"OPENAI_API_KEY\")\n\n# Define a simple config dictionary. You can expand this as needed.\nconfig = {\"base_dir\": \"\"}  # Leave blank to prompt for base directory",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "125",
    "name": "mp3-csv 3.py",
    "path": "01_core_ai_analysis/transcription/mp3-csv 3.py",
    "category": "01_core_ai_analysis",
    "type": "audio_processing",
    "lines": 107,
    "size": 4019,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis",
      "transcription"
    ],
    "functions": [
      "format_timestamp",
      "transcribe_audio",
      "analyze_text_for_section",
      "process_audio_file",
      "process_audio_directory"
    ],
    "classes": [],
    "imports": [
      "os",
      "openai",
      "dotenv"
    ],
    "preview": "import os\n\nimport openai\nfrom dotenv import load_dotenv\n\n# Load environment variables\nload_dotenv()\nopenai.api_key = os.getenv(\"OPENAI_API_KEY\")\nif not openai.api_key:\n    raise EnvironmentError(\"OpenAI API key not found. Please check your .env file.\")\n\n\ndef format_timestamp(seconds):\n    \"\"\"Format seconds into MM:SS timestamp.\"\"\"\n    minutes = int(seconds // 60)\n    seconds = seconds % 60\n    return f\"{minutes:02d}:{int(seconds):02d}\"\n\n\ndef transcribe_audio(file_path):",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "126",
    "name": "2.0.py",
    "path": "01_core_ai_analysis/transcription/2.0.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 59,
    "size": 1713,
    "docstring": "",
    "keywords": [
      "data_processing",
      "openai",
      "analysis",
      "web_tools"
    ],
    "functions": [
      "generate_content_based_on_prompt"
    ],
    "classes": [],
    "imports": [
      "os",
      "openai",
      "io",
      "pandas",
      "requests",
      "PIL",
      "tqdm"
    ],
    "preview": "import os\n\nfrom openai import OpenAI\n\nclient = OpenAI(api_key=\"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\")\nimport io\n\nimport pandas as pd\nimport requests\nfrom PIL import Image\nfrom tqdm import tqdm\n\n# Set OpenAI and Stability API keys\nstability_ai_key = \"sk-2O6mutk6X4HI9olxMeGKYv0MpXcNRzs6fVRPJMdEvgEzRRII\"\n\n\n# Function to generate content based on the prompt\ndef generate_content_based_on_prompt(prompt):\n    response = client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",",
    "last_modified": "2025-05-04T22:47:11.582301"
  },
  {
    "id": "127",
    "name": "adown.py",
    "path": "01_core_ai_analysis/transcription/adown.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 110,
    "size": 2335,
    "docstring": "",
    "keywords": [
      "youtube"
    ],
    "functions": [
      "list_remote_files",
      "list_local_files",
      "download_missing_files"
    ],
    "classes": [],
    "imports": [
      "os",
      "paramiko"
    ],
    "preview": "import os\n\nimport paramiko\n\n# SSH Configuration\nhostname = \"access981577610.webspace-data.io\"\nusername = \"u114071855\"\npassword = \"A^p1yT@AHn*akbhs\"\nlocal_dir = \"/Users/steven/AvaTarArTs\"\nremote_dirs = [\n    \"/2025\",\n    \"/Users\",\n    \"/all\",\n    \"/audio-texts\",\n    \"/blog\",\n    \"/build\",\n    \"/card\",\n    \"/city\",\n    \"/clickandbuilds\",\n    \"/convo\",",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "128",
    "name": "analyze 1.py",
    "path": "01_core_ai_analysis/transcription/analyze 1.py",
    "category": "01_core_ai_analysis",
    "type": "analysis",
    "lines": 223,
    "size": 9464,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis",
      "transcription"
    ],
    "functions": [
      "format_timestamp",
      "parse_transcript",
      "transcribe_audio",
      "analyze_text_for_section",
      "link_timestamps_to_analysis",
      "process_audio_file",
      "process_audio_directory"
    ],
    "classes": [],
    "imports": [
      "logging",
      "os",
      "sys",
      "time",
      "concurrent.futures",
      "dotenv",
      "openai",
      "termcolor",
      "tqdm"
    ],
    "preview": "import logging\nimport os\nimport sys\nimport time\nfrom concurrent.futures import ThreadPoolExecutor\n\nfrom dotenv import load_dotenv\nfrom openai import OpenAI\nfrom termcolor import colored\nfrom tqdm import tqdm\n\nclient = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n\n\n# Load environment variables from .env (make sure your OPENAI_API_KEY is stored here)\nenv_path = os.path.expanduser(\"~/.env\")\nload_dotenv(dotenv_path=env_path)\n\n# Configure logging for error reporting\nlogging.basicConfig(",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "129",
    "name": "migrate_remaining.py",
    "path": "01_core_ai_analysis/transcription/migrate_remaining.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 268,
    "size": 13298,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "create_additional_directories",
      "migrate_remaining_files",
      "migrate_remaining_directories",
      "main"
    ],
    "classes": [],
    "imports": [
      "os",
      "shutil",
      "pathlib"
    ],
    "preview": "#!/usr/bin/env python3\n\"\"\"\nSecond Migration Pass - Organize Remaining Python Files\n\"\"\"\n\nimport os\nimport shutil\nfrom pathlib import Path\n\ndef create_additional_directories():\n    \"\"\"Create additional directories for remaining files.\"\"\"\n    base_path = Path(\"/Users/steven/Documents/python\")\n    \n    additional_dirs = [\n        \"01_core_tools/text_processors\",\n        \"05_audio_video/image_processors\", \n        \"05_audio_video/audio_processors/quiz_tts\",\n        \"06_utilities/converters\",\n        \"06_utilities/data_processors\",\n        \"07_experimental/web_tools\",",
    "last_modified": "2025-10-09T05:31:11.505816"
  },
  {
    "id": "130",
    "name": "analyze_all_images_20250530220437.py",
    "path": "01_core_ai_analysis/transcription/analyze_all_images_20250530220437.py",
    "category": "01_core_ai_analysis",
    "type": "analysis",
    "lines": 429,
    "size": 17578,
    "docstring": "batch_image_seo_pipeline.py\n\n1) Discovers all image files under a given folder.\n2) Extracts technical metadata via Pillow.\n3) Calls GPT-4o Vision (multimodal) using a \u201csystem\u201d + \u201cuser\u201d prompt.\n4) Parses the JSON response into structured fields:\n   main_subject, style, color_palette, tags, orientation, suggested_products,\n   SEO_title, SEO_description, emotion, safety_rating, dominant_keyword.\n5) Builds a \u201csource\u201d tag: folder-subfolder-YYYYMMDD.\n6) Applies a \u201cTop 5% SEO Analytics\u201d framework to generate SEO columns:\n   \u2013 SEO Keywords, Traffic Source, CRO Tactic, Backlink Source, Engagement Rate.\n7) Adds niche-specific design prompts (Geeky, Dark Humor, Anime).\n8) Writes everything into a final CSV for bulk upload.\n\nDependencies:\n    pip install openai python-dotenv pillow tqdm backoff",
    "keywords": [
      "openai"
    ],
    "functions": [
      "load_openai_client",
      "retry_on_api_error",
      "call_gpt4o",
      "get_image_tech_meta",
      "build_source_tag",
      "discover_images",
      "build_gpt_messages",
      "parse_args",
      "main"
    ],
    "classes": [],
    "imports": [
      "argparse",
      "csv",
      "json",
      "logging",
      "os",
      "sys",
      "time",
      "datetime",
      "pathlib",
      "typing"
    ],
    "preview": "#!/usr/bin/env python3\n\"\"\"\nbatch_image_seo_pipeline.py\n\n1) Discovers all image files under a given folder.\n2) Extracts technical metadata via Pillow.\n3) Calls GPT-4o Vision (multimodal) using a \u201csystem\u201d + \u201cuser\u201d prompt.\n4) Parses the JSON response into structured fields:\n   main_subject, style, color_palette, tags, orientation, suggested_products,\n   SEO_title, SEO_description, emotion, safety_rating, dominant_keyword.\n5) Builds a \u201csource\u201d tag: folder-subfolder-YYYYMMDD.\n6) Applies a \u201cTop 5% SEO Analytics\u201d framework to generate SEO columns:\n   \u2013 SEO Keywords, Traffic Source, CRO Tactic, Backlink Source, Engagement Rate.\n7) Adds niche-specific design prompts (Geeky, Dark Humor, Anime).\n8) Writes everything into a final CSV for bulk upload.\n\nDependencies:\n    pip install openai python-dotenv pillow tqdm backoff\n\"\"\"\n",
    "last_modified": "2025-09-13T05:53:25.324048"
  },
  {
    "id": "131",
    "name": "thumbs.py",
    "path": "01_core_ai_analysis/transcription/thumbs.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 19,
    "size": 343,
    "docstring": "",
    "keywords": [],
    "functions": [
      "export_thumbnail"
    ],
    "classes": [],
    "imports": [
      "__future__",
      "subprocess"
    ],
    "preview": "from __future__ import annotations\n\nimport subprocess\n\n\ndef export_thumbnail(video_path: str, out_png: str, ss: float = 0.25):\n    cmd = [\n        \"ffmpeg\",\n        \"-y\",\n        \"-i\",\n        video_path,\n        \"-vf\",\n        \"thumbnail,scale=1080:-2\",\n        \"-frames:v\",\n        \"1\",\n        out_png,\n    ]\n    subprocess.check_call(cmd)\n",
    "last_modified": "2025-09-11T13:26:59.332635"
  },
  {
    "id": "132",
    "name": "info 3.py",
    "path": "01_core_ai_analysis/transcription/info 3.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 159,
    "size": 7063,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis",
      "transcription"
    ],
    "functions": [
      "split_media_to_segments",
      "transcribe_media_segment",
      "format_timestamp",
      "analyze_text_for_section",
      "process_media_by_section"
    ],
    "classes": [],
    "imports": [
      "os",
      "subprocess",
      "openai",
      "dotenv",
      "sys"
    ],
    "preview": "import os\nimport subprocess\n\nimport openai\nfrom dotenv import load_dotenv\n\n# Load environment variables from .env (make sure your OPENAI_API_KEY is stored here)\nload_dotenv(\"/Users/steven/.env\")\nopenai.api_key = os.getenv(\"OPENAI_API_KEY\")\n\n# Prompt for directory paths\nproject_root = input(\"Please enter the project root directory path: \").strip()\n\n# Directory paths based on user input\nMEDIA_DIR = os.path.join(project_root, \"Media\")  # Directory containing media files\nTRANSCRIPT_DIR = os.path.join(MEDIA_DIR, \"transcript\")  # Directory to save transcripts\nANALYSIS_DIR = os.path.join(MEDIA_DIR, \"analysis\")  # Directory to save the analysis files\n\n# Create output directories if they don't exist\nos.makedirs(TRANSCRIPT_DIR, exist_ok=True)",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "133",
    "name": "test copy.py",
    "path": "01_core_ai_analysis/transcription/test copy.py",
    "category": "01_core_ai_analysis",
    "type": "testing",
    "lines": 90,
    "size": 3032,
    "docstring": "",
    "keywords": [
      "image_processing",
      "openai",
      "analysis",
      "web_tools"
    ],
    "functions": [
      "generate_base_prompt",
      "refine_prompt",
      "create_image",
      "upscale_image",
      "text_to_speech"
    ],
    "classes": [],
    "imports": [
      "csv",
      "io",
      "pathlib",
      "requests",
      "openai",
      "PIL"
    ],
    "preview": "import csv\nfrom io import BytesIO\nfrom pathlib import Path\n\nimport requests\nfrom openai import OpenAI\nfrom PIL import Image\n\n# Initialize the OpenAI client\nclient = OpenAI()\n\ndef generate_base_prompt(question, options):\n    # Combine question and options to generate a base prompt\n    return f\"Question: {question} Options: {', '.join(options)}\"\n\ndef refine_prompt(prompt):\n    # Refine the prompt using ChatGPT for more creativity\n    response = client.chat.completions.create(\n        model=\"gpt-4\",\n        messages=[{\"role\": \"system\", \"content\": \"You are a creative writer.\"},",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "134",
    "name": "pipeline.py",
    "path": "01_core_ai_analysis/transcription/pipeline.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 197,
    "size": 6563,
    "docstring": "",
    "keywords": [
      "transcription"
    ],
    "functions": [
      "__init__",
      "transcribe",
      "_hook_score",
      "candidates_from_transcript",
      "select_topk",
      "export_clip",
      "run"
    ],
    "classes": [
      "ClipCandidate",
      "OpusClonePipeline"
    ],
    "imports": [
      "__future__",
      "json",
      "math",
      "os",
      "re",
      "shutil",
      "subprocess",
      "tempfile",
      "dataclasses",
      "typing"
    ],
    "preview": "from __future__ import annotations\n\nimport json\nimport math\nimport os\nimport re\nimport shutil\nimport subprocess\nimport tempfile\nfrom dataclasses import dataclass\nfrom typing import Any, Dict, List, Tuple\n\nimport numpy as np\nfrom moviepy.editor import VideoFileClip\nfrom tqdm import tqdm\n\nimport whisper\n\nfrom .brand import BrandTemplate\nfrom .captions import burn_captions_ffmpeg, write_srt",
    "last_modified": "2025-09-13T05:55:09.593682"
  },
  {
    "id": "135",
    "name": "deep_analyze.py",
    "path": "01_core_ai_analysis/transcription/deep_analyze.py",
    "category": "01_core_ai_analysis",
    "type": "analysis",
    "lines": 107,
    "size": 5108,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis",
      "transcription"
    ],
    "functions": [
      "format_timestamp",
      "transcribe_audio",
      "analyze_text",
      "process_media_directory"
    ],
    "classes": [],
    "imports": [
      "logging",
      "os",
      "sys",
      "pathlib",
      "openai",
      "dotenv"
    ],
    "preview": "import logging\nimport os\nimport sys\nfrom pathlib import Path\nimport openai\nfrom dotenv import load_dotenv\n\n# Setup logging\nlogging.basicConfig(level=logging.INFO, format=\"%(asctime)s [%(levelname)s] %(message)s\")\n\n# Load environment variables\nenv_path = Path(\"/Users/steven/.env\")\nload_dotenv(dotenv_path=env_path)\nopenai.api_key = os.getenv(\"OPENAI_API_KEY\")\n\n# Base config\nconfig = {\"base_dir\": \"\"}\nif not config.get(\"base_dir\"):\n    base_dir_input = input(\"Enter base directory (leave blank for current): \").strip()\n    config[\"base_dir\"] = base_dir_input or os.getcwd()",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "136",
    "name": "analyze-mp4s.py",
    "path": "01_core_ai_analysis/transcription/analyze-mp4s.py",
    "category": "01_core_ai_analysis",
    "type": "analysis",
    "lines": 114,
    "size": 5693,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis",
      "transcription"
    ],
    "functions": [
      "format_timestamp",
      "transcribe_audio",
      "analyze_text_for_section",
      "process_audio_directory"
    ],
    "classes": [],
    "imports": [
      "logging",
      "os",
      "openai",
      "dotenv",
      "termcolor",
      "tqdm"
    ],
    "preview": "import logging\nimport os\n\nimport openai\nfrom dotenv import load_dotenv\nfrom termcolor import colored\nfrom tqdm import tqdm\n\n# Load environment variables from .env\nenv_path = os.path.expanduser(\"~/.env\")\nload_dotenv(dotenv_path=env_path)\n\n# Set OpenAI API key\nopenai.api_key = os.getenv(\"OPENAI_API_KEY\")\nif not openai.api_key:\n    raise EnvironmentError(\"OpenAI API key not found. Please check your .env file.\")\n\n\ndef format_timestamp(seconds):\n    minutes = int(seconds // 60)",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "137",
    "name": "generate_speech.py",
    "path": "01_core_ai_analysis/transcription/generate_speech.py",
    "category": "01_core_ai_analysis",
    "type": "transcription",
    "lines": 36,
    "size": 1234,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis"
    ],
    "functions": [
      "generate_speech",
      "main"
    ],
    "classes": [],
    "imports": [
      "csv",
      "openai"
    ],
    "preview": "import csv\n\nfrom openai import OpenAI\n\nclient = OpenAI(api_key=\"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\")\n\n# Replace 'your_api_key_here' with your actual OpenAI API key\n\n\ndef generate_speech(text, voice=\"shimmer\", output_path=\"speech.mp3\"):\n    response = client.audio.create(\n        model=\"tts-1-hd\",  # Ensure this model supports the audio creation\n        input=text,\n        voice=voice,\n        format=\"mp3\",\n    )\n    with open(output_path, \"wb\") as file:\n        file.write(response.content)\n\n",
    "last_modified": "2025-09-13T05:53:51.036050"
  },
  {
    "id": "138",
    "name": "Quiz22sec (1).py",
    "path": "01_core_ai_analysis/transcription/Quiz22sec (1).py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 108,
    "size": 4050,
    "docstring": "",
    "keywords": [
      "analysis",
      "web_tools"
    ],
    "functions": [
      "generate_speech",
      "calculate_text_duration",
      "process_csv_and_generate_speech"
    ],
    "classes": [],
    "imports": [
      "csv",
      "os",
      "requests",
      "pydub"
    ],
    "preview": "import csv\nimport os\n\nimport requests  # Make sure to install this package\nfrom pydub import AudioSegment  # Make sure to install pydub package\n\n\ndef generate_speech(input_text, output_path, api_key):\n    \"\"\"\n    Generates speech from text using an API (e.g., OpenAI's text-to-speech API).\n    \"\"\"\n    url = \"https://api.openai.com/v1/audio/speech\"\n    headers = {\n        \"Authorization\": f\"Bearer {api_key}\",\n        \"Content-Type\": \"application/json\",\n    }\n    data = {\n        \"model\": \"tts-1\",  # Update this according to the specific model you're using\n        \"input\": input_text,\n        \"voice\": \"shimmer\",  # Update the voice parameter as needed",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "139",
    "name": "mp3-csv 2.py",
    "path": "01_core_ai_analysis/transcription/mp3-csv 2.py",
    "category": "01_core_ai_analysis",
    "type": "audio_processing",
    "lines": 107,
    "size": 4019,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis",
      "transcription"
    ],
    "functions": [
      "format_timestamp",
      "transcribe_audio",
      "analyze_text_for_section",
      "process_audio_file",
      "process_audio_directory"
    ],
    "classes": [],
    "imports": [
      "os",
      "openai",
      "dotenv"
    ],
    "preview": "import os\n\nimport openai\nfrom dotenv import load_dotenv\n\n# Load environment variables\nload_dotenv()\nopenai.api_key = os.getenv(\"OPENAI_API_KEY\")\nif not openai.api_key:\n    raise EnvironmentError(\"OpenAI API key not found. Please check your .env file.\")\n\n\ndef format_timestamp(seconds):\n    \"\"\"Format seconds into MM:SS timestamp.\"\"\"\n    minutes = int(seconds // 60)\n    seconds = seconds % 60\n    return f\"{minutes:02d}:{int(seconds):02d}\"\n\n\ndef transcribe_audio(file_path):",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "140",
    "name": "mp4-mp3-analyze_20250531041622.py",
    "path": "01_core_ai_analysis/transcription/mp4-mp3-analyze_20250531041622.py",
    "category": "01_core_ai_analysis",
    "type": "analysis",
    "lines": 174,
    "size": 6778,
    "docstring": "",
    "keywords": [
      "video_processing",
      "openai",
      "analysis",
      "transcription"
    ],
    "functions": [
      "split_video_to_segments",
      "transcribe_video_segment",
      "format_timestamp",
      "analyze_text_for_section",
      "process_video_by_section"
    ],
    "classes": [],
    "imports": [
      "os",
      "subprocess",
      "openai",
      "dotenv",
      "sys"
    ],
    "preview": "import os\nimport subprocess\n\nimport openai\nfrom dotenv import load_dotenv\n\n# Load environment variables from .env (make sure your OPENAI_API_KEY is stored here)\nload_dotenv()\nopenai.api_key = os.getenv(\"OPENAI_API_KEY\")\n\n# Directory paths\nVIDEO_DIR = (\n    \"/Users/steven/AvaTarArTs/canva/Video\"  # Directory containing MP4 files\n)\nTRANSCRIPT_DIR = \"/Users/steven/AvaTarArTs/canva/Video/transcript\"  # Directory to save transcripts\nANALYSIS_DIR = \"/Users/steven/AvaTarArTs/canva/Video/analysis\"  # Directory to save the analysis files\n\n# Create output directories if they don't exist\nos.makedirs(TRANSCRIPT_DIR, exist_ok=True)\nos.makedirs(ANALYSIS_DIR, exist_ok=True)",
    "last_modified": "2025-05-31T07:03:53.918581"
  },
  {
    "id": "141",
    "name": "createimages.py",
    "path": "01_core_ai_analysis/transcription/createimages.py",
    "category": "01_core_ai_analysis",
    "type": "image_processing",
    "lines": 150,
    "size": 5687,
    "docstring": "",
    "keywords": [
      "data_processing",
      "openai",
      "analysis",
      "web_tools"
    ],
    "functions": [
      "generate_clickable_title",
      "generate_description",
      "generate_image_prompt",
      "generate_tags"
    ],
    "classes": [],
    "imports": [
      "os",
      "openai",
      "io",
      "pandas",
      "requests",
      "PIL",
      "tqdm"
    ],
    "preview": "import os\n\nfrom openai import OpenAI\n\nclient = OpenAI(api_key=\"YOUR_OPEN_AI_KEY\")\nimport io\n\nimport pandas as pd\nimport requests\nfrom PIL import Image, ImageDraw\nfrom tqdm import tqdm\n\n# Set OpenAI and Stability.ai API keys https://platform.openai.com/account/api-keys AND https://beta.dreamstudio.ai/account\nstability_ai_key = \"YOUR_STABILITY_AI_KEY\"\n\n# Function to generate a clickable title using GPT-3.5-turbo\n\n# Change this prompt if you are changing the product type, right now it's Acrylic Wall Art Panels\n\n",
    "last_modified": "2025-05-04T22:47:11.599251"
  },
  {
    "id": "142",
    "name": "transcribe.py",
    "path": "01_core_ai_analysis/transcription/transcribe.py",
    "category": "01_core_ai_analysis",
    "type": "transcription",
    "lines": 26,
    "size": 594,
    "docstring": "",
    "keywords": [
      "openai",
      "transcription"
    ],
    "functions": [
      "transcribe_audio"
    ],
    "classes": [],
    "imports": [
      "openai",
      "os",
      "dotenv",
      "sys"
    ],
    "preview": "from openai import OpenAI\n\nclient = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\nimport os\n\nfrom dotenv import load_dotenv\n\n# Load environment variables from .env\nload_dotenv()\n\n\ndef transcribe_audio(file_path):\n    with open(file_path, \"rb\") as audio_file:\n        transcript = client.audio.transcribe(\"whisper-1\", audio_file)\n        return transcript.text\n\n\nif __name__ == \"__main__\":\n    import sys\n",
    "last_modified": "2025-05-04T22:47:11"
  },
  {
    "id": "143",
    "name": "help_Nekmo_ffmpeg.py",
    "path": "01_core_ai_analysis/transcription/help_Nekmo_ffmpeg.py",
    "category": "01_core_ai_analysis",
    "type": "video_processing",
    "lines": 159,
    "size": 5150,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "logging",
      "asyncio",
      "os",
      "time",
      "hachoir.metadata",
      "hachoir.parser"
    ],
    "preview": "import logging\n\nlogging.basicConfig(\n    level=logging.DEBUG, format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\n)\nlogger = logging.getLogger(__name__)\n\n\nimport asyncio\nimport os\nimport time\n\nfrom hachoir.metadata import extractMetadata\nfrom hachoir.parser import createParser\n\n\nasync def place_water_mark(input_file, output_file, water_mark_file):\n    watermarked_file = output_file + \".watermark.png\"\n    metadata = extractMetadata(createParser(input_file))\n    width = metadata.get(\"width\")",
    "last_modified": "2025-05-06T04:35:15.017373"
  },
  {
    "id": "144",
    "name": "sub.py",
    "path": "01_core_ai_analysis/transcription/sub.py",
    "category": "01_core_ai_analysis",
    "type": "transcription",
    "lines": 92,
    "size": 2610,
    "docstring": "This script is used to generate a transcript from an audio file using AssemblyAI api.",
    "keywords": [
      "transcription",
      "youtube",
      "web_tools"
    ],
    "functions": [
      "read_file",
      "uploadFile",
      "transcribe",
      "poll",
      "get_transcription_result_url",
      "save_transcript"
    ],
    "classes": [],
    "imports": [
      "os",
      "time",
      "requests",
      "config"
    ],
    "preview": "\"\"\"\nThis script is used to generate a transcript from an audio file using AssemblyAI api.\n\"\"\"\n\nimport os\nimport time\n\nimport requests\n\nimport config\n\nupload_url = \"https://api.assemblyai.com/v2/upload\"\ntranscribe_url = \"https://api.assemblyai.com/v2/transcript\"\nsrt_endpoint = \"https://api.assemblyai.com/v2/transcript/\"  # YOUR-TRANSCRIPT-ID-HERE/srt\nheaders = {\"authorization\": config.assemblyai}\n\n\n# Read the audio file to verify\ndef read_file(filename, chunk_size=5242880):\n    with open(filename, \"rb\") as _file:",
    "last_modified": "2025-09-13T05:53:29.691898"
  },
  {
    "id": "145",
    "name": "trans.py",
    "path": "01_core_ai_analysis/transcription/trans.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 44,
    "size": 1393,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "os",
      "pydub",
      "whisper"
    ],
    "preview": "import os\n\nfrom pydub import AudioSegment\n\nimport whisper\n\n# Initialize Whisper model\nmodel = whisper.load_model(\"base\")\n\n# Directory containing the MP3 files\naudio_dir = os.path.expanduser(\"/Users/steven/Movies/2025-Vid/done\")\n\n# Loop over each MP3 file in the directory\nfor audio_file in os.listdir(audio_dir):\n    if audio_file.endswith(\".mp3\"):\n        # Full path to the audio file\n        audio_path = os.path.join(audio_dir, audio_file)\n\n        # Load audio file\n        audio = AudioSegment.from_mp3(audio_path)",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "146",
    "name": "analyze-shorts-1.py",
    "path": "01_core_ai_analysis/transcription/analyze-shorts-1.py",
    "category": "01_core_ai_analysis",
    "type": "analysis",
    "lines": 122,
    "size": 5310,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis",
      "transcription"
    ],
    "functions": [
      "transcribe_audio",
      "format_timestamp",
      "analyze_text_for_section",
      "process_audio_directory"
    ],
    "classes": [],
    "imports": [
      "logging",
      "os",
      "sys",
      "time",
      "concurrent.futures",
      "openai",
      "dotenv",
      "termcolor",
      "tqdm"
    ],
    "preview": "import logging\nimport os\nimport sys\nimport time\nfrom concurrent.futures import ThreadPoolExecutor\n\nimport openai\nfrom dotenv import load_dotenv\nfrom termcolor import colored\nfrom tqdm import tqdm\n\n# Load environment variables from .env (make sure your OPENAI_API_KEY is stored here)\nenv_path = os.path.expanduser(\"~/.env\")\n # Update this path if necessary\nload_dotenv(dotenv_path=env_path)\n\n# Initialize OpenAI API key\nopenai.api_key = os.getenv('OPENAI_API_KEY')\n\n# Error checking for OpenAI API key",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "147",
    "name": "python-analyze-html.py",
    "path": "01_core_ai_analysis/transcription/python-analyze-html.py",
    "category": "01_core_ai_analysis",
    "type": "analysis",
    "lines": 160,
    "size": 5191,
    "docstring": "",
    "keywords": [
      "data_processing",
      "analysis"
    ],
    "functions": [
      "analyze_ast",
      "categorize_heuristic",
      "esc"
    ],
    "classes": [],
    "imports": [
      "ast",
      "csv",
      "html",
      "json",
      "os",
      "re",
      "dataclasses",
      "pathlib",
      "typing",
      "pandas"
    ],
    "preview": "# Analyze the Python files you've uploaded here (a subset of your /Users/steven/Documents/python)\n# and generate a compact report (CSV + simple HTML) along with an on-screen table.\n\nimport ast\nimport csv\nimport html\nimport json\nimport os\nimport re\nfrom dataclasses import dataclass, field\nfrom pathlib import Path\nfrom typing import List, Tuple\n\nimport pandas as pd\nfrom caas_jupyter_tools import display_dataframe_to_user\n\n# Files visible in this workspace (mirror of some of your local files)\nroot = Path(\"/Users/steven/Documents/python\")\npy_files = sorted([p for p in root.iterdir() if p.suffix == \".py\"])\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "148",
    "name": "quiz-20.py",
    "path": "01_core_ai_analysis/transcription/quiz-20.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 103,
    "size": 3633,
    "docstring": "",
    "keywords": [
      "analysis",
      "web_tools"
    ],
    "functions": [
      "generate_speech",
      "calculate_text_duration",
      "process_csv_and_generate_speech"
    ],
    "classes": [],
    "imports": [
      "csv",
      "os",
      "requests",
      "dotenv",
      "pydub"
    ],
    "preview": "import csv\nimport os\n\nimport requests  # Install using `pip install requests`\nfrom dotenv import load_dotenv\nfrom pydub import AudioSegment  # Install using `pip install pydub`\n\n\ndef generate_speech(input_text, output_path, api_key):\n    \"\"\"\n    Generates speech from text using OpenAI's text-to-speech API.\n    \"\"\"\n    url = \"https://api.openai.com/v1/audio/speech\"\n    headers = {\n        \"Authorization\": f\"Bearer {api_key}\",\n        \"Content-Type\": \"application/json\",\n    }\n    data = {\n        \"model\": \"tts-1\",\n        \"input\": input_text,",
    "last_modified": "2025-09-13T05:53:55.412139"
  },
  {
    "id": "149",
    "name": "conda_consolidator.py",
    "path": "01_core_ai_analysis/transcription/conda_consolidator.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 499,
    "size": 18738,
    "docstring": "Conda Environment Consolidator\nInteractive tool to safely remove and combine conda environments.",
    "keywords": [
      "analysis",
      "organization"
    ],
    "functions": [
      "print_colored",
      "load_report",
      "export_environment",
      "get_package_list",
      "compare_environments",
      "create_merged_environment",
      "remove_environment",
      "interactive_removal",
      "interactive_merge",
      "clean_conda_cache"
    ],
    "classes": [
      "Colors"
    ],
    "imports": [
      "json",
      "subprocess",
      "sys",
      "pathlib",
      "datetime",
      "shutil"
    ],
    "preview": "#!/usr/bin/env python3\n\"\"\"\nConda Environment Consolidator\nInteractive tool to safely remove and combine conda environments.\n\"\"\"\n\nimport json\nimport subprocess\nimport sys\nfrom pathlib import Path\nfrom datetime import datetime\nimport shutil\n\nclass Colors:\n    \"\"\"ANSI color codes for terminal output.\"\"\"\n    HEADER = '\\033[95m'\n    BLUE = '\\033[94m'\n    CYAN = '\\033[96m'\n    GREEN = '\\033[92m'\n    YELLOW = '\\033[93m'",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "150",
    "name": "scan.py",
    "path": "01_core_ai_analysis/transcription/scan.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 112,
    "size": 3032,
    "docstring": "",
    "keywords": [],
    "functions": [
      "is_excluded",
      "scan_directory",
      "save_csv",
      "main"
    ],
    "classes": [],
    "imports": [
      "csv",
      "os",
      "re",
      "datetime"
    ],
    "preview": "import csv\nimport os\nimport re\nfrom datetime import datetime\n\n# Define base directories to scan\nBASE_DIRS = [\n    \"/Users/steven/Documents/Python_backup\",\n    \"/Users/steven/Documents/Python\",\n    \"/Users/steven/Music/nocTurneMeLoDieS/lyrics-keys-indo\",\n    \"/Users/steven/Music/nocTurneMeLoDieS/mp3-analyze-transcribe\",\n]\n\n# Regex patterns for exclusions\nEXCLUDED_PATTERNS = [\n    r\"^\\..*\",\n    r\".*/venv/.*\",\n    r\".*/\\.venv/.*\",\n    r\".*/lib/.*\",\n    r\".*/\\.lib/.*\",",
    "last_modified": "2025-09-06T12:24:11.720416"
  },
  {
    "id": "151",
    "name": "analyze11.py",
    "path": "01_core_ai_analysis/transcription/analyze11.py",
    "category": "01_core_ai_analysis",
    "type": "analysis",
    "lines": 223,
    "size": 9464,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis",
      "transcription"
    ],
    "functions": [
      "format_timestamp",
      "parse_transcript",
      "transcribe_audio",
      "analyze_text_for_section",
      "link_timestamps_to_analysis",
      "process_audio_file",
      "process_audio_directory"
    ],
    "classes": [],
    "imports": [
      "logging",
      "os",
      "sys",
      "time",
      "concurrent.futures",
      "dotenv",
      "openai",
      "termcolor",
      "tqdm"
    ],
    "preview": "import logging\nimport os\nimport sys\nimport time\nfrom concurrent.futures import ThreadPoolExecutor\n\nfrom dotenv import load_dotenv\nfrom openai import OpenAI\nfrom termcolor import colored\nfrom tqdm import tqdm\n\nclient = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n\n\n# Load environment variables from .env (make sure your OPENAI_API_KEY is stored here)\nenv_path = os.path.expanduser(\"~/.env\")\nload_dotenv(dotenv_path=env_path)\n\n# Configure logging for error reporting\nlogging.basicConfig(",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "152",
    "name": "cover2.py",
    "path": "01_core_ai_analysis/transcription/cover2.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 51,
    "size": 1566,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis",
      "web_tools"
    ],
    "functions": [
      "generate_cover_image_with_dalle",
      "convert_mp3_to_mp4",
      "process_directory"
    ],
    "classes": [],
    "imports": [
      "openai",
      "glob",
      "os",
      "io",
      "requests",
      "moviepy.editor",
      "PIL",
      "sys"
    ],
    "preview": "from openai import OpenAI\n\nclient = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\nimport glob\nimport os\nfrom io import BytesIO\n\nimport requests\nfrom moviepy.editor import AudioFileClip, ImageClip\nfrom PIL import Image\n\n\ndef generate_cover_image_with_dalle(file_name, output_path):\n    prompt = f\"lets create a series of typography cover image for '{file_name}' in the Font and style and contexts to tell the story'\"\n    response = client.images.generate(prompt=prompt, n=1, size=\"1024x1024\")\n    image_url = response.data[0].url\n    response = requests.get(image_url)\n    img = Image.open(BytesIO(response.content))\n    img.save(output_path)\n",
    "last_modified": "2025-05-04T22:47:12.620496"
  },
  {
    "id": "153",
    "name": "generate_speech 2 2.py",
    "path": "01_core_ai_analysis/transcription/generate_speech 2 2.py",
    "category": "01_core_ai_analysis",
    "type": "transcription",
    "lines": 44,
    "size": 1447,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis"
    ],
    "functions": [
      "generate_speech",
      "main"
    ],
    "classes": [],
    "imports": [
      "csv",
      "openai"
    ],
    "preview": "import csv\n\nfrom openai import OpenAI\n\nclient = OpenAI(api_key='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')\n\n# Replace 'your_api_key_here' with your actual OpenAI API key\n\n\ndef generate_speech(text, voice='shimmer', output_path='speech.mp3'):\n    response = client.audio.create(model=\"tts-1-hd\",  # Ensure this model supports the audio creation\n                                   input=text,\n                                   voice=voice,\n                                   format=\"mp3\")\n    with open(output_path, \"wb\") as file:\n        file.write(response.content)\n\n\ndef main():\n    # Update this path to where your CSV is located",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "154",
    "name": "audio_chunker.py",
    "path": "01_core_ai_analysis/transcription/audio_chunker.py",
    "category": "01_core_ai_analysis",
    "type": "transcription",
    "lines": 174,
    "size": 7538,
    "docstring": "Audio chunking utilities for handling long files",
    "keywords": [
      "organization"
    ],
    "functions": [
      "__init__",
      "get_audio_duration",
      "should_split_file",
      "calculate_chunks",
      "split_audio_file",
      "merge_transcripts",
      "_format_timestamp",
      "cleanup_chunks"
    ],
    "classes": [
      "AudioChunker"
    ],
    "imports": [
      "os",
      "logging",
      "pathlib",
      "typing",
      "moviepy.editor",
      "config"
    ],
    "preview": "\"\"\"\nAudio chunking utilities for handling long files\n\"\"\"\n\nimport os\nimport logging\nfrom pathlib import Path\nfrom typing import List, Tuple, Optional\nfrom moviepy.editor import AudioFileClip\nimport config\n\nlogger = logging.getLogger(__name__)\n\nclass AudioChunker:\n    def __init__(self):\n        self.max_chunk_duration = config.MAX_CHUNK_DURATION_MINUTES * 60  # Convert to seconds\n        self.chunk_overlap = config.CHUNK_OVERLAP_SECONDS\n        self.min_chunk_duration = config.MIN_CHUNK_DURATION_MINUTES * 60  # Convert to seconds\n    \n    def get_audio_duration(self, audio_path: str) -> float:",
    "last_modified": "2025-10-09T05:16:30.799674"
  },
  {
    "id": "155",
    "name": "pydoc-analyze.py",
    "path": "01_core_ai_analysis/transcription/pydoc-analyze.py",
    "category": "01_core_ai_analysis",
    "type": "analysis",
    "lines": 121,
    "size": 4155,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis"
    ],
    "functions": [
      "get_openai_api_key",
      "get_directory_path",
      "get_pydocgen_paths",
      "generate_docs",
      "enhance_docs",
      "run_flake8"
    ],
    "classes": [],
    "imports": [
      "os",
      "openai",
      "subprocess"
    ],
    "preview": "import os\n\nfrom openai import OpenAI\n\nclient = OpenAI(api_key=get_openai_api_key())\nimport subprocess\n\n\n# Prompt the user to input the OpenAI API key if it's not found in environment variables\ndef get_openai_api_key():\n    api_key = os.getenv(\"OPENAI_API_KEY\")\n    if not api_key:\n        api_key = input(\"Enter your OpenAI API key: \").strip()\n    return api_key\n\n\n# Prompt the user for directory paths if not provided\ndef get_directory_path(prompt_message, default_path):\n    directory = input(f\"{prompt_message} (default: {default_path}): \").strip()\n    return directory if directory else default_path",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "156",
    "name": "start-with-openai.py",
    "path": "01_core_ai_analysis/transcription/start-with-openai.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 11,
    "size": 265,
    "docstring": "",
    "keywords": [
      "openai"
    ],
    "functions": [],
    "classes": [],
    "imports": [
      "openai"
    ],
    "preview": "import openai\n\n# Change YOUR_API_KEY with your actual API key\nopenai.api_key = \"YOUR_API_KEY\"\n\n# Request is saved in a variable\nresponse = openai.Completion.create(engine=\"davinci\", prompt=\"Hello, world!\")\n\n# Prints out the Response\nprint(response.choices[0].text)\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "157",
    "name": "captions.py",
    "path": "01_core_ai_analysis/transcription/captions.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 40,
    "size": 1343,
    "docstring": "",
    "keywords": [],
    "functions": [
      "write_srt",
      "burn_captions_ffmpeg",
      "fmt_time"
    ],
    "classes": [],
    "imports": [
      "__future__",
      "json",
      "math",
      "os",
      "subprocess",
      "tempfile",
      "typing"
    ],
    "preview": "from __future__ import annotations\n\nimport json\nimport math\nimport os\nimport subprocess\nimport tempfile\nfrom typing import Any, Dict, List\n\n\ndef write_srt(segments: List[Dict[str, Any]], srt_path: str):\n    def fmt_time(t):\n        ms = int((t - int(t)) * 1000)\n        h = int(t // 3600)\n        m = int((t % 3600) // 60)\n        s = int(t % 60)\n        return f\"{h:02d}:{m:02d}:{s:02d},{ms:03d}\"\n\n    lines = []\n    for i, seg in enumerate(segments, 1):",
    "last_modified": "2025-09-13T05:55:09.460705"
  },
  {
    "id": "158",
    "name": "mp3-trans-storytime.py",
    "path": "01_core_ai_analysis/transcription/mp3-trans-storytime.py",
    "category": "01_core_ai_analysis",
    "type": "audio_processing",
    "lines": 136,
    "size": 5114,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis",
      "transcription"
    ],
    "functions": [
      "format_timestamp",
      "transcribe_file",
      "analyze_text_for_section",
      "process_media_directory"
    ],
    "classes": [],
    "imports": [
      "os",
      "subprocess",
      "openai",
      "dotenv"
    ],
    "preview": "import os\nimport subprocess\n\nfrom openai import OpenAI\n\nclient = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\nfrom dotenv import load_dotenv\n\n# Load environment variables from .env\nenv_path = \"/Users/steven/.env\"  # Update the path if needed\nload_dotenv(dotenv_path=env_path)\n\n# Initialize OpenAI API key\n\n# Error checking for OpenAI API key\nif not openai.api_key:\n    raise EnvironmentError(\"OpenAI API key not found. Please check your .env file.\")\n\n\n# Helper to format timestamps",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "159",
    "name": "elevenlabs.py",
    "path": "01_core_ai_analysis/transcription/elevenlabs.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 43,
    "size": 1099,
    "docstring": "",
    "keywords": [],
    "functions": [
      "__init__",
      "run",
      "randomvoice"
    ],
    "classes": [
      "elevenlabs"
    ],
    "imports": [
      "random",
      "elevenlabs",
      "utils"
    ],
    "preview": "import random\n\nfrom elevenlabs import generate, save\n\nfrom utils import settings\n\nvoices = [\n    \"Adam\",\n    \"Antoni\",\n    \"Arnold\",\n    \"Bella\",\n    \"Domi\",\n    \"Elli\",\n    \"Josh\",\n    \"Rachel\",\n    \"Sam\",\n]\n\n\nclass elevenlabs:",
    "last_modified": "2025-09-13T05:53:59.626342"
  },
  {
    "id": "160",
    "name": "batch_image_seo_pipeline_20250530222615.py",
    "path": "01_core_ai_analysis/transcription/batch_image_seo_pipeline_20250530222615.py",
    "category": "01_core_ai_analysis",
    "type": "image_processing",
    "lines": 122,
    "size": 4843,
    "docstring": "",
    "keywords": [
      "openai"
    ],
    "functions": [
      "call_gpt4o_analysis_and_prompts",
      "batch_flexible_analysis"
    ],
    "classes": [],
    "imports": [
      "csv",
      "json",
      "logging",
      "os",
      "pathlib",
      "dotenv",
      "openai",
      "tqdm"
    ],
    "preview": "import csv\nimport json\nimport logging\nimport os\nfrom pathlib import Path\n\nfrom dotenv import load_dotenv\nfrom openai import OpenAI\nfrom tqdm import tqdm\n\n# CONFIG\nload_dotenv(os.path.expanduser(\"~/.env\"))\nOPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\nif not OPENAI_API_KEY:\n    raise EnvironmentError(\"OPENAI_API_KEY not found in ~/.env\")\nclient = OpenAI(api_key=OPENAI_API_KEY)\n\nLOG_FILE = \"avatararts_flexible_analysis.log\"\nlogging.basicConfig(\n    level=logging.INFO,",
    "last_modified": "2025-09-13T05:53:26.316918"
  },
  {
    "id": "161",
    "name": "evaluate.py",
    "path": "01_core_ai_analysis/transcription/evaluate.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 251,
    "size": 9083,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis"
    ],
    "functions": [
      "generate_answers",
      "evaluate_answer",
      "evaluate_batch",
      "evaluate_answers",
      "check_if_huggingface_model_exists",
      "check_if_huggingface_dataset_exists",
      "format"
    ],
    "classes": [],
    "imports": [
      "concurrent.futures",
      "gc",
      "json",
      "os",
      "datasets",
      "huggingface_hub",
      "huggingface_hub.utils",
      "openai",
      "tqdm.auto",
      "vllm"
    ],
    "preview": "import concurrent.futures\nimport gc\nimport json\nimport os\n\nfrom datasets import Dataset, load_dataset\nfrom huggingface_hub import HfApi\nfrom huggingface_hub.utils import RepositoryNotFoundError\nfrom openai import OpenAI\nfrom tqdm.auto import tqdm\nfrom vllm import LLM, SamplingParams\n\nOPENAI_API_KEY = os.environ[\"OPENAI_API_KEY\"]\nDATASET_HUGGINGFACE_WORKSPACE = os.environ[\"DATASET_HUGGINGFACE_WORKSPACE\"]\nMODEL_HUGGINGFACE_WORKSPACE = os.environ[\"MODEL_HUGGINGFACE_WORKSPACE\"]\nIS_DUMMY = os.environ.get(\"IS_DUMMY\", False)\n\nprint(\"====== EVAL PARAMETERS ======\")  # noqa\nprint(f\"{DATASET_HUGGINGFACE_WORKSPACE=}\")  # noqa\nprint(f\"{MODEL_HUGGINGFACE_WORKSPACE=}\")  # noqa",
    "last_modified": "2025-09-13T05:53:42.236007"
  },
  {
    "id": "162",
    "name": "transcribe (1).py",
    "path": "01_core_ai_analysis/transcription/transcribe (1).py",
    "category": "01_core_ai_analysis",
    "type": "transcription",
    "lines": 57,
    "size": 1994,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis",
      "transcription"
    ],
    "functions": [
      "transcribe_audio",
      "analyze_text"
    ],
    "classes": [],
    "imports": [
      "openai",
      "os",
      "dotenv",
      "sys"
    ],
    "preview": "from openai import OpenAI\n\nclient = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\nimport os\n\nfrom dotenv import load_dotenv\n\n# Load environment variables from .env\nload_dotenv()\n\n\n# Function to transcribe audio using OpenAI Whisper\ndef transcribe_audio(file_path):\n    with open(file_path, \"rb\") as audio_file:\n        transcript = client.audio.transcribe(\"whisper-1\", audio_file)\n        return transcript.text\n\n\n# Function to analyze the transcript using GPT\ndef analyze_text(text):",
    "last_modified": "2025-05-04T22:47:13.352714"
  },
  {
    "id": "163",
    "name": "transcribe-mp3.py",
    "path": "01_core_ai_analysis/transcription/transcribe-mp3.py",
    "category": "01_core_ai_analysis",
    "type": "transcription",
    "lines": 48,
    "size": 1347,
    "docstring": "",
    "keywords": [
      "analysis",
      "transcription"
    ],
    "functions": [
      "transcribe_audio",
      "save_transcription",
      "process_directory",
      "main"
    ],
    "classes": [],
    "imports": [
      "os",
      "whisper"
    ],
    "preview": "import os\n\nimport whisper\n\n\ndef transcribe_audio(file_path):\n    # Load the Whisper model\n    model = whisper.load_model(\"base\")\n\n    # Transcribe the audio file\n    result = model.transcribe(file_path)\n\n    return result[\"segments\"]\n\n\ndef save_transcription(segments, output_file):\n    with open(output_file, \"w\") as f:\n        for segment in segments:\n            start = segment[\"start\"]\n            end = segment[\"end\"]",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "164",
    "name": "batch_image_seo_pipeline_20250530221306.py",
    "path": "01_core_ai_analysis/transcription/batch_image_seo_pipeline_20250530221306.py",
    "category": "01_core_ai_analysis",
    "type": "analysis",
    "lines": 422,
    "size": 17257,
    "docstring": "batch_image_seo_pipeline.py\n\n1) Discovers all image files under a given folder.\n2) Extracts technical metadata via Pillow.\n3) Calls GPT-4o Vision (multimodal) using a \u201csystem\u201d + \u201cuser\u201d prompt.\n4) Parses the JSON response into structured fields:\n   main_subject, style, color_palette, tags, orientation, suggested_products,\n   SEO_title, SEO_description, emotion, safety_rating, dominant_keyword.\n5) Builds a \u201csource\u201d tag: folder-subfolder-YYYYMMDD.\n6) Applies a \u201cTop 5% SEO Analytics\u201d framework to generate SEO columns:\n   \u2013 SEO Keywords, Traffic Source, CRO Tactic, Backlink Source, Engagement Rate.\n7) Adds niche-specific design prompts (Geeky, Dark Humor, Anime).\n8) Writes everything into a final CSV for bulk upload.\n\nDependencies:\n    pip install openai python-dotenv pillow tqdm backoff",
    "keywords": [
      "openai"
    ],
    "functions": [
      "load_openai_client",
      "retry_on_exception",
      "call_gpt4o",
      "get_image_tech_meta",
      "build_source_tag",
      "discover_images",
      "build_gpt_messages",
      "parse_args",
      "main"
    ],
    "classes": [],
    "imports": [
      "argparse",
      "csv",
      "json",
      "logging",
      "os",
      "sys",
      "datetime",
      "pathlib",
      "typing",
      "backoff"
    ],
    "preview": "#!/usr/bin/env python3\n\"\"\"\nbatch_image_seo_pipeline.py\n\n1) Discovers all image files under a given folder.\n2) Extracts technical metadata via Pillow.\n3) Calls GPT-4o Vision (multimodal) using a \u201csystem\u201d + \u201cuser\u201d prompt.\n4) Parses the JSON response into structured fields:\n   main_subject, style, color_palette, tags, orientation, suggested_products,\n   SEO_title, SEO_description, emotion, safety_rating, dominant_keyword.\n5) Builds a \u201csource\u201d tag: folder-subfolder-YYYYMMDD.\n6) Applies a \u201cTop 5% SEO Analytics\u201d framework to generate SEO columns:\n   \u2013 SEO Keywords, Traffic Source, CRO Tactic, Backlink Source, Engagement Rate.\n7) Adds niche-specific design prompts (Geeky, Dark Humor, Anime).\n8) Writes everything into a final CSV for bulk upload.\n\nDependencies:\n    pip install openai python-dotenv pillow tqdm backoff\n\"\"\"\n",
    "last_modified": "2025-09-13T05:53:26.033209"
  },
  {
    "id": "165",
    "name": "SpeechReco.py",
    "path": "01_core_ai_analysis/transcription/SpeechReco.py",
    "category": "01_core_ai_analysis",
    "type": "transcription",
    "lines": 87,
    "size": 3672,
    "docstring": "",
    "keywords": [],
    "functions": [
      "mp3_to_wav_Conversion",
      "split_files_with_timestamp",
      "writeInFile_key_value"
    ],
    "classes": [],
    "imports": [
      "os",
      "subprocess",
      "time",
      "speech_recognition",
      "pydub",
      "pydub.utils"
    ],
    "preview": "# PLEASE READ THE COMMENTS and PROVIDE REVIEW COMMENTS IN THE CODE\n# This file take mp3 file as an input and produces <key,value> pair of <timestamp,transcribed text> in a single file using Google's Speech recognition library.\n# This pair can later be pre-processed for deleting the articles, prepositions, conjections etc.. and can be added to the DB\n# The granularity is currently set as 10 seconds. we can reduce or increase it depending.\n\n\n# Trial Number#1: with 532 recording on Oct12.(1:15 hrs) <-- the transcribed text does not match with the actual words due to an unique accent.\n# Trial Number#2: with 661 recording by Brian Levine(20 mins) <-- the transcribed output is perfect.\n\n# TBD::: what is the time granularity? which timestamp style do we use? Need american accent dataset, to get proper result.\n# Next step of code addition:::\n#       Loop through multiple videos\n#       Timestamp correction\n#       add the results in the DB\n#       eliminate unimportant words\n\nimport os\nimport subprocess\nimport time\n",
    "last_modified": "2025-03-28T18:37:11"
  },
  {
    "id": "166",
    "name": "cover2 2.py",
    "path": "01_core_ai_analysis/transcription/cover2 2.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 51,
    "size": 1566,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis",
      "web_tools"
    ],
    "functions": [
      "generate_cover_image_with_dalle",
      "convert_mp3_to_mp4",
      "process_directory"
    ],
    "classes": [],
    "imports": [
      "glob",
      "os",
      "io",
      "requests",
      "moviepy.editor",
      "openai",
      "PIL",
      "sys"
    ],
    "preview": "import glob\nimport os\nfrom io import BytesIO\n\nimport requests\nfrom moviepy.editor import AudioFileClip, ImageClip\nfrom openai import OpenAI\nfrom PIL import Image\n\nclient = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n\n\ndef generate_cover_image_with_dalle(file_name, output_path):\n    prompt = f\"lets create a series of typography cover image for '{file_name}' in the Font and style and contexts to tell the story'\"\n    response = client.images.generate(prompt=prompt, n=1, size=\"1024x1024\")\n    image_url = response.data[0].url\n    response = requests.get(image_url)\n    img = Image.open(BytesIO(response.content))\n    img.save(output_path)\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "167",
    "name": "convert-webm-mp3.py",
    "path": "01_core_ai_analysis/transcription/convert-webm-mp3.py",
    "category": "01_core_ai_analysis",
    "type": "audio_processing",
    "lines": 43,
    "size": 1273,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "os",
      "subprocess"
    ],
    "preview": "import os\nimport subprocess\n\n# Path to the TXT file containing .webm file paths\ntxt_file_path = \"/Users/steven/webm.txt\"\n\n# Read the file paths from the TXT file\nwith open(txt_file_path, \"r\") as file:\n    webm_paths = file.readlines()\n\n# Loop through each path and convert to .mp3\nfor webm_path in webm_paths:\n    webm_path = webm_path.strip()  # Remove newline and extra spaces\n    if webm_path.endswith(\".webm\"):\n        # Define the output .mp3 path\n        mp3_path = webm_path.replace(\".webm\", \".mp3\")\n\n        # Ensure the output directory exists\n        os.makedirs(os.path.dirname(mp3_path), exist_ok=True)\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "168",
    "name": "fancyimg (1) 1.py",
    "path": "01_core_ai_analysis/transcription/fancyimg (1) 1.py",
    "category": "01_core_ai_analysis",
    "type": "image_processing",
    "lines": 84,
    "size": 3000,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis"
    ],
    "functions": [
      "generate_filename_with_gpt",
      "pair_and_rename_images"
    ],
    "classes": [],
    "imports": [
      "os",
      "openai",
      "csv",
      "dotenv"
    ],
    "preview": "import os\n\nfrom openai import OpenAI\n\nclient = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\nimport csv\n\nfrom dotenv import load_dotenv\n\n# Load API key from .env file\nload_dotenv(\"/Users/steven/.env\")\n\n\n# Function to generate a filename using GPT based on the prompt\ndef generate_filename_with_gpt(prompt):\n    response = client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        messages=[\n            {\n                \"role\": \"system\",",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "169",
    "name": "trek 3 2.py",
    "path": "01_core_ai_analysis/transcription/trek 3 2.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 70,
    "size": 2498,
    "docstring": "",
    "keywords": [
      "analysis",
      "web_tools"
    ],
    "functions": [
      "generate_speech",
      "process_csv_and_generate_speech"
    ],
    "classes": [],
    "imports": [
      "csv",
      "os",
      "requests"
    ],
    "preview": "import csv\nimport os\n\nimport requests  # Make sure to install this package\n\n\ndef generate_speech(input_text, output_path, api_key):\n    \"\"\"\n    Generates speech from text using an API (e.g., OpenAI's text-to-speech API).\n    \"\"\"\n    url = \"https://api.openai.com/v1/audio/speech\"\n    headers = {\n        \"Authorization\": f\"Bearer {api_key}\",\n        \"Content-Type\": \"application/json\",\n    }\n    data = {\n        \"model\": \"tts-1\",  # Update this according to the specific model you're using\n        \"input\": input_text,\n        \"voice\": \"shimmer\",  # Update the voice parameter as needed\n    }",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "170",
    "name": "media_processor.py",
    "path": "01_core_ai_analysis/transcription/media_processor.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 131,
    "size": 5982,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis",
      "transcription"
    ],
    "functions": [
      "format_timestamp",
      "transcribe_audio",
      "analyze_text_for_section",
      "process_media_directory"
    ],
    "classes": [],
    "imports": [
      "logging",
      "os",
      "sys",
      "openai",
      "dotenv"
    ],
    "preview": "import logging\nimport os\nimport sys\n\nimport openai\nfrom dotenv import load_dotenv\n\n# Setup basic logging\nlogging.basicConfig(level=logging.INFO, format=\"%(asctime)s [%(levelname)s] %(message)s\")\n\n# Load environment variables from .env (ensure your OPENAI_API_KEY is stored here)\nenv_path = \"/Users/steven/.env\"\nload_dotenv(dotenv_path=env_path)\n\n# Set the OpenAI API key\nopenai.api_key = os.getenv(\"OPENAI_API_KEY\")\n\n# Define a simple config dictionary. You can expand this as needed.\nconfig = {\"base_dir\": \"\"}  # Leave blank to prompt for base directory\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "171",
    "name": "ana-trans.py",
    "path": "01_core_ai_analysis/transcription/ana-trans.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 158,
    "size": 7062,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis",
      "transcription"
    ],
    "functions": [
      "split_media_to_segments",
      "transcribe_media_segment",
      "format_timestamp",
      "analyze_text_for_section",
      "process_media_by_section"
    ],
    "classes": [],
    "imports": [
      "openai",
      "os",
      "subprocess",
      "dotenv",
      "sys"
    ],
    "preview": "import openai\nimport os\nimport subprocess\nfrom dotenv import load_dotenv\n\n# Load environment variables from .env (make sure your OPENAI_API_KEY is stored here)\nload_dotenv(\"/Users/steven/.env\")\nopenai.api_key = os.getenv(\"OPENAI_API_KEY\")\n\n# Prompt for directory paths\nproject_root = input(\"Please enter the project root directory path: \").strip()\n\n# Directory paths based on user input\nMEDIA_DIR = os.path.join(project_root, \"Media\")  # Directory containing media files\nTRANSCRIPT_DIR = os.path.join(MEDIA_DIR, \"transcript\")  # Directory to save transcripts\nANALYSIS_DIR = os.path.join(MEDIA_DIR, \"analysis\")  # Directory to save the analysis files\n\n# Create output directories if they don't exist\nos.makedirs(TRANSCRIPT_DIR, exist_ok=True)\nos.makedirs(ANALYSIS_DIR, exist_ok=True)",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "172",
    "name": "song-process-remix.py",
    "path": "01_core_ai_analysis/transcription/song-process-remix.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 241,
    "size": 8710,
    "docstring": "",
    "keywords": [
      "analysis",
      "transcription"
    ],
    "functions": [
      "extract_duration_from_transcript",
      "get_all_song_data",
      "read_file_content",
      "write_to_csv",
      "format_timestamp",
      "transcribe_audio",
      "analyze_text_for_section",
      "process_audio_file",
      "process_audio_directory"
    ],
    "classes": [],
    "imports": [
      "csv",
      "os",
      "sys",
      "dotenv"
    ],
    "preview": "import csv\nimport os\nimport sys\n\nfrom dotenv import load_dotenv\n\n# BASE_DIR = \"/Users/steven/Music/NocTurnE-meLoDieS/mp3\"\n# CSV_OUTPUT = os.path.join(BASE_DIR, \"final_song_data.csv\")\n\n\ndef extract_duration_from_transcript(transcript_text):\n    lines = transcript_text.strip().split(\"\\n\")\n    last_line = lines[-1] if lines else \"\"\n    if \"--\" in last_line:\n        time_range = last_line.split(\"--\")[-1].split(\":\")\n        minutes = int(time_range[0].strip())\n        seconds = int(time_range[1].strip().split()[0])\n        return f\"{minutes}:{str(seconds).zfill(2)}\"\n    return \"0:00\"\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "173",
    "name": "audiototext.py",
    "path": "01_core_ai_analysis/transcription/audiototext.py",
    "category": "01_core_ai_analysis",
    "type": "transcription",
    "lines": 753,
    "size": 28563,
    "docstring": "AudioToText.ipynb",
    "keywords": [
      "openai",
      "transcription"
    ],
    "functions": [
      "get_audio",
      "add_chunk",
      "raw_split",
      "write_result",
      "write_result"
    ],
    "classes": [
      "WriteText(WriteTXT)"
    ],
    "imports": [
      "subprocess",
      "sys",
      "platform",
      "io",
      "base64",
      "os.path",
      "ffmpeg",
      "numpy",
      "google.colab.output",
      "IPython.display"
    ],
    "preview": "# -*- coding: utf-8 -*-\n\"\"\"AudioToText.ipynb\n\nAutomatically generated by Colab.\n\nOriginal file is located at\n    https://colab.research.google.com/github/Carleslc/AudioToText/blob/master/AudioToText.ipynb\n\n# \ud83d\udde3\ufe0f [**AudioToText**](https://github.com/Carleslc/AudioToText)\n\n[![Donate](https://www.ko-fi.com/img/githubbutton_sm.svg)](https://ko-fi.com/carleslc)\n\n### \ud83d\udee0 [Whisper by OpenAI](https://github.com/openai/whisper)\n\n## [Step 1] \u2699\ufe0f Install the required libraries\n\nClick \u25b6\ufe0f button below to install the dependencies for this notebook.\n\"\"\"\n\n#@title { display-mode: \"form\" }",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "174",
    "name": "Quiz22sec.py",
    "path": "01_core_ai_analysis/transcription/Quiz22sec.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 98,
    "size": 3336,
    "docstring": "",
    "keywords": [
      "analysis",
      "web_tools"
    ],
    "functions": [
      "generate_speech",
      "calculate_text_duration",
      "process_csv_and_generate_speech"
    ],
    "classes": [],
    "imports": [
      "csv",
      "os",
      "requests",
      "dotenv",
      "pydub"
    ],
    "preview": "import csv\nimport os\n\nimport requests  # Install using `pip install requests`\nfrom dotenv import load_dotenv\nfrom pydub import AudioSegment  # Install using `pip install pydub`\n\n\ndef generate_speech(input_text, output_path, api_key):\n    \"\"\"\n    Generates speech from text using OpenAI's text-to-speech API.\n    \"\"\"\n    url = \"https://api.openai.com/v1/audio/speech\"\n    headers = {\n        \"Authorization\": f\"Bearer {api_key}\",\n        \"Content-Type\": \"application/json\",\n    }\n    data = {\n        \"model\": \"tts-1\",\n        \"input\": input_text,",
    "last_modified": "2025-09-13T05:53:54.781162"
  },
  {
    "id": "175",
    "name": "speech.py",
    "path": "01_core_ai_analysis/transcription/speech.py",
    "category": "01_core_ai_analysis",
    "type": "transcription",
    "lines": 35,
    "size": 1144,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis"
    ],
    "functions": [
      "generate_speech"
    ],
    "classes": [],
    "imports": [
      "csv",
      "pathlib",
      "openai"
    ],
    "preview": "import csv\nfrom pathlib import Path\n\nfrom openai import OpenAI\n\n# Initialize the OpenAI client\nclient = OpenAI()\n\n# Define the path to the CSV file\ncsv_path = \"quiz329.csv\"\n\n# Define the directory to save the audio files\noutput_dir = Path(__file__).parent / \"speech\"\noutput_dir.mkdir(exist_ok=True)  # Create the directory if it doesn't exist\n\n\n# Function to generate speech with pauses\ndef generate_speech(text, file_path):\n    response = client.audio.speech.create(\n        model=\"tts-1\", voice=\"shimmer\", input=text, ssml=True  # Enable SSML processing",
    "last_modified": "2025-05-04T22:47:13.048193"
  },
  {
    "id": "176",
    "name": "analyze_remaining.py",
    "path": "01_core_ai_analysis/transcription/analyze_remaining.py",
    "category": "01_core_ai_analysis",
    "type": "analysis",
    "lines": 506,
    "size": 23831,
    "docstring": "Analyze remaining Python files and directories for second migration pass",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "analyze_remaining_files",
      "show_remaining_analysis",
      "create_second_migration_script",
      "main"
    ],
    "classes": [],
    "imports": [
      "os",
      "re",
      "pathlib",
      "collections"
    ],
    "preview": "#!/usr/bin/env python3\n\"\"\"\nAnalyze remaining Python files and directories for second migration pass\n\"\"\"\n\nimport os\nimport re\nfrom pathlib import Path\nfrom collections import defaultdict\n\ndef analyze_remaining_files():\n    \"\"\"Analyze all remaining Python files and directories.\"\"\"\n    base_path = Path(\"/Users/steven/Documents/python\")\n    \n    # Categories for remaining files\n    categories = {\n        \"image_processing\": [],\n        \"quiz_tts\": [],\n        \"conversion_tools\": [],\n        \"gallery_html\": [],",
    "last_modified": "2025-10-09T05:30:43.977227"
  },
  {
    "id": "177",
    "name": "Multi-Modal.py",
    "path": "01_core_ai_analysis/transcription/Multi-Modal.py",
    "category": "01_core_ai_analysis",
    "type": "analysis",
    "lines": 649,
    "size": 24805,
    "docstring": "ULTIMATE Media Analysis Pipeline - Multi-API Edition\nEnhanced with large file support (2GB+ MP4s)\nCompatible with Pydantic 2.x",
    "keywords": [
      "analysis",
      "openai",
      "transcription",
      "organization"
    ],
    "functions": [
      "main",
      "__init__",
      "extract_audio_from_large_video",
      "cleanup_temp_files",
      "__init__",
      "setup_directories",
      "setup_clients",
      "get_file_hash",
      "check_cache",
      "save_to_cache"
    ],
    "classes": [
      "LargeFileHandler",
      "UltimateMediaAnalyzer"
    ],
    "imports": [
      "os",
      "logging",
      "time",
      "json",
      "subprocess",
      "hashlib",
      "pathlib",
      "typing",
      "dotenv",
      "openai"
    ],
    "preview": "#!/usr/bin/env python3\n\"\"\"\nULTIMATE Media Analysis Pipeline - Multi-API Edition\nEnhanced with large file support (2GB+ MP4s)\nCompatible with Pydantic 2.x\n\"\"\"\n\nimport os\nimport logging\nimport time\nimport json\nimport subprocess\nimport hashlib\nfrom pathlib import Path\nfrom typing import Optional, Dict, Any, List\nfrom dotenv import load_dotenv\n\n# Load environment variables first\nload_dotenv()\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "178",
    "name": "engine_wrapper.py",
    "path": "01_core_ai_analysis/transcription/engine_wrapper.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 183,
    "size": 7785,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "process_text",
      "__init__",
      "add_periods",
      "run",
      "split_post",
      "call_tts",
      "create_silence_mp3"
    ],
    "classes": [
      "TTSEngine"
    ],
    "imports": [
      "os",
      "re",
      "pathlib",
      "typing",
      "numpy",
      "translators",
      "moviepy.audio.AudioClip",
      "moviepy.audio.fx.volumex",
      "moviepy.editor",
      "rich.progress"
    ],
    "preview": "import os\nimport re\nfrom pathlib import Path\nfrom typing import Tuple\n\nimport numpy as np\nimport translators\nfrom moviepy.audio.AudioClip import AudioClip\nfrom moviepy.audio.fx.volumex import volumex\nfrom moviepy.editor import AudioFileClip\nfrom rich.progress import track\n\nfrom utils import settings\nfrom utils.console import print_step, print_substep\nfrom utils.voice import sanitize_text\n\nDEFAULT_MAX_LENGTH: int = (\n    50  # Video length variable, edit this on your own risk. It should work, but it's not supported\n)\n",
    "last_modified": "2025-09-13T05:53:59.726247"
  },
  {
    "id": "179",
    "name": "mp3-csv.py",
    "path": "01_core_ai_analysis/transcription/mp3-csv.py",
    "category": "01_core_ai_analysis",
    "type": "audio_processing",
    "lines": 107,
    "size": 4019,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis",
      "transcription"
    ],
    "functions": [
      "format_timestamp",
      "transcribe_audio",
      "analyze_text_for_section",
      "process_audio_file",
      "process_audio_directory"
    ],
    "classes": [],
    "imports": [
      "os",
      "openai",
      "dotenv"
    ],
    "preview": "import os\n\nimport openai\nfrom dotenv import load_dotenv\n\n# Load environment variables\nload_dotenv()\nopenai.api_key = os.getenv(\"OPENAI_API_KEY\")\nif not openai.api_key:\n    raise EnvironmentError(\"OpenAI API key not found. Please check your .env file.\")\n\n\ndef format_timestamp(seconds):\n    \"\"\"Format seconds into MM:SS timestamp.\"\"\"\n    minutes = int(seconds // 60)\n    seconds = seconds % 60\n    return f\"{minutes:02d}:{int(seconds):02d}\"\n\n\ndef transcribe_audio(file_path):",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "180",
    "name": "import os.py",
    "path": "01_core_ai_analysis/transcription/import os.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 61,
    "size": 1969,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis"
    ],
    "functions": [
      "get_openai_category",
      "create_directory",
      "save_to_category",
      "categorize_files"
    ],
    "classes": [],
    "imports": [
      "os",
      "openai"
    ],
    "preview": "import os\n\nfrom openai import OpenAI\n\nclient = OpenAI(api_key=api_key)\n\n# Load the API key from the environment variable\napi_key = os.getenv(\"OPENAI_API_KEY\")\nif not api_key:\n    raise ValueError(\"API key not found. Please ensure it is set in your environment variables.\")\n\n\ndef get_openai_category(script_content):\n    try:\n        response = client.chat.completions.create(\n            model=\"gpt-3.5-turbo\",\n            messages=[\n                {\"role\": \"system\", \"content\": \"You are an expert Python programmer.\"},\n                {\n                    \"role\": \"user\",",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "181",
    "name": "pyrepo_doc_organizer.py",
    "path": "01_core_ai_analysis/transcription/pyrepo_doc_organizer.py",
    "category": "01_core_ai_analysis",
    "type": "analysis",
    "lines": 478,
    "size": 14913,
    "docstring": "pyrepo_doc_organizer.py\nA comprehensive CLI tool to:\n  1) Audit a Python repository using AST to summarize modules, functions, classes, and imports.\n  2) Generate Markdown summaries and a CSV index for each module.\n  3) Categorize scripts using heuristics and/or OpenAI (optional).\n  4) (Optional) Export simple HTML index (blog-like list) from CSV.\n\nSafe defaults:\n- Works offline (std. library only). OpenAI integration is optional.\n- Does NOT modify your repo (read-only); all outputs go to an output folder.\n- Cross-platform (macOS/Linux/Windows) for Python 3.10+.\n\nUsage examples:\n  # Basic: analyze the current directory and write outputs to ./_py_audit\n  python pyrepo_doc_organizer.py .\n\n  # Include virtualenv directories\n  python pyrepo_doc_organizer.py . --include-venv\n\n  # Ask OpenAI to help with titles/descriptions/categories (requires OPENAI_API_KEY)\n  python pyrepo_doc_organizer.py . --ai-describe --ai-categorize --model gpt-4o-mini\n\n  # Also emit a minimal HTML index (from the CSV) for quick browsing\n  python pyrepo_doc_organizer.py . --html\n\nAuthor: ChatGPT for Steven (TechnoMancer)\nLicense: MIT",
    "keywords": [
      "openai",
      "analysis"
    ],
    "functions": [
      "iter_py_files",
      "read_text",
      "analyze_ast",
      "categorize_heuristic",
      "sanitize_filename",
      "write_markdown",
      "write_csv",
      "write_html_from_csv",
      "ensure_openai_client",
      "ai_describe_and_categorize"
    ],
    "classes": [
      "ModuleInfo"
    ],
    "imports": [
      "__future__",
      "argparse",
      "ast",
      "csv",
      "html",
      "json",
      "os",
      "re",
      "sys",
      "dataclasses"
    ],
    "preview": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\npyrepo_doc_organizer.py\nA comprehensive CLI tool to:\n  1) Audit a Python repository using AST to summarize modules, functions, classes, and imports.\n  2) Generate Markdown summaries and a CSV index for each module.\n  3) Categorize scripts using heuristics and/or OpenAI (optional).\n  4) (Optional) Export simple HTML index (blog-like list) from CSV.\n\nSafe defaults:\n- Works offline (std. library only). OpenAI integration is optional.\n- Does NOT modify your repo (read-only); all outputs go to an output folder.\n- Cross-platform (macOS/Linux/Windows) for Python 3.10+.\n\nUsage examples:\n  # Basic: analyze the current directory and write outputs to ./_py_audit\n  python pyrepo_doc_organizer.py .\n\n  # Include virtualenv directories",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "182",
    "name": "reddit.py",
    "path": "01_core_ai_analysis/transcription/reddit.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 57,
    "size": 1627,
    "docstring": "",
    "keywords": [
      "data_processing"
    ],
    "functions": [
      "comment_html",
      "gen_comment_image"
    ],
    "classes": [],
    "imports": [
      "datetime",
      "pandas",
      "praw",
      "moviepy.editor"
    ],
    "preview": "import datetime as dt\n\nimport pandas as pd\nimport praw\nfrom moviepy.editor import *\n\n\ndef comment_html(username, content):\n    str = (\n        \"<!DOCTYPE html><html><head>\"\n        \"<style>body {background-color: rgb(26, 26, 27);color: white;font-family: BentonSans, sans-serif;}.username {color: rgb(79, 188, 255);}.content {padding: 5px}.header {padding: 0 0 0 5px}</style>\"\n        \"</head>\"\n        \"<body><div><div class = 'header'><span class=username>\"\n        + username\n        + \"</span></div><div class = 'content'>\"\n        + content\n        + \"</div></div></body></html>\"\n    )\n    return str\n",
    "last_modified": "2025-05-04T23:28:22.832649"
  },
  {
    "id": "183",
    "name": "enhanced_image_pipeline.py",
    "path": "01_core_ai_analysis/transcription/enhanced_image_pipeline.py",
    "category": "01_core_ai_analysis",
    "type": "analysis",
    "lines": 1299,
    "size": 51574,
    "docstring": "Hybrid Image Analysis Pipeline\n==============================\n\nA cost-optimized pipeline combining Google Cloud Vision API for technical analysis\nwith OpenAI GPT-4 Vision for complex semantic understanding. This approach reduces\nAPI costs by 60-80% while maintaining high-quality results.\n\nArchitecture:\n- Google Cloud Vision: Object detection, label classification, color analysis, OCR\n- OpenAI GPT-4 Vision: Emotional context, style descriptions, marketing copy, SEO optimization\n\nFeatures:\n- Intelligent API selection based on analysis type\n- Cost tracking and optimization\n- Fallback mechanisms for API failures\n- Batch processing with rate limiting\n- Comprehensive error handling and retry logic\n\nAuthor: Enhanced by Claude\nVersion: 2.1 (Hybrid)",
    "keywords": [
      "openai",
      "analysis"
    ],
    "functions": [
      "setup_logging",
      "parse_arguments",
      "validate_environment",
      "main",
      "__init__",
      "load_config",
      "_deep_merge",
      "save_config",
      "get",
      "__init__"
    ],
    "classes": [
      "ImageMetadata",
      "GoogleVisionResults",
      "OpenAIResults",
      "CombinedAnalysis",
      "ProcessingResult",
      "Config",
      "CostTracker",
      "GoogleVisionClient",
      "OpenAIClient",
      "HybridImageProcessor",
      "CSVWriter",
      "HybridImagePipeline"
    ],
    "imports": [
      "argparse",
      "base64",
      "csv",
      "json",
      "logging",
      "os",
      "sys",
      "time",
      "concurrent.futures",
      "datetime"
    ],
    "preview": "#!/usr/bin/env python3\n\"\"\"\nHybrid Image Analysis Pipeline\n==============================\n\nA cost-optimized pipeline combining Google Cloud Vision API for technical analysis\nwith OpenAI GPT-4 Vision for complex semantic understanding. This approach reduces\nAPI costs by 60-80% while maintaining high-quality results.\n\nArchitecture:\n- Google Cloud Vision: Object detection, label classification, color analysis, OCR\n- OpenAI GPT-4 Vision: Emotional context, style descriptions, marketing copy, SEO optimization\n\nFeatures:\n- Intelligent API selection based on analysis type\n- Cost tracking and optimization\n- Fallback mechanisms for API failures\n- Batch processing with rate limiting\n- Comprehensive error handling and retry logic\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "184",
    "name": "organize_avatararts.py",
    "path": "01_core_ai_analysis/transcription/organize_avatararts.py",
    "category": "01_core_ai_analysis",
    "type": "organization",
    "lines": 95,
    "size": 2281,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "os",
      "shutil",
      "pathlib"
    ],
    "preview": "import os\nimport shutil\nfrom pathlib import Path\n\nbase_path = Path(\"/Users/steven/AvaTarArTs\")\n\nstructure_map = {\n    \"site\": [\n        \"index.html\",\n        \"playlist.html\",\n        \"globe.html\",\n        \"glitch.html\",\n        \"404.md\",\n        \"style.css\",\n        \"styles.css\",\n        \"styles\",\n        \"script.js\",\n        \"js\",\n        \"img\",\n        \"site.html\",",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "185",
    "name": "video_analyzer.py",
    "path": "01_core_ai_analysis/transcription/video_analyzer.py",
    "category": "01_core_ai_analysis",
    "type": "analysis",
    "lines": 114,
    "size": 5696,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis",
      "transcription"
    ],
    "functions": [
      "format_timestamp",
      "transcribe_audio",
      "analyze_text_for_section",
      "process_audio_directory"
    ],
    "classes": [],
    "imports": [
      "logging",
      "os",
      "openai",
      "shared.config",
      "termcolor",
      "tqdm",
      "shared.openai_client"
    ],
    "preview": "import logging\nimport os\n\nimport openai\nfrom shared.config import *\nfrom termcolor import colored\nfrom tqdm import tqdm\n\n# Load environment variables from .env\nenv_path = os.path.expanduser(\"~/.env\")\nload_dotenv(dotenv_path=env_path)\n\n# Set OpenAI API key\nfrom shared.openai_client import get_openai_client\nif not openai.api_key:\n    raise EnvironmentError(\"OpenAI API key not found. Please check your .env file.\")\n\n\ndef format_timestamp(seconds):\n    minutes = int(seconds // 60)",
    "last_modified": "2025-10-09T05:27:15.573723"
  },
  {
    "id": "186",
    "name": "mp4-mp3-analyze.py",
    "path": "01_core_ai_analysis/transcription/mp4-mp3-analyze.py",
    "category": "01_core_ai_analysis",
    "type": "analysis",
    "lines": 174,
    "size": 6748,
    "docstring": "",
    "keywords": [
      "video_processing",
      "openai",
      "analysis",
      "transcription"
    ],
    "functions": [
      "split_video_to_segments",
      "transcribe_video_segment",
      "format_timestamp",
      "analyze_text_for_section",
      "process_video_by_section"
    ],
    "classes": [],
    "imports": [
      "os",
      "subprocess",
      "openai",
      "dotenv",
      "sys"
    ],
    "preview": "import os\nimport subprocess\n\nimport openai\nfrom dotenv import load_dotenv\n\n# Load environment variables from .env (make sure your OPENAI_API_KEY is stored here)\nload_dotenv()\nopenai.api_key = os.getenv(\"OPENAI_API_KEY\")\n\n# Directory paths\nVIDEO_DIR = (\n    \"/Users/steven/Movies/j2025\"  # Directory containing MP4 files\n)\nTRANSCRIPT_DIR = \"/Users/steven/Movies/j2025/transcript\"  # Directory to save transcripts\nANALYSIS_DIR = \"/Users/steven/Movies/j2025/analysis\"  # Directory to save the analysis files\n\n# Create output directories if they don't exist\nos.makedirs(TRANSCRIPT_DIR, exist_ok=True)\nos.makedirs(ANALYSIS_DIR, exist_ok=True)",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "187",
    "name": "analyze-shorts.py",
    "path": "01_core_ai_analysis/transcription/analyze-shorts.py",
    "category": "01_core_ai_analysis",
    "type": "analysis",
    "lines": 123,
    "size": 5264,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis",
      "transcription"
    ],
    "functions": [
      "transcribe_audio",
      "format_timestamp",
      "analyze_text_for_section",
      "process_audio_directory"
    ],
    "classes": [],
    "imports": [
      "openai",
      "logging",
      "os",
      "sys",
      "time",
      "concurrent.futures",
      "dotenv",
      "termcolor",
      "tqdm"
    ],
    "preview": "from openai import OpenAI\n\nclient = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))\nimport logging\nimport os\nimport sys\nimport time\nfrom concurrent.futures import ThreadPoolExecutor\n\nfrom dotenv import load_dotenv\nfrom termcolor import colored\nfrom tqdm import tqdm\n\n# Load environment variables from .env (make sure your OPENAI_API_KEY is stored here)\nenv_path = os.path.expanduser(\"~/.env\")\n # Update this path if necessary\nload_dotenv(dotenv_path=env_path)\n\n# Initialize OpenAI API key\n",
    "last_modified": "2025-05-04T22:47:13.343475"
  },
  {
    "id": "188",
    "name": "info.py",
    "path": "01_core_ai_analysis/transcription/info.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 159,
    "size": 7063,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis",
      "transcription"
    ],
    "functions": [
      "split_media_to_segments",
      "transcribe_media_segment",
      "format_timestamp",
      "analyze_text_for_section",
      "process_media_by_section"
    ],
    "classes": [],
    "imports": [
      "os",
      "subprocess",
      "openai",
      "dotenv",
      "sys"
    ],
    "preview": "import os\nimport subprocess\n\nimport openai\nfrom dotenv import load_dotenv\n\n# Load environment variables from .env (make sure your OPENAI_API_KEY is stored here)\nload_dotenv(\"/Users/steven/.env\")\nopenai.api_key = os.getenv(\"OPENAI_API_KEY\")\n\n# Prompt for directory paths\nproject_root = input(\"Please enter the project root directory path: \").strip()\n\n# Directory paths based on user input\nMEDIA_DIR = os.path.join(project_root, \"Media\")  # Directory containing media files\nTRANSCRIPT_DIR = os.path.join(MEDIA_DIR, \"transcript\")  # Directory to save transcripts\nANALYSIS_DIR = os.path.join(MEDIA_DIR, \"analysis\")  # Directory to save the analysis files\n\n# Create output directories if they don't exist\nos.makedirs(TRANSCRIPT_DIR, exist_ok=True)",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "189",
    "name": "batch_image_seo_pipeline_20250530220526.py",
    "path": "01_core_ai_analysis/transcription/batch_image_seo_pipeline_20250530220526.py",
    "category": "01_core_ai_analysis",
    "type": "image_processing",
    "lines": 409,
    "size": 16734,
    "docstring": "",
    "keywords": [
      "openai"
    ],
    "functions": [
      "load_openai_client",
      "retry_on_api_error",
      "call_gpt4o",
      "get_image_tech_meta",
      "build_source_tag",
      "discover_images",
      "build_gpt_messages",
      "parse_args",
      "main"
    ],
    "classes": [],
    "imports": [
      "argparse",
      "csv",
      "json",
      "logging",
      "os",
      "sys",
      "time",
      "datetime",
      "pathlib",
      "typing"
    ],
    "preview": "import argparse\nimport csv\nimport json\nimport logging\nimport os\nimport sys\nimport time\nfrom datetime import datetime\nfrom pathlib import Path\nfrom typing import Any, Dict, List, Optional\n\nimport backoff  # pip install backoff\nfrom dotenv import load_dotenv\nfrom openai import OpenAI, OpenAIAPIError\nfrom PIL import Image, UnidentifiedImageError\nfrom tqdm import tqdm\n\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n# CONFIGURATION\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500",
    "last_modified": "2025-09-13T05:53:26.971810"
  },
  {
    "id": "190",
    "name": "main (1).py",
    "path": "01_core_ai_analysis/transcription/main (1).py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 21,
    "size": 626,
    "docstring": "",
    "keywords": [],
    "functions": [
      "main"
    ],
    "classes": [],
    "imports": [
      "csv",
      "generate_speech"
    ],
    "preview": "import csv\n\nfrom generate_speech import generate_speech\n\n\ndef main():\n    csv_path = \"/Users/steven/Music/quiz-talk/Gtrivia - Sheet1.csv\"\n\n    with open(csv_path, newline=\"\", encoding=\"utf-8\") as csvfile:\n        reader = csv.DictReader(csvfile)\n        for i, row in enumerate(reader):\n            # Assuming 'Question' is the column name\n            question_text = row[\"Question\"]\n            output_path = f\"/ Users / steven / Music / quiz - talk / speech / question_{\n                i + 1}.mp3\"\n            generate_speech(question_text, voice=\"shimmer\", output_path=output_path)\n\n\nif __name__ == \"__main__\":\n    main()",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "191",
    "name": "mp4-mp3-analyze_20250531070353.py",
    "path": "01_core_ai_analysis/transcription/mp4-mp3-analyze_20250531070353.py",
    "category": "01_core_ai_analysis",
    "type": "analysis",
    "lines": 174,
    "size": 6748,
    "docstring": "",
    "keywords": [
      "video_processing",
      "openai",
      "analysis",
      "transcription"
    ],
    "functions": [
      "split_video_to_segments",
      "transcribe_video_segment",
      "format_timestamp",
      "analyze_text_for_section",
      "process_video_by_section"
    ],
    "classes": [],
    "imports": [
      "os",
      "subprocess",
      "openai",
      "dotenv",
      "sys"
    ],
    "preview": "import os\nimport subprocess\n\nimport openai\nfrom dotenv import load_dotenv\n\n# Load environment variables from .env (make sure your OPENAI_API_KEY is stored here)\nload_dotenv()\nopenai.api_key = os.getenv(\"OPENAI_API_KEY\")\n\n# Directory paths\nVIDEO_DIR = (\n    \"/Users/steven/Movies/j2025\"  # Directory containing MP4 files\n)\nTRANSCRIPT_DIR = \"/Users/steven/Movies/j2025/transcript\"  # Directory to save transcripts\nANALYSIS_DIR = \"/Users/steven/Movies/j2025/analysis\"  # Directory to save the analysis files\n\n# Create output directories if they don't exist\nos.makedirs(TRANSCRIPT_DIR, exist_ok=True)\nos.makedirs(ANALYSIS_DIR, exist_ok=True)",
    "last_modified": "2025-05-31T07:03:53.971234"
  },
  {
    "id": "192",
    "name": "quiz-time.py",
    "path": "01_core_ai_analysis/transcription/quiz-time.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 121,
    "size": 4449,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis",
      "web_tools"
    ],
    "functions": [
      "generate_speech",
      "calculate_text_duration",
      "process_csv_and_generate_speech"
    ],
    "classes": [],
    "imports": [
      "csv",
      "os",
      "requests",
      "openai",
      "pydub"
    ],
    "preview": "import csv\nimport os\n\nimport requests\nfrom openai import OpenAI\nfrom pydub import AudioSegment\n\n# Initialize the OpenAI client\n\n\ndef generate_speech(input_text, output_path, api_key):\n    \"\"\"\n    Generates speech from text using an API (e.g., OpenAI's text-to-speech API).\n    \"\"\"\n    url = \"https://api.openai.com/v1/audio/speech\"\n    headers = {\n        \"Authorization\": f\"Bearer {api_key}\",\n        \"Content-Type\": \"application/json\",\n    }\n    data = {",
    "last_modified": "2025-09-13T05:53:51.009657"
  },
  {
    "id": "193",
    "name": "named.py",
    "path": "01_core_ai_analysis/transcription/named.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 198,
    "size": 6669,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis"
    ],
    "functions": [
      "sanitize_filename",
      "get_closest_aspect_ratio",
      "resize_image",
      "process_batch",
      "process_images_and_generate_csv",
      "write_csv",
      "main"
    ],
    "classes": [],
    "imports": [
      "csv",
      "os",
      "time",
      "datetime",
      "dotenv",
      "openai",
      "PIL"
    ],
    "preview": "import csv\nimport os\nimport time\nfrom datetime import datetime\n\nfrom dotenv import load_dotenv\nfrom openai import OpenAI\nfrom PIL import Image, UnidentifiedImageError\n\n# Load environment variables\nenv_path = \"/Users/steven/.env\"\nload_dotenv(dotenv_path=env_path)\n\n# Initialize OpenAI API key\napi_key = os.getenv(\"OPENAI_API_KEY\")\nif not api_key:\n    raise EnvironmentError(\"OpenAI API key not found. Please check your .env file.\")\n\n# Constants\nMAX_WIDTH, MAX_HEIGHT = 4500, 5400",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "194",
    "name": "mp4tomp3.py",
    "path": "01_core_ai_analysis/transcription/mp4tomp3.py",
    "category": "01_core_ai_analysis",
    "type": "video_processing",
    "lines": 59,
    "size": 1759,
    "docstring": "",
    "keywords": [
      "analysis",
      "transcription"
    ],
    "functions": [
      "convert_mp4_to_mp3",
      "transcribe_audio",
      "save_transcription",
      "process_directory",
      "main"
    ],
    "classes": [],
    "imports": [
      "os",
      "subprocess",
      "whisper"
    ],
    "preview": "import os\nimport subprocess\n\nimport whisper\n\n\ndef convert_mp4_to_mp3(mp4_file, mp3_file):\n    # Use ffmpeg to convert mp4 to mp3\n    subprocess.run([\"ffmpeg\", \"-i\", mp4_file, \"-q:a\", \"0\", \"-map\", \"a\", mp3_file])\n\n\ndef transcribe_audio(file_path):\n    # Load the Whisper model\n    model = whisper.load_model(\"base\")\n\n    # Transcribe the audio file\n    result = model.transcribe(file_path)\n\n    return result[\"segments\"]\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "195",
    "name": "media-trans-analyze.py",
    "path": "01_core_ai_analysis/transcription/media-trans-analyze.py",
    "category": "01_core_ai_analysis",
    "type": "analysis",
    "lines": 159,
    "size": 7063,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis",
      "transcription"
    ],
    "functions": [
      "split_media_to_segments",
      "transcribe_media_segment",
      "format_timestamp",
      "analyze_text_for_section",
      "process_media_by_section"
    ],
    "classes": [],
    "imports": [
      "os",
      "subprocess",
      "openai",
      "dotenv",
      "sys"
    ],
    "preview": "import os\nimport subprocess\n\nimport openai\nfrom dotenv import load_dotenv\n\n# Load environment variables from .env (make sure your OPENAI_API_KEY is stored here)\nload_dotenv(\"/Users/steven/.env\")\nopenai.api_key = os.getenv(\"OPENAI_API_KEY\")\n\n# Prompt for directory paths\nproject_root = input(\"Please enter the project root directory path: \").strip()\n\n# Directory paths based on user input\nMEDIA_DIR = os.path.join(project_root, \"Media\")  # Directory containing media files\nTRANSCRIPT_DIR = os.path.join(MEDIA_DIR, \"transcript\")  # Directory to save transcripts\nANALYSIS_DIR = os.path.join(MEDIA_DIR, \"analysis\")  # Directory to save the analysis files\n\n# Create output directories if they don't exist\nos.makedirs(TRANSCRIPT_DIR, exist_ok=True)",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "196",
    "name": "untitled.py",
    "path": "01_core_ai_analysis/transcription/untitled.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 142,
    "size": 6647,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis",
      "transcription"
    ],
    "functions": [
      "format_timestamp",
      "transcribe_file",
      "analyze_text_for_section",
      "process_media_directory"
    ],
    "classes": [],
    "imports": [
      "os",
      "subprocess",
      "openai",
      "dotenv"
    ],
    "preview": "import os\nimport subprocess\n\nfrom openai import OpenAI\n\nclient = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\nfrom dotenv import load_dotenv\n\n# Load environment variables from .env\nenv_path = os.path.expanduser(\"~/.env\")\n# Update the path if needed\nload_dotenv(dotenv_path=env_path)\n\n# Initialize OpenAI API key\n\n# Error checking for OpenAI API key\nif not openai.api_key:\n    raise EnvironmentError(\"OpenAI API key not found. Please check your .env file.\")\n\n",
    "last_modified": "2025-09-13T05:53:55.629302"
  },
  {
    "id": "197",
    "name": "migrate_remaining_fixed.py",
    "path": "01_core_ai_analysis/transcription/migrate_remaining_fixed.py",
    "category": "01_core_ai_analysis",
    "type": "organization",
    "lines": 301,
    "size": 14502,
    "docstring": "Second Migration Pass - Organize Remaining Python Files (Fixed Version)",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "create_additional_directories",
      "migrate_remaining_files",
      "migrate_remaining_directories",
      "archive_remaining_duplicates",
      "main"
    ],
    "classes": [],
    "imports": [
      "os",
      "shutil",
      "pathlib"
    ],
    "preview": "#!/usr/bin/env python3\n\"\"\"\nSecond Migration Pass - Organize Remaining Python Files (Fixed Version)\n\"\"\"\n\nimport os\nimport shutil\nfrom pathlib import Path\n\ndef create_additional_directories():\n    \"\"\"Create additional directories for remaining files.\"\"\"\n    base_path = Path(\"/Users/steven/Documents/python\")\n    \n    additional_dirs = [\n        \"01_core_tools/text_processors\",\n        \"05_audio_video/image_processors\", \n        \"05_audio_video/audio_processors/quiz_tts\",\n        \"06_utilities/converters\",\n        \"06_utilities/data_processors\",\n        \"07_experimental/web_tools\",",
    "last_modified": "2025-10-09T05:32:18.982367"
  },
  {
    "id": "198",
    "name": "batch_image_seo_pipeline.py",
    "path": "01_core_ai_analysis/transcription/batch_image_seo_pipeline.py",
    "category": "01_core_ai_analysis",
    "type": "image_processing",
    "lines": 421,
    "size": 16834,
    "docstring": "",
    "keywords": [
      "openai"
    ],
    "functions": [
      "load_openai_client",
      "retry_on_api_error",
      "call_gpt4o",
      "get_image_tech_meta",
      "build_source_tag",
      "discover_images",
      "build_gpt_messages",
      "parse_args",
      "main"
    ],
    "classes": [],
    "imports": [
      "argparse",
      "csv",
      "json",
      "logging",
      "os",
      "sys",
      "time",
      "datetime",
      "pathlib",
      "typing"
    ],
    "preview": "import argparse\nimport csv\nimport json\nimport logging\nimport os\nimport sys\nimport time\nfrom datetime import datetime\nfrom pathlib import Path\nfrom typing import Any, Dict, List, Optional\n\nimport backoff  # pip install backoff\nfrom dotenv import load_dotenv\nfrom openai import OpenAI, OpenAIAPIError\nfrom PIL import Image, UnidentifiedImageError\nfrom tqdm import tqdm\n\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n# CONFIGURATION\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "199",
    "name": "analyzer-prompt.py",
    "path": "01_core_ai_analysis/transcription/analyzer-prompt.py",
    "category": "01_core_ai_analysis",
    "type": "analysis",
    "lines": 156,
    "size": 7044,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis",
      "transcription"
    ],
    "functions": [
      "split_video_to_segments",
      "transcribe_video_segment",
      "format_timestamp",
      "analyze_text_for_section",
      "process_video_by_section"
    ],
    "classes": [],
    "imports": [
      "os",
      "subprocess",
      "openai",
      "dotenv",
      "sys"
    ],
    "preview": "import os\nimport subprocess\n\nimport openai\nfrom dotenv import load_dotenv\n\n# Load environment variables from .env (make sure your OPENAI_API_KEY is stored here)\nload_dotenv()\nopenai.api_key = os.getenv(\"OPENAI_API_KEY\")\n\n# Prompt user for the video directory path\nVIDEO_DIR = input(\"\ud83c\udfa5 Please enter the directory path containing your MP4 files: \")\n\n# Directory paths\nTRANSCRIPT_DIR = os.path.join(VIDEO_DIR, \"transcript\")  # Directory to save transcripts\nANALYSIS_DIR = os.path.join(VIDEO_DIR, \"analysis\")  # Directory to save the analysis files\n\n# Create output directories if they don't exist\nos.makedirs(TRANSCRIPT_DIR, exist_ok=True)\nos.makedirs(ANALYSIS_DIR, exist_ok=True)",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "200",
    "name": "batch_image_seo_pipeline_20250530222614.py",
    "path": "01_core_ai_analysis/transcription/batch_image_seo_pipeline_20250530222614.py",
    "category": "01_core_ai_analysis",
    "type": "image_processing",
    "lines": 122,
    "size": 4843,
    "docstring": "",
    "keywords": [
      "openai"
    ],
    "functions": [
      "call_gpt4o_analysis_and_prompts",
      "batch_flexible_analysis"
    ],
    "classes": [],
    "imports": [
      "csv",
      "json",
      "logging",
      "os",
      "pathlib",
      "dotenv",
      "openai",
      "tqdm"
    ],
    "preview": "import csv\nimport json\nimport logging\nimport os\nfrom pathlib import Path\n\nfrom dotenv import load_dotenv\nfrom openai import OpenAI\nfrom tqdm import tqdm\n\n# CONFIG\nload_dotenv(os.path.expanduser(\"~/.env\"))\nOPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\nif not OPENAI_API_KEY:\n    raise EnvironmentError(\"OPENAI_API_KEY not found in ~/.env\")\nclient = OpenAI(api_key=OPENAI_API_KEY)\n\nLOG_FILE = \"avatararts_flexible_analysis.log\"\nlogging.basicConfig(\n    level=logging.INFO,",
    "last_modified": "2025-09-13T05:53:26.215232"
  },
  {
    "id": "201",
    "name": "gpt4.py",
    "path": "01_core_ai_analysis/transcription/gpt4.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 85,
    "size": 2501,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis",
      "web_tools"
    ],
    "functions": [
      "analyze_image_with_gpt4_vision",
      "upscale_image",
      "text_to_speech"
    ],
    "classes": [],
    "imports": [
      "csv",
      "io",
      "openai",
      "requests",
      "PIL"
    ],
    "preview": "import csv\nfrom io import BytesIO\n\nimport openai\nimport requests\nfrom PIL import Image\n\n# Initialize the OpenAI client\nclient = openai(api_key=\"sk-acw35nWnLFMd23JUzcQoQ7QBhg4y5wLxoQJpR64ITBWHqBT7\")\n\n\ndef analyze_image_with_gpt4_vision(image_url):\n    # Analyze the image using GPT-4 with Vision\n    response = client.chat.completions.create(\n        model=\"gpt-4-vision-preview\",\n        messages=[\n            {\n                \"role\": \"user\",\n                \"content\": [\n                    {\"type\": \"text\", \"text\": \"What\u2019s in this image?\"},",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "202",
    "name": "sora.py",
    "path": "01_core_ai_analysis/transcription/sora.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 12,
    "size": 202,
    "docstring": "",
    "keywords": [
      "openai"
    ],
    "functions": [],
    "classes": [],
    "imports": [
      "os",
      "openai"
    ],
    "preview": "import os\n\nimport openai\n\nresponse = openai.Video.create(\n    model=\"sora\",\n    prompt=\"A futuristic cityscape with flying cars\",\n    duration=60,  # Duration in seconds\n)\n\nprint(response[\"video_url\"])\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "203",
    "name": "speak (1).py",
    "path": "01_core_ai_analysis/transcription/speak (1).py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 55,
    "size": 1741,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis"
    ],
    "functions": [
      "generate_speech",
      "main"
    ],
    "classes": [],
    "imports": [
      "csv",
      "openai"
    ],
    "preview": "import csv\n\nimport openai\n\n# Initialize the OpenAI client\nclient = openai()\n\n\ndef generate_speech(text, pause_duration=\"5s\", voice=\"shimmer\", output_path=\"speech.mp3\"):\n    # Adding a pause using the SSML <break> tag\n    # Assuming that 'text' contains something like \"Option 3: <your text>\"\n    # and you want to insert a pause right after this before continuing with\n    # the answer\n    modified_text = text.replace(\"Option 3:\", f'Option 3:\"/>')\n\n    response = OpenAI.Audio.create(\n        model=\"tts-1-hd\",  # Ensure this model supports the audio creation\n        input=modified_text,\n        voice=voice,\n        format=\"mp3\",",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "204",
    "name": "yt-dalle-prompt.py",
    "path": "01_core_ai_analysis/transcription/yt-dalle-prompt.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 77,
    "size": 2509,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis",
      "web_tools"
    ],
    "functions": [
      "upscale_image",
      "generate_youtube_content",
      "analyze_and_generate"
    ],
    "classes": [],
    "imports": [
      "csv",
      "io",
      "requests",
      "openai",
      "PIL"
    ],
    "preview": "import csv\nfrom io import BytesIO\n\nimport requests\nfrom openai import OpenAI\nfrom PIL import Image\n\nclient = OpenAI(api_key=\"sk-acw35nWnLFMd23JUzcQoQ7QBhg4y5wLxoQJpR64ITBWHqBT7\")\n\n# Set your OpenAI API key here\n\n\ndef upscale_image(image_url):\n    # Fetch the image\n    response = requests.get(image_url)\n    image = Image.open(BytesIO(response.content))\n\n    # Calculate the new size, doubling the width and height\n    new_size = (image.width * 2, image.height * 2)\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "205",
    "name": "QuizPrompts.py",
    "path": "01_core_ai_analysis/transcription/QuizPrompts.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 100,
    "size": 3394,
    "docstring": "",
    "keywords": [
      "analysis",
      "web_tools"
    ],
    "functions": [
      "generate_speech",
      "calculate_text_duration",
      "process_csv_and_generate_speech"
    ],
    "classes": [],
    "imports": [
      "csv",
      "os",
      "requests",
      "dotenv",
      "pydub"
    ],
    "preview": "import csv\nimport os\n\nimport requests  # Install using `pip install requests`\nfrom dotenv import load_dotenv\nfrom pydub import AudioSegment  # Install using `pip install pydub`\n\n\ndef generate_speech(input_text, output_path, api_key):\n    \"\"\"\n    Generates speech from text using OpenAI's text-to-speech API.\n    \"\"\"\n    url = \"https://api.openai.com/v1/audio/speech\"\n    headers = {\n        \"Authorization\": f\"Bearer {api_key}\",\n        \"Content-Type\": \"application/json\",\n    }\n    data = {\n        \"model\": \"tts-1\",\n        \"input\": input_text,",
    "last_modified": "2025-09-13T05:53:54.821397"
  },
  {
    "id": "206",
    "name": "batch_image_seo_pipeline_20250530221257.py",
    "path": "01_core_ai_analysis/transcription/batch_image_seo_pipeline_20250530221257.py",
    "category": "01_core_ai_analysis",
    "type": "analysis",
    "lines": 428,
    "size": 17474,
    "docstring": "batch_image_seo_pipeline.py\n\n1) Discovers all image files under a given folder.\n2) Extracts technical metadata via Pillow.\n3) Calls GPT-4o Vision (multimodal) using a \u201csystem\u201d + \u201cuser\u201d prompt.\n4) Parses the JSON response into structured fields:\n   main_subject, style, color_palette, tags, orientation, suggested_products,\n   SEO_title, SEO_description, emotion, safety_rating, dominant_keyword.\n5) Builds a \u201csource\u201d tag: folder-subfolder-YYYYMMDD.\n6) Applies a \u201cTop 5% SEO Analytics\u201d framework to generate SEO columns:\n   \u2013 SEO Keywords, Traffic Source, CRO Tactic, Backlink Source, Engagement Rate.\n7) Adds niche-specific design prompts (Geeky, Dark Humor, Anime).\n8) Writes everything into a final CSV for bulk upload.\n\nDependencies:\n    pip install openai python-dotenv pillow tqdm backoff",
    "keywords": [
      "openai"
    ],
    "functions": [
      "load_openai_client",
      "retry_on_api_error",
      "call_gpt4o",
      "get_image_tech_meta",
      "build_source_tag",
      "discover_images",
      "build_gpt_messages",
      "parse_args",
      "main"
    ],
    "classes": [],
    "imports": [
      "argparse",
      "csv",
      "json",
      "logging",
      "os",
      "sys",
      "datetime",
      "pathlib",
      "typing",
      "backoff"
    ],
    "preview": "#!/usr/bin/env python3\n\"\"\"\nbatch_image_seo_pipeline.py\n\n1) Discovers all image files under a given folder.\n2) Extracts technical metadata via Pillow.\n3) Calls GPT-4o Vision (multimodal) using a \u201csystem\u201d + \u201cuser\u201d prompt.\n4) Parses the JSON response into structured fields:\n   main_subject, style, color_palette, tags, orientation, suggested_products,\n   SEO_title, SEO_description, emotion, safety_rating, dominant_keyword.\n5) Builds a \u201csource\u201d tag: folder-subfolder-YYYYMMDD.\n6) Applies a \u201cTop 5% SEO Analytics\u201d framework to generate SEO columns:\n   \u2013 SEO Keywords, Traffic Source, CRO Tactic, Backlink Source, Engagement Rate.\n7) Adds niche-specific design prompts (Geeky, Dark Humor, Anime).\n8) Writes everything into a final CSV for bulk upload.\n\nDependencies:\n    pip install openai python-dotenv pillow tqdm backoff\n\"\"\"\n",
    "last_modified": "2025-09-13T05:53:25.864120"
  },
  {
    "id": "207",
    "name": "main 2 2.py",
    "path": "01_core_ai_analysis/transcription/main 2 2.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 21,
    "size": 588,
    "docstring": "",
    "keywords": [],
    "functions": [
      "main"
    ],
    "classes": [],
    "imports": [
      "csv",
      "generate_speech"
    ],
    "preview": "import csv\n\nfrom generate_speech import generate_speech\n\n\ndef main():\n    csv_path = \"quiz329.csv\"\n\n    with open(csv_path, newline=\"\", encoding=\"utf-8\") as csvfile:\n        reader = csv.DictReader(csvfile)\n        for i, row in enumerate(reader):\n            # Assuming 'Question' is the column name\n            question_text = row[\"Question\"]\n            output_path = f\"/Users/steven/Documents/quiz-talk/quiz329/question/question_{\n                i + 1}.mp3\"\n            generate_speech(question_text, voice=\"shimmer\", output_path=output_path)\n\n\nif __name__ == \"__main__\":\n    main()",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "208",
    "name": "analyze-prompt1.py",
    "path": "01_core_ai_analysis/transcription/analyze-prompt1.py",
    "category": "01_core_ai_analysis",
    "type": "analysis",
    "lines": 64,
    "size": 3523,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "analyze_text"
    ],
    "classes": [],
    "imports": [
      "os",
      "dotenv",
      "sys"
    ],
    "preview": "vfrom openai import OpenAI\n\nclient = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\nimport os\n\nfrom dotenv import load_dotenv\n\n# Load environment variables from .env\nenv_path = os.path.expanduser(\"~/.env\")\n # Update the path if needed\nload_dotenv(dotenv_path=env_path)\n\n# Initialize openai API key\n\n# Error checking for openai API key\nif not openai.api_key:\n    raise EnvironmentError(\"openai API key not found. Please check your .env file.\")\n\ndef analyze_text(text):\n    response = client.chat.completions.create(",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "209",
    "name": "analyze-prompt (1).py",
    "path": "01_core_ai_analysis/transcription/analyze-prompt (1).py",
    "category": "01_core_ai_analysis",
    "type": "analysis",
    "lines": 99,
    "size": 4036,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis",
      "transcription"
    ],
    "functions": [
      "transcribe_audio",
      "format_timestamp",
      "analyze_text_for_section",
      "process_audio_directory"
    ],
    "classes": [],
    "imports": [
      "openai",
      "os",
      "subprocess",
      "dotenv"
    ],
    "preview": "from openai import OpenAI\n\nclient = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\nimport os\nimport subprocess\n\nfrom dotenv import load_dotenv\n\n# Load environment variables from .env (make sure your OPENAI_API_KEY is stored here)\nenv_path = \"/Users/steven/.env\"\nload_dotenv(dotenv_path=env_path)\n\n# Directory paths\nAUDIO_DIR = (\n    \"/Users/steven/Music/NocTurnE-meLoDieS/Discography/Audio\"  # Directory containing MP3 files\n)\nTRANSCRIPT_DIR = \"/Users/steven/Music/NocTurnE-meLoDieS/Discography/Audio/transcript\"  # Directory to save transcripts\nANALYSIS_DIR = \"/Users/steven/Music/NocTurnE-meLoDieS/Discography/Audio/analysis\"  # Directory to save the analysis files\n\n# Create output directories if they don't exist",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "210",
    "name": "image-gpt.py",
    "path": "01_core_ai_analysis/content_analysis/image-gpt.py",
    "category": "01_core_ai_analysis",
    "type": "image_processing",
    "lines": 59,
    "size": 1831,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis"
    ],
    "functions": [
      "generate_description"
    ],
    "classes": [],
    "imports": [
      "csv",
      "openai"
    ],
    "preview": "import csv\n\nfrom openai import OpenAI\n\n# Initialize the OpenAI client\nclient = OpenAI()\n\n\ndef generate_description(image_url):\n    response = client.chat.completions.create(\n        model=\"gpt-4-vision-preview\",\n        messages=[\n            {\n                \"role\": \"user\",\n                \"content\": [\n                    {\n                        \"type\": \"text\",\n                        \"text\": \"Can you create a detailed and descriptive image prompt for the image as if you were to recreate it?\",\n                    },\n                    {",
    "last_modified": "2025-09-13T05:53:55.205434"
  },
  {
    "id": "211",
    "name": "testing2.py",
    "path": "01_core_ai_analysis/content_analysis/testing2.py",
    "category": "01_core_ai_analysis",
    "type": "testing",
    "lines": 119,
    "size": 5316,
    "docstring": "",
    "keywords": [
      "analysis",
      "openai",
      "youtube"
    ],
    "functions": [
      "upload_file",
      "wait_for_run_completion",
      "get_internal_links",
      "process_blog_post",
      "process_content_plan"
    ],
    "classes": [],
    "imports": [
      "csv",
      "os",
      "time",
      "openai",
      "tqdm"
    ],
    "preview": "import csv\nimport os\nimport time\n\nimport openai\nfrom tqdm import tqdm\n\n# Set your OpenAI API key\nOPENAI_API_TOKEN = \"put_your_secret_key_here\"\nos.environ[\"OPENAI_API_KEY\"] = OPENAI_API_TOKEN\n\n# Initialize the OpenAI client\nclient = openai.OpenAI()\n\n\n# Function to upload a file to OpenAI\ndef upload_file(file_path, purpose):\n    with open(file_path, \"rb\") as file:\n        response = client.files.create(file=file, purpose=purpose)\n    return response.id",
    "last_modified": "2025-09-13T05:54:17.514006"
  },
  {
    "id": "212",
    "name": "pydescribe.py",
    "path": "01_core_ai_analysis/content_analysis/pydescribe.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 96,
    "size": 3258,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis"
    ],
    "functions": [
      "extract_functions_and_classes",
      "save_to_text_files",
      "get_description_from_file",
      "analyze_scripts"
    ],
    "classes": [],
    "imports": [
      "openai",
      "os",
      "re"
    ],
    "preview": "from openai import OpenAI\n\nclient = OpenAI(api_key=api_key)\nimport os\nimport re\n\n# Load the API key from the environment variable\napi_key = os.getenv(\"OPENAI_API_KEY\")\nif not api_key:\n    raise ValueError(\"API key not found. Please ensure it is set in your environment variables.\")\n\n\ndef extract_functions_and_classes(content):\n    \"\"\"\n    Extracts top-level functions and classes from the script content.\n    \"\"\"\n    pattern = re.compile(r\"^\\s*(def|class)\\s+\\w+\\s*\\(.*?\\):\", re.MULTILINE)\n    matches = pattern.split(content)\n    headers = pattern.findall(content)\n    functions_and_classes = []",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "213",
    "name": "batch copy.py",
    "path": "01_core_ai_analysis/content_analysis/batch copy.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 85,
    "size": 2693,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis"
    ],
    "functions": [
      "get_openai_batch_titles",
      "suggest_script_titles_batch",
      "process_directory_with_batching"
    ],
    "classes": [],
    "imports": [
      "os",
      "openai"
    ],
    "preview": "import os\n\nfrom openai import OpenAI\n\nclient = OpenAI()\n\n\ndef get_openai_batch_titles(script_contents):\n    # Prepare messages for the batch request\n    messages = [\n        {\n            \"role\": \"system\",\n            \"content\": \"You are an expert Python programmer. Suggest appropriate titles for the following scripts.\",\n        },\n        {\n            \"role\": \"user\",\n            \"content\": \"\\n\\n\".join(\n                f\"Script {i+1}:\\n{content[:1000]}\" for i, content in enumerate(script_contents)\n            ),\n        },",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "214",
    "name": "song--analyze-keys.py",
    "path": "01_core_ai_analysis/content_analysis/song--analyze-keys.py",
    "category": "01_core_ai_analysis",
    "type": "analysis",
    "lines": 42,
    "size": 1100,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis"
    ],
    "functions": [
      "analyze_text"
    ],
    "classes": [],
    "imports": [
      "openai",
      "os",
      "dotenv",
      "sys"
    ],
    "preview": "from openai import OpenAI\n\nclient = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\nimport os\n\nfrom dotenv import load_dotenv\n\n# Load environment variables from .env\nload_dotenv()\n\n\ndef analyze_text(text):\n    response = client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",  # Ensure you're using a chat model like gpt-3.5-turbo\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": \"You are a helpful assistant that analyzes song lyrics.\",\n            },\n            {",
    "last_modified": "2025-05-04T22:47:13.351254"
  },
  {
    "id": "215",
    "name": "batch_image_seo_pipeline_20250530221254.py",
    "path": "01_core_ai_analysis/content_analysis/batch_image_seo_pipeline_20250530221254.py",
    "category": "01_core_ai_analysis",
    "type": "image_processing",
    "lines": 6,
    "size": 406,
    "docstring": "",
    "keywords": [
      "openai"
    ],
    "functions": [],
    "classes": [],
    "imports": [
      "openai.error"
    ],
    "preview": " python /Users/steven/Documents/python/batch/batch_image_seo_pipeline.py                                           \u2039git:main \u2718\u203a 22:11.04 Fri May 30 2025 >>> \nTraceback (most recent call last):\n  File \"/Users/steven/Documents/python/batch/batch_image_seo_pipeline.py\", line 35, in <module>\n    from openai.error import OpenAIError\nModuleNotFoundError: No module named 'openai.error'\n(media_tools) <<< ",
    "last_modified": "2025-05-30T22:12:54.349281"
  },
  {
    "id": "216",
    "name": "whispertext-combine.py",
    "path": "01_core_ai_analysis/content_analysis/whispertext-combine.py",
    "category": "01_core_ai_analysis",
    "type": "transcription",
    "lines": 72,
    "size": 1917,
    "docstring": "",
    "keywords": [
      "data_processing"
    ],
    "functions": [],
    "classes": [],
    "imports": [
      "json",
      "os",
      "glob",
      "pandas",
      "natsort"
    ],
    "preview": "import json\nimport os\nfrom glob import glob\n\nimport pandas as pd\nfrom natsort import natsorted\n\nROOT_DIR = \"/Users/steven/Documents/Whisper-Text/ALL\"\n\njson_files = glob(os.path.join(ROOT_DIR, \"**/*.json\"), recursive=True)\njson_files = natsorted(json_files)\n\nHEADERS = [\n    \"Title\",\n    \"Summary\",\n    \"Quotes\",\n    \"Chapters\",\n    \"Show Notes\",\n    \"Newsletter\",\n    \"Blog post\",",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "217",
    "name": "testing3.py",
    "path": "01_core_ai_analysis/content_analysis/testing3.py",
    "category": "01_core_ai_analysis",
    "type": "testing",
    "lines": 117,
    "size": 6790,
    "docstring": "",
    "keywords": [
      "analysis",
      "openai",
      "youtube"
    ],
    "functions": [
      "upload_file",
      "wait_for_run_completion",
      "get_internal_links",
      "process_blog_post",
      "process_content_plan"
    ],
    "classes": [],
    "imports": [
      "csv",
      "os",
      "time",
      "openai",
      "tqdm"
    ],
    "preview": "import csv\nimport os\nimport time\n\nimport openai\nfrom tqdm import tqdm\n\n# Set your OpenAI API key\nOPENAI_API_TOKEN = \"JUST_ADD_API_KEY_HERE\"\nos.environ[\"OPENAI_API_KEY\"] = OPENAI_API_TOKEN\n\n# Initialize the OpenAI client\nclient = openai.OpenAI()\n\n\n# Function to upload a file to OpenAI\ndef upload_file(file_path, purpose):\n    with open(file_path, \"rb\") as file:\n        response = client.files.create(file=file, purpose=purpose)\n    return response.id",
    "last_modified": "2025-09-13T05:54:17.587676"
  },
  {
    "id": "218",
    "name": "breakdown.py",
    "path": "01_core_ai_analysis/content_analysis/breakdown.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 51,
    "size": 1669,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis"
    ],
    "functions": [
      "get_script_description",
      "analyze_scripts"
    ],
    "classes": [],
    "imports": [
      "openai",
      "os"
    ],
    "preview": "from openai import OpenAI\n\nclient = OpenAI(api_key=api_key)\nimport os\n\n# Load the API key from the environment variable\napi_key = os.getenv(\"OPENAI_API_KEY\")\nif not api_key:\n    raise ValueError(\"API key not found. Please ensure it is set in your environment variables.\")\n\n\ndef get_script_description(file_path):\n    with open(file_path, \"r\") as file:\n        script_content = file.read()\n\n    # OpenAI API call to get the script description\n    response = client.chat.completions.create(\n        model=\"gpt-4\",\n        messages=[\n            {\"role\": \"system\", \"content\": \"You are an expert Python programmer.\"},",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "219",
    "name": "pyorganize.py",
    "path": "01_core_ai_analysis/content_analysis/pyorganize.py",
    "category": "01_core_ai_analysis",
    "type": "organization",
    "lines": 73,
    "size": 2358,
    "docstring": "",
    "keywords": [
      "openai"
    ],
    "functions": [
      "extract_functions_and_classes",
      "get_script_description",
      "categorize_scripts"
    ],
    "classes": [],
    "imports": [
      "openai",
      "os",
      "re"
    ],
    "preview": "from openai import OpenAI\n\nclient = OpenAI(api_key=api_key)\nimport os\nimport re\n\n# Load the API key from the environment variable\napi_key = os.getenv(\"OPENAI_API_KEY\")\nif not api_key:\n    raise ValueError(\"API key not found. Please ensure it is set in your environment variables.\")\n\n\ndef extract_functions_and_classes(content):\n    \"\"\"\n    Extracts top-level functions and classes from the script content.\n    \"\"\"\n    pattern = re.compile(r\"^\\s*(def|class)\\s+\\w+\\s*\\(.*?\\):\", re.MULTILINE)\n    matches = pattern.findall(content)\n    headers = pattern.findall(content)\n    functions_and_classes = []",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "220",
    "name": "fancy 1.py",
    "path": "01_core_ai_analysis/content_analysis/fancy 1.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 113,
    "size": 3902,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis"
    ],
    "functions": [
      "generate_creative_filename",
      "pair_and_rename_images",
      "main"
    ],
    "classes": [],
    "imports": [
      "csv",
      "os",
      "datetime",
      "dotenv",
      "openai"
    ],
    "preview": "import csv\nimport os\nfrom datetime import datetime\n\nfrom dotenv import load_dotenv\nfrom openai import OpenAI\n\n# Load environment variables from the specified .env file\nenv_path = \"/Users/steven/.env\"\nload_dotenv(dotenv_path=env_path)\n\n# Initialize OpenAI API key\napi_key = os.getenv(\"OPENAI_API_KEY\")\n\n# Error checking for OpenAI API key\nif not api_key:\n    raise EnvironmentError(\"OpenAI API key not found. Please check your .env file.\")\n\n# Initialize OpenAI client\nclient = OpenAI(api_key=api_key)",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "221",
    "name": "fancyname.py",
    "path": "01_core_ai_analysis/content_analysis/fancyname.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 101,
    "size": 3334,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis"
    ],
    "functions": [
      "generate_creative_filename",
      "process_images",
      "main"
    ],
    "classes": [],
    "imports": [
      "csv",
      "os",
      "datetime",
      "dotenv",
      "openai"
    ],
    "preview": "import csv\nimport os\nfrom datetime import datetime\n\nfrom dotenv import load_dotenv\nfrom openai import OpenAI\n\n# Load environment variables from the specified .env file\nenv_path = \"/Users/steven/.env\"\nload_dotenv(dotenv_path=env_path)\n\n# Initialize OpenAI API key\napi_key = os.getenv(\"OPENAI_API_KEY\")\n\n# Error checking for OpenAI API key\nif not api_key:\n    raise EnvironmentError(\"OpenAI API key not found. Please check your .env file.\")\n\n# Initialize OpenAI client\nclient = OpenAI(api_key=api_key)",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "222",
    "name": "emport_prompts.py",
    "path": "01_core_ai_analysis/content_analysis/emport_prompts.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 108,
    "size": 4158,
    "docstring": "",
    "keywords": [
      "openai"
    ],
    "functions": [
      "call_gpt4o_analysis_and_prompts",
      "batch_flexible_analysis"
    ],
    "classes": [],
    "imports": [
      "csv",
      "json",
      "logging",
      "os",
      "pathlib",
      "dotenv",
      "openai",
      "tqdm"
    ],
    "preview": "# image_flexible_analysis.py\nimport csv\nimport json\nimport logging\nimport os\nfrom pathlib import Path\n\nfrom dotenv import load_dotenv\nfrom openai import OpenAI\nfrom tqdm import tqdm\n\nload_dotenv(os.path.expanduser(\"~/.env\"))\nOPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\nif not OPENAI_API_KEY:\n    raise EnvironmentError(\"OPENAI_API_KEY not found in ~/.env\")\nclient = OpenAI(api_key=OPENAI_API_KEY)\n\nSYSTEM_PROMPT = (\n    \"You are an expert AI creative director, SEO specialist, and product designer for print-on-demand. \"\n    \"For each image, analyze and output a JSON object with: main_subject, style, color_palette, tags (list), \"",
    "last_modified": "2025-09-13T05:53:48.121869"
  },
  {
    "id": "223",
    "name": "batch-info.py",
    "path": "01_core_ai_analysis/content_analysis/batch-info.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 147,
    "size": 4910,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis"
    ],
    "functions": [
      "get_openai_batch_titles",
      "suggest_script_titles_batch",
      "process_directory_with_batching"
    ],
    "classes": [],
    "imports": [
      "os",
      "openai",
      "dotenv"
    ],
    "preview": "import os\n\nfrom openai import OpenAI\n\nclient = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\nfrom dotenv import load_dotenv\n\n# Load environment variables from the specified .env file\nload_dotenv(\"/Users/steven/Documents/python/.env\")\n\n# Set OpenAI API key from the environment variable\n\n\ndef get_openai_batch_titles(script_contents):\n    \"\"\"\n    Prepare a batch request to OpenAI for suggesting appropriate titles for scripts.\n\n    Args:\n        script_contents (list): List of script contents as strings.\n",
    "last_modified": "2025-09-13T05:53:55.716688"
  },
  {
    "id": "224",
    "name": "vision.py",
    "path": "01_core_ai_analysis/content_analysis/vision.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 60,
    "size": 1845,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis"
    ],
    "functions": [
      "generate_description"
    ],
    "classes": [],
    "imports": [
      "csv",
      "openai"
    ],
    "preview": "import csv\n\nfrom openai import OpenAI\n\n# Initialize the OpenAI client\nclient = OpenAI()\n\n\ndef generate_description(image_url):\n    response = client.chat.completions.create(\n        model=\"gpt-4-vision-preview\",\n        messages=[\n            {\n                \"role\": \"user\",\n                \"content\": [\n                    {\n                        \"type\": \"text\",\n                        \"text\": \"Can you create a detailed and descriptive image prompt for the image as if you were to recreate it?\",\n                    },\n                    {",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "225",
    "name": "rename.py",
    "path": "01_core_ai_analysis/content_analysis/rename.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 104,
    "size": 3887,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis"
    ],
    "functions": [
      "generate_filename_from_image",
      "rename_and_copy_files"
    ],
    "classes": [],
    "imports": [
      "csv",
      "os",
      "shutil",
      "openai",
      "dotenv"
    ],
    "preview": "import csv\nimport os\nimport shutil\n\nimport openai\nfrom dotenv import load_dotenv\n\n# Load API Key from .env\nload_dotenv()\nopenai.api_key = os.getenv(\"OPENAI_API_KEY\")\n\n\n# Function to generate a filename from image content\ndef generate_filename_from_image(filepath):\n    \"\"\"\n    Uses OpenAI to describe the image content and generate a filename.\n    \"\"\"\n    try:\n        # Describe the image content (simplified example using keywords)\n        response = openai.ChatCompletion.create(",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "226",
    "name": "better.py",
    "path": "01_core_ai_analysis/content_analysis/better.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 17,
    "size": 553,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [],
    "preview": "\nmodel=\"gpt-3.5-turbo\",\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": (\n                    \"You are a witty and imaginative assistant. Generate playful, creative, and descriptive filenames \"\n                    \"for digital products based on provided prompts.\"\n                ),\n            },\n            {\n                \"role\": \"user\",\n                \"content\": f\"Create a unique filename for this design prompt: '{prompt}'\",\n            },\n        ],\n        max_tokens=20,\n        temperature=0.7",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "227",
    "name": "finaltry.py",
    "path": "01_core_ai_analysis/content_analysis/finaltry.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 81,
    "size": 2761,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis",
      "web_tools"
    ],
    "functions": [
      "generate_image_prompt",
      "generate_youtube_content",
      "create_image_with_dalle",
      "process_csv_and_generate_content"
    ],
    "classes": [],
    "imports": [
      "csv",
      "requests",
      "openai"
    ],
    "preview": "import csv\n\nimport requests\nfrom openai import OpenAI\n\n# Initialize the OpenAI client\n\n# Function to generate an image prompt based on the input data\n\n\ndef generate_image_prompt(question, description, keywords):\n    prompt = f\"Create a detailed and descriptive image based on the following: {description}. Related to the question: '{question}'. Keywords: {keywords}\"\n    return prompt\n\n\n# Function to generate YouTube titles and descriptions with emojis\n\n\ndef generate_youtube_content(prompt):\n    response = openai.Completion.create(",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "228",
    "name": "openai_local.py",
    "path": "01_core_ai_analysis/content_analysis/openai_local.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 27,
    "size": 4524,
    "docstring": "",
    "keywords": [
      "openai"
    ],
    "functions": [],
    "classes": [],
    "imports": [
      "openai"
    ],
    "preview": "from openai import OpenAI\n\nclient = OpenAI()\n\nresponse = client.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=[\n        {\n            \"role\": \"system\",\n            \"content\": '\\n1. Image Generation Guidelines:\\n   - Utilize DALL-E 3 for image creation.\\n   - Ensure each prompt is creatively detailed.\\n   - Strictly adhere to the Prompt Guidelines.\\n\\n2. Image Batching Protocol:\\n   - Generate images in separate batches: one image per sentence, matching the total number of sentences.\\n\\n3. Conceptualization for New Ideas:\\n   - After image generation, suggest four new, simple, yet related concepts based on the original theme.\\n\\nDefault Settings (Unless Specified Otherwise):\\n1. Aspect Ratio: Adopt a 9:16 ratio (1080x1920px, Vertical).\\n2. Image Quantity: Always produce four separate images.\\n3. Background Setting: Isolate graphics on a solid color background.\\n4. Post-Creation Requirements:\\n   a) Title: Create titles (50-60 characters) suitable for SEO, enhanced for YouTube Shorts, reflecting content, theme, and story.\\n   b) Keywords: List a minimum of 20 relevant SEO keywords, emphasizing the strongest. Exclude artistic style, camera type, and setup.\\n   c) Description: Write a compelling description (max 256 characters, no longer than one paragraph), incorporating relevant keywords and narrative style.\\n\\nNote: To initiate the next set of image generations, respond with \"DO\" for creating images based on the four newly suggested ideas.\\n',\n        },\n        {\n            \"role\": \"user\",\n            \"content\": 'Solarflare (Real Name: Ethan Clarke)\\nNormal Life: Ethan was an astrophysicist, fascinated by the mysteries of the universe.\\nTransformation: Gains solar energy powers and becomes a hero, later succumbing to the lure of power and turning into a villain.\\nAqua-Mistress (Real Name: Mia Waters)\\nNormal Life: Mia was a dedicated marine biologist, advocating for ocean conservation.\\nTransformation: Discovers mystical powers over water, initially uses them for good, but later exploits them for personal gain.\\nGaia (Real Name: Ivy Green)\\nNormal Life: Ivy, an environmental activist, fought tirelessly to protect natural habitats.\\nTransformation: Gains earth-related powers, starts as a guardian of nature but becomes more extreme in her methods.\\nWind Warden (Real Name: Leo Storm)\\nNormal Life: Leo was a charismatic and adventurous pilot.\\nTransformation: After gaining air control powers, he starts as a hero but gets carried away by the thrill of his abilities.\\nQuantum (Real Name: Dr. Max Quantum)\\nNormal Life: A brilliant physicist researching quantum mechanics.\\nTransformation: His powers of teleportation and time manipulation lead him down a path from hero to rogue scientist.\\nSpectra (Real Name: Natasha \"Tasha\" Blake)\\nNormal Life: Tasha was a skilled intelligence agent known for her stealth.\\nTransformation: Gains invisibility and force fields, initially fights crime, but later uses her powers for espionage and personal agendas.\\nThunderstrike (Real Name: Chris Bolt)\\nNormal Life: A popular professional athlete known for his electrifying personality.\\nTransformation: Chris becomes a superhuman with electrical powers, but his desire for fame corrupts his heroism.\\n',\n        },\n        {\n            \"role\": \"assistant\",\n            \"content\": 'Image Generation: In Progress\\n\\n1. Solarflare (Ethan Clarke) as an astrophysicist staring into the heart of a solar storm.\\n   - Title: \"Astrophysicist Glimpses Solar Storm Secret\"\\n   - Keywords: solarflare, astrophysicist, solar storm, universe, Ethan Clarke, mysteries, powerful gaze, cosmic event, hero-to-villain, energy powers.\\n   - Description: Astrophysicist Ethan Clarke, soon-to-be Solarflare, peers into the dazzling abyss of a solar storm, moments before a fateful transformation.\\n\\n2. Aqua-Mistress (Mia Waters) at a coral reef conducting research.\\n   - Title: \"Marine Biologist\\'s Encounter at Vibrant Coral Reef\"\\n   - Keywords: Aqua-Mistress, marine biologist, coral reef, ocean conservation, Mia Waters, mystical powers, water hero, underwater research, marine life.\\n   - Description: Mia Waters, destined to become Aqua-Mistress, passionately studies the teeming life within a vibrant coral reef, echoing her future connection to the seas.\\n\\n3. Gaia (Ivy Green) planting trees in a deforested area.\\n   - Title: \"Activist Restores Forests Before Becoming',\n        },\n    ],",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "229",
    "name": "filenamer 1.py",
    "path": "01_core_ai_analysis/content_analysis/filenamer 1.py",
    "category": "01_core_ai_analysis",
    "type": "organization",
    "lines": 100,
    "size": 3150,
    "docstring": "",
    "keywords": [
      "data_processing",
      "openai",
      "analysis"
    ],
    "functions": [
      "analyze_image",
      "process_images"
    ],
    "classes": [],
    "imports": [
      "json",
      "logging",
      "os",
      "pathlib",
      "pandas",
      "dotenv",
      "openai",
      "tenacity"
    ],
    "preview": "import json\nimport logging\nimport os\nfrom pathlib import Path\n\nimport pandas as pd\nfrom dotenv import load_dotenv\nfrom openai import OpenAI\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\n# Configure logging\nlogging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n\n# Load environment variables\nenv_path = Path(\".\") / \".env\"\nload_dotenv(dotenv_path=env_path)\n\n# Initialize OpenAI client\ntry:\n    client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "230",
    "name": "riddle.py",
    "path": "01_core_ai_analysis/content_analysis/riddle.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 96,
    "size": 2754,
    "docstring": "",
    "keywords": [
      "openai"
    ],
    "functions": [
      "fetch_riddles",
      "play_dynamic_game_with_choices",
      "output_to_csv"
    ],
    "classes": [],
    "imports": [
      "openai",
      "csv",
      "os"
    ],
    "preview": "from openai import OpenAI\n\nclient = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\nimport csv\nimport os\n\n# Set up OpenAI API key\n\n\ndef fetch_riddles(number_of_riddles=3):\n    prompt_text = \"Generate a sphinx riddle with a question, an answer, a correct response message, and an incorrect response message.\"\n    messages = [\n        {\n            \"role\": \"system\",\n            \"content\": \"You are about to generate a series of sphinx riddles.\",\n        },\n        {\"role\": \"user\", \"content\": prompt_text * number_of_riddles},\n    ]\n\n    try:",
    "last_modified": "2025-05-04T22:47:11.911164"
  },
  {
    "id": "231",
    "name": "classify.py",
    "path": "01_core_ai_analysis/content_analysis/classify.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 169,
    "size": 5837,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis"
    ],
    "functions": [
      "generate_category_with_openai",
      "classify_script",
      "scan_directory",
      "save_classification_to_csv"
    ],
    "classes": [],
    "imports": [
      "ast",
      "os",
      "openai",
      "csv",
      "datetime",
      "dotenv"
    ],
    "preview": "import ast\nimport os\n\nfrom openai import OpenAI\n\nclient = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\nimport csv\nfrom datetime import datetime\n\nfrom dotenv import load_dotenv\n\n# Load environment variables from .env\nenv_path = \"/Users/steven/.env\"\nload_dotenv(dotenv_path=env_path)\n\n# Initialize OpenAI API key\n\n# Error checking for OpenAI API key\nif not openai.api_key:\n    raise EnvironmentError(\"OpenAI API key not found. Please check your .env file.\")",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "232",
    "name": "emport_prompts_20250530225121.py",
    "path": "01_core_ai_analysis/content_analysis/emport_prompts_20250530225121.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 108,
    "size": 4158,
    "docstring": "",
    "keywords": [
      "openai"
    ],
    "functions": [
      "call_gpt4o_analysis_and_prompts",
      "batch_flexible_analysis"
    ],
    "classes": [],
    "imports": [
      "csv",
      "json",
      "logging",
      "os",
      "pathlib",
      "dotenv",
      "openai",
      "tqdm"
    ],
    "preview": "# image_flexible_analysis.py\nimport csv\nimport json\nimport logging\nimport os\nfrom pathlib import Path\n\nfrom dotenv import load_dotenv\nfrom openai import OpenAI\nfrom tqdm import tqdm\n\nload_dotenv(os.path.expanduser(\"~/.env\"))\nOPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\nif not OPENAI_API_KEY:\n    raise EnvironmentError(\"OPENAI_API_KEY not found in ~/.env\")\nclient = OpenAI(api_key=OPENAI_API_KEY)\n\nSYSTEM_PROMPT = (\n    \"You are an expert AI creative director, SEO specialist, and product designer for print-on-demand. \"\n    \"For each image, analyze and output a JSON object with: main_subject, style, color_palette, tags (list), \"",
    "last_modified": "2025-09-13T05:53:27.123820"
  },
  {
    "id": "233",
    "name": "batch.py",
    "path": "01_core_ai_analysis/content_analysis/batch.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 146,
    "size": 4763,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis"
    ],
    "functions": [
      "get_openai_batch_titles",
      "suggest_script_titles_batch",
      "process_directory_with_batching"
    ],
    "classes": [],
    "imports": [
      "os",
      "openai",
      "dotenv"
    ],
    "preview": "import os\n\nfrom openai import OpenAI\n\nclient = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\nfrom dotenv import load_dotenv\n\n# Load environment variables from .env file\nload_dotenv()\n\n# Set OpenAI API key from the environment variable\n\n\ndef get_openai_batch_titles(script_contents):\n    \"\"\"\n    Prepare a batch request to OpenAI for suggesting appropriate titles for scripts.\n\n    Args:\n        script_contents (list): List of script contents as strings.\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "234",
    "name": "pydescriber.py",
    "path": "01_core_ai_analysis/content_analysis/pydescriber.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 96,
    "size": 3258,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis"
    ],
    "functions": [
      "extract_functions_and_classes",
      "save_to_text_files",
      "get_description_from_file",
      "analyze_scripts"
    ],
    "classes": [],
    "imports": [
      "openai",
      "os",
      "re"
    ],
    "preview": "from openai import OpenAI\n\nclient = OpenAI(api_key=api_key)\nimport os\nimport re\n\n# Load the API key from the environment variable\napi_key = os.getenv(\"OPENAI_API_KEY\")\nif not api_key:\n    raise ValueError(\"API key not found. Please ensure it is set in your environment variables.\")\n\n\ndef extract_functions_and_classes(content):\n    \"\"\"\n    Extracts top-level functions and classes from the script content.\n    \"\"\"\n    pattern = re.compile(r\"^\\s*(def|class)\\s+\\w+\\s*\\(.*?\\):\", re.MULTILINE)\n    matches = pattern.split(content)\n    headers = pattern.findall(content)\n    functions_and_classes = []",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "235",
    "name": "__init__.py",
    "path": "01_core_ai_analysis/content_analysis/__init__.py",
    "category": "01_core_ai_analysis",
    "type": "setup",
    "lines": 7,
    "size": 162,
    "docstring": "Shared utilities for Python projects",
    "keywords": [
      "openai"
    ],
    "functions": [],
    "classes": [],
    "imports": [
      "config",
      "openai_client",
      "file_utils"
    ],
    "preview": "\"\"\"\nShared utilities for Python projects\n\"\"\"\nfrom .config import *\nfrom .openai_client import get_openai_client\nfrom .file_utils import ensure_dir, get_file_size\n",
    "last_modified": "2025-10-09T05:27:15.569506"
  },
  {
    "id": "236",
    "name": "scanpythons.py",
    "path": "01_core_ai_analysis/content_analysis/scanpythons.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 66,
    "size": 2149,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis"
    ],
    "functions": [
      "extract_functions_and_classes",
      "get_script_description",
      "analyze_scripts"
    ],
    "classes": [],
    "imports": [
      "openai",
      "os",
      "re"
    ],
    "preview": "from openai import OpenAI\n\nclient = OpenAI(api_key=api_key)\nimport os\nimport re\n\n# Load the API key from the environment variable\napi_key = os.getenv(\"OPENAI_API_KEY\")\nif not api_key:\n    raise ValueError(\"API key not found. Please ensure it is set in your environment variables.\")\n\n\ndef extract_functions_and_classes(content):\n    \"\"\"\n    Extracts top-level functions and classes from the script content.\n    \"\"\"\n    pattern = re.compile(r\"^\\s*(def|class)\\s+\\w+\\s*\\(.*?\\):\", re.MULTILINE)\n    matches = pattern.findall(content)\n    return matches\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "237",
    "name": "enhance_img_csv.py",
    "path": "01_core_ai_analysis/content_analysis/enhance_img_csv.py",
    "category": "01_core_ai_analysis",
    "type": "image_processing",
    "lines": 297,
    "size": 10725,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis"
    ],
    "functions": [
      "analyze_image_gpt4o",
      "get_image_tech_meta",
      "build_source_tag_from_csv_row",
      "main"
    ],
    "classes": [],
    "imports": [
      "csv",
      "json",
      "os",
      "datetime",
      "pathlib",
      "typing",
      "openai",
      "dotenv",
      "PIL",
      "tqdm"
    ],
    "preview": "#!/usr/bin/env python3\nimport csv\nimport json\nimport os\nfrom datetime import datetime\nfrom pathlib import Path\nfrom typing import Any, Dict, List, Optional\n\nimport openai\nfrom dotenv import load_dotenv\nfrom PIL import Image, UnidentifiedImageError\nfrom tqdm import tqdm\n\n# \u2500\u2500\u2500 CONFIGURATION \u2500\u2500\u2500\n# 1) Path to your existing CSV file (prompted at runtime)\n# 2) Output CSV file name\n# 3) Base directory for building \u201csource\u201d tags (optional; default = parent folder of CSV)\nload_dotenv(Path.home() / \".env\")\nOPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "238",
    "name": "analyze_img_csv 1.py",
    "path": "01_core_ai_analysis/content_analysis/analyze_img_csv 1.py",
    "category": "01_core_ai_analysis",
    "type": "analysis",
    "lines": 246,
    "size": 8267,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis"
    ],
    "functions": [
      "analyze_image_gpt4o",
      "get_image_tech_meta",
      "discover_images",
      "build_source_tag",
      "main"
    ],
    "classes": [],
    "imports": [
      "csv",
      "json",
      "os",
      "datetime",
      "pathlib",
      "typing",
      "openai",
      "dotenv",
      "PIL",
      "tqdm"
    ],
    "preview": "import csv\nimport json\nimport os\nfrom datetime import datetime\nfrom pathlib import Path\nfrom typing import Any, Dict, List, Optional\n\nimport openai\nfrom dotenv import load_dotenv\nfrom PIL import Image, UnidentifiedImageError\nfrom tqdm import tqdm\n\n# --- CONFIGURATION ---\n# Load environment variables from ~/.env\nload_dotenv(Path.home() / \".env\")\nOPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n\nif not OPENAI_API_KEY:\n    raise ValueError(\"No OpenAI API key found! Please set it in ~/.env as OPENAI_API_KEY=...\")\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "239",
    "name": "self_query.py",
    "path": "01_core_ai_analysis/content_analysis/self_query.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 51,
    "size": 1554,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis"
    ],
    "functions": [
      "generate"
    ],
    "classes": [
      "SelfQuery"
    ],
    "imports": [
      "opik",
      "langchain_openai",
      "llm_engineering.application",
      "llm_engineering.domain.documents",
      "llm_engineering.domain.queries",
      "llm_engineering.settings",
      "loguru",
      "base",
      "prompt_templates"
    ],
    "preview": "import opik\nfrom langchain_openai import ChatOpenAI\nfrom llm_engineering.application import utils\nfrom llm_engineering.domain.documents import UserDocument\nfrom llm_engineering.domain.queries import Query\nfrom llm_engineering.settings import settings\nfrom loguru import logger\n\nfrom .base import RAGStep\nfrom .prompt_templates import SelfQueryTemplate\n\n\nclass SelfQuery(RAGStep):\n    @opik.track(name=\"SelfQuery.generate\")\n    def generate(self, query: Query) -> Query:\n        if self._mock:\n            return query\n\n        prompt = SelfQueryTemplate().create_template()\n        model = ChatOpenAI(",
    "last_modified": "2025-05-06T04:35:14.983667"
  },
  {
    "id": "240",
    "name": "autofill.py",
    "path": "01_core_ai_analysis/content_analysis/autofill.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 96,
    "size": 3448,
    "docstring": "",
    "keywords": [
      "openai"
    ],
    "functions": [
      "gpt_fill",
      "fill_missing_fields"
    ],
    "classes": [],
    "imports": [
      "csv",
      "json",
      "os",
      "pathlib",
      "dotenv",
      "openai",
      "tqdm"
    ],
    "preview": "import csv\nimport json\nimport os\nfrom pathlib import Path\n\nfrom dotenv import load_dotenv\nfrom openai import OpenAI\nfrom tqdm import tqdm\n\n# CONFIG\nload_dotenv(os.path.expanduser(\"~/.env\"))\nOPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\nclient = OpenAI(api_key=OPENAI_API_KEY)\nCSV_PATH = \"/Users/steven/Documents/python/clean/CSV/prompts_expanded_image_data-05-30-22-21.csv\"\nOUT_CSV = (\n    \"/Users/steven/Documents/python/clean/CSV/filled_prompts_expanded_image_data-05-30-22-21.csv\"\n)\nLOG_PATH = \"/Users/steven/Documents/python/clean/CSV/fill_log.txt\"\n\n# List analytic fields and prompt fields to fill",
    "last_modified": "2025-09-13T05:53:47.685388"
  },
  {
    "id": "241",
    "name": "autofill_20250530225752.py",
    "path": "01_core_ai_analysis/content_analysis/autofill_20250530225752.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 96,
    "size": 3448,
    "docstring": "",
    "keywords": [
      "openai"
    ],
    "functions": [
      "gpt_fill",
      "fill_missing_fields"
    ],
    "classes": [],
    "imports": [
      "csv",
      "json",
      "os",
      "pathlib",
      "dotenv",
      "openai",
      "tqdm"
    ],
    "preview": "import csv\nimport json\nimport os\nfrom pathlib import Path\n\nfrom dotenv import load_dotenv\nfrom openai import OpenAI\nfrom tqdm import tqdm\n\n# CONFIG\nload_dotenv(os.path.expanduser(\"~/.env\"))\nOPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\nclient = OpenAI(api_key=OPENAI_API_KEY)\nCSV_PATH = \"/Users/steven/Documents/python/clean/CSV/prompts_expanded_image_data-05-30-22-21.csv\"\nOUT_CSV = (\n    \"/Users/steven/Documents/python/clean/CSV/filled_prompts_expanded_image_data-05-30-22-21.csv\"\n)\nLOG_PATH = \"/Users/steven/Documents/python/clean/CSV/fill_log.txt\"\n\n# List analytic fields and prompt fields to fill",
    "last_modified": "2025-09-13T05:53:47.415963"
  },
  {
    "id": "242",
    "name": "filenamer.py",
    "path": "01_core_ai_analysis/content_analysis/filenamer.py",
    "category": "01_core_ai_analysis",
    "type": "organization",
    "lines": 100,
    "size": 3150,
    "docstring": "",
    "keywords": [
      "data_processing",
      "openai",
      "analysis"
    ],
    "functions": [
      "analyze_image",
      "process_images"
    ],
    "classes": [],
    "imports": [
      "json",
      "logging",
      "os",
      "pathlib",
      "pandas",
      "dotenv",
      "openai",
      "tenacity"
    ],
    "preview": "import json\nimport logging\nimport os\nfrom pathlib import Path\n\nimport pandas as pd\nfrom dotenv import load_dotenv\nfrom openai import OpenAI\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\n# Configure logging\nlogging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n\n# Load environment variables\nenv_path = Path(\".\") / \".env\"\nload_dotenv(dotenv_path=env_path)\n\n# Initialize OpenAI client\ntry:\n    client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "243",
    "name": "fancy.py",
    "path": "01_core_ai_analysis/content_analysis/fancy.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 113,
    "size": 3902,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis"
    ],
    "functions": [
      "generate_creative_filename",
      "pair_and_rename_images",
      "main"
    ],
    "classes": [],
    "imports": [
      "csv",
      "os",
      "datetime",
      "dotenv",
      "openai"
    ],
    "preview": "import csv\nimport os\nfrom datetime import datetime\n\nfrom dotenv import load_dotenv\nfrom openai import OpenAI\n\n# Load environment variables from the specified .env file\nenv_path = \"/Users/steven/.env\"\nload_dotenv(dotenv_path=env_path)\n\n# Initialize OpenAI API key\napi_key = os.getenv(\"OPENAI_API_KEY\")\n\n# Error checking for OpenAI API key\nif not api_key:\n    raise EnvironmentError(\"OpenAI API key not found. Please check your .env file.\")\n\n# Initialize OpenAI client\nclient = OpenAI(api_key=api_key)",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "244",
    "name": "fancyimg copy.py",
    "path": "01_core_ai_analysis/content_analysis/fancyimg copy.py",
    "category": "01_core_ai_analysis",
    "type": "image_processing",
    "lines": 84,
    "size": 2952,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis"
    ],
    "functions": [
      "generate_filename_with_gpt",
      "pair_and_rename_images"
    ],
    "classes": [],
    "imports": [
      "os",
      "openai",
      "csv",
      "dotenv"
    ],
    "preview": "import os\n\nfrom openai import OpenAI\n\nclient = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\nimport csv\n\nfrom dotenv import load_dotenv\n\n# Load API key from .env file\nload_dotenv(\"/Users/steven/.env\")\n\n\n# Function to generate a filename using GPT based on the prompt\ndef generate_filename_with_gpt(prompt):\n    response = client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        messages=[\n            {\n                \"role\": \"system\",",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "245",
    "name": "whisper.py",
    "path": "01_core_ai_analysis/content_analysis/whisper.py",
    "category": "01_core_ai_analysis",
    "type": "transcription",
    "lines": 59,
    "size": 1425,
    "docstring": "",
    "keywords": [
      "data_processing"
    ],
    "functions": [],
    "classes": [],
    "imports": [
      "json",
      "os",
      "glob",
      "pandas"
    ],
    "preview": "import json\nimport os\nfrom glob import glob\n\nimport pandas as pd\n\n# Define the directory path where JSON files are stored\nDIR_PATH = \"/Users/steven/Documents/Whisper-Text/content34\"\n\n# Get all JSON files in the directory\njson_files = sorted(\n    glob(os.path.join(DIR_PATH, \"content*.json\")),\n    key=lambda x: (int(\"\".join(filter(str.isdigit, x))) if any(c.isdigit() for c in x) else 0),\n)\n\ndata_list = []\n\n# Define the updated headers we want in the CSV\nHEADERS = [\n    \"Title\",",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "246",
    "name": "ident.py",
    "path": "01_core_ai_analysis/content_analysis/ident.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 172,
    "size": 5935,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis"
    ],
    "functions": [
      "get_openai_batch_titles",
      "suggest_script_titles_batch",
      "process_directory_with_batching"
    ],
    "classes": [],
    "imports": [
      "os",
      "openai",
      "csv",
      "dotenv"
    ],
    "preview": "import os\n\nfrom openai import OpenAI\n\nclient = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\nimport csv\n\nfrom dotenv import load_dotenv\n\n# Load environment variables from the specified .env file\nload_dotenv(\"/Users/steven/Documents/python/.env\")\n\n# Set OpenAI API key from the environment variable\n\n\ndef get_openai_batch_titles(script_contents):\n    \"\"\"\n    Prepare a batch request to OpenAI for suggesting appropriate titles for scripts.\n\n    Args:",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "247",
    "name": "groq.py",
    "path": "01_core_ai_analysis/content_analysis/groq.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 42,
    "size": 942,
    "docstring": "",
    "keywords": [
      "openai"
    ],
    "functions": [],
    "classes": [],
    "imports": [
      "os",
      "dotenv",
      "openai"
    ],
    "preview": "import os\n\nfrom dotenv import load_dotenv\nfrom openai import OpenAI\n\n# Load environment variables from .env file\nload_dotenv(dotenv_path=os.path.expanduser(\"~/.env\"))\n\nXAI_API_KEY = os.getenv(\"XAI_API_KEY\")\nimage_url = \"https://science.nasa.gov/wp-content/uploads/2023/09/web-first-images-release.png\"\n\nclient = OpenAI(\n    api_key=XAI_API_KEY,\n    base_url=\"https://api.x.ai/v1\",\n)\n\nmessages = [\n    {\n        \"role\": \"user\",\n        \"content\": [",
    "last_modified": "2025-09-13T05:53:54.172126"
  },
  {
    "id": "248",
    "name": "sagemaker.py",
    "path": "01_core_ai_analysis/content_analysis/sagemaker.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 64,
    "size": 2011,
    "docstring": "",
    "keywords": [],
    "functions": [
      "run_evaluation_on_sagemaker"
    ],
    "classes": [],
    "imports": [
      "pathlib",
      "huggingface_hub",
      "loguru",
      "llm_engineering",
      "sagemaker.huggingface"
    ],
    "preview": "from pathlib import Path\n\nfrom huggingface_hub import HfApi\nfrom loguru import logger\n\ntry:\n    from sagemaker.huggingface import HuggingFaceProcessor\nexcept ModuleNotFoundError:\n    logger.warning(\n        \"Couldn't load SageMaker imports. Run 'poetry install --with aws' to support AWS.\"\n    )\n\nfrom llm_engineering import settings\n\nevaluation_dir = Path(__file__).resolve().parent\nevaluation_requirements_path = evaluation_dir / \"requirements.txt\"\n\n\ndef run_evaluation_on_sagemaker(is_dummy: bool = True) -> None:\n    assert settings.HUGGINGFACE_ACCESS_TOKEN, \"Hugging Face access token is required.\"",
    "last_modified": "2025-09-13T05:53:42.270688"
  },
  {
    "id": "249",
    "name": "narrative_gen.py",
    "path": "01_core_ai_analysis/content_analysis/narrative_gen.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 182,
    "size": 6374,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis"
    ],
    "functions": [
      "generate_theme_title",
      "construct_image_prompt",
      "generate_batch",
      "generate_continuation_concepts",
      "save_output"
    ],
    "classes": [],
    "imports": [
      "json",
      "logging",
      "os",
      "random",
      "pathlib",
      "typing",
      "dotenv",
      "openai"
    ],
    "preview": "import json\nimport logging\nimport os\nimport random\nfrom pathlib import Path\nfrom typing import Dict, List\n\nfrom dotenv import load_dotenv\nfrom openai import APIError, OpenAI\n\n# Configure logging\nlogging.basicConfig(\n    level=logging.INFO,\n    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n    handlers=[logging.FileHandler(\"narrative_gen.log\"), logging.StreamHandler()],\n)\n\n# Load environment variables\nenv_path = Path(\".\") / \".env\"\nload_dotenv(dotenv_path=env_path)",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "250",
    "name": "my_openai_utils.py",
    "path": "01_core_ai_analysis/content_analysis/my_openai_utils.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 27,
    "size": 4524,
    "docstring": "",
    "keywords": [
      "openai"
    ],
    "functions": [],
    "classes": [],
    "imports": [
      "openai"
    ],
    "preview": "from openai import OpenAI\n\nclient = OpenAI()\n\nresponse = client.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=[\n        {\n            \"role\": \"system\",\n            \"content\": '\\n1. Image Generation Guidelines:\\n   - Utilize DALL-E 3 for image creation.\\n   - Ensure each prompt is creatively detailed.\\n   - Strictly adhere to the Prompt Guidelines.\\n\\n2. Image Batching Protocol:\\n   - Generate images in separate batches: one image per sentence, matching the total number of sentences.\\n\\n3. Conceptualization for New Ideas:\\n   - After image generation, suggest four new, simple, yet related concepts based on the original theme.\\n\\nDefault Settings (Unless Specified Otherwise):\\n1. Aspect Ratio: Adopt a 9:16 ratio (1080x1920px, Vertical).\\n2. Image Quantity: Always produce four separate images.\\n3. Background Setting: Isolate graphics on a solid color background.\\n4. Post-Creation Requirements:\\n   a) Title: Create titles (50-60 characters) suitable for SEO, enhanced for YouTube Shorts, reflecting content, theme, and story.\\n   b) Keywords: List a minimum of 20 relevant SEO keywords, emphasizing the strongest. Exclude artistic style, camera type, and setup.\\n   c) Description: Write a compelling description (max 256 characters, no longer than one paragraph), incorporating relevant keywords and narrative style.\\n\\nNote: To initiate the next set of image generations, respond with \"DO\" for creating images based on the four newly suggested ideas.\\n',\n        },\n        {\n            \"role\": \"user\",\n            \"content\": 'Solarflare (Real Name: Ethan Clarke)\\nNormal Life: Ethan was an astrophysicist, fascinated by the mysteries of the universe.\\nTransformation: Gains solar energy powers and becomes a hero, later succumbing to the lure of power and turning into a villain.\\nAqua-Mistress (Real Name: Mia Waters)\\nNormal Life: Mia was a dedicated marine biologist, advocating for ocean conservation.\\nTransformation: Discovers mystical powers over water, initially uses them for good, but later exploits them for personal gain.\\nGaia (Real Name: Ivy Green)\\nNormal Life: Ivy, an environmental activist, fought tirelessly to protect natural habitats.\\nTransformation: Gains earth-related powers, starts as a guardian of nature but becomes more extreme in her methods.\\nWind Warden (Real Name: Leo Storm)\\nNormal Life: Leo was a charismatic and adventurous pilot.\\nTransformation: After gaining air control powers, he starts as a hero but gets carried away by the thrill of his abilities.\\nQuantum (Real Name: Dr. Max Quantum)\\nNormal Life: A brilliant physicist researching quantum mechanics.\\nTransformation: His powers of teleportation and time manipulation lead him down a path from hero to rogue scientist.\\nSpectra (Real Name: Natasha \"Tasha\" Blake)\\nNormal Life: Tasha was a skilled intelligence agent known for her stealth.\\nTransformation: Gains invisibility and force fields, initially fights crime, but later uses her powers for espionage and personal agendas.\\nThunderstrike (Real Name: Chris Bolt)\\nNormal Life: A popular professional athlete known for his electrifying personality.\\nTransformation: Chris becomes a superhuman with electrical powers, but his desire for fame corrupts his heroism.\\n',\n        },\n        {\n            \"role\": \"assistant\",\n            \"content\": 'Image Generation: In Progress\\n\\n1. Solarflare (Ethan Clarke) as an astrophysicist staring into the heart of a solar storm.\\n   - Title: \"Astrophysicist Glimpses Solar Storm Secret\"\\n   - Keywords: solarflare, astrophysicist, solar storm, universe, Ethan Clarke, mysteries, powerful gaze, cosmic event, hero-to-villain, energy powers.\\n   - Description: Astrophysicist Ethan Clarke, soon-to-be Solarflare, peers into the dazzling abyss of a solar storm, moments before a fateful transformation.\\n\\n2. Aqua-Mistress (Mia Waters) at a coral reef conducting research.\\n   - Title: \"Marine Biologist\\'s Encounter at Vibrant Coral Reef\"\\n   - Keywords: Aqua-Mistress, marine biologist, coral reef, ocean conservation, Mia Waters, mystical powers, water hero, underwater research, marine life.\\n   - Description: Mia Waters, destined to become Aqua-Mistress, passionately studies the teeming life within a vibrant coral reef, echoing her future connection to the seas.\\n\\n3. Gaia (Ivy Green) planting trees in a deforested area.\\n   - Title: \"Activist Restores Forests Before Becoming',\n        },\n    ],",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "251",
    "name": "fancyname 1.py",
    "path": "01_core_ai_analysis/content_analysis/fancyname 1.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 101,
    "size": 3334,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis"
    ],
    "functions": [
      "generate_creative_filename",
      "process_images",
      "main"
    ],
    "classes": [],
    "imports": [
      "csv",
      "os",
      "datetime",
      "dotenv",
      "openai"
    ],
    "preview": "import csv\nimport os\nfrom datetime import datetime\n\nfrom dotenv import load_dotenv\nfrom openai import OpenAI\n\n# Load environment variables from the specified .env file\nenv_path = \"/Users/steven/.env\"\nload_dotenv(dotenv_path=env_path)\n\n# Initialize OpenAI API key\napi_key = os.getenv(\"OPENAI_API_KEY\")\n\n# Error checking for OpenAI API key\nif not api_key:\n    raise EnvironmentError(\"OpenAI API key not found. Please check your .env file.\")\n\n# Initialize OpenAI client\nclient = OpenAI(api_key=api_key)",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "252",
    "name": "whisper2.py",
    "path": "01_core_ai_analysis/content_analysis/whisper2.py",
    "category": "01_core_ai_analysis",
    "type": "transcription",
    "lines": 64,
    "size": 1635,
    "docstring": "",
    "keywords": [
      "data_processing"
    ],
    "functions": [],
    "classes": [],
    "imports": [
      "json",
      "os",
      "glob",
      "pandas"
    ],
    "preview": "import json\nimport os\nfrom glob import glob\n\nimport pandas as pd\n\n# Define the root directory path where JSON files are stored\nROOT_DIR = \"/Users/steven/Library/Application Support/WhisperTranscribe/library\"\n\n# Recursively find all JSON files in subdirectories\njson_files = sorted(\n    glob(os.path.join(ROOT_DIR, \"**/*.json\"), recursive=True),\n    key=lambda x: (int(\"\".join(filter(str.isdigit, x))) if any(c.isdigit() for c in x) else 0),\n)\n\ndata_list = []\n\n# Define the updated headers we want in the CSV\nHEADERS = [\n    \"Title\",",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "253",
    "name": "pyorganizerr.py",
    "path": "01_core_ai_analysis/content_analysis/pyorganizerr.py",
    "category": "01_core_ai_analysis",
    "type": "organization",
    "lines": 73,
    "size": 2358,
    "docstring": "",
    "keywords": [
      "openai"
    ],
    "functions": [
      "extract_functions_and_classes",
      "get_script_description",
      "categorize_scripts"
    ],
    "classes": [],
    "imports": [
      "openai",
      "os",
      "re"
    ],
    "preview": "from openai import OpenAI\n\nclient = OpenAI(api_key=api_key)\nimport os\nimport re\n\n# Load the API key from the environment variable\napi_key = os.getenv(\"OPENAI_API_KEY\")\nif not api_key:\n    raise ValueError(\"API key not found. Please ensure it is set in your environment variables.\")\n\n\ndef extract_functions_and_classes(content):\n    \"\"\"\n    Extracts top-level functions and classes from the script content.\n    \"\"\"\n    pattern = re.compile(r\"^\\s*(def|class)\\s+\\w+\\s*\\(.*?\\):\", re.MULTILINE)\n    matches = pattern.findall(content)\n    headers = pattern.findall(content)\n    functions_and_classes = []",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "254",
    "name": "textgenerator.py",
    "path": "01_core_ai_analysis/content_analysis/textgenerator.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 36,
    "size": 982,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis",
      "organization"
    ],
    "functions": [
      "clean_response",
      "generate_text_list"
    ],
    "classes": [],
    "imports": [
      "os",
      "dotenv",
      "openai",
      "config"
    ],
    "preview": "import os\n\nfrom dotenv import load_dotenv\nfrom openai import OpenAI\n\nfrom config import API_PARAM, PROMPT_TEMPLATE\n\nclient = OpenAI(api_key=os.getenv(\"OPENAI_TOKEN\"))\n\nload_dotenv()  # Load environment variables from .env.\n\n# Define the API key\n\n\ndef clean_response(text: str) -> list[str]:\n    text = text.strip().strip(\"[]\").split(\"\\n\")\n    return [t.strip().strip(\",\").strip('\"') for t in text]\n\n\ndef generate_text_list(date: str) -> str:",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "255",
    "name": "combiner.py",
    "path": "01_core_ai_analysis/content_analysis/combiner.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 72,
    "size": 1914,
    "docstring": "",
    "keywords": [
      "data_processing"
    ],
    "functions": [],
    "classes": [],
    "imports": [
      "json",
      "os",
      "glob",
      "pandas",
      "natsort"
    ],
    "preview": "import json\nimport os\nfrom glob import glob\n\nimport pandas as pd\nfrom natsort import natsorted\n\nROOT_DIR = \"/Users/steven/Documents/Whisper-Text/\"\n\njson_files = glob(os.path.join(ROOT_DIR, \"**/*.json\"), recursive=True)\njson_files = natsorted(json_files)\n\nHEADERS = [\n    \"Title\",\n    \"Summary\",\n    \"Quotes\",\n    \"Chapters\",\n    \"Show Notes\",\n    \"Newsletter\",\n    \"Blog post\",",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "256",
    "name": "overlay_theme_engines (3).py",
    "path": "01_core_ai_analysis/content_analysis/overlay_theme_engines (3).py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 67,
    "size": 2394,
    "docstring": "",
    "keywords": [],
    "functions": [
      "get_overlay_filter",
      "_graffiti_filter",
      "_sticker_filter",
      "_doodle_filter",
      "_ascii_filter",
      "_scanlines_filter"
    ],
    "classes": [
      "OverlayEngine",
      "ThemeEngine"
    ],
    "imports": [
      "__future__",
      "random",
      "typing"
    ],
    "preview": "\"\"\"\nOverlay and Theme engines for enhanced visual effects\n\"\"\"\nfrom __future__ import annotations\n\nimport random\nfrom typing import Any, Dict, List\n\n\nclass OverlayEngine:\n    \"\"\"Generates overlay effects for clips\"\"\"\n    \n    def get_overlay_filter(self, overlay_type: str) -> str:\n        \"\"\"Generate FFmpeg filter for overlay effects\"\"\"\n        overlay_type = overlay_type.lower()\n        \n        if overlay_type == \"graffiti\":\n            # Random graffiti-style elements\n            return self._graffiti_filter()\n        elif overlay_type == \"stickers\":",
    "last_modified": "2025-09-11T13:26:59.346287"
  },
  {
    "id": "257",
    "name": "verify_connections2.py",
    "path": "01_core_ai_analysis/content_analysis/verify_connections2.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 52,
    "size": 1645,
    "docstring": "",
    "keywords": [],
    "functions": [
      "verify_ssh_connection",
      "list_repo_contents"
    ],
    "classes": [],
    "imports": [
      "os",
      "paramiko",
      "dotenv"
    ],
    "preview": "import os\n\nimport paramiko\nfrom dotenv import load_dotenv\n\n# Load environment variables from .env (ensure your OPENAI_API_KEY is stored here)\nenv_path = \"/Users/steven/.env\"\nload_dotenv(dotenv_path=env_path)\n\nhostname = os.getenv(\"HOSTNAME\")\nusername = os.getenv(\"USERNAME\")\npassword = os.getenv(\"PASSWORD\")\nremote_dir = \"/repo/zip\"  # Adjust this path if needed\n\n\ndef verify_ssh_connection():\n    \"\"\"Run a simple command (like 'who') to verify an SSH connection.\"\"\"\n    client = paramiko.SSHClient()\n    client.set_missing_host_key_policy(paramiko.AutoAddPolicy())\n    try:",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "258",
    "name": "whisper-combiner.py",
    "path": "01_core_ai_analysis/content_analysis/whisper-combiner.py",
    "category": "01_core_ai_analysis",
    "type": "transcription",
    "lines": 72,
    "size": 1944,
    "docstring": "",
    "keywords": [
      "data_processing"
    ],
    "functions": [],
    "classes": [],
    "imports": [
      "json",
      "os",
      "glob",
      "pandas",
      "natsort"
    ],
    "preview": "import json\nimport os\nfrom glob import glob\n\nimport pandas as pd\nfrom natsort import natsorted\n\nROOT_DIR = \"/Users/steven/Library/Application Support/WhisperTranscribe/library\"\n\njson_files = glob(os.path.join(ROOT_DIR, \"**/*.json\"), recursive=True)\njson_files = natsorted(json_files)\n\nHEADERS = [\n    \"Title\",\n    \"Summary\",\n    \"Quotes\",\n    \"Chapters\",\n    \"Show Notes\",\n    \"Newsletter\",\n    \"Blog post\",",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "259",
    "name": "fancyimg 1.py",
    "path": "01_core_ai_analysis/content_analysis/fancyimg 1.py",
    "category": "01_core_ai_analysis",
    "type": "image_processing",
    "lines": 84,
    "size": 3036,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis"
    ],
    "functions": [
      "generate_filename_with_gpt",
      "pair_and_rename_images"
    ],
    "classes": [],
    "imports": [
      "os",
      "openai",
      "csv",
      "dotenv"
    ],
    "preview": "import os\n\nfrom openai import OpenAI\n\nclient = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\nimport csv\n\nfrom dotenv import load_dotenv\n\n# Load API key from .env file\nload_dotenv(\"/Users/steven/.env\")\n\n\n# Function to generate a filename using GPT based on the prompt\ndef generate_filename_with_gpt(prompt):\n    response = client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        messages=[\n            {\n                \"role\": \"system\",",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "260",
    "name": "fancyimg.py",
    "path": "01_core_ai_analysis/content_analysis/fancyimg.py",
    "category": "01_core_ai_analysis",
    "type": "image_processing",
    "lines": 84,
    "size": 2936,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis"
    ],
    "functions": [
      "generate_filename_with_gpt",
      "pair_and_rename_images"
    ],
    "classes": [],
    "imports": [
      "os",
      "openai",
      "csv",
      "dotenv"
    ],
    "preview": "import os\n\nfrom openai import OpenAI\n\nclient = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\nimport csv\n\nfrom dotenv import load_dotenv\n\n# Load API key from .env file\nload_dotenv(\"/Users/steven/.env\")\n\n\n# Function to generate a filename using GPT based on the prompt\ndef generate_filename_with_gpt(prompt):\n    response = client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        messages=[\n            {\n                \"role\": \"system\",",
    "last_modified": "2025-05-04T22:47:13.345543"
  },
  {
    "id": "261",
    "name": "openai_client.py",
    "path": "01_core_ai_analysis/content_analysis/openai_client.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 12,
    "size": 237,
    "docstring": "Centralized OpenAI client for all projects",
    "keywords": [
      "openai"
    ],
    "functions": [
      "get_openai_client"
    ],
    "classes": [],
    "imports": [
      "openai",
      "config"
    ],
    "preview": "\"\"\"\nCentralized OpenAI client for all projects\n\"\"\"\nfrom openai import OpenAI\nfrom .config import OPENAI_API_KEY\n\nclient = OpenAI(api_key=OPENAI_API_KEY)\n\ndef get_openai_client():\n    \"\"\"Get configured OpenAI client.\"\"\"\n    return client\n",
    "last_modified": "2025-10-09T05:27:15.568968"
  },
  {
    "id": "262",
    "name": "fancyimg copy 1.py",
    "path": "01_core_ai_analysis/content_analysis/fancyimg copy 1.py",
    "category": "01_core_ai_analysis",
    "type": "image_processing",
    "lines": 84,
    "size": 2952,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis"
    ],
    "functions": [
      "generate_filename_with_gpt",
      "pair_and_rename_images"
    ],
    "classes": [],
    "imports": [
      "os",
      "openai",
      "csv",
      "dotenv"
    ],
    "preview": "import os\n\nfrom openai import OpenAI\n\nclient = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\nimport csv\n\nfrom dotenv import load_dotenv\n\n# Load API key from .env file\nload_dotenv(\"/Users/steven/.env\")\n\n\n# Function to generate a filename using GPT based on the prompt\ndef generate_filename_with_gpt(prompt):\n    response = client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        messages=[\n            {\n                \"role\": \"system\",",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "263",
    "name": "batch_img_seo_pipeline.py",
    "path": "01_core_ai_analysis/content_analysis/batch_img_seo_pipeline.py",
    "category": "01_core_ai_analysis",
    "type": "image_processing",
    "lines": 258,
    "size": 8436,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis"
    ],
    "functions": [
      "analyze_image_gpt4o",
      "get_image_tech_meta",
      "discover_images",
      "build_source_tag",
      "parse_args",
      "process_image",
      "main"
    ],
    "classes": [],
    "imports": [
      "argparse",
      "csv",
      "json",
      "logging",
      "os",
      "concurrent.futures",
      "datetime",
      "pathlib",
      "typing",
      "backoff"
    ],
    "preview": "import argparse\nimport csv\nimport json\nimport logging\nimport os\nfrom concurrent.futures import ThreadPoolExecutor, as_completed\nfrom datetime import datetime\nfrom pathlib import Path\nfrom typing import Any, Dict, List, Optional\n\nimport backoff\nimport openai\nfrom dotenv import load_dotenv\nfrom PIL import Image, UnidentifiedImageError\nfrom tqdm import tqdm\n\n# --- CONFIGURATION ---\nload_dotenv(Path.home() / \".env\")\nOPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "264",
    "name": "query_expanison.py",
    "path": "01_core_ai_analysis/content_analysis/query_expanison.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 50,
    "size": 1619,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis"
    ],
    "functions": [
      "generate"
    ],
    "classes": [
      "QueryExpansion"
    ],
    "imports": [
      "opik",
      "langchain_openai",
      "llm_engineering.domain.queries",
      "llm_engineering.settings",
      "loguru",
      "base",
      "prompt_templates"
    ],
    "preview": "import opik\nfrom langchain_openai import ChatOpenAI\nfrom llm_engineering.domain.queries import Query\nfrom llm_engineering.settings import settings\nfrom loguru import logger\n\nfrom .base import RAGStep\nfrom .prompt_templates import QueryExpansionTemplate\n\n\nclass QueryExpansion(RAGStep):\n    @opik.track(name=\"QueryExpansion.generate\")\n    def generate(self, query: Query, expand_to_n: int) -> list[Query]:\n        assert expand_to_n > 0, f\"'expand_to_n' should be greater than 0. Got {expand_to_n}.\"\n\n        if self._mock:\n            return [query for _ in range(expand_to_n)]\n\n        query_expansion_template = QueryExpansionTemplate()\n        prompt = query_expansion_template.create_template(expand_to_n - 1)",
    "last_modified": "2025-09-13T05:53:41.743127"
  },
  {
    "id": "265",
    "name": "batch.py.bak.py",
    "path": "01_core_ai_analysis/content_analysis/batch.py.bak.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 146,
    "size": 4763,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis"
    ],
    "functions": [
      "get_openai_batch_titles",
      "suggest_script_titles_batch",
      "process_directory_with_batching"
    ],
    "classes": [],
    "imports": [
      "os",
      "openai",
      "dotenv"
    ],
    "preview": "import os\n\nfrom openai import OpenAI\n\nclient = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\nfrom dotenv import load_dotenv\n\n# Load environment variables from .env file\nload_dotenv()\n\n# Set OpenAI API key from the environment variable\n\n\ndef get_openai_batch_titles(script_contents):\n    \"\"\"\n    Prepare a batch request to OpenAI for suggesting appropriate titles for scripts.\n\n    Args:\n        script_contents (list): List of script contents as strings.\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "266",
    "name": "content-new.py",
    "path": "01_core_ai_analysis/content_analysis/content-new.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 83,
    "size": 2377,
    "docstring": "",
    "keywords": [
      "data_processing"
    ],
    "functions": [],
    "classes": [],
    "imports": [
      "json",
      "os",
      "datetime",
      "glob",
      "pandas"
    ],
    "preview": "import json\nimport os\nfrom datetime import datetime\nfrom glob import glob\n\nimport pandas as pd\n\n# Define the root directory path where JSON files are stored\nROOT_DIR = \"/Users/steven/Library/Application Support/WhisperTranscribe/library\"\n\n# Recursively find all JSON files in subdirectories\njson_files = sorted(\n    glob(os.path.join(ROOT_DIR, \"**/*.json\"), recursive=True),\n    key=lambda x: (int(\"\".join(filter(str.isdigit, x))) if any(c.isdigit() for c in x) else 0),\n)\n\ndata_list = []\n\n# Define the headers we want in the CSV\nHEADERS = [",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "267",
    "name": "analyze_img_csv.py",
    "path": "01_core_ai_analysis/content_analysis/analyze_img_csv.py",
    "category": "01_core_ai_analysis",
    "type": "analysis",
    "lines": 246,
    "size": 8267,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis"
    ],
    "functions": [
      "analyze_image_gpt4o",
      "get_image_tech_meta",
      "discover_images",
      "build_source_tag",
      "main"
    ],
    "classes": [],
    "imports": [
      "csv",
      "json",
      "os",
      "datetime",
      "pathlib",
      "typing",
      "openai",
      "dotenv",
      "PIL",
      "tqdm"
    ],
    "preview": "import csv\nimport json\nimport os\nfrom datetime import datetime\nfrom pathlib import Path\nfrom typing import Any, Dict, List, Optional\n\nimport openai\nfrom dotenv import load_dotenv\nfrom PIL import Image, UnidentifiedImageError\nfrom tqdm import tqdm\n\n# --- CONFIGURATION ---\n# Load environment variables from ~/.env\nload_dotenv(Path.home() / \".env\")\nOPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n\nif not OPENAI_API_KEY:\n    raise ValueError(\"No OpenAI API key found! Please set it in ~/.env as OPENAI_API_KEY=...\")\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "268",
    "name": "openai-Utilize DALL-E 3 for image creation.py",
    "path": "01_core_ai_analysis/content_analysis/openai-Utilize DALL-E 3 for image creation.py",
    "category": "01_core_ai_analysis",
    "type": "image_processing",
    "lines": 27,
    "size": 4524,
    "docstring": "",
    "keywords": [
      "openai"
    ],
    "functions": [],
    "classes": [],
    "imports": [
      "openai"
    ],
    "preview": "from openai import OpenAI\n\nclient = OpenAI()\n\nresponse = client.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=[\n        {\n            \"role\": \"system\",\n            \"content\": '\\n1. Image Generation Guidelines:\\n   - Utilize DALL-E 3 for image creation.\\n   - Ensure each prompt is creatively detailed.\\n   - Strictly adhere to the Prompt Guidelines.\\n\\n2. Image Batching Protocol:\\n   - Generate images in separate batches: one image per sentence, matching the total number of sentences.\\n\\n3. Conceptualization for New Ideas:\\n   - After image generation, suggest four new, simple, yet related concepts based on the original theme.\\n\\nDefault Settings (Unless Specified Otherwise):\\n1. Aspect Ratio: Adopt a 9:16 ratio (1080x1920px, Vertical).\\n2. Image Quantity: Always produce four separate images.\\n3. Background Setting: Isolate graphics on a solid color background.\\n4. Post-Creation Requirements:\\n   a) Title: Create titles (50-60 characters) suitable for SEO, enhanced for YouTube Shorts, reflecting content, theme, and story.\\n   b) Keywords: List a minimum of 20 relevant SEO keywords, emphasizing the strongest. Exclude artistic style, camera type, and setup.\\n   c) Description: Write a compelling description (max 256 characters, no longer than one paragraph), incorporating relevant keywords and narrative style.\\n\\nNote: To initiate the next set of image generations, respond with \"DO\" for creating images based on the four newly suggested ideas.\\n',\n        },\n        {\n            \"role\": \"user\",\n            \"content\": 'Solarflare (Real Name: Ethan Clarke)\\nNormal Life: Ethan was an astrophysicist, fascinated by the mysteries of the universe.\\nTransformation: Gains solar energy powers and becomes a hero, later succumbing to the lure of power and turning into a villain.\\nAqua-Mistress (Real Name: Mia Waters)\\nNormal Life: Mia was a dedicated marine biologist, advocating for ocean conservation.\\nTransformation: Discovers mystical powers over water, initially uses them for good, but later exploits them for personal gain.\\nGaia (Real Name: Ivy Green)\\nNormal Life: Ivy, an environmental activist, fought tirelessly to protect natural habitats.\\nTransformation: Gains earth-related powers, starts as a guardian of nature but becomes more extreme in her methods.\\nWind Warden (Real Name: Leo Storm)\\nNormal Life: Leo was a charismatic and adventurous pilot.\\nTransformation: After gaining air control powers, he starts as a hero but gets carried away by the thrill of his abilities.\\nQuantum (Real Name: Dr. Max Quantum)\\nNormal Life: A brilliant physicist researching quantum mechanics.\\nTransformation: His powers of teleportation and time manipulation lead him down a path from hero to rogue scientist.\\nSpectra (Real Name: Natasha \"Tasha\" Blake)\\nNormal Life: Tasha was a skilled intelligence agent known for her stealth.\\nTransformation: Gains invisibility and force fields, initially fights crime, but later uses her powers for espionage and personal agendas.\\nThunderstrike (Real Name: Chris Bolt)\\nNormal Life: A popular professional athlete known for his electrifying personality.\\nTransformation: Chris becomes a superhuman with electrical powers, but his desire for fame corrupts his heroism.\\n',\n        },\n        {\n            \"role\": \"assistant\",\n            \"content\": 'Image Generation: In Progress\\n\\n1. Solarflare (Ethan Clarke) as an astrophysicist staring into the heart of a solar storm.\\n   - Title: \"Astrophysicist Glimpses Solar Storm Secret\"\\n   - Keywords: solarflare, astrophysicist, solar storm, universe, Ethan Clarke, mysteries, powerful gaze, cosmic event, hero-to-villain, energy powers.\\n   - Description: Astrophysicist Ethan Clarke, soon-to-be Solarflare, peers into the dazzling abyss of a solar storm, moments before a fateful transformation.\\n\\n2. Aqua-Mistress (Mia Waters) at a coral reef conducting research.\\n   - Title: \"Marine Biologist\\'s Encounter at Vibrant Coral Reef\"\\n   - Keywords: Aqua-Mistress, marine biologist, coral reef, ocean conservation, Mia Waters, mystical powers, water hero, underwater research, marine life.\\n   - Description: Mia Waters, destined to become Aqua-Mistress, passionately studies the teeming life within a vibrant coral reef, echoing her future connection to the seas.\\n\\n3. Gaia (Ivy Green) planting trees in a deforested area.\\n   - Title: \"Activist Restores Forests Before Becoming',\n        },\n    ],",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "269",
    "name": "category.py",
    "path": "01_core_ai_analysis/content_analysis/category.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 137,
    "size": 4604,
    "docstring": "",
    "keywords": [
      "openai"
    ],
    "functions": [
      "determine_category",
      "split_content",
      "get_openai_category",
      "sanitize_category",
      "categorize_files"
    ],
    "classes": [],
    "imports": [
      "openai",
      "os",
      "re",
      "shutil"
    ],
    "preview": "from openai import OpenAI\n\nclient = OpenAI(api_key=api_key)\nimport os\nimport re\nimport shutil\n\n# Load the API key from the environment variable\napi_key = os.getenv(\"OPENAI_API_KEY\")\nif not api_key:\n    raise ValueError(\"API key not found. Please ensure it is set in your environment variables.\")\n\n\n# Define initial categories and their corresponding keywords\nCATEGORIES = {\n    \"data_processing\": [\"pandas\", \"numpy\", \"dataframe\"],\n    \"web_scraping\": [\"requests\", \"BeautifulSoup\", \"scrapy\"],\n    \"machine_learning\": [\"sklearn\", \"tensorflow\", \"keras\"],\n    \"utilities\": [\"os\", \"sys\", \"argparse\"],\n    \"networking\": [\"socket\", \"http.server\", \"flask\"],",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "270",
    "name": "rephrase.py",
    "path": "01_core_ai_analysis/content_analysis/rephrase.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 49,
    "size": 1437,
    "docstring": "",
    "keywords": [
      "openai"
    ],
    "functions": [
      "__init__",
      "rephrase_with_gpt",
      "rephrase_sentence"
    ],
    "classes": [
      "Rephrase"
    ],
    "imports": [
      "openai",
      "logging",
      "utilities.const"
    ],
    "preview": "from openai import OpenAI\n\nclient = OpenAI(api_key=YOUR_OPENAI_API_KEY)\nimport logging\n\nfrom utilities.const import GPT_MODEL, YOUR_OPENAI_API_KEY\n\n# Configure OpenAI API\n\n# Configure logger\nlogger = logging.getLogger(__name__)\nlogger.setLevel(logging.INFO)\nformatter = logging.Formatter(\"%(asctime)s - %(levelname)s - %(message)s\")\nfile_handler = logging.FileHandler(\"rephraser.log\")\nfile_handler.setLevel(logging.INFO)\nfile_handler.setFormatter(formatter)\nlogger.addHandler(file_handler)\n\n\nclass Rephrase:",
    "last_modified": "2025-09-13T05:53:28.762167"
  },
  {
    "id": "271",
    "name": "scanpython.py",
    "path": "01_core_ai_analysis/content_analysis/scanpython.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 52,
    "size": 1681,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis"
    ],
    "functions": [
      "get_script_description",
      "analyze_scripts"
    ],
    "classes": [],
    "imports": [
      "openai",
      "os"
    ],
    "preview": "from openai import OpenAI\n\nclient = OpenAI(api_key=api_key)\nimport os\n\n# Load the API key from the environment variable\napi_key = os.getenv(\"OPENAI_API_KEY\")\nif not api_key:\n    raise ValueError(\"API key not found. Please ensure it is set in your environment variables.\")\n\n\ndef get_script_description(file_path):\n    with open(file_path, \"r\") as file:\n        script_content = file.read()\n\n    # OpenAI API call to get the script description\n    response = client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        messages=[\n            {\"role\": \"system\", \"content\": \"You are an expert Python programmer.\"},",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "272",
    "name": "verify_connections.py",
    "path": "01_core_ai_analysis/content_analysis/verify_connections.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 52,
    "size": 1645,
    "docstring": "",
    "keywords": [],
    "functions": [
      "verify_ssh_connection",
      "list_repo_contents"
    ],
    "classes": [],
    "imports": [
      "os",
      "paramiko",
      "dotenv"
    ],
    "preview": "import os\n\nimport paramiko\nfrom dotenv import load_dotenv\n\n# Load environment variables from .env (ensure your OPENAI_API_KEY is stored here)\nenv_path = \"/Users/steven/.env\"\nload_dotenv(dotenv_path=env_path)\n\nhostname = os.getenv(\"HOSTNAME\")\nusername = os.getenv(\"USERNAME\")\npassword = os.getenv(\"PASSWORD\")\nremote_dir = \"/repo/zip\"  # Adjust this path if needed\n\n\ndef verify_ssh_connection():\n    \"\"\"Run a simple command (like 'who') to verify an SSH connection.\"\"\"\n    client = paramiko.SSHClient()\n    client.set_missing_host_key_policy(paramiko.AutoAddPolicy())\n    try:",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "273",
    "name": "generate_songs_csv.py_02.py",
    "path": "01_core_ai_analysis/generate_songs_csv.py_consolidated/generate_songs_csv.py_02.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 64,
    "size": 2088,
    "docstring": "",
    "keywords": [],
    "functions": [
      "get_matching_files"
    ],
    "classes": [],
    "imports": [
      "csv",
      "os"
    ],
    "preview": "import csv\nimport os\n\n# Define the directories\nmp3_dir = \"/Users/steven/Music/suno/mp3\"\ntxt_dir = \"/Users/steven/Music/suno/txt\"\ncsv_output = \"/Users/steven/Music/suno/music_project/songs_data.csv\"\n\n# Collect the list of MP3 and text files\nmp3_files = [f for f in os.listdir(mp3_dir) if f.endswith(\".mp3\")]\ntxt_files = [f for f in os.listdir(txt_dir) if f.endswith(\".txt\")]\n\n\n# Function to match song with corresponding text files\ndef get_matching_files(song_title, txt_files):\n    analysis_file = None\n    transcript_file = None\n    base_title = song_title.replace(\".mp3\", \"\")\n    for txt in txt_files:\n        if base_title in txt:",
    "last_modified": "2025-09-13T05:53:55.953591"
  },
  {
    "id": "274",
    "name": "generate_songs_csv.py.py",
    "path": "01_core_ai_analysis/generate_songs_csv.py_consolidated/generate_songs_csv.py.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 45,
    "size": 1665,
    "docstring": "",
    "keywords": [
      "data_processing"
    ],
    "functions": [],
    "classes": [],
    "imports": [
      "pandas",
      "bs4",
      "ace_tools"
    ],
    "preview": "import pandas as pd\nfrom bs4 import BeautifulSoup\n\n# Prompt the user for the directory\ndirectory = input(\"Enter the directory containing the files: \").strip()\n\n# Load the newly uploaded HTML file for processing\nfile_path_new_html = input(\"Enter the HTML Url: \").strip()\nwith open(file_path_new_html, \"r\", encoding=\"utf-8\") as file:\n    html_content_new_html = file.read()\n\n# Parsing the HTML content with BeautifulSoup\nsoup = BeautifulSoup(html_content_new_html, \"html.parser\")\n\n# Extracting relevant details\nsong_details_new_html = []\nfor item in soup.find_all(\"div\", class_=\"css-79jxux\"):  # Adjust the class if necessary\n    time_element = item.find_previous(\"span\", class_=\"font-mono\")\n    title_element = item.find(\"span\", class_=\"text-primary\")\n    song_url_element = item.find(\"a\", href=True)",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "275",
    "name": "autofixer.py",
    "path": "01_core_ai_analysis/data_processing/autofixer.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 109,
    "size": 3630,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "create_backup",
      "apply_autopep8",
      "run_pylint",
      "process_directory"
    ],
    "classes": [],
    "imports": [
      "csv",
      "os",
      "shutil",
      "subprocess",
      "datetime"
    ],
    "preview": "import csv\nimport os\nimport shutil\nimport subprocess\nfrom datetime import datetime\n\n\ndef create_backup(file_path, backup_dir):\n    \"\"\"\n    Create a backup of the given file in the specified backup directory.\n\n    :param file_path: Path to the original file.\n    :param backup_dir: Path to the backup directory.\n    :return: None\n    \"\"\"\n    os.makedirs(backup_dir, exist_ok=True)\n    file_name = os.path.basename(file_path)\n    backup_path = os.path.join(backup_dir, file_name)\n    shutil.copy(file_path, backup_path)\n    print(f\"Backup created for {file_path} at {backup_path}\")",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "276",
    "name": "try.py",
    "path": "01_core_ai_analysis/data_processing/try.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 79,
    "size": 3493,
    "docstring": "",
    "keywords": [
      "data_processing",
      "web_tools"
    ],
    "functions": [],
    "classes": [],
    "imports": [
      "base64",
      "pandas",
      "requests"
    ],
    "preview": "import base64\n\nimport pandas as pd\nimport requests\n\n# Set your API credentials\naccess_token = \"eyJ0eXAiOiJKV1QiLCJhbGciOiJSUzI1NiJ9.eyJhdWQiOiIzN2Q0YmQzMDM1ZmUxMWU5YTgwM2FiN2VlYjNjY2M5NyIsImp0aSI6IjQyMmQ1MWFlODI0YjEyMGY2MmY5N2YwZWExOWQ1YzRjMjhlZGM5OTRjYWQ0MmJmNzViOTZlNTdkZWQ3Mzk3YWUyZmQxZDAxMjJhZWRkMTU2IiwiaWF0IjoxNzAyMzYwNjg5LjA0MDcyMywibmJmIjoxNzAyMzYwNjg5LjA0MDcyNiwiZXhwIjoxNzMzOTgzMDg5LjAzNDUzNiwic3ViIjoiMTEzMjQxNTMiLCJzY29wZXMiOlsic2hvcHMubWFuYWdlIiwic2hvcHMucmVhZCIsImNhdGFsb2cucmVhZCIsIm9yZGVycy5yZWFkIiwib3JkZXJzLndyaXRlIiwicHJvZHVjdHMucmVhZCIsInByb2R1Y3RzLndyaXRlIiwid2ViaG9va3MucmVhZCIsIndlYmhvb2tzLndyaXRlIiwidXBsb2Fkcy5yZWFkIiwidXBsb2Fkcy53cml0ZSIsInByaW50X3Byb3ZpZGVycy5yZWFkIl19.AkWVerYdnoGceJShqCIhFpw6o0m7Nz0mqE6moOvuAdy9F4YS-G4rVePuxPp6u9C-y9VxF2pGDYF3yg6kQEo\"  # Replace with your actual access token\nshop_id = \"6511744\"  # Replace with your actual shop ID\n\n# Set the URL for the API endpoints\nbase_url = \"https://api.printify.com/v1\"\nupload_url = f\"{base_url}/uploads/images.json\"\nproduct_url = f\"{base_url}/shops/{shop_id}/products.json\"\n\n# Load the CSV file\ncsv_path = \"product_information.csv\"  # Update this to your CSV file path\nimage_df = pd.read_csv(csv_path)\n\n# Set headers for requests\nheaders = {",
    "last_modified": "2025-09-13T05:53:45.998793"
  },
  {
    "id": "277",
    "name": "quiz-tts.py",
    "path": "01_core_ai_analysis/data_processing/quiz-tts.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 38,
    "size": 1165,
    "docstring": "",
    "keywords": [
      "data_processing",
      "analysis"
    ],
    "functions": [
      "generate_trivia_quiz"
    ],
    "classes": [],
    "imports": [
      "pandas"
    ],
    "preview": "import pandas as pd\n\n\n# Function to read the CSV file and generate trivia quiz questions\ndef generate_trivia_quiz(csv_path):\n    # Read the CSV file into a pandas DataFrame\n    df = pd.read_csv(csv_path)\n\n    # Iterate over each row in the DataFrame\n    for index, row in df.iterrows():\n        # Extract question and options from the row\n        question = row[\"Question\"]\n        options = [row[\"Option 1\"], row[\"Option 2\"], row[\"Option 3\"], row]\n\n        # Print the question\n        print(f\"Question {index + 1}: {question}\")\n\n        # Print the options\n        for i, option in enumerate(options):\n            print(f\"{i + 1}. {option}\")",
    "last_modified": "2025-05-04T22:47:13.049111"
  },
  {
    "id": "278",
    "name": "csv-output.py",
    "path": "01_core_ai_analysis/data_processing/csv-output.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 26,
    "size": 772,
    "docstring": "",
    "keywords": [
      "data_processing"
    ],
    "functions": [],
    "classes": [],
    "imports": [
      "pandas"
    ],
    "preview": "import pandas as pd\n\n# Load the CSV file\nfile_path = \"/Users/steven/Pictures/DALLe/pic.csv\"  # Replace with your file path\ndf = pd.read_csv(file_path)\n\n# Extract URLs (assuming they start with \"http\")\nurls = df[df.iloc[:, 0].str.startswith(\"http\")]\n\n# Extract info (assuming they don't start with \"http\")\ninfo = df[~df.iloc[:, 0].str.startswith(\"http\")]\n\n# Resetting indices\nurls.reset_index(drop=True, inplace=True)\ninfo.reset_index(drop=True, inplace=True)\n\n# Combine into a new dataframe\nresult_df = pd.DataFrame({\"URL\": urls.iloc[:, 0], \"Info\": info.iloc[:, 0]})\n\n# Save to CSV",
    "last_modified": "2025-05-04T22:47:11.552797"
  },
  {
    "id": "279",
    "name": "vance.py",
    "path": "01_core_ai_analysis/data_processing/vance.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 106,
    "size": 4205,
    "docstring": "",
    "keywords": [
      "data_processing",
      "analysis",
      "web_tools"
    ],
    "functions": [
      "upscale_image",
      "process_image",
      "process_directory"
    ],
    "classes": [],
    "imports": [
      "logging",
      "os",
      "time",
      "concurrent.futures",
      "pandas",
      "requests"
    ],
    "preview": "import logging\nimport os\nimport time\nfrom concurrent.futures import ThreadPoolExecutor, as_completed\n\nimport pandas as pd\nimport requests\n\n# Set up logging\nlogging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n\n\ndef upscale_image(api_token, image_path, scale):\n    try:\n        url = \"https://api-service.vanceai.com/web_api/v1/enlarge3\"\n        headers = {\"Authorization\": f\"Bearer {api_token}\"}\n\n        with open(image_path, \"rb\") as image_file:\n            files = {\"image\": image_file}\n            params = {",
    "last_modified": "2025-09-13T05:55:29.391468"
  },
  {
    "id": "280",
    "name": "outs.py",
    "path": "01_core_ai_analysis/data_processing/outs.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 28,
    "size": 891,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "csv"
    ],
    "preview": "import csv\n\n# File paths\ninput_file = \"/Users/steven/avatararts/dall-e.txt\"\noutput_file = \"output_image_data.csv\"\n\n# List to store extracted data\ndata = []\n\n# Read the file and process the data\nwith open(input_file, \"r\", encoding=\"utf-8\") as file:\n    content = file.read().strip()\n    entries = content.split(\"\\n\\n\")  # each entry is separated by two newlines\n\n    for entry in entries:\n        lines = entry.split(\"\\n\", 1)\n        if len(lines) == 2:\n            url, description = lines\n            data.append([url.strip(), description.strip()])\n",
    "last_modified": "2025-05-04T22:47:13"
  },
  {
    "id": "281",
    "name": "bot_support.py",
    "path": "01_core_ai_analysis/data_processing/bot_support.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 105,
    "size": 4805,
    "docstring": "Support instabot's methods.",
    "keywords": [],
    "functions": [
      "check_if_file_exists",
      "read_list_from_file",
      "console_print",
      "extract_urls"
    ],
    "classes": [],
    "imports": [
      "__future__",
      "codecs",
      "os",
      "re",
      "sys",
      "huepy"
    ],
    "preview": "\"\"\"\nSupport instabot's methods.\n\"\"\"\n\nfrom __future__ import unicode_literals\n\nimport codecs\nimport os\nimport re\nimport sys\n\nimport huepy\n\n\ndef check_if_file_exists(file_path, quiet=False):\n    if not os.path.exists(file_path):\n        if not quiet:\n            print(\"Can't find '%s' file.\" % file_path)\n        return False\n    return True",
    "last_modified": "2025-03-28T18:35:47"
  },
  {
    "id": "282",
    "name": "convert 3.py",
    "path": "01_core_ai_analysis/data_processing/convert 3.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 78,
    "size": 2071,
    "docstring": "",
    "keywords": [],
    "functions": [
      "convert_to_colab"
    ],
    "classes": [],
    "imports": [
      "os",
      "re"
    ],
    "preview": "import os\nimport re\n\n\ndef convert_to_colab(python_code):\n    # Install necessary libraries\n    python_code = re.sub(\n        r\"^\\s*#\\s*Install\\s*required\\s*libraries\\s*\",\n        \"\",\n        python_code,\n        flags=re.MULTILINE,\n    )\n    python_code = re.sub(\n        r\"^\\s*!pip\\s*install\\s*(\\w+)\",\n        r\"!pip install \\1\",\n        python_code,\n        flags=re.MULTILINE,\n    )\n\n    # Adjust file paths to be compatible with Colab",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "283",
    "name": "mask.py",
    "path": "01_core_ai_analysis/data_processing/mask.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 71,
    "size": 2445,
    "docstring": "",
    "keywords": [
      "opencv",
      "analysis"
    ],
    "functions": [
      "apply_circular_mask",
      "process_images_in_directory"
    ],
    "classes": [],
    "imports": [
      "os",
      "cv2",
      "numpy"
    ],
    "preview": "import os\n\nimport cv2\nimport numpy as np\n\n\ndef apply_circular_mask(image_path, output_path):\n    \"\"\"\n    Applies a circular mask to the input image and saves it with transparency.\n\n    Args:\n        image_path (str): Path to the input image file.\n        output_path (str): Path to save the masked image.\n    \"\"\"\n    # Load the image\n    image = cv2.imread(image_path, cv2.IMREAD_UNCHANGED)\n\n    # If the image is not loaded correctly, skip it\n    if image is None:\n        print(f\"Failed to load image: {image_path}\")",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "284",
    "name": "process_csv.py",
    "path": "01_core_ai_analysis/data_processing/process_csv.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 14,
    "size": 405,
    "docstring": "",
    "keywords": [
      "data_processing"
    ],
    "functions": [
      "read_csv"
    ],
    "classes": [],
    "imports": [
      "pandas"
    ],
    "preview": "import pandas as pd\n\n\ndef read_csv():\n    # Prompt the user for the path to the CSV file\n    file_path = input(\"Enter the path to the CSV file: \")\n    # Load the CSV data\n    data = pd.read_csv(file_path, header=None, names=['date', 'url'])\n    return data\n\n# Test the function\ndata = read_csv()\nprint(data.head())  # Print the first 5 rows to check the data\n/Users/steven/Pictures/mid-date/3/march.csv   ",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "285",
    "name": "img-resizer.py",
    "path": "01_core_ai_analysis/data_processing/img-resizer.py",
    "category": "01_core_ai_analysis",
    "type": "image_processing",
    "lines": 121,
    "size": 4392,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "setup_logging",
      "process_image",
      "load_paths_from_csv",
      "main"
    ],
    "classes": [],
    "imports": [
      "argparse",
      "csv",
      "logging",
      "os",
      "concurrent.futures",
      "PIL",
      "tqdm"
    ],
    "preview": "#!/usr/bin/env python3\nimport argparse\nimport csv\nimport logging\nimport os\nfrom concurrent.futures import ThreadPoolExecutor\n\nfrom PIL import Image, UnidentifiedImageError\nfrom tqdm import tqdm\n\n\n# Configure logging\ndef setup_logging(log_file=None, verbose=False):\n    level = logging.DEBUG if verbose else logging.INFO\n    handlers = [logging.StreamHandler()]\n    if log_file:\n        handlers.append(logging.FileHandler(log_file))\n    logging.basicConfig(\n        level=level, format=\"%(asctime)s [%(levelname)s] %(message)s\", handlers=handlers\n    )",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "286",
    "name": "album_sorter.py",
    "path": "01_core_ai_analysis/data_processing/album_sorter.py",
    "category": "01_core_ai_analysis",
    "type": "organization",
    "lines": 93,
    "size": 2825,
    "docstring": "",
    "keywords": [
      "data_processing"
    ],
    "functions": [
      "extract_album"
    ],
    "classes": [],
    "imports": [
      "os",
      "shutil",
      "pandas",
      "collections"
    ],
    "preview": "import os\nimport shutil\n\nimport pandas as pd\n\n# Input file locations (change if needed)\nVIDS_CSV = \"/Users/steven/Movies/project2025/vids-05-31-18:16.csv\"\nDOCS_CSV = \"/Users/steven/Movies/project2025/docs-05-31-18:16.csv\"\nPATHS_TXT = \"/Users/steven/Movies/project2025/paths-2025.txt\"\n\n# Album file types to sort (ignore .png, .jpg, etc.)\nSUFFIXES = [\".mp4\", \".mp3\", \"_analysis.txt\", \"_transcript.txt\"]\nMP4_BASE = \"/Users/steven/Movies/project2025/mp4\"\n\n\ndef extract_album(filename):\n    for suf in SUFFIXES:\n        if filename.endswith(suf):\n            return filename[: -len(suf)]\n    return None",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "287",
    "name": "fixer.py",
    "path": "01_core_ai_analysis/data_processing/fixer.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 50,
    "size": 1449,
    "docstring": "",
    "keywords": [
      "analysis",
      "organization"
    ],
    "functions": [
      "format_with_black",
      "sort_imports_with_isort",
      "analyze_script"
    ],
    "classes": [],
    "imports": [
      "ast",
      "csv",
      "os",
      "subprocess",
      "io",
      "pylint.lint",
      "radon.complexity"
    ],
    "preview": "import ast\nimport csv\nimport os\nimport subprocess\nfrom io import StringIO\n\nfrom pylint.lint import Run\nfrom radon.complexity import cc_visit\n\n\n# Function to auto-format code with black\ndef format_with_black(filepath):\n    try:\n        subprocess.run([\"black\", filepath], check=True)\n        print(f\"Formatted {filepath} with black.\")\n    except subprocess.CalledProcessError as e:\n        print(f\"Error formatting {filepath}: {e}\")\n\n\n# Function to auto-sort imports with isort",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "288",
    "name": "resize-img.py",
    "path": "01_core_ai_analysis/data_processing/resize-img.py",
    "category": "01_core_ai_analysis",
    "type": "image_processing",
    "lines": 201,
    "size": 5873,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "get_closest_aspect_ratio",
      "get_creation_date",
      "optimize_file_size",
      "upscale_image",
      "get_image_metadata",
      "process_image",
      "process_images",
      "save_log",
      "save_last_directory",
      "load_last_directory"
    ],
    "classes": [],
    "imports": [
      "csv",
      "os",
      "re",
      "time",
      "datetime",
      "PIL",
      "tqdm"
    ],
    "preview": "import csv\nimport os\nimport re\nimport time\nfrom datetime import datetime\n\nfrom PIL import Image, UnidentifiedImageError\nfrom tqdm import tqdm\n\n# \ud83c\udf1f Constants\nMAX_WIDTH, MAX_HEIGHT = 4550, 5400\nTARGET_DPI = 300\nUPSCALE_MULTIPLIER = 2\nBATCH_SIZE = 50\nPAUSE_DURATION = 3\nSIZE_THRESHOLD_MB = 9\nTARGET_MAX_FILE_SIZE_MB = 10\nLAST_DIRECTORY_FILE = \"image_data.txt\"\n\n# \ud83d\udcca Common Aspect Ratios",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "289",
    "name": "re-size.py",
    "path": "01_core_ai_analysis/data_processing/re-size.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 146,
    "size": 4974,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "get_closest_aspect_ratio",
      "apply_dpi",
      "resize_image",
      "process_batch",
      "process_images",
      "main"
    ],
    "classes": [],
    "imports": [
      "os",
      "time",
      "datetime",
      "PIL",
      "tqdm"
    ],
    "preview": "import os\nimport time\nfrom datetime import datetime\n\nfrom PIL import Image, UnidentifiedImageError\nfrom tqdm import tqdm\n\n# Constants\nMAX_WIDTH, MAX_HEIGHT = 4500, 5400\nTARGET_DPI = 300\nBATCH_SIZE = 50\nPAUSE_DURATION = 5\nMAX_FILE_SIZE_BYTES = 8 * 1024 * 1024  # 8MB in bytes\n\n# Aspect Ratio Minimums\nASPECT_RATIO_MINIMUMS = {\n    \"16:9\": (720, 1280),  # Landscape\n    \"9:16\": (1080, 1920),  # Portrait\n    \"1:1\": (1024, 1024),  # Square\n    \"3:4\": (768, 1024),  # Portrait typically used for photography",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "290",
    "name": "noted.py",
    "path": "01_core_ai_analysis/data_processing/noted.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 55,
    "size": 1648,
    "docstring": "",
    "keywords": [
      "data_processing"
    ],
    "functions": [
      "parse_markdown_to_table"
    ],
    "classes": [],
    "imports": [
      "re",
      "pandas"
    ],
    "preview": "import re\n\nimport pandas as pd\n\n\ndef parse_markdown_to_table(md_file):\n    \"\"\"\n    Parses a Markdown file to extract trends, designs, and tags into a structured table.\n    \"\"\"\n    with open(md_file, \"r\") as file:\n        content = file.read()\n\n    # Regex patterns to match required components\n    trend_pattern = r\"\\| (\\d+)\\s+\\|\"\n    title_pattern = r\"\\| \\[(.*?)\\]\\((.*?)\\)\"\n    tags_pattern = r\"\\| ([^\\|]+)\\s*\\|$\"\n\n    # Splitting lines to process each row\n    lines = content.splitlines()\n    data = []",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "291",
    "name": "create_csv_from_json.py",
    "path": "01_core_ai_analysis/data_processing/create_csv_from_json.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 75,
    "size": 2218,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "process_json_to_csv",
      "combine_json_to_csv"
    ],
    "classes": [],
    "imports": [
      "csv",
      "gzip",
      "json",
      "os"
    ],
    "preview": "import csv\nimport gzip\nimport json\nimport os\n\n# Configuration\nOUTPUT_DIR = \"/Users/steven/Movies/printtricks/metadata.json\"\nCSV_FILE = os.path.join(OUTPUT_DIR, \"leonardo.csv\")\n\n# Headers for CSV\nHEADERS = [\n    \"id\",\n    \"prompt\",\n    \"negativePrompt\",\n    \"motionStrength\",\n    \"createdAt\",\n    \"image_url\",\n    \"motion_url\",\n    \"local_image_path\",\n    \"local_motion_path\",",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "292",
    "name": "expand_prompts_by_style.py",
    "path": "01_core_ai_analysis/data_processing/expand_prompts_by_style.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 56,
    "size": 1990,
    "docstring": "",
    "keywords": [],
    "functions": [
      "collect_all_styles",
      "expand_prompts"
    ],
    "classes": [],
    "imports": [
      "csv",
      "json",
      "pathlib"
    ],
    "preview": "import csv\nimport json\nfrom pathlib import Path\n\nINPUT_CSV = \"/Users/steven/Documents/python/clean/CSV/flexible_analyzed_image_data-05-30-22-21.csv\"\nOUTPUT_CSV = \"/Users/steven/Documents/python/clean/CSV/prompts_expanded_image_data-05-30-22-21.csv\"\nPROMPTS_FIELD = \"design_prompts_json\"\n\n\ndef collect_all_styles(input_csv):\n    styles = set()\n    with open(input_csv, newline=\"\", encoding=\"utf-8\") as infile:\n        reader = csv.DictReader(infile)\n        for row in reader:\n            prompt_json = row.get(PROMPTS_FIELD, \"\")\n            if prompt_json:\n                try:\n                    prompts = json.loads(prompt_json)\n                    for style in prompts.keys():\n                        styles.add(style)",
    "last_modified": "2025-09-13T05:53:48.150952"
  },
  {
    "id": "293",
    "name": "process_leonardo_20250102110033.py",
    "path": "01_core_ai_analysis/data_processing/process_leonardo_20250102110033.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 75,
    "size": 2220,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "process_json_to_csv",
      "combine_json_to_csv"
    ],
    "classes": [],
    "imports": [
      "csv",
      "gzip",
      "json",
      "os"
    ],
    "preview": "import csv\nimport gzip\nimport json\nimport os\n\n# Configuration\nOUTPUT_DIR = \"/Users/steven/Downloads/leonardo_images\"\nCSV_FILE = os.path.join(OUTPUT_DIR, \"leonardo_metadata.csv\")\n\n# Headers for CSV\nHEADERS = [\n    \"id\",\n    \"prompt\",\n    \"negativePrompt\",\n    \"motionStrength\",\n    \"createdAt\",\n    \"image_url\",\n    \"motion_url\",\n    \"local_image_path\",\n    \"local_motion_path\",",
    "last_modified": "2025-03-28T18:37:01.031009"
  },
  {
    "id": "294",
    "name": "quiz.py",
    "path": "01_core_ai_analysis/data_processing/quiz.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 39,
    "size": 1207,
    "docstring": "",
    "keywords": [
      "data_processing",
      "analysis"
    ],
    "functions": [
      "generate_trivia_quiz"
    ],
    "classes": [],
    "imports": [
      "pandas"
    ],
    "preview": "import pandas as pd\n\n# Function to read the CSV file and generate trivia quiz questions\n\n\ndef generate_trivia_quiz(csv_path):\n    # Read the CSV file into a pandas DataFrame\n    df = pd.read_csv(csv_path)\n\n    # Iterate over each row in the DataFrame\n    for index, row in df.iterrows():\n        # Extract question and options from the row\n        question = row[\"Question\"]\n        options = [row[\"Option1\"], row[\"Option2\"], row[\"Option3\"], row[\"Option4\"]]\n\n        # Print the question\n        print(f\"Question {index + 1}: {question}\")\n\n        # Print the options\n        for i, option in enumerate(options):",
    "last_modified": "2025-05-04T22:47:13.044891"
  },
  {
    "id": "295",
    "name": "yplaylist.py",
    "path": "01_core_ai_analysis/data_processing/yplaylist.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 31,
    "size": 942,
    "docstring": "",
    "keywords": [
      "data_processing"
    ],
    "functions": [],
    "classes": [],
    "imports": [
      "pandas",
      "pytube"
    ],
    "preview": "import pandas as pd\nfrom pytube import Playlist\n\ntry:\n    playlist_url = \"https://www.youtube.com/playlist?list=PLfudK7D_bQIjRgAqVU_jkbnb1N5V_pNiG\"\n    playlist = Playlist(playlist_url)\n\n    videos_info = {\n        \"Title\": [],\n        \"Video URL\": [],\n        \"Length (seconds)\": [],\n        \"Views\": [],\n        \"Thumbnail URL\": [],\n        \"Description\": [],\n    }\n\n    for video in playlist.videos:\n        videos_info[\"Title\"].append(video.title)\n        videos_info[\"Video URL\"].append(video.watch_url)\n        videos_info[\"Length (seconds)\"].append(video.length)",
    "last_modified": "2025-09-13T05:53:55.644704"
  },
  {
    "id": "296",
    "name": "custom_thumbnail.py",
    "path": "01_core_ai_analysis/data_processing/custom_thumbnail.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 159,
    "size": 5719,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "logging",
      "os",
      "time",
      "numpy",
      "PIL",
      "pyrogram",
      "translation",
      "helper_funcs.chat_base",
      "sample_config",
      "config"
    ],
    "preview": "import logging\n\nlogging.basicConfig(\n    level=logging.DEBUG, format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\n)\nlogger = logging.getLogger(__name__)\n\nimport os\nimport time\n\nimport numpy\nfrom PIL import Image\n\n# the secret configuration specific things\nif bool(os.environ.get(\"WEBHOOK\", False)):\n    from sample_config import Config\nelse:\n    from config import Config\n\n# the Strings used for this \"thing\"",
    "last_modified": "2025-09-13T05:53:43.860320"
  },
  {
    "id": "297",
    "name": "upscalerr.py",
    "path": "01_core_ai_analysis/data_processing/upscalerr.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 168,
    "size": 5941,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "get_user_choice",
      "apply_dpi",
      "resize_image",
      "upscale_image",
      "process_batch",
      "process_images",
      "write_log",
      "main"
    ],
    "classes": [],
    "imports": [
      "csv",
      "os",
      "time",
      "datetime",
      "PIL",
      "tqdm"
    ],
    "preview": "import csv\nimport os\nimport time\nfrom datetime import datetime\n\nfrom PIL import Image, UnidentifiedImageError\nfrom tqdm import tqdm\n\n# \ud83d\ude80 Constants\nTARGET_DPI = 300\nUPSCALE_MULTIPLIER = 2  # How much to enlarge small images\nBATCH_SIZE = 50\nPAUSE_DURATION = 3  # Just for dramatic effect \ud83c\udfad\nSIZE_THRESHOLD_MB = 9  # The Holy Grail of size rules\n\n# \ud83d\udcdc Log Data\nlog_data = []\n\n\n# \ud83c\udfc6 Ask the user for the processing mode",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "298",
    "name": "generate_song_csv.py",
    "path": "01_core_ai_analysis/data_processing/generate_song_csv.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 89,
    "size": 2892,
    "docstring": "",
    "keywords": [
      "data_processing"
    ],
    "functions": [
      "prompt_for_files",
      "predefined_file_paths",
      "extract_song_details",
      "main"
    ],
    "classes": [],
    "imports": [
      "os",
      "pandas",
      "bs4"
    ],
    "preview": "import os\n\nimport pandas as pd\nfrom bs4 import BeautifulSoup\n\n\n# Option A: Prompt for file paths\ndef prompt_for_files():\n    print(\"Enter the file paths for the HTML files (comma-separated):\")\n    file_paths = input().split(\",\")\n    return [file.strip() for file in file_paths]\n\n\n# Option B: Predefined file paths\ndef predefined_file_paths():\n    return [\"/Users/steven/Music/NocTurnE-meLoDieS/suno/01.html\"]\n\n\n# Main function to process HTML files\ndef extract_song_details(file_paths):",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "299",
    "name": "txt-csv.py",
    "path": "01_core_ai_analysis/data_processing/txt-csv.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 41,
    "size": 1270,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "csv",
      "re"
    ],
    "preview": "import csv\nimport re\n\n# Prompt the user for the input and output file paths\ninput_file_path = input(\"Enter the path to the input .txt file: \").strip()\noutput_file_path = input(\"Enter the path for the output .csv file: \").strip()\n\n# Define the regex pattern to extract URL, Info, and Keywords\npattern = re.compile(r'\\[src=\"(https[^\"]+)\" alt=\"([^\"]+)\"\\]')\n\n# List to store the extracted data\ndata = []\n\n# Read the input file and extract data\ntry:\n    with open(input_file_path, \"r\") as file:\n        for line in file:\n            match = pattern.search(line)\n            if match:\n                url = match.group(1)",
    "last_modified": "2025-05-04T22:47:11.553258"
  },
  {
    "id": "300",
    "name": "gthumb.py",
    "path": "01_core_ai_analysis/data_processing/gthumb.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 485,
    "size": 46791,
    "docstring": "",
    "keywords": [
      "data_processing"
    ],
    "functions": [],
    "classes": [],
    "imports": [
      "pandas"
    ],
    "preview": "import pandas as pd\n\n# Sample data - replace this with your actual DataFrame loading method\ndata = {\n    \"URL\": [\n        \"https://www.youtube.com/watch?v=2bbQdfVWr_8\",\n        \"https://www.youtube.com/watch?v=KZ4WHCmB51c\",\n        \"https://www.youtube.com/watch?v=n-o0a_DvhA8\",\n        \"https://www.youtube.com/watch?v=6XgEyP4PBxA\",\n        \"https://www.youtube.com/watch?v=ItQuj0NQCxk\",\n        \"https://www.youtube.com/watch?v=JB8bsCJzZaw\",\n        \"https://www.youtube.com/watch?v=6yRFOeb1nZI\",\n        \"https://www.youtube.com/watch?v=92jt7rgPPt0\",\n        \"https://www.youtube.com/watch?v=jBrEhPdY6Mw\",\n        \"https://www.youtube.com/watch?v=Rn5VognknAs\",\n        \"https://www.youtube.com/watch?v=J4kjVVxokKg\",\n        \"https://www.youtube.com/watch?v=8fDbfexgO3E\",\n        \"https://www.youtube.com/watch?v=MyJW1wVxR3s\",\n        \"https://www.youtube.com/watch?v=NdiDxzm4VcI\",\n        \"https://www.youtube.com/watch?v=RL1v9FCaO5s\",",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "301",
    "name": "ai_methods.py",
    "path": "01_core_ai_analysis/data_processing/ai_methods.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 67,
    "size": 2805,
    "docstring": "",
    "keywords": [
      "organization"
    ],
    "functions": [
      "mean_pooling",
      "sort_by_similarity"
    ],
    "classes": [],
    "imports": [
      "numpy",
      "torch",
      "transformers"
    ],
    "preview": "import numpy as np\nimport torch\nfrom transformers import AutoModel, AutoTokenizer\n\n\n# Mean Pooling - Take attention mask into account for correct averaging\ndef mean_pooling(model_output, attention_mask):\n    token_embeddings = model_output[\n        0\n    ]  # First element of model_output contains all token embeddings\n    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(\n        input_mask_expanded.sum(1), min=1e-9\n    )\n\n\n# This function sort the given threads based on their total similarity with the given keywords\ndef sort_by_similarity(thread_objects, keywords):\n    # Initialize tokenizer + model.\n    tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")",
    "last_modified": "2025-09-13T05:54:00.046373"
  },
  {
    "id": "302",
    "name": "generate-info 1.py",
    "path": "01_core_ai_analysis/data_processing/generate-info 1.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 94,
    "size": 3021,
    "docstring": "",
    "keywords": [
      "data_processing"
    ],
    "functions": [
      "prompt_for_files",
      "predefined_file_paths",
      "extract_song_details",
      "main"
    ],
    "classes": [],
    "imports": [
      "os",
      "pandas",
      "bs4"
    ],
    "preview": "import os\n\nimport pandas as pd\nfrom bs4 import BeautifulSoup\n\n\n# Option A: Prompt for file paths\ndef prompt_for_files():\n    print(\"Enter the file paths for the HTML files (comma-separated):\")\n    file_paths = input().split(\",\")\n    return [file.strip() for file in file_paths]\n\n\n# Option B: Predefined file paths\ndef predefined_file_paths():\n    return [\n        \"/Users/steven/Music/suno/1.html\",\n        \"/Users/steven/Music/suno/2.html\",\n        \"/Users/steven/Music/suno/3.html\",\n        \"/Users/steven/Music/suno/4.html\",",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "303",
    "name": "organize_csv.py",
    "path": "01_core_ai_analysis/data_processing/organize_csv.py",
    "category": "01_core_ai_analysis",
    "type": "organization",
    "lines": 114,
    "size": 4573,
    "docstring": "",
    "keywords": [
      "data_processing",
      "analysis"
    ],
    "functions": [
      "analyze_df",
      "standardize_columns"
    ],
    "classes": [],
    "imports": [
      "pandas"
    ],
    "preview": "import pandas as pd\n\n# --------------------------------------------------------------------------------\n# 1) Read the CSV files\n# --------------------------------------------------------------------------------\n\ndf_vids = pd.read_csv(\"/Users/steven/clean/vids-03-29-17-53.csv\")\ndf_other = pd.read_csv(\"/Users/steven/clean/other-03-29-17-52.csv\")\ndf_img = pd.read_csv(\"/Users/steven/clean/image_data-03-29-17-51.csv\")\ndf_docs = pd.read_csv(\"/Users/steven/clean/docs-03-29-17-49.csv\")\ndf_audio = pd.read_csv(\"/Users/steven/clean/audio-03-29-17-49.csv\")\n\n# --------------------------------------------------------------------------------\n# 2) Basic analysis: shapes, duplicates, and a few lines to see structure\n# --------------------------------------------------------------------------------\n\n\ndef analyze_df(df, df_name):\n    print(f\"--- Analyzing {df_name} ---\")\n    print(\"Shape (rows, columns):\", df.shape)",
    "last_modified": "2025-09-13T05:53:48.775045"
  },
  {
    "id": "304",
    "name": "suno-song-info.py",
    "path": "01_core_ai_analysis/data_processing/suno-song-info.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 34,
    "size": 891,
    "docstring": "",
    "keywords": [
      "data_processing"
    ],
    "functions": [],
    "classes": [],
    "imports": [
      "re",
      "pandas",
      "ace_tools"
    ],
    "preview": "import re\n\nimport pandas as pd\n\n# Load HTML content from file\nfile_path = \"/Users/steven/Music/suno/1.html\"\nwith open(file_path, \"r\", encoding=\"utf-8\") as file:\n    html_content = file.read()\n\n# Adjusted regex pattern to extract src, title, song_href, style_href, and style\npattern = r'src=\"([^\"]+)\".*?title=\"([^\"]+)\".*?href=\"([^\"]+)\".*?href=\"([^\"]+)\">([^<]+)'\n\n# Extract matches\nmatches = re.findall(pattern, html_content)\n\n# Prepare results for output\nresults = []\nfor match in matches:\n    results.append(\n        {",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "305",
    "name": "YTubeDLthumbs (1).py",
    "path": "01_core_ai_analysis/data_processing/YTubeDLthumbs (1).py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 59,
    "size": 1918,
    "docstring": "",
    "keywords": [
      "data_processing",
      "web_tools"
    ],
    "functions": [
      "fetch_video_details"
    ],
    "classes": [],
    "imports": [
      "os",
      "pandas",
      "requests",
      "googleapiclient.discovery"
    ],
    "preview": "import os\n\nimport pandas as pd  # This line is necessary to use pandas in your script\nimport requests\nfrom googleapiclient.discovery import build\n\n# Initialize YouTube API\napi_key = \"AIzaSyCyfGm1oF2syfzfsPeCRHb10tMKmC0dbCo\"\nyoutube = build(\"youtube\", \"v3\", developerKey=api_key)\n\n# Define paths\ncsv_path = \"/Users/steven/Downloads/Misc/ytube - youtube_videos.csv\"\nthumbnail_dir = \"/Users/steven/Downloads/Misc/Thumbnails/d2\"\nos.makedirs(thumbnail_dir, exist_ok=True)\n\n# Load CSV\ndf = pd.read_csv(csv_path)\n\n\n# Function to fetch video details and download thumbnail",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "306",
    "name": "suno-extract-song.py",
    "path": "01_core_ai_analysis/data_processing/suno-extract-song.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 113,
    "size": 3802,
    "docstring": "",
    "keywords": [
      "data_processing"
    ],
    "functions": [
      "prompt_for_files",
      "predefined_file_paths",
      "extract_song_details",
      "main"
    ],
    "classes": [],
    "imports": [
      "os",
      "re",
      "pandas",
      "bs4"
    ],
    "preview": "import os\nimport re\n\nimport pandas as pd\nfrom bs4 import BeautifulSoup\n\n\n# Option A: Prompt for file paths\ndef prompt_for_files():\n    print(\"Enter the file paths for the HTML files (comma-separated):\")\n    file_paths = input().split(\",\")\n    return [file.strip() for file in file_paths]\n\n\n# Option B: Predefined file paths\ndef predefined_file_paths():\n    return [\"pg1-310.html\"]\n\n\n# Main function to process HTML files",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "307",
    "name": "gen-songs.py",
    "path": "01_core_ai_analysis/data_processing/gen-songs.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 45,
    "size": 1665,
    "docstring": "",
    "keywords": [
      "data_processing"
    ],
    "functions": [],
    "classes": [],
    "imports": [
      "pandas",
      "bs4",
      "ace_tools"
    ],
    "preview": "import pandas as pd\nfrom bs4 import BeautifulSoup\n\n# Prompt the user for the directory\ndirectory = input(\"Enter the directory containing the files: \").strip()\n\n# Load the newly uploaded HTML file for processing\nfile_path_new_html = input(\"Enter the HTML Url: \").strip()\nwith open(file_path_new_html, \"r\", encoding=\"utf-8\") as file:\n    html_content_new_html = file.read()\n\n# Parsing the HTML content with BeautifulSoup\nsoup = BeautifulSoup(html_content_new_html, \"html.parser\")\n\n# Extracting relevant details\nsong_details_new_html = []\nfor item in soup.find_all(\"div\", class_=\"css-79jxux\"):  # Adjust the class if necessary\n    time_element = item.find_previous(\"span\", class_=\"font-mono\")\n    title_element = item.find(\"span\", class_=\"text-primary\")\n    song_url_element = item.find(\"a\", href=True)",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "308",
    "name": "check.py",
    "path": "01_core_ai_analysis/data_processing/check.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 67,
    "size": 2389,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "check_with_pylint",
      "check_with_flake8",
      "process_directory"
    ],
    "classes": [],
    "imports": [
      "os",
      "subprocess"
    ],
    "preview": "import os\nimport subprocess\n\n\ndef check_with_pylint(file_path, log_file):\n    \"\"\"\n    Run pylint on the given Python file to check for errors and style issues.\n\n    :param file_path: Path to the Python file.\n    :param log_file: File object to log the errors.\n    :return: None\n    \"\"\"\n    result = subprocess.run([\"pylint\", file_path], capture_output=True, text=True)\n    if result.returncode != 0:\n        log_file.write(f\"Pylint issues in {file_path}:\\n\")\n        log_file.write(result.stdout + \"\\n\")\n        print(f\"Pylint issues in {file_path}:\\n{result.stdout}\")\n    else:\n        log_file.write(f\"No pylint issues found in {file_path}\\n\")\n        print(f\"No pylint issues found in {file_path}\")",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "309",
    "name": "netlify_uploader.py",
    "path": "01_core_ai_analysis/data_processing/netlify_uploader.py",
    "category": "01_core_ai_analysis",
    "type": "youtube",
    "lines": 245,
    "size": 8349,
    "docstring": "",
    "keywords": [
      "youtube",
      "analysis",
      "web_tools"
    ],
    "functions": [
      "get_netlify_site_id",
      "deploy_to_netlify",
      "create_website_zip",
      "get_params",
      "render_page",
      "process_index",
      "process_token",
      "process_error",
      "log_message",
      "do_GET"
    ],
    "classes": [
      "SimplePhotoGalleryHTTPServer",
      "SimplePhotoGalleryHTTPRequestHandler",
      "NetlifyUploader"
    ],
    "imports": [
      "json",
      "os",
      "tempfile",
      "webbrowser",
      "zipfile",
      "http.server",
      "urllib",
      "jinja2",
      "pkg_resources",
      "requests"
    ],
    "preview": "import json\nimport os\nimport tempfile\nimport webbrowser\nimport zipfile\nfrom http.server import BaseHTTPRequestHandler, HTTPServer\nfrom urllib import parse\n\nimport jinja2\nimport pkg_resources\nimport requests\nimport simplegallery.common as spg_common\nfrom simplegallery.upload.base_uploader import BaseUploader\n\n\nclass SimplePhotoGalleryHTTPServer(HTTPServer):\n    \"\"\"\n    Class deriving from the HTTPServer, defining new properties\n    \"\"\"\n",
    "last_modified": "2025-09-13T05:53:53.351252"
  },
  {
    "id": "310",
    "name": "csvmerge.py",
    "path": "01_core_ai_analysis/data_processing/csvmerge.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 67,
    "size": 1981,
    "docstring": "",
    "keywords": [
      "data_processing"
    ],
    "functions": [],
    "classes": [],
    "imports": [
      "os",
      "pandas"
    ],
    "preview": "import os\n\nimport pandas as pd\n\n# Directory containing the CSV files\ncsv_directory = \"/Users/steven/Pictures/mydesign\"\n\n# List to hold dataframes\ndataframes = []\n\n# Define the expected headers\nexpected_headers = [\n    \"ID\",\n    \"Default_slot_file_name\",\n    \"Default_slot_image_url\",\n    \"Digital Image_slot_file_name\",\n    \"Digital Image_slot_image_url\",\n    \"Boy_slot_file_name\",\n    \"Boy_slot_image_url\",\n    \"VID_slot_file_name\",",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "311",
    "name": "euckrprober.py",
    "path": "01_core_ai_analysis/data_processing/euckrprober.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 48,
    "size": 1753,
    "docstring": "",
    "keywords": [],
    "functions": [
      "__init__",
      "charset_name",
      "language"
    ],
    "classes": [
      "EUCKRProber"
    ],
    "imports": [
      "chardistribution",
      "codingstatemachine",
      "mbcharsetprober",
      "mbcssm"
    ],
    "preview": "######################## BEGIN LICENSE BLOCK ########################\n# The Original Code is mozilla.org code.\n#\n# The Initial Developer of the Original Code is\n# Netscape Communications Corporation.\n# Portions created by the Initial Developer are Copyright (C) 1998\n# the Initial Developer. All Rights Reserved.\n#\n# Contributor(s):\n#   Mark Pilgrim - port to Python\n#\n# This library is free software; you can redistribute it and/or\n# modify it under the terms of the GNU Lesser General Public\n# License as published by the Free Software Foundation; either\n# version 2.1 of the License, or (at your option) any later version.\n#\n# This library is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU\n# Lesser General Public License for more details.",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "312",
    "name": "upload.py",
    "path": "01_core_ai_analysis/data_processing/upload.py",
    "category": "01_core_ai_analysis",
    "type": "youtube",
    "lines": 82,
    "size": 3510,
    "docstring": "",
    "keywords": [
      "data_processing",
      "web_tools"
    ],
    "functions": [],
    "classes": [],
    "imports": [
      "base64",
      "os",
      "pandas",
      "requests"
    ],
    "preview": "import base64\nimport os\n\nimport pandas as pd\nimport requests\n\n# Set your API credentials\naccess_token = \"eyJ0eXAiOiJKV1QiLCJhbGciOiJSUzI1NiJ9.eyJhdWQiOiIzN2Q0YmQzMDM1ZmUxMWU5YTgwM2FiN2VlYjNjY2M5NyIsImp0aSI6IjQyMmQ1MWFlODI0YjEyMGY2MmY5N2YwZWExOWQ1YzRjMjhlZGM5OTRjYWQ0MmJmNzViOTZlNTdkZWQ3Mzk3YWUyZmQxZDAxMjJhZWRkMTU2IiwiaWF0IjoxNzAyMzYwNjg5LjA0MDcyMywibmJmIjoxNzAyMzYwNjg5LjA0MDcyNiwiZXhwIjoxNzMzOTgzMDg5LjAzNDUzNiwic3ViIjoiMTEzMjQxNTMiLCJzY29wZXMiOlsic2hvcHMubWFuYWdlIiwic2hvcHMucmVhZCIsImNhdGFsb2cucmVhZCIsIm9yZGVycy5yZWFkIiwib3JkZXJzLndyaXRlIiwicHJvZHVjdHMucmVhZCIsInByb2R1Y3RzLndyaXRlIiwid2ViaG9va3MucmVhZCIsIndlYmhvb2tzLndyaXRlIiwidXBsb2Fkcy5yZWFkIiwidXBsb2Fkcy53cml0ZSIsInByaW50X3Byb3ZpZGVycy5yZWFkIl19.AkWVerYdnoGceJShqCIhFpw6o0m7Nz0mqE6moOvuAdy9F4YS-G4rVePuxPp6u9C-y9VxF2pGDYF3yg6kQEo\"\n\n# Find your shop ID by running this: curl -X GET https://api.printify.com/v1/shops.json --header \"Authorization: Bearer YOUR_PRINTIFY_API_KEY\"\n\nshop_id = \"6511744\"\n\n# Set the URL for the API endpoints\nbase_url = \"https://api.printify.com/v1\"\nupload_url = f\"{base_url}/uploads/images.json\"\nproduct_url = f\"{base_url}/shops/{shop_id}/products.json\"\n\n# Load the CSV file\ncsv_path = \"trick.csv\"  # Update this to your CSV file path",
    "last_modified": "2025-09-13T05:53:46.060530"
  },
  {
    "id": "313",
    "name": "process_leonardo_20250102110137.py",
    "path": "01_core_ai_analysis/data_processing/process_leonardo_20250102110137.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 75,
    "size": 2240,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "process_json_to_csv",
      "combine_json_to_csv"
    ],
    "classes": [],
    "imports": [
      "csv",
      "gzip",
      "json",
      "os"
    ],
    "preview": "import csv\nimport gzip\nimport json\nimport os\n\n# Configuration\nOUTPUT_DIR = \"/Users/steven/Documents/python/leonardo/process_leonardo.py\"\nCSV_FILE = os.path.join(OUTPUT_DIR, \"leonardo_metadata.csv\")\n\n# Headers for CSV\nHEADERS = [\n    \"id\",\n    \"prompt\",\n    \"negativePrompt\",\n    \"motionStrength\",\n    \"createdAt\",\n    \"image_url\",\n    \"motion_url\",\n    \"local_image_path\",\n    \"local_motion_path\",",
    "last_modified": "2025-03-28T18:37:01.032665"
  },
  {
    "id": "314",
    "name": "div.py",
    "path": "01_core_ai_analysis/data_processing/div.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 35,
    "size": 1243,
    "docstring": "",
    "keywords": [],
    "functions": [
      "extract_product_info_to_csv"
    ],
    "classes": [],
    "imports": [
      "csv",
      "re"
    ],
    "preview": "import csv\nimport re\n\n\ndef extract_product_info_to_csv(input_file_path, output_csv_path):\n    with open(input_file_path, \"r\", encoding=\"utf-8\") as file:\n        content = file.read()\n\n    # Regular expression to match product titles and image URLs\n    pattern = re.compile(\n        r\"(.*?)\\n(https://images-na\\.ssl-images-amazon\\.com/images/W/MEDIAX_792452-T2/images/I/[^\\s]+\\.jpg)\",\n        re.DOTALL,\n    )\n\n    # Write the extracted data to a CSV file\n    with open(output_csv_path, \"w\", newline=\"\", encoding=\"utf-8\") as csvfile:\n        csvwriter = csv.writer(csvfile)\n        for match in pattern.finditer(content):\n            title = match.group(1).strip()\n            urls = re.findall(",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "315",
    "name": "idt.py",
    "path": "01_core_ai_analysis/data_processing/idt.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 34,
    "size": 1067,
    "docstring": "",
    "keywords": [
      "data_processing"
    ],
    "functions": [],
    "classes": [],
    "imports": [
      "os",
      "pandas"
    ],
    "preview": "import os\n\nimport pandas as pd\n\n# Load your YouTube video data\ncsv_path = \"/Users/steven/Downloads/Misc/ytube - youtube_videos.csv\"\ndf = pd.read_csv(csv_path)\n\n# Directory containing the downloaded thumbnails\nthumbnail_dir = \"/Users/steven/Downloads/Misc/Thumbnails/idT\"\n\n# Add a new column for the thumbnail path if it doesn't exist\nif \"Thumbnail Path\" not in df.columns:\n    df[\"Thumbnail Path\"] = \"\"\n\n# Iterate through the thumbnails in the directory\nfor filename in os.listdir(thumbnail_dir):\n    if filename.endswith(\".jpg\") or filename.endswith(\".png\"):\n        # Extract the video ID from the filename\n        video_id = filename.split(\".\")[0]",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "316",
    "name": "dbl.py",
    "path": "01_core_ai_analysis/data_processing/dbl.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 186,
    "size": 6189,
    "docstring": "",
    "keywords": [
      "data_processing",
      "analysis"
    ],
    "functions": [
      "is_excluded",
      "compute_md5",
      "generate_detailed_duplicate_report",
      "prompt_for_csv_files"
    ],
    "classes": [],
    "imports": [
      "csv",
      "hashlib",
      "os",
      "re",
      "collections",
      "pandas"
    ],
    "preview": "import csv\nimport hashlib\nimport os\nimport re\nfrom collections import defaultdict\n\nimport pandas as pd\n\n\ndef is_excluded(path, patterns):\n    \"\"\"\n    Check if a given path matches any of the exclusion patterns.\n\n    Parameters:\n    path (str): The path to check.\n    patterns (list): A list of regex patterns for exclusion.\n\n    Returns:\n    bool: True if path matches any pattern, False otherwise.\n    \"\"\"",
    "last_modified": "2025-09-13T05:54:29.153600"
  },
  {
    "id": "317",
    "name": "resize-skip-8below.py",
    "path": "01_core_ai_analysis/data_processing/resize-skip-8below.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 134,
    "size": 4444,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "get_closest_aspect_ratio",
      "resize_image",
      "process_batch",
      "process_images",
      "main"
    ],
    "classes": [],
    "imports": [
      "os",
      "time",
      "datetime",
      "PIL"
    ],
    "preview": "import os\nimport time\nfrom datetime import datetime\n\nfrom PIL import Image, UnidentifiedImageError\n\n# Constants\nMAX_WIDTH, MAX_HEIGHT = 4500, 5400\nTARGET_DPI = 300\nBATCH_SIZE = 50\nPAUSE_DURATION = 5\nMIN_FILE_SIZE_BYTES = 8 * 1024 * 1024  # 8MB in bytes\n\n# Aspect Ratio Minimums\nASPECT_RATIO_MINIMUMS = {\n    \"16:9\": (720, 1280),  # Landscape\n    \"9:16\": (1080, 1920),  # Portrait\n    \"1:1\": (1024, 1024),  # Square\n}\n",
    "last_modified": "2025-09-13T05:53:48.995810"
  },
  {
    "id": "318",
    "name": "deepseek_python_20250530205148.py",
    "path": "01_core_ai_analysis/data_processing/deepseek_python_20250530205148.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 497,
    "size": 17786,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "__init__",
      "get_system_info",
      "analyze",
      "analyze_file",
      "analyze_path_relationships",
      "get_file_metrics",
      "ast_analysis",
      "run_analysis_tools",
      "complexity_analysis",
      "update_summary"
    ],
    "classes": [
      "AdvancedPythonAnalyzer"
    ],
    "imports": [
      "ast",
      "csv",
      "datetime",
      "json",
      "os",
      "platform",
      "subprocess",
      "sys",
      "collections",
      "matplotlib.pyplot"
    ],
    "preview": "import ast\nimport csv\nimport datetime\nimport json\nimport os\nimport platform\nimport subprocess\nimport sys\nfrom collections import defaultdict\n\nimport matplotlib.pyplot as plt\nimport networkx as nx\nimport radon\nfrom pylint import epylint as lint\nfrom radon.complexity import cc_visit\nfrom radon.metrics import mi_visit\n\n\nclass AdvancedPythonAnalyzer:\n    def __init__(self, directory):",
    "last_modified": "2025-09-13T05:53:27.372995"
  },
  {
    "id": "319",
    "name": "mydesigner.py",
    "path": "01_core_ai_analysis/data_processing/mydesigner.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 97,
    "size": 3180,
    "docstring": "",
    "keywords": [
      "data_processing",
      "analysis",
      "youtube",
      "web_tools"
    ],
    "functions": [
      "ensure_directories",
      "download_image",
      "process_images",
      "write_log_to_csv",
      "main"
    ],
    "classes": [],
    "imports": [
      "os",
      "datetime",
      "pandas",
      "requests"
    ],
    "preview": "import os\nfrom datetime import datetime\n\nimport pandas as pd\nimport requests\n\n\n# Ensure that CSV and output directories exist based on the user prompt\ndef ensure_directories(base_dir):\n    output_dir = os.path.join(base_dir, \"processed_images\")\n    os.makedirs(output_dir, exist_ok=True)\n    return output_dir\n\n\n# Function to download an image\ndef download_image(url, filename):\n    try:\n        response = requests.get(url, timeout=10)\n        if response.status_code == 200:\n            with open(filename, \"wb\") as f:",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "320",
    "name": "etsy-csv-sort.py",
    "path": "01_core_ai_analysis/data_processing/etsy-csv-sort.py",
    "category": "01_core_ai_analysis",
    "type": "organization",
    "lines": 75,
    "size": 2679,
    "docstring": "",
    "keywords": [
      "data_processing",
      "organization"
    ],
    "functions": [
      "organize_etsy_csv"
    ],
    "classes": [],
    "imports": [
      "pandas"
    ],
    "preview": "import pandas as pd\n\n\ndef organize_etsy_csv():\n    # Prompt for file locations\n    input_file_path = input(\"Enter the path to your input CSV file: \")\n    output_file_path = input(\n        \"Enter the path to save the organized output CSV file (with .csv extension): \"\n    )\n\n    # Read the input CSV file\n    try:\n        df = pd.read_csv(input_file_path)\n\n        # Define expected columns and provide options to rename if needed\n        required_columns = {\n            \"iD\": \"Listing URL\",\n            \"Title\": \"Etsy Listing\",\n            \"price\": \"price\",\n            \"est_total_sales\": \"est_total_sales\",",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "321",
    "name": "sjisprober.py",
    "path": "01_core_ai_analysis/data_processing/sjisprober.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 104,
    "size": 3961,
    "docstring": "",
    "keywords": [],
    "functions": [
      "__init__",
      "reset",
      "charset_name",
      "language",
      "feed",
      "get_confidence"
    ],
    "classes": [
      "SJISProber"
    ],
    "imports": [
      "typing",
      "chardistribution",
      "codingstatemachine",
      "enums",
      "jpcntx",
      "mbcharsetprober",
      "mbcssm"
    ],
    "preview": "######################## BEGIN LICENSE BLOCK ########################\n# The Original Code is mozilla.org code.\n#\n# The Initial Developer of the Original Code is\n# Netscape Communications Corporation.\n# Portions created by the Initial Developer are Copyright (C) 1998\n# the Initial Developer. All Rights Reserved.\n#\n# Contributor(s):\n#   Mark Pilgrim - port to Python\n#\n# This library is free software; you can redistribute it and/or\n# modify it under the terms of the GNU Lesser General Public\n# License as published by the Free Software Foundation; either\n# version 2.1 of the License, or (at your option) any later version.\n#\n# This library is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU\n# Lesser General Public License for more details.",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "322",
    "name": "csv-download.py",
    "path": "01_core_ai_analysis/data_processing/csv-download.py",
    "category": "01_core_ai_analysis",
    "type": "youtube",
    "lines": 35,
    "size": 990,
    "docstring": "",
    "keywords": [
      "data_processing",
      "youtube",
      "web_tools"
    ],
    "functions": [
      "read_csv",
      "download_images"
    ],
    "classes": [],
    "imports": [
      "os",
      "pandas",
      "requests"
    ],
    "preview": "import os\n\nimport pandas as pd\nimport requests\n\n\ndef read_csv():\n    # Prompt the user for the path to the CSV file\n    file_path = input(\"Enter the path to the CSV file: \")\n    # Load the CSV data\n    data = pd.read_csv(file_path, header=None, names=[\"date\", \"url\"])\n    return data\n\n\ndef download_images(data):\n    # Prompt the user for the output directory\n    output_directory = input(\"Enter the directory where images should be saved: \")\n    for index, row in data.iterrows():\n        url = row[\"url\"]\n        date = row[\"date\"].replace(\" \", \"_\").replace(\":\", \"-\")",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "323",
    "name": "cp949prober.py",
    "path": "01_core_ai_analysis/data_processing/cp949prober.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 50,
    "size": 1860,
    "docstring": "",
    "keywords": [],
    "functions": [
      "__init__",
      "charset_name",
      "language"
    ],
    "classes": [
      "CP949Prober"
    ],
    "imports": [
      "chardistribution",
      "codingstatemachine",
      "mbcharsetprober",
      "mbcssm"
    ],
    "preview": "######################## BEGIN LICENSE BLOCK ########################\n# The Original Code is mozilla.org code.\n#\n# The Initial Developer of the Original Code is\n# Netscape Communications Corporation.\n# Portions created by the Initial Developer are Copyright (C) 1998\n# the Initial Developer. All Rights Reserved.\n#\n# Contributor(s):\n#   Mark Pilgrim - port to Python\n#\n# This library is free software; you can redistribute it and/or\n# modify it under the terms of the GNU Lesser General Public\n# License as published by the Free Software Foundation; either\n# version 2.1 of the License, or (at your option) any later version.\n#\n# This library is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU\n# Lesser General Public License for more details.",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "324",
    "name": "yt-thumbnail-dl.py",
    "path": "01_core_ai_analysis/data_processing/yt-thumbnail-dl.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 58,
    "size": 1883,
    "docstring": "",
    "keywords": [
      "data_processing",
      "web_tools"
    ],
    "functions": [
      "fetch_video_details"
    ],
    "classes": [],
    "imports": [
      "os",
      "pandas",
      "requests",
      "googleapiclient.discovery"
    ],
    "preview": "import os\n\nimport pandas as pd  # This line is necessary to use pandas in your script\nimport requests\nfrom googleapiclient.discovery import build\n\n# Initialize YouTube API\napi_key = \"AIzaSyCyfGm1oF2syfzfsPeCRHb10tMKmC0dbCo\"\nyoutube = build(\"youtube\", \"v3\", developerKey=api_key)\n\n# Define paths\ncsv_path = \"/Users/steven/etsy-automation/ytube - youtube_videos.csv\"\nthumbnail_dir = \"/Users/steven/Downloads/Misc/Thumbnails\"\nos.makedirs(thumbnail_dir, exist_ok=True)\n\n# Load CSV\ndf = pd.read_csv(csv_path)\n\n# Function to fetch video details and download thumbnail\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "325",
    "name": "ythumb.py",
    "path": "01_core_ai_analysis/data_processing/ythumb.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 46,
    "size": 1506,
    "docstring": "",
    "keywords": [
      "data_processing",
      "youtube",
      "web_tools"
    ],
    "functions": [
      "download_thumbnail"
    ],
    "classes": [],
    "imports": [
      "os",
      "pandas",
      "requests"
    ],
    "preview": "import os\n\nimport pandas as pd\nimport requests\n\n# YTUBE API: AIzaSyCyfGm1oF2syfzfsPeCRHb10tMKmC0dbCo\n# Path to the CSV file\ncsv_path = \"/Users/steven/Downloads/Misc/ytube - youtube_videos.csv\"\n\n# Load the CSV file containing your YouTube video data\ndf = pd.read_csv(csv_path)\n\n# Directory where you want to save the thumbnails\nthumbnail_dir = \"/Users/steven/Downloads/Misc/Thumbnails\"\nos.makedirs(thumbnail_dir, exist_ok=True)\n\n\ndef download_thumbnail(url, video_id):\n    # Construct the URL for the video's max resolution thumbnail\n    thumbnail_url = f\"https://img.youtube.com/vi/{video_id}/maxresdefault.jpg\"",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "326",
    "name": "cleanup2.py",
    "path": "01_core_ai_analysis/data_processing/cleanup2.py",
    "category": "01_core_ai_analysis",
    "type": "organization",
    "lines": 157,
    "size": 5003,
    "docstring": "",
    "keywords": [
      "data_processing"
    ],
    "functions": [
      "is_excluded",
      "prompt_for_csv_file",
      "prompt_for_output_directory",
      "prompt_for_action",
      "remove_duplicates"
    ],
    "classes": [],
    "imports": [
      "csv",
      "os",
      "re",
      "datetime",
      "pandas"
    ],
    "preview": "import csv\nimport os\nimport re\nfrom datetime import datetime\n\nimport pandas as pd\n\n\ndef is_excluded(path, patterns):\n    \"\"\"\n    Check if a given path matches any of the exclusion patterns.\n\n    Parameters:\n    path (str): The path to check.\n    patterns (list): A list of regex patterns for exclusion.\n\n    Returns:\n    bool: True if path matches any pattern, False otherwise.\n    \"\"\"\n    for pattern in patterns:",
    "last_modified": "2025-09-13T05:54:29.070546"
  },
  {
    "id": "327",
    "name": "expand_prompts_by_style_20250530223233.py",
    "path": "01_core_ai_analysis/data_processing/expand_prompts_by_style_20250530223233.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 56,
    "size": 1990,
    "docstring": "",
    "keywords": [],
    "functions": [
      "collect_all_styles",
      "expand_prompts"
    ],
    "classes": [],
    "imports": [
      "csv",
      "json",
      "pathlib"
    ],
    "preview": "import csv\nimport json\nfrom pathlib import Path\n\nINPUT_CSV = \"/Users/steven/Documents/python/clean/CSV/flexible_analyzed_image_data-05-30-22-21.csv\"\nOUTPUT_CSV = \"/Users/steven/Documents/python/clean/CSV/prompts_expanded_image_data-05-30-22-21.csv\"\nPROMPTS_FIELD = \"design_prompts_json\"\n\n\ndef collect_all_styles(input_csv):\n    styles = set()\n    with open(input_csv, newline=\"\", encoding=\"utf-8\") as infile:\n        reader = csv.DictReader(infile)\n        for row in reader:\n            prompt_json = row.get(PROMPTS_FIELD, \"\")\n            if prompt_json:\n                try:\n                    prompts = json.loads(prompt_json)\n                    for style in prompts.keys():\n                        styles.add(style)",
    "last_modified": "2025-09-13T05:53:27.159445"
  },
  {
    "id": "328",
    "name": "convert.py",
    "path": "01_core_ai_analysis/data_processing/convert.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 78,
    "size": 2071,
    "docstring": "",
    "keywords": [],
    "functions": [
      "convert_to_colab"
    ],
    "classes": [],
    "imports": [
      "os",
      "re"
    ],
    "preview": "import os\nimport re\n\n\ndef convert_to_colab(python_code):\n    # Install necessary libraries\n    python_code = re.sub(\n        r\"^\\s*#\\s*Install\\s*required\\s*libraries\\s*\",\n        \"\",\n        python_code,\n        flags=re.MULTILINE,\n    )\n    python_code = re.sub(\n        r\"^\\s*!pip\\s*install\\s*(\\w+)\",\n        r\"!pip install \\1\",\n        python_code,\n        flags=re.MULTILINE,\n    )\n\n    # Adjust file paths to be compatible with Colab",
    "last_modified": "2025-09-13T05:53:49.156763"
  },
  {
    "id": "329",
    "name": "resize-skip-8below2.py",
    "path": "01_core_ai_analysis/data_processing/resize-skip-8below2.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 134,
    "size": 4366,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "get_closest_aspect_ratio",
      "apply_dpi",
      "resize_image",
      "process_batch",
      "process_images",
      "main"
    ],
    "classes": [],
    "imports": [
      "os",
      "time",
      "datetime",
      "PIL",
      "tqdm"
    ],
    "preview": "import os\nimport time\nfrom datetime import datetime\n\nfrom PIL import Image, UnidentifiedImageError\nfrom tqdm import tqdm\n\n# Constants\nMAX_WIDTH, MAX_HEIGHT = 4500, 5400\nTARGET_DPI = 300\nBATCH_SIZE = 50\nPAUSE_DURATION = 5\nMAX_FILE_SIZE_BYTES = 8 * 1024 * 1024  # 8MB in bytes\n\n# Aspect Ratio Minimums\nASPECT_RATIO_MINIMUMS = {\n    \"16:9\": (720, 1280),  # Landscape\n    \"9:16\": (1080, 1920),  # Portrait\n    \"1:1\": (1024, 1024),  # Square\n}",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "330",
    "name": "quantum_media_processor.py",
    "path": "01_core_ai_analysis/data_processing/quantum_media_processor.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 32,
    "size": 1295,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "__init__",
      "_quantum_dct",
      "process_image"
    ],
    "classes": [
      "QuantumMediaProcessor"
    ],
    "imports": [
      "numpy",
      "PIL",
      "scipy.fftpack"
    ],
    "preview": "import numpy as np\nfrom PIL import Image, ImageOps\nfrom scipy.fftpack import dct, idct\n\n\nclass QuantumMediaProcessor:\n    def __init__(self, chaos_factor=0.07):\n        self.chaos_factor = chaos_factor\n\n    def _quantum_dct(self, block):\n        coeffs = dct(dct(block.T, norm=\"ortho\").T, norm=\"ortho\")\n        threshold = np.percentile(np.abs(coeffs), 95) * self.chaos_factor\n        coeffs[np.abs(coeffs) < threshold] *= np.random.choice(\n            [0, 1], p=[0.3, 0.7], size=coeffs.shape\n        )\n        return coeffs\n\n    def process_image(self, image_path, output_path):\n        img = ImageOps.exif_transpose(Image.open(image_path))\n        ycbcr = img.convert(\"YCbCr\")",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "331",
    "name": "resize-skip-8below-og.py",
    "path": "01_core_ai_analysis/data_processing/resize-skip-8below-og.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 134,
    "size": 4444,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "get_closest_aspect_ratio",
      "resize_image",
      "process_batch",
      "process_images",
      "main"
    ],
    "classes": [],
    "imports": [
      "os",
      "time",
      "datetime",
      "PIL"
    ],
    "preview": "import os\nimport time\nfrom datetime import datetime\n\nfrom PIL import Image, UnidentifiedImageError\n\n# Constants\nMAX_WIDTH, MAX_HEIGHT = 4500, 5400\nTARGET_DPI = 300\nBATCH_SIZE = 50\nPAUSE_DURATION = 5\nMIN_FILE_SIZE_BYTES = 8 * 1024 * 1024  # 8MB in bytes\n\n# Aspect Ratio Minimums\nASPECT_RATIO_MINIMUMS = {\n    \"16:9\": (720, 1280),  # Landscape\n    \"9:16\": (1080, 1920),  # Portrait\n    \"1:1\": (1024, 1024),  # Square\n}\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "332",
    "name": "resize-skip-image-fixer.py",
    "path": "01_core_ai_analysis/data_processing/resize-skip-image-fixer.py",
    "category": "01_core_ai_analysis",
    "type": "image_processing",
    "lines": 168,
    "size": 5941,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "get_user_choice",
      "apply_dpi",
      "resize_image",
      "upscale_image",
      "process_batch",
      "process_images",
      "write_log",
      "main"
    ],
    "classes": [],
    "imports": [
      "csv",
      "os",
      "time",
      "datetime",
      "PIL",
      "tqdm"
    ],
    "preview": "import csv\nimport os\nimport time\nfrom datetime import datetime\n\nfrom PIL import Image, UnidentifiedImageError\nfrom tqdm import tqdm\n\n# \ud83d\ude80 Constants\nTARGET_DPI = 300\nUPSCALE_MULTIPLIER = 2  # How much to enlarge small images\nBATCH_SIZE = 50\nPAUSE_DURATION = 3  # Just for dramatic effect \ud83c\udfad\nSIZE_THRESHOLD_MB = 9  # The Holy Grail of size rules\n\n# \ud83d\udcdc Log Data\nlog_data = []\n\n\n# \ud83c\udfc6 Ask the user for the processing mode",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "333",
    "name": "deepseek_python-copy.py",
    "path": "01_core_ai_analysis/data_processing/deepseek_python-copy.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 481,
    "size": 18382,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "__init__",
      "get_system_info",
      "analyze",
      "analyze_file",
      "analyze_path_relationships",
      "get_file_metrics",
      "ast_analysis",
      "run_analysis_tools",
      "complexity_analysis",
      "update_summary"
    ],
    "classes": [
      "AdvancedPythonAnalyzer"
    ],
    "imports": [
      "ast",
      "csv",
      "datetime",
      "json",
      "os",
      "platform",
      "subprocess",
      "sys",
      "collections",
      "matplotlib.pyplot"
    ],
    "preview": "import ast\nimport csv\nimport datetime\nimport json\nimport os\nimport platform\nimport subprocess\nimport sys\nfrom collections import defaultdict\n\nimport matplotlib.pyplot as plt\nimport networkx as nx\nimport radon\nfrom pylint import epylint as lint\nfrom radon.complexity import cc_visit\nfrom radon.metrics import mi_visit\n\n\nclass AdvancedPythonAnalyzer:\n    def __init__(self, directory):",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "334",
    "name": "mydesign-csv-download.py",
    "path": "01_core_ai_analysis/data_processing/mydesign-csv-download.py",
    "category": "01_core_ai_analysis",
    "type": "youtube",
    "lines": 77,
    "size": 2705,
    "docstring": "",
    "keywords": [
      "data_processing",
      "web_tools"
    ],
    "functions": [
      "sanitize_title"
    ],
    "classes": [],
    "imports": [
      "pathlib",
      "pandas",
      "requests"
    ],
    "preview": "from pathlib import Path\n\nimport pandas as pd\nimport requests\n\n\n# Function to sanitize the title to create a valid directory name\ndef sanitize_title(title):\n    return title.replace(\" \", \"_\").replace(\"/\", \"_\").replace(\"|\", \"\").replace(\",\", \"\")\n\n\n# Read the original CSV file\ncsv_file = Path(\"/Users/steven/Documents/python/x-python/mydesigns-export.CSV\")\ndf = pd.read_csv(csv_file)\n\n# Directory where images will be downloaded\nbase_dir = Path(\"/Users/steven/Pictures/etsy/mydesign\")\nbase_dir.mkdir(exist_ok=True)\n\n# Process each row in the DataFrame",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "335",
    "name": "printify_matcher.py",
    "path": "01_core_ai_analysis/data_processing/printify_matcher.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 179,
    "size": 6590,
    "docstring": "",
    "keywords": [
      "opencv",
      "analysis"
    ],
    "functions": [
      "get_dominant_color",
      "detect_text",
      "match_product",
      "process_directory",
      "main"
    ],
    "classes": [],
    "imports": [
      "csv",
      "os",
      "datetime",
      "cv2",
      "numpy",
      "pytesseract",
      "PIL"
    ],
    "preview": "import csv\nimport os\nfrom datetime import datetime\n\nimport cv2\nimport numpy as np\nimport pytesseract\nfrom PIL import Image\n\n# Configure Tesseract for macOS (adjust for your system if needed)\npytesseract.pytesseract.tesseract_cmd = \"/usr/local/bin/tesseract\"\n\n# \ud83c\udfaf Etsy & TikTok Bestsellers\nPLATFORMS = {\n    \"tiktok\": {\n        \"hoodie\": [\"bold colors\", \"dark tones\", \"statement text\"],\n        \"t-shirt\": [\"minimalist\", \"memes\", \"high contrast\"],\n        \"tote bag\": [\"artistic\", \"neutral tones\", \"simple graphics\"],\n        \"phone case\": [\"vibrant\", \"pop culture\", \"sharp details\"],\n        \"sticker\": [\"high contrast\", \"small details\", \"text-heavy\"],",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "336",
    "name": "ascii-python.py",
    "path": "01_core_ai_analysis/data_processing/ascii-python.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 236,
    "size": 8302,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "generate_ascii_art",
      "__init__",
      "get_system_info",
      "analyze",
      "analyze_file",
      "run_pylint",
      "run_analysis_tools",
      "generate_html_report"
    ],
    "classes": [
      "PythonAnalyzer"
    ],
    "imports": [
      "os",
      "ast",
      "sys",
      "json",
      "csv",
      "platform",
      "datetime",
      "subprocess",
      "radon",
      "networkx"
    ],
    "preview": "import os\nimport ast\nimport sys\nimport json\nimport csv\nimport platform\nimport datetime\nimport subprocess\nimport radon\nimport networkx as nx\nimport matplotlib.pyplot as plt\nfrom collections import defaultdict\nfrom radon.complexity import cc_visit\nfrom radon.metrics import mi_visit\n\n# Fixed import - removed problematic epylint import\n# Added ASCII art generation function\ndef generate_ascii_art(text):\n    \"\"\"Generate ASCII art text using simple character patterns\"\"\"\n    art_map = {",
    "last_modified": "2025-05-30T22:44:18.020078"
  },
  {
    "id": "337",
    "name": "doubles.py",
    "path": "01_core_ai_analysis/data_processing/doubles.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 143,
    "size": 4754,
    "docstring": "",
    "keywords": [
      "data_processing",
      "analysis"
    ],
    "functions": [
      "is_excluded",
      "compute_md5",
      "generate_detailed_duplicate_report",
      "prompt_for_csv_files"
    ],
    "classes": [],
    "imports": [
      "csv",
      "hashlib",
      "os",
      "collections",
      "pandas"
    ],
    "preview": "import csv\nimport hashlib\nimport os\nfrom collections import defaultdict\n\nimport pandas as pd\n\n\ndef is_excluded(path, patterns):\n    \"\"\"\n    Check if a given path matches any of the exclusion patterns.\n\n    Parameters:\n    path (str): The path to check.\n    patterns (list): A list of regex patterns for exclusion.\n\n    Returns:\n    bool: True if path matches any pattern, False otherwise.\n    \"\"\"\n    for pattern in patterns:",
    "last_modified": "2025-09-13T05:54:29.310043"
  },
  {
    "id": "338",
    "name": "diag_20250530223533.py",
    "path": "01_core_ai_analysis/data_processing/diag_20250530223533.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 55,
    "size": 2012,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "csv",
      "json"
    ],
    "preview": "import csv\nimport json\n\nINPUT_CSV = \"/Users/steven/Documents/python/clean/CSV/flexible_analyzed_image_data-05-30-22-21.csv\"\nOUTPUT_CSV = \"/Users/steven/Documents/python/clean/CSV/prompts_expanded_image_data-05-30-22-21.csv\"\nPROMPTS_FIELD = \"design_prompts_json\"\nLOG_FILE = \"/Users/steven/Documents/python/clean/CSV/prompt_expand_issues.log\"\n\n# First pass: find all unique styles\nall_styles = set()\nrows = []\nwith open(INPUT_CSV, newline=\"\", encoding=\"utf-8\") as infile:\n    reader = csv.DictReader(infile)\n    for row in reader:\n        prompt_json = row.get(PROMPTS_FIELD, \"\")\n        if prompt_json:\n            try:\n                prompts = json.loads(prompt_json)\n                if isinstance(prompts, dict):\n                    all_styles.update(prompts.keys())",
    "last_modified": "2025-09-13T05:53:27.041579"
  },
  {
    "id": "339",
    "name": "csvsort.py",
    "path": "01_core_ai_analysis/data_processing/csvsort.py",
    "category": "01_core_ai_analysis",
    "type": "organization",
    "lines": 55,
    "size": 1717,
    "docstring": "",
    "keywords": [
      "data_processing",
      "youtube",
      "web_tools"
    ],
    "functions": [
      "safe_filename",
      "download_image"
    ],
    "classes": [],
    "imports": [
      "os",
      "pathlib",
      "urllib.parse",
      "pandas",
      "requests"
    ],
    "preview": "import os\nfrom pathlib import Path\nfrom urllib.parse import urlparse\n\nimport pandas as pd\nimport requests\n\n\ndef safe_filename(title):\n    \"\"\"Create a safe filename from a title.\"\"\"\n    return \"\".join([c if c.isalnum() else \"_\" for c in title])\n\n\ndef download_image(url, path):\n    \"\"\"Download an image from a URL and save it to a path.\"\"\"\n    try:\n        response = requests.get(url, stream=True)\n        if response.status_code == 200:\n            with open(path, \"wb\") as file:\n                for chunk in response.iter_content(1024):",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "340",
    "name": "resize-skip-8below3.py",
    "path": "01_core_ai_analysis/data_processing/resize-skip-8below3.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 177,
    "size": 6353,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "get_closest_aspect_ratio",
      "resize_image_to_target_size",
      "process_image",
      "process_images_in_directory",
      "write_log_to_csv",
      "display_summary",
      "main"
    ],
    "classes": [],
    "imports": [
      "csv",
      "os",
      "datetime",
      "PIL",
      "tqdm"
    ],
    "preview": "import csv\nimport os\nfrom datetime import datetime\n\nfrom PIL import Image, UnidentifiedImageError\nfrom tqdm import tqdm\n\n# Constants\nTARGET_DPI = 300\nTARGET_MIN_FILE_SIZE_MB = 9  # Minimum file size in MB\nTARGET_MAX_FILE_SIZE_MB = 10  # Maximum file size in MB\nASPECT_RATIOS = [(9, 16), (16, 9), (1, 1), (2, 3)]  # Supported aspect ratios\n\n# Initialize log data\nlog_data = []\n\n\n# Function to dynamically calculate the closest aspect ratio\ndef get_closest_aspect_ratio(width, height):\n    current_ratio = width / height",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "341",
    "name": "deepseek_python.py",
    "path": "01_core_ai_analysis/data_processing/deepseek_python.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 489,
    "size": 18399,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "main",
      "__init__",
      "get_system_info",
      "analyze",
      "analyze_file",
      "analyze_path_relationships",
      "get_file_metrics",
      "ast_analysis",
      "run_analysis_tools",
      "complexity_analysis"
    ],
    "classes": [
      "AdvancedPythonAnalyzer"
    ],
    "imports": [
      "ast",
      "csv",
      "datetime",
      "json",
      "os",
      "platform",
      "subprocess",
      "sys",
      "collections",
      "matplotlib.pyplot"
    ],
    "preview": "import ast\nimport csv\nimport datetime\nimport json\nimport os\nimport platform\nimport subprocess\nimport sys\nfrom collections import defaultdict\n\nimport matplotlib.pyplot as plt\nimport networkx as nx\nimport radon\nfrom pylint import epylint as lint\nfrom radon.complexity import cc_visit\nfrom radon.metrics import mi_visit\n\n\nclass AdvancedPythonAnalyzer:\n    def __init__(self, directory):",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "342",
    "name": "tts.py",
    "path": "01_core_ai_analysis/data_processing/tts.py",
    "category": "01_core_ai_analysis",
    "type": "organization",
    "lines": 166,
    "size": 5686,
    "docstring": "Reformat 'As a Man Thinketh' Markdown into clean assets:\n- cleaned .md\n- chapters.jsonl (order, slug, title, text)\n- toc.json\n\nDefaults to: /Users/steven/Documents/AS A MAN THINKETH.md\nLoads secrets from ~/.env (ignored if absent).",
    "keywords": [
      "organization"
    ],
    "functions": [
      "slugify",
      "read_and_normalize",
      "extract_epigraph",
      "find_chapter_spans",
      "clean_for_tts",
      "main"
    ],
    "classes": [],
    "imports": [
      "json",
      "re",
      "sys",
      "pathlib",
      "typing",
      "tqdm",
      "dotenv"
    ],
    "preview": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\nReformat 'As a Man Thinketh' Markdown into clean assets:\n- cleaned .md\n- chapters.jsonl (order, slug, title, text)\n- toc.json\n\nDefaults to: /Users/steven/Documents/AS A MAN THINKETH.md\nLoads secrets from ~/.env (ignored if absent).\n\"\"\"\n\nimport json\nimport re\nimport sys\nfrom pathlib import Path\nfrom typing import List, Tuple\nfrom tqdm import tqdm\n\n# Optional: load ~/.env as you prefer",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "343",
    "name": "mp4-mp4.py",
    "path": "01_core_ai_analysis/data_processing/mp4-mp4.py",
    "category": "01_core_ai_analysis",
    "type": "video_processing",
    "lines": 60,
    "size": 1819,
    "docstring": "",
    "keywords": [
      "opencv",
      "analysis"
    ],
    "functions": [
      "process_frame",
      "process_large_video"
    ],
    "classes": [],
    "imports": [
      "cv2"
    ],
    "preview": "import cv2\n\n\ndef process_frame(frame):\n    \"\"\"\n    Example processing function for large video files.\n    Modify this to apply your custom image processing logic.\n    \"\"\"\n    # Example: Convert frame to grayscale\n    processed_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n    return processed_frame\n\n\ndef process_large_video(input_video_path, output_video_path):\n    \"\"\"\n    Process large MP4 video files efficiently.\n    \"\"\"\n    # Open the video file\n    cap = cv2.VideoCapture(input_video_path)\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "344",
    "name": "suno-csv-card-html-seo1.py",
    "path": "01_core_ai_analysis/data_processing/suno-csv-card-html-seo1.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 121,
    "size": 4202,
    "docstring": "",
    "keywords": [
      "data_processing"
    ],
    "functions": [],
    "classes": [],
    "imports": [
      "os",
      "datetime",
      "pandas"
    ],
    "preview": "import os\nfrom datetime import datetime\n\nimport pandas as pd\n\n# Load and preprocess CSV data\ndf = pd.read_csv(\n    \"/Users/steven/Music/nocTurne MeLoDieS/Song-origins-html/Discography-ALL.csv\"\n).fillna(\"\")\n\n# HTML template\nhtml_template = \"\"\"\n<!DOCTYPE html>\n<html lang=\"en\">\n<head>\n    <meta charset=\"UTF-8\">\n    <title>Avatar Arts Full Discography</title>\n    <style>\n        body {{ font-family: -apple-system, BlinkMacSystemFont, \"Segoe UI\", sans-serif; \n               max-width: 1200px; margin: 0 auto; padding: 2rem; background: #f5f5f5; }}",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "345",
    "name": "analyze 6.py",
    "path": "01_core_ai_analysis/data_processing/analyze 6.py",
    "category": "01_core_ai_analysis",
    "type": "analysis",
    "lines": 65,
    "size": 2019,
    "docstring": "Analyze batches command.",
    "keywords": [],
    "functions": [
      "Args",
      "Run"
    ],
    "classes": [
      "Analyze"
    ],
    "imports": [
      "__future__",
      "googlecloudsdk.api_lib.dataproc",
      "googlecloudsdk.api_lib.dataproc",
      "googlecloudsdk.calliope",
      "googlecloudsdk.command_lib.dataproc"
    ],
    "preview": "# -*- coding: utf-8 -*- #\n# Copyright 2024 Google LLC. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Analyze batches command.\"\"\"\n\nfrom __future__ import absolute_import, division, unicode_literals\n\nfrom googlecloudsdk.api_lib.dataproc import dataproc as dp",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "346",
    "name": "dthumb.py",
    "path": "01_core_ai_analysis/data_processing/dthumb.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 58,
    "size": 1881,
    "docstring": "",
    "keywords": [
      "data_processing",
      "web_tools"
    ],
    "functions": [
      "fetch_video_details"
    ],
    "classes": [],
    "imports": [
      "os",
      "pandas",
      "requests",
      "googleapiclient.discovery"
    ],
    "preview": "import os\n\nimport pandas as pd  # This line is necessary to use pandas in your script\nimport requests\nfrom googleapiclient.discovery import build\n\n# Initialize YouTube API\napi_key = \"AIzaSyCyfGm1oF2syfzfsPeCRHb10tMKmC0dbCo\"\nyoutube = build(\"youtube\", \"v3\", developerKey=api_key)\n\n# Define paths\ncsv_path = \"/Users/steven/etsy-automation/ytube - youtube_videos.csv\"\nthumbnail_dir = \"/Users/steven/Downloads/Misc/Thumbnails\"\nos.makedirs(thumbnail_dir, exist_ok=True)\n\n# Load CSV\ndf = pd.read_csv(csv_path)\n\n# Function to fetch video details and download thumbnail\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "347",
    "name": "extract_topic.py",
    "path": "01_core_ai_analysis/data_processing/extract_topic.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 54,
    "size": 2201,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "__init__",
      "process_data",
      "polly_tts"
    ],
    "classes": [
      "ExtractNews"
    ],
    "imports": [
      "logging",
      "topic.news",
      "tts.polly",
      "utilities.const"
    ],
    "preview": "import logging\n\nfrom topic.news import NEWS\nfrom tts.polly import AudioProcessor\nfrom utilities.const import LOG_PATH, TECH_NEWS\n\nlogging.basicConfig(level=logging.INFO, format=\"[%(levelname)s] %(message)s\", filename=LOG_PATH)\n\n\nclass ExtractNews:\n    def __init__(self, news_url):\n        self.url = news_url\n        self.generated_files = []\n\n    @property\n    def process_data(self):\n        try:\n            news = NEWS(self.url)\n            news_response = news.getnews()\n            logging.info(f\"news_response: {news_response}\")",
    "last_modified": "2025-09-13T05:53:28.738519"
  },
  {
    "id": "348",
    "name": "sort-tik-etsy-best-seller.py",
    "path": "01_core_ai_analysis/data_processing/sort-tik-etsy-best-seller.py",
    "category": "01_core_ai_analysis",
    "type": "organization",
    "lines": 69,
    "size": 1859,
    "docstring": "",
    "keywords": [
      "data_processing"
    ],
    "functions": [
      "categorize_platform"
    ],
    "classes": [],
    "imports": [
      "os",
      "pandas"
    ],
    "preview": "import os\n\nimport pandas as pd\n\n\n# \ud83d\ude80 Your Custom Sorting Logic \u2013 Because Machines Should Think for You\ndef categorize_platform(filename):\n    \"\"\"Determines if an image belongs on Etsy or TikTok based on filename magic.\"\"\"\n    etsy_keywords = [\n        \"vintage\",\n        \"handmade\",\n        \"rustic\",\n        \"cozy\",\n        \"aesthetic\",\n        \"custom\",\n        \"wedding\",\n        \"gift\",\n        \"doormat\",\n        \"candle\",\n    ]",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "349",
    "name": "cleanup.py",
    "path": "01_core_ai_analysis/data_processing/cleanup.py",
    "category": "01_core_ai_analysis",
    "type": "organization",
    "lines": 123,
    "size": 3922,
    "docstring": "",
    "keywords": [
      "data_processing"
    ],
    "functions": [
      "is_excluded",
      "prompt_for_csv_file",
      "prompt_for_action",
      "remove_duplicates"
    ],
    "classes": [],
    "imports": [
      "csv",
      "os",
      "re",
      "pandas"
    ],
    "preview": "import csv\nimport os\nimport re\n\nimport pandas as pd\n\n\ndef is_excluded(path, patterns):\n    \"\"\"\n    Check if a given path matches any of the exclusion patterns.\n\n    Parameters:\n    path (str): The path to check.\n    patterns (list): A list of regex patterns for exclusion.\n\n    Returns:\n    bool: True if path matches any pattern, False otherwise.\n    \"\"\"\n    for pattern in patterns:\n        if re.search(pattern, path):",
    "last_modified": "2025-09-13T05:54:29.015795"
  },
  {
    "id": "350",
    "name": "etsycsv.py",
    "path": "01_core_ai_analysis/data_processing/etsycsv.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 36,
    "size": 911,
    "docstring": "",
    "keywords": [
      "data_processing"
    ],
    "functions": [],
    "classes": [],
    "imports": [
      "pandas"
    ],
    "preview": "import pandas as pd\n\n# Assuming 'image_file_names.txt' is the file with image file names, one\n# per line\nfile_path = \"/Users/steven/Documents/etsypng.txt\"\n\n# Read the file and split each line into a list of file names\nwith open(file_path, \"r\") as file:\n    lines = file.readlines()\n    image_lines = [line.strip().split() for line in lines]\n\n# Create a DataFrame to hold the table structure\ncolumns = [\n    \"IMAGE1\",\n    \"IMAGE2\",\n    \"IMAGE3\",\n    \"IMAGE4\",\n    \"IMAGE5\",\n    \"IMAGE6\",\n    \"IMAGE7\",",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "351",
    "name": "docs4.py",
    "path": "01_core_ai_analysis/data_processing/docs4.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 67,
    "size": 2260,
    "docstring": "",
    "keywords": [
      "data_processing"
    ],
    "functions": [
      "combine_csvs"
    ],
    "classes": [],
    "imports": [
      "os",
      "pandas",
      "pandas.errors"
    ],
    "preview": "import os\n\nimport pandas as pd\nfrom pandas.errors import EmptyDataError\n\n\ndef combine_csvs():\n    \"\"\"\n    Reads multiple CSV files from specified paths, concatenates them, and\n    saves the result as 'all_combined.csv' (or whatever you name it).\n    Handles empty or missing files gracefully.\n    \"\"\"\n\n    # List of CSV file paths\n    CSV_PATHS = [\n        \"/Users/steven/clean/clean-csv/combined_csv.csv\",\n        \"/Users/steven/clean/clean-csv/docs-03-28-18-52.csv\",\n        \"/Users/steven/clean/clean-csv/docs-03-28-18-57.csv\",\n        \"/Users/steven/clean/clean-csv/docs-03-29-06-11.csv\",\n        \"/Users/steven/clean/clean-csv/docs-03-29-11-47.csv\",",
    "last_modified": "2025-09-13T05:53:47.806686"
  },
  {
    "id": "352",
    "name": "bubblespider-amazon-scraper.py",
    "path": "01_core_ai_analysis/data_processing/bubblespider-amazon-scraper.py",
    "category": "01_core_ai_analysis",
    "type": "web_tools",
    "lines": 39,
    "size": 1256,
    "docstring": "",
    "keywords": [],
    "functions": [
      "extract_amazon_urls_to_csv"
    ],
    "classes": [],
    "imports": [
      "csv",
      "os",
      "re"
    ],
    "preview": "import csv\nimport os\nimport re\n\n\ndef extract_amazon_urls_to_csv(input_file_path):\n    # Regular expressions for matching the URLs\n    product_url_pattern = re.compile(r\"https:\\/\\/www\\.amazon\\.com\\/dp\\/[A-Za-z0-9]+\")\n    image_url_pattern = re.compile(\n        r\"https:\\/\\/m\\.media-amazon\\.com\\/images\\/I\\/[A-Za-z0-9_-]+\\.\\w+\"\n    )\n\n    with open(input_file_path, \"r\", encoding=\"utf-8\") as file:\n        content = file.read()\n\n    # Find all matching URLs\n    product_urls = product_url_pattern.findall(content)\n    image_urls = image_url_pattern.findall(content)\n\n    # Prepare the output CSV file name",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "353",
    "name": "gb2312prober.py",
    "path": "01_core_ai_analysis/data_processing/gb2312prober.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 48,
    "size": 1759,
    "docstring": "",
    "keywords": [],
    "functions": [
      "__init__",
      "charset_name",
      "language"
    ],
    "classes": [
      "GB2312Prober"
    ],
    "imports": [
      "chardistribution",
      "codingstatemachine",
      "mbcharsetprober",
      "mbcssm"
    ],
    "preview": "######################## BEGIN LICENSE BLOCK ########################\n# The Original Code is mozilla.org code.\n#\n# The Initial Developer of the Original Code is\n# Netscape Communications Corporation.\n# Portions created by the Initial Developer are Copyright (C) 1998\n# the Initial Developer. All Rights Reserved.\n#\n# Contributor(s):\n#   Mark Pilgrim - port to Python\n#\n# This library is free software; you can redistribute it and/or\n# modify it under the terms of the GNU Lesser General Public\n# License as published by the Free Software Foundation; either\n# version 2.1 of the License, or (at your option) any later version.\n#\n# This library is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU\n# Lesser General Public License for more details.",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "354",
    "name": "process_leonardo_20250102110148.py",
    "path": "01_core_ai_analysis/data_processing/process_leonardo_20250102110148.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 75,
    "size": 2220,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "process_json_to_csv",
      "combine_json_to_csv"
    ],
    "classes": [],
    "imports": [
      "csv",
      "gzip",
      "json",
      "os"
    ],
    "preview": "import csv\nimport gzip\nimport json\nimport os\n\n# Configuration\nOUTPUT_DIR = \"/Users/steven/Downloads/leonardo_images\"\nCSV_FILE = os.path.join(OUTPUT_DIR, \"leonardo_metadata.csv\")\n\n# Headers for CSV\nHEADERS = [\n    \"id\",\n    \"prompt\",\n    \"negativePrompt\",\n    \"motionStrength\",\n    \"createdAt\",\n    \"image_url\",\n    \"motion_url\",\n    \"local_image_path\",\n    \"local_motion_path\",",
    "last_modified": "2025-03-28T18:37:01.035450"
  },
  {
    "id": "355",
    "name": "printify-organize.py",
    "path": "01_core_ai_analysis/data_processing/printify-organize.py",
    "category": "01_core_ai_analysis",
    "type": "organization",
    "lines": 205,
    "size": 6941,
    "docstring": "",
    "keywords": [
      "opencv",
      "analysis"
    ],
    "functions": [
      "get_dominant_color",
      "detect_text",
      "suggest_product",
      "process_batch",
      "process_images",
      "write_csv",
      "main"
    ],
    "classes": [],
    "imports": [
      "csv",
      "os",
      "time",
      "datetime",
      "cv2",
      "numpy",
      "pytesseract",
      "PIL",
      "tqdm"
    ],
    "preview": "import csv\nimport os\nimport time\nfrom datetime import datetime\n\nimport cv2\nimport numpy as np\nimport pytesseract\nfrom PIL import Image, UnidentifiedImageError\nfrom tqdm import tqdm\n\n# Configure Tesseract for OCR\npytesseract.pytesseract.tesseract_cmd = \"/usr/local/bin/tesseract\"  # Adjust path as needed\n\n# Constants\nBATCH_SIZE = 50\nPAUSE_DURATION = 2  # Pause between batches for readability\nCSV_FIELDS = [\n    \"Platform\",\n    \"File\",",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "356",
    "name": "csvr.py",
    "path": "01_core_ai_analysis/data_processing/csvr.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 52,
    "size": 1731,
    "docstring": "",
    "keywords": [
      "data_processing",
      "web_tools"
    ],
    "functions": [
      "sanitize_title"
    ],
    "classes": [],
    "imports": [
      "pathlib",
      "pandas",
      "requests"
    ],
    "preview": "from pathlib import Path\n\nimport pandas as pd\nimport requests\n\n\ndef sanitize_title(title):\n    # Sanitize the title to create a valid file name\n    return title.replace(\" \", \"_\").replace(\"/\", \"_\").replace(\"|\", \"\").replace(\",\", \"\")\n\n\n# Read the original CSV file\ncsv_file = Path(\"/Users/steven/Downloads/NeAt/Misc/reformatted_mydesigns - Sheet1.csv\")\ndf = pd.read_csv(csv_file)\n\n# Directory where images will be downloaded\nbase_dir = Path(\"/Users/steven/csv2/\")\nbase_dir.mkdir(exist_ok=True)\n\n# Process each row in the DataFrame",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "357",
    "name": "YTubeDLthumbs.py",
    "path": "01_core_ai_analysis/data_processing/YTubeDLthumbs.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 60,
    "size": 1919,
    "docstring": "",
    "keywords": [
      "data_processing",
      "web_tools"
    ],
    "functions": [
      "fetch_video_details"
    ],
    "classes": [],
    "imports": [
      "os",
      "pandas",
      "requests",
      "googleapiclient.discovery"
    ],
    "preview": "import os\n\nimport pandas as pd  # This line is necessary to use pandas in your script\nimport requests\nfrom googleapiclient.discovery import build\n\n# Initialize YouTube API\napi_key = \"AIzaSyCyfGm1oF2syfzfsPeCRHb10tMKmC0dbCo\"\nyoutube = build(\"youtube\", \"v3\", developerKey=api_key)\n\n# Define paths\ncsv_path = \"/Users/steven/Downloads/Misc/ytube - youtube_videos.csv\"\nthumbnail_dir = \"/Users/steven/Downloads/Misc/Thumbnails/d2\"\nos.makedirs(thumbnail_dir, exist_ok=True)\n\n# Load CSV\ndf = pd.read_csv(csv_path)\n\n# Function to fetch video details and download thumbnail\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "358",
    "name": "300dpi.py",
    "path": "01_core_ai_analysis/data_processing/300dpi.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 58,
    "size": 2080,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "set_dpi",
      "process_directory"
    ],
    "classes": [],
    "imports": [
      "os",
      "PIL"
    ],
    "preview": "import os\n\nfrom PIL import Image, ImageResampling\n\n\ndef set_dpi(image_path, output_path, dpi=(300, 300)):\n    \"\"\"\n    Set the DPI of an image and save it to a new file.\n\n    :param image_path: Path to the input image file.\n    :param output_path: Path to save the output image file.\n    :param dpi: Tuple containing the DPI to set for the image.\n    \"\"\"\n    # Open the image file\n    with Image.open(image_path) as img:\n        # Save the image with the new DPI\n        img.save(output_path, dpi=dpi)\n    print(f\"Image saved with {dpi} DPI at {output_path}\")\n\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "359",
    "name": "bak.py",
    "path": "01_core_ai_analysis/data_processing/bak.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 76,
    "size": 3347,
    "docstring": "",
    "keywords": [
      "data_processing",
      "web_tools"
    ],
    "functions": [],
    "classes": [],
    "imports": [
      "base64",
      "pandas",
      "requests"
    ],
    "preview": "import base64\n\nimport pandas as pd\nimport requests\n\n# Set your API credentials\naccess_token = \"eyJ0eXAiOiJKV1QiLCJhbGciOiJSUzI1NiJ9.eyJhdWQiOiIzN2Q0YmQzMDM1ZmUxMWU5YTgwM2FiN2VlYjNjY2M5NyIsImp0aSI6IjQyMmQ1MWFlODI0YjEyMGY2MmY5N2YwZWExOWQ1YzRjMjhlZGM5OTRjYWQ0MmJmNzViOTZlNTdkZWQ3Mzk3YWUyZmQxZDAxMjJhZWRkMTU2IiwiaWF0IjoxNzAyMzYwNjg5LjA0MDcyMywibmJmIjoxNzAyMzYwNjg5LjA0MDcyNiwiZXhwIjoxNzMzOTgzMDg5LjAzNDUzNiwic3ViIjoiMTEzMjQxNTMiLCJzY29wZXMiOlsic2hvcHMubWFuYWdlIiwic2hvcHMucmVhZCIsImNhdGFsb2cucmVhZCIsIm9yZGVycy5yZWFkIiwib3JkZXJzLndyaXRlIiwicHJvZHVjdHMucmVhZCIsInByb2R1Y3RzLndyaXRlIiwid2ViaG9va3MucmVhZCIsIndlYmhvb2tzLndyaXRlIiwidXBsb2Fkcy5yZWFkIiwidXBsb2Fkcy53cml0ZSIsInByaW50X3Byb3ZpZGVycy5yZWFkIl19.AkWVerYdnoGceJShqCIhFpw6o0m7Nz0mqE6moOvuAdy9F4YS-G4rVePuxPp6u9C-y9VxF2pGDYF3yg6kQEo\"\nshop_id = \"6511744\"\n\n# Set the URL for the API endpoints\nbase_url = \"https://api.printify.com/v1\"\nupload_url = f\"{base_url}/uploads/images.json\"\nproduct_url = f\"{base_url}/shops/{shop_id}/products.json\"\n\n# Load the CSV file\ncsv_path = \"product_information.csv\"  # Update this to your CSV file path\nimage_df = pd.read_csv(csv_path)\n\n# Set headers for requests\nheaders = {",
    "last_modified": "2025-09-13T05:53:45.903007"
  },
  {
    "id": "360",
    "name": "mbcharsetprober.py",
    "path": "01_core_ai_analysis/data_processing/mbcharsetprober.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 96,
    "size": 3715,
    "docstring": "",
    "keywords": [],
    "functions": [
      "__init__",
      "reset",
      "feed",
      "get_confidence"
    ],
    "classes": [
      "MultiByteCharSetProber"
    ],
    "imports": [
      "typing",
      "chardistribution",
      "charsetprober",
      "codingstatemachine",
      "enums"
    ],
    "preview": "######################## BEGIN LICENSE BLOCK ########################\n# The Original Code is Mozilla Universal charset detector code.\n#\n# The Initial Developer of the Original Code is\n# Netscape Communications Corporation.\n# Portions created by the Initial Developer are Copyright (C) 2001\n# the Initial Developer. All Rights Reserved.\n#\n# Contributor(s):\n#   Mark Pilgrim - port to Python\n#   Shy Shalom - original C code\n#   Proofpoint, Inc.\n#\n# This library is free software; you can redistribute it and/or\n# modify it under the terms of the GNU Lesser General Public\n# License as published by the Free Software Foundation; either\n# version 2.1 of the License, or (at your option) any later version.\n#\n# This library is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "361",
    "name": "uploadimages.py",
    "path": "01_core_ai_analysis/data_processing/uploadimages.py",
    "category": "01_core_ai_analysis",
    "type": "youtube",
    "lines": 89,
    "size": 4369,
    "docstring": "",
    "keywords": [
      "data_processing",
      "web_tools"
    ],
    "functions": [],
    "classes": [],
    "imports": [
      "base64",
      "os",
      "pandas",
      "requests"
    ],
    "preview": "import base64\nimport os\n\nimport pandas as pd\nimport requests\n\n# Set your API credentials \naccess_token = \"eyJ0eXAiOiJKV1QiLCJhbGciOiJSUzI1NiJ9.eyJhdWQiOiIzN2Q0YmQzMDM1ZmUxMWU5YTgwM2FiN2VlYjNjY2M5NyIsImp0aSI6IjQyMmQ1MWFlODI0YjEyMGY2MmY5N2YwZWExOWQ1YzRjMjhlZGM5OTRjYWQ0MmJmNzViOTZlNTdkZWQ3Mzk3YWUyZmQxZDAxMjJhZWRkMTU2IiwiaWF0IjoxNzAyMzYwNjg5LjA0MDcyMywibmJmIjoxNzAyMzYwNjg5LjA0MDcyNiwiZXhwIjoxNzMzOTgzMDg5LjAzNDUzNiwic3ViIjoiMTEzMjQxNTMiLCJzY29wZXMiOlsic2hvcHMubWFuYWdlIiwic2hvcHMucmVhZCIsImNhdGFsb2cucmVhZCIsIm9yZGVycy5yZWFkIiwib3JkZXJzLndyaXRlIiwicHJvZHVjdHMucmVhZCIsInByb2R1Y3RzLndyaXRlIiwid2ViaG9va3MucmVhZCIsIndlYmhvb2tzLndyaXRlIiwidXBsb2Fkcy5yZWFkIiwidXBsb2Fkcy53cml0ZSIsInByaW50X3Byb3ZpZGVycy5yZWFkIl19.AkWVerYdnoGceJShqCIhFpw6o0m7Nz0mqE6moOvuAdy9F4YS-G4rVePuxPp6u9C-y9VxF2pGDYF3yg6kQEo\"\n\n# Find your shop ID by running this: curl -X GET https://api.printify.com/v1/shops.json --header \"Authorization: Bearer YOUR_SECRET_KEY\"\n\nshop_id = \"6511744\"\n\n# Set the URL for the API endpoints\nbase_url = \"https://api.printify.com/v1\"\nupload_url = f\"{base_url}/uploads/images.json\"\nproduct_url = f\"{base_url}/shops/{shop_id}/products.json\"\n\n# Load the CSV file\ncsv_path = \"product_information.csv\"  # Update this to your CSV file path",
    "last_modified": "2025-05-04T22:47:11.603355"
  },
  {
    "id": "362",
    "name": "download_etsy_images.py",
    "path": "01_core_ai_analysis/data_processing/download_etsy_images.py",
    "category": "01_core_ai_analysis",
    "type": "youtube",
    "lines": 92,
    "size": 2953,
    "docstring": "",
    "keywords": [
      "data_processing",
      "analysis",
      "youtube",
      "web_tools"
    ],
    "functions": [
      "download_image",
      "process_csv"
    ],
    "classes": [],
    "imports": [
      "os",
      "pandas",
      "requests"
    ],
    "preview": "import os\n\nimport pandas as pd\nimport requests\n\n# Save all images into this one folder\noutput_dir = \"/Users/steven/Pictures/etsy/MyDesign-IMGs\"\nos.makedirs(output_dir, exist_ok=True)\n\n# Define the CSVs and which image URL columns to use\nfiles_info = {\n    \"cookieshirt\": {\n        \"path\": \"/Users/steven/Pictures/etsy/cookieshirt.CSV\",\n        \"url_columns\": [\n            \"Default_slot_image_url\",\n            \"Digital Image_slot_image_url\",\n            \"Boy_slot_image_url\",\n            \"VID_slot_image_url\",\n            \"boy size_slot_image_url\",\n            \"girl size_slot_image_url\",",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "363",
    "name": "yplaylist 2.py",
    "path": "01_core_ai_analysis/data_processing/yplaylist 2.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 31,
    "size": 942,
    "docstring": "",
    "keywords": [
      "data_processing"
    ],
    "functions": [],
    "classes": [],
    "imports": [
      "pandas",
      "pytube"
    ],
    "preview": "import pandas as pd\nfrom pytube import Playlist\n\ntry:\n    playlist_url = \"https://www.youtube.com/playlist?list=PLfudK7D_bQIjRgAqVU_jkbnb1N5V_pNiG\"\n    playlist = Playlist(playlist_url)\n\n    videos_info = {\n        \"Title\": [],\n        \"Video URL\": [],\n        \"Length (seconds)\": [],\n        \"Views\": [],\n        \"Thumbnail URL\": [],\n        \"Description\": [],\n    }\n\n    for video in playlist.videos:\n        videos_info[\"Title\"].append(video.title)\n        videos_info[\"Video URL\"].append(video.watch_url)\n        videos_info[\"Length (seconds)\"].append(video.length)",
    "last_modified": "2025-09-13T05:54:10.627026"
  },
  {
    "id": "364",
    "name": "suno-csv-card-html-seo2.py",
    "path": "01_core_ai_analysis/data_processing/suno-csv-card-html-seo2.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 136,
    "size": 3976,
    "docstring": "",
    "keywords": [
      "data_processing"
    ],
    "functions": [],
    "classes": [],
    "imports": [
      "datetime",
      "pandas"
    ],
    "preview": "from datetime import datetime\n\nimport pandas as pd\n\n# Read CSV data\ndf = pd.read_csv(\n    \"/Users/steven/Music/nocTurne MeLoDieS/Song-origins-html/Discography_Reformatted.csv\"\n)\n# Add this after reading the CSV\ndf[\"content\"] = df[\"content\"].fillna(\"\")  # Handle missing content\ndf[\"Analysis\"] = df[\"Analysis\"].fillna(\"\")  # Handle missing analysis\n\n# Then modify the bullet points section:\nbullet_points = (\n    \"\\n\".join(\n        [\n            f\"<li>{bp.strip()}</li>\"\n            for bp in str(row[\"content\"]).split(\"\u2022\")[1:]  # Ensure string conversion\n        ]\n    ),",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "365",
    "name": "suno-csv-card-html-seo.py",
    "path": "01_core_ai_analysis/data_processing/suno-csv-card-html-seo.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 125,
    "size": 4323,
    "docstring": "",
    "keywords": [
      "data_processing"
    ],
    "functions": [],
    "classes": [],
    "imports": [
      "datetime",
      "pandas"
    ],
    "preview": "from datetime import datetime\n\nimport pandas as pd\n\n# Load CSV and fill missing data\ndf = pd.read_csv(\"Discography_Reformatted.csv\").fillna(\"\")\n\n# Add this after reading the CSV\ndf[\"content\"] = df[\"content\"].fillna(\"\").astype(str)\n\n# HTML template (escaped braces for CSS)\nhtml_template = \"\"\"\n<!DOCTYPE html>\n<html lang=\"en\">\n<head>\n    <meta charset=\"UTF-8\">\n    <title>Avatar Arts Full Discography</title>\n    <style>\n        body {{ font-family: -apple-system, BlinkMacSystemFont, \"Segoe UI\", sans-serif; \n               max-width: 1200px; margin: 0 auto; padding: 2rem; background: #f5f5f5; }}",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "366",
    "name": "docs3.py",
    "path": "01_core_ai_analysis/data_processing/docs3.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 115,
    "size": 4879,
    "docstring": "",
    "keywords": [
      "data_processing"
    ],
    "functions": [],
    "classes": [],
    "imports": [
      "pandas"
    ],
    "preview": "import pandas as pd\n\n# ------------------------------------------------------------------------------\n# 1) Read the TWO old CSVs and COMBINE them into one DataFrame\n# ------------------------------------------------------------------------------\n\nOLD_CSV_1 = \"/Users/steven/Documents/DeepSeek/docs-03-29-17_49.csv\"\nOLD_CSV_2 = \"/Users/steven/clean/docs-03-29-17-49.csv\"\nOLD_CSV_3 =\"/Users/steven/clean/clean-csv/docs-03-28-18-52.csv\"\nOLD_CSV_4 =\"/Users/steven/clean/clean-csv/docs-03-28-18-57.csv\"\nOLD_CSV_5 =\"/Users/steven/clean/clean-csv/docs-03-29-06-11.csv\"\nOLD_CSV_6 =\"/Users/steven/clean/clean-csv/docs-03-29-11-47.csv\"\nOLD_CSV_7 =\"/Users/steven/clean/clean-csv/docs-03-29-17_49_1.csv\"\nOLD_CSV_8 =\"/Users/steven/clean/clean-csv/docs-03-29-17_49.csv\"\nOLD_CSV_9 =\"/Users/steven/clean/clean-csv/docs-03-29-17-49.csv\"\nOLD_CSV_10 =\"/Users/steven/clean/clean-csv/python-newcho.csv\"\n\ndf_old_1 = pd.read_csv(OLD_CSV_1)\ndf_old_2 = pd.read_csv(OLD_CSV_2)\n",
    "last_modified": "2025-03-30T13:14:06.848356"
  },
  {
    "id": "367",
    "name": "resize.py",
    "path": "01_core_ai_analysis/data_processing/resize.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 187,
    "size": 6431,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "get_closest_aspect_ratio",
      "resize_image_to_target_size",
      "process_image",
      "process_images_in_directory",
      "write_log_to_csv",
      "display_summary",
      "main"
    ],
    "classes": [],
    "imports": [
      "csv",
      "os",
      "datetime",
      "PIL",
      "tqdm"
    ],
    "preview": "import csv\nimport os\nfrom datetime import datetime\n\nfrom PIL import Image, UnidentifiedImageError\nfrom tqdm import tqdm\n\n# Constants\nTARGET_DPI = 300\nTARGET_MIN_FILE_SIZE_MB = 5  # Minimum file size in MB\nTARGET_MAX_FILE_SIZE_MB = 9  # Maximum file size in MB\nASPECT_RATIOS = [\n    (9, 16),\n    (16, 9),\n    (1, 1),\n    (2, 3),\n    (3, 2),\n    (4, 5),\n    (5, 4),\n    (1, 2),",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "368",
    "name": "euctwprober.py",
    "path": "01_core_ai_analysis/data_processing/euctwprober.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 48,
    "size": 1753,
    "docstring": "",
    "keywords": [],
    "functions": [
      "__init__",
      "charset_name",
      "language"
    ],
    "classes": [
      "EUCTWProber"
    ],
    "imports": [
      "chardistribution",
      "codingstatemachine",
      "mbcharsetprober",
      "mbcssm"
    ],
    "preview": "######################## BEGIN LICENSE BLOCK ########################\n# The Original Code is mozilla.org code.\n#\n# The Initial Developer of the Original Code is\n# Netscape Communications Corporation.\n# Portions created by the Initial Developer are Copyright (C) 1998\n# the Initial Developer. All Rights Reserved.\n#\n# Contributor(s):\n#   Mark Pilgrim - port to Python\n#\n# This library is free software; you can redistribute it and/or\n# modify it under the terms of the GNU Lesser General Public\n# License as published by the Free Software Foundation; either\n# version 2.1 of the License, or (at your option) any later version.\n#\n# This library is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU\n# Lesser General Public License for more details.",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "369",
    "name": "suno.py",
    "path": "01_core_ai_analysis/data_processing/suno.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 55,
    "size": 2013,
    "docstring": "",
    "keywords": [
      "data_processing"
    ],
    "functions": [],
    "classes": [],
    "imports": [
      "re",
      "pandas"
    ],
    "preview": "import re\n\nimport pandas as pd\n\n# Define the adjusted regex pattern\npattern = r'src=\"([^\"]+)\".*?title=\"([^\"]+)\".*?href=\"([^\"]+)\".*?href=\"([^\"]+)\">([^<]+)'\n\n# Prompt the user for HTML file paths\nprint(\"Enter the paths to your HTML files, separated by commas:\")\nfile_input = input(\"> \")\n\n# Convert the input into a list of file paths\nhtml_files = [path.strip() for path in file_input.split(\",\")]\n\n# Initialize a list to store the extracted results\nresults = []\n\n# Process each file to extract matches\nfor html_file in html_files:\n    try:",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "370",
    "name": "printful.py",
    "path": "01_core_ai_analysis/data_processing/printful.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 82,
    "size": 4226,
    "docstring": "",
    "keywords": [
      "data_processing",
      "web_tools"
    ],
    "functions": [],
    "classes": [],
    "imports": [
      "base64",
      "os",
      "pandas",
      "requests"
    ],
    "preview": "import base64\nimport os\n\nimport pandas as pd\nimport requests\n\n# Set your API credentials\naccess_token = \"eyJ0eXAiOiJKV1QiLCJhbGciOiJSUzI1NiJ9.eyJhdWQiOiIzN2Q0YmQzMDM1ZmUxMWU5YTgwM2FiN2VlYjNjY2M5NyIsImp0aSI6ImVjYjNlYTBlMDk0OWIzZDA1Y2ZiODk4N2EwYzU5NzU5ZTE0NzBmMjIzYzQ0ODViN2M0OWU0YWFkNTY5MGMxODM4MTc5N2Y4Y2RkZDE0MzBjIiwiaWF0IjoxNzM4MTk4MTA4LjkwNDQxMiwibmJmIjoxNzM4MTk4MTA4LjkwNDQxNCwiZXhwIjoxNzY5NzM0MTA4Ljg5NzE0Nywic3ViIjoiMTEzMjQxNTMiLCJzY29wZXMiOlsic2hvcHMubWFuYWdlIiwic2hvcHMucmVhZCIsImNhdGFsb2cucmVhZCIsIm9yZGVycy5yZWFkIiwib3JkZXJzLndyaXRlIiwicHJvZHVjdHMucmVhZCIsInByb2R1Y3RzLndyaXRlIiwid2ViaG9va3MucmVhZCIsIndlYmhvb2tzLndyaXRlIiwidXBsb2Fkcy5yZWFkIiwidXBsb2Fkcy53cml0ZSIsInByaW50X3Byb3ZpZGVycy5yZWFkIiwidXNlci5pbmZvIl19.AHO8pARL74uPg4bASasg_De9T9X50a29R014NE7TwFkPpT2R58TfeCA2Ygsyj4TQSXhg0g54cou2uY0ifJg\"\n\n# Find your shop ID by running this: curl -X GET https://api.printify.com/v1/catalog/blueprints.json --header \"Authorization: Bearer eyJ0eXAiOiJKV1QiLCJhbGciOiJSUzI1NiJ9.eyJhdWQiOiIzN2Q0YmQzMDM1ZmUxMWU5YTgwM2FiN2VlYjNjY2M5NyIsImp0aSI6ImVjYjNlYTBlMDk0OWIzZDA1Y2ZiODk4N2EwYzU5NzU5ZTE0NzBmMjIzYzQ0ODViN2M0OWU0YWFkNTY5MGMxODM4MTc5N2Y4Y2RkZDE0MzBjIiwiaWF0IjoxNzM4MTk4MTA4LjkwNDQxMiwibmJmIjoxNzM4MTk4MTA4LjkwNDQxNCwiZXhwIjoxNzY5NzM0MTA4Ljg5NzE0Nywic3ViIjoiMTEzMjQxNTMiLCJzY29wZXMiOlsic2hvcHMubWFuYWdlIiwic2hvcHMucmVhZCIsImNhdGFsb2cucmVhZCIsIm9yZGVycy5yZWFkIiwib3JkZXJzLndyaXRlIiwicHJvZHVjdHMucmVhZCIsInByb2R1Y3RzLndyaXRlIiwid2ViaG9va3MucmVhZCIsIndlYmhvb2tzLndyaXRlIiwidXBsb2Fkcy5yZWFkIiwidXBsb2Fkcy53cml0ZSIsInByaW50X3Byb3ZpZGVycy5yZWFkIiwidXNlci5pbmZvIl19.AHO8pARL74uPg4bASasg_De9T9X50a29R014NE7TwFkPpT2R58TfeCA2Ygsyj4TQSXhg0g54cou2uY0ifJg\"\n\nshop_id = \"19528660\"\n\n# Set the URL for the API endpoints\nbase_url = \"https://api.printify.com/v1\"\nupload_url = f\"{base_url}/uploads/images.json\"\nproduct_url = f\"{base_url}/shops/{shop_id}/products.json\"\n\n# Load the CSV file\ncsv_path = \"product_information.csv\"  # Update this to your CSV file path",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "371",
    "name": "file_analyzer.py",
    "path": "01_core_ai_analysis/data_processing/file_analyzer.py",
    "category": "01_core_ai_analysis",
    "type": "analysis",
    "lines": 124,
    "size": 4407,
    "docstring": "",
    "keywords": [
      "data_processing",
      "analysis"
    ],
    "functions": [
      "run_fdupes",
      "parse_fdupes_output",
      "run_diff",
      "is_text_file",
      "analyze_text_files",
      "generate_report",
      "main"
    ],
    "classes": [],
    "imports": [
      "subprocess",
      "os",
      "re",
      "pandas",
      "datetime",
      "pathlib"
    ],
    "preview": "import subprocess\nimport os\nimport re\nimport pandas as pd\nfrom datetime import datetime\nfrom pathlib import Path\n\n# Define base directory and file extensions to analyze\nBASE_DIR = \"/Users/steven/SUNO\"\nTEXT_EXTENSIONS = {'.md', '.html', '.txt', '.py', '.json', '.csv'}\nREPORT_FILE = \"suno_file_analysis_report.txt\"\n\ndef run_fdupes(directory):\n    \"\"\"Run fdupes to find duplicate files and return the output.\"\"\"\n    try:\n        result = subprocess.run(\n            ['fdupes', '-r', directory],\n            capture_output=True, text=True, check=True\n        )\n        return result.stdout",
    "last_modified": "2025-10-09T05:27:15.578398"
  },
  {
    "id": "372",
    "name": "video-ocr.py",
    "path": "01_core_ai_analysis/data_processing/video-ocr.py",
    "category": "01_core_ai_analysis",
    "type": "youtube",
    "lines": 58,
    "size": 2244,
    "docstring": "",
    "keywords": [
      "opencv",
      "analysis"
    ],
    "functions": [
      "extract_text_from_video",
      "process_videos_in_folder"
    ],
    "classes": [],
    "imports": [
      "os",
      "cv2",
      "pytesseract"
    ],
    "preview": "import os\n\nimport cv2\nimport pytesseract\n\n# If you encounter issues with pytesseract finding Tesseract-OCR, specify the path as shown below\n# pytesseract.pytesseract.tesseract_cmd = '/path/to/tesseract'  # Update this path if necessary\n\n\ndef extract_text_from_video(video_path, output_file):\n    \"\"\"\n    Extracts text from the given video file using OCR and saves the text to an output file.\n\n    Args:\n    video_path (str): The path to the video file.\n    output_file (file): An open file object to write the extracted text.\n    \"\"\"\n    cap = cv2.VideoCapture(video_path)\n    frameRate = cap.get(5)  # frame rate\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "373",
    "name": "batch-img.py",
    "path": "01_core_ai_analysis/data_processing/batch-img.py",
    "category": "01_core_ai_analysis",
    "type": "image_processing",
    "lines": 73,
    "size": 2589,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "convert_image_to_jpg",
      "process_batch",
      "batch_process_file_list"
    ],
    "classes": [],
    "imports": [
      "os",
      "PIL"
    ],
    "preview": "import os\n\nfrom PIL import Image\n\n\ndef convert_image_to_jpg(file_path):\n    try:\n        with Image.open(file_path) as img:\n            # Convert to RGB (for JPEG compatibility)\n            rgb_img = img.convert(\"RGB\")\n            new_path = os.path.splitext(file_path)[0] + \".jpg\"\n            rgb_img.save(new_path, \"JPEG\")\n        os.remove(file_path)\n        return new_path\n    except Exception as e:\n        print(f\"\u274c Error converting {file_path}: {e}\")\n        return None\n\n\ndef process_batch(batch, batch_index, total_batches, start_file, total_files):",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "374",
    "name": "YTdownloadthumb copy.py",
    "path": "01_core_ai_analysis/data_processing/YTdownloadthumb copy.py",
    "category": "01_core_ai_analysis",
    "type": "youtube",
    "lines": 58,
    "size": 1881,
    "docstring": "",
    "keywords": [
      "data_processing",
      "web_tools"
    ],
    "functions": [
      "fetch_video_details"
    ],
    "classes": [],
    "imports": [
      "os",
      "pandas",
      "requests",
      "googleapiclient.discovery"
    ],
    "preview": "import os\n\nimport pandas as pd  # This line is necessary to use pandas in your script\nimport requests\nfrom googleapiclient.discovery import build\n\n# Initialize YouTube API\napi_key = \"AIzaSyCyfGm1oF2syfzfsPeCRHb10tMKmC0dbCo\"\nyoutube = build(\"youtube\", \"v3\", developerKey=api_key)\n\n# Define paths\ncsv_path = \"/Users/steven/etsy-automation/ytube - youtube_videos.csv\"\nthumbnail_dir = \"/Users/steven/Downloads/Misc/Thumbnails\"\nos.makedirs(thumbnail_dir, exist_ok=True)\n\n# Load CSV\ndf = pd.read_csv(csv_path)\n\n# Function to fetch video details and download thumbnail\n",
    "last_modified": "2025-05-04T22:47:12"
  },
  {
    "id": "375",
    "name": "YTdownloadthumb.py",
    "path": "01_core_ai_analysis/data_processing/YTdownloadthumb.py",
    "category": "01_core_ai_analysis",
    "type": "youtube",
    "lines": 57,
    "size": 1880,
    "docstring": "",
    "keywords": [
      "data_processing",
      "web_tools"
    ],
    "functions": [
      "fetch_video_details"
    ],
    "classes": [],
    "imports": [
      "os",
      "pandas",
      "requests",
      "googleapiclient.discovery"
    ],
    "preview": "import os\n\nimport pandas as pd  # This line is necessary to use pandas in your script\nimport requests\nfrom googleapiclient.discovery import build\n\n# Initialize YouTube API\napi_key = \"AIzaSyCyfGm1oF2syfzfsPeCRHb10tMKmC0dbCo\"\nyoutube = build(\"youtube\", \"v3\", developerKey=api_key)\n\n# Define paths\ncsv_path = \"/Users/steven/etsy-automation/ytube - youtube_videos.csv\"\nthumbnail_dir = \"/Users/steven/Downloads/Misc/Thumbnails\"\nos.makedirs(thumbnail_dir, exist_ok=True)\n\n# Load CSV\ndf = pd.read_csv(csv_path)\n\n\n# Function to fetch video details and download thumbnail",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "376",
    "name": "taggy.py",
    "path": "01_core_ai_analysis/data_processing/taggy.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 153,
    "size": 5230,
    "docstring": "",
    "keywords": [
      "opencv",
      "web_tools"
    ],
    "functions": [
      "get_clarifai_tags",
      "get_rekognition_tags",
      "get_imagga_tags",
      "extract_video_frame",
      "extract_and_tag"
    ],
    "classes": [],
    "imports": [
      "csv",
      "os",
      "subprocess",
      "tempfile",
      "zipfile",
      "boto3",
      "cv2",
      "requests",
      "clarifai.client.model",
      "dotenv"
    ],
    "preview": "import csv\nimport os\nimport subprocess\nimport tempfile\nimport zipfile\n\nimport boto3\nimport cv2\nimport requests\nfrom clarifai.client.model import Model\nfrom dotenv import load_dotenv\nfrom PIL import Image\n\nload_dotenv(os.path.expanduser(\"~/.env\"))\n\n# API keys\nCLARIFAI_PAT = os.getenv(\"CLARIFAI_PAT\")\nAWS_ACCESS_KEY_ID = os.getenv(\"AWS_ACCESS_KEY_ID\")\nAWS_SECRET_ACCESS_KEY = os.getenv(\"AWS_SECRET_ACCESS_KEY\")\nAWS_REGION = os.getenv(\"AWS_REGION\")",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "377",
    "name": "diag.py",
    "path": "01_core_ai_analysis/data_processing/diag.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 55,
    "size": 2012,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "csv",
      "json"
    ],
    "preview": "import csv\nimport json\n\nINPUT_CSV = \"/Users/steven/Documents/python/clean/CSV/flexible_analyzed_image_data-05-30-22-21.csv\"\nOUTPUT_CSV = \"/Users/steven/Documents/python/clean/CSV/prompts_expanded_image_data-05-30-22-21.csv\"\nPROMPTS_FIELD = \"design_prompts_json\"\nLOG_FILE = \"/Users/steven/Documents/python/clean/CSV/prompt_expand_issues.log\"\n\n# First pass: find all unique styles\nall_styles = set()\nrows = []\nwith open(INPUT_CSV, newline=\"\", encoding=\"utf-8\") as infile:\n    reader = csv.DictReader(infile)\n    for row in reader:\n        prompt_json = row.get(PROMPTS_FIELD, \"\")\n        if prompt_json:\n            try:\n                prompts = json.loads(prompt_json)\n                if isinstance(prompts, dict):\n                    all_styles.update(prompts.keys())",
    "last_modified": "2025-08-02T18:25:57.950990"
  },
  {
    "id": "378",
    "name": "yt-upload.py",
    "path": "01_core_ai_analysis/data_processing/yt-upload.py",
    "category": "01_core_ai_analysis",
    "type": "youtube",
    "lines": 81,
    "size": 2423,
    "docstring": "",
    "keywords": [
      "data_processing",
      "youtube"
    ],
    "functions": [
      "authenticate",
      "upload_video",
      "main"
    ],
    "classes": [],
    "imports": [
      "os",
      "google_auth_oauthlib.flow",
      "googleapiclient.discovery",
      "googleapiclient.errors",
      "pandas"
    ],
    "preview": "import os\n\nimport google_auth_oauthlib.flow\nimport googleapiclient.discovery\nimport googleapiclient.errors\nimport pandas as pd\n\n# Path to the client secrets file\nCLIENT_SECRETS_FILE = \"/Users/steven/Documents/Python/Youtube/clientsecret.json\"\n\n# Define the required scope for uploading videos\nSCOPES = [\"https://www.googleapis.com/auth/youtube.upload\"]\n\n\ndef authenticate():\n    \"\"\"\n    Authenticates the user and returns a YouTube API client.\n    \"\"\"\n    # Get credentials and create an API client\n    flow = google_auth_oauthlib.flow.InstalledAppFlow.from_client_secrets_file(",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "379",
    "name": "suno 1.py",
    "path": "01_core_ai_analysis/data_processing/suno 1.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 55,
    "size": 2013,
    "docstring": "",
    "keywords": [
      "data_processing"
    ],
    "functions": [],
    "classes": [],
    "imports": [
      "re",
      "pandas"
    ],
    "preview": "import re\n\nimport pandas as pd\n\n# Define the adjusted regex pattern\npattern = r'src=\"([^\"]+)\".*?title=\"([^\"]+)\".*?href=\"([^\"]+)\".*?href=\"([^\"]+)\">([^<]+)'\n\n# Prompt the user for HTML file paths\nprint(\"Enter the paths to your HTML files, separated by commas:\")\nfile_input = input(\"> \")\n\n# Convert the input into a list of file paths\nhtml_files = [path.strip() for path in file_input.split(\",\")]\n\n# Initialize a list to store the extracted results\nresults = []\n\n# Process each file to extract matches\nfor html_file in html_files:\n    try:",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "380",
    "name": "taggy 1.py",
    "path": "01_core_ai_analysis/data_processing/taggy 1.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 153,
    "size": 5230,
    "docstring": "",
    "keywords": [
      "opencv",
      "web_tools"
    ],
    "functions": [
      "get_clarifai_tags",
      "get_rekognition_tags",
      "get_imagga_tags",
      "extract_video_frame",
      "extract_and_tag"
    ],
    "classes": [],
    "imports": [
      "csv",
      "os",
      "subprocess",
      "tempfile",
      "zipfile",
      "boto3",
      "cv2",
      "requests",
      "clarifai.client.model",
      "dotenv"
    ],
    "preview": "import csv\nimport os\nimport subprocess\nimport tempfile\nimport zipfile\n\nimport boto3\nimport cv2\nimport requests\nfrom clarifai.client.model import Model\nfrom dotenv import load_dotenv\nfrom PIL import Image\n\nload_dotenv(os.path.expanduser(\"~/.env\"))\n\n# API keys\nCLARIFAI_PAT = os.getenv(\"CLARIFAI_PAT\")\nAWS_ACCESS_KEY_ID = os.getenv(\"AWS_ACCESS_KEY_ID\")\nAWS_SECRET_ACCESS_KEY = os.getenv(\"AWS_SECRET_ACCESS_KEY\")\nAWS_REGION = os.getenv(\"AWS_REGION\")",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "381",
    "name": "gdrive.py",
    "path": "01_core_ai_analysis/data_processing/gdrive.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 72,
    "size": 2066,
    "docstring": "",
    "keywords": [],
    "functions": [
      "convert_to_colab"
    ],
    "classes": [],
    "imports": [
      "os",
      "re"
    ],
    "preview": "import os\nimport re\n\n\ndef convert_to_colab(file_path, google_drive_path):\n    # Read the content of the specified file\n    with open(file_path, \"r\") as file:\n        python_code = file.read()\n\n    # Install necessary libraries\n    python_code = re.sub(\n        r\"^\\s*#\\s*Install\\s*required\\s*libraries\\s*\",\n        \"\",\n        python_code,\n        flags=re.MULTILINE,\n    )\n    python_code = re.sub(\n        r\"^\\s*!pip\\s*install\\s*(\\w+)\",\n        r\"!pip install \\1\",\n        python_code,",
    "last_modified": "2025-09-13T05:53:49.178052"
  },
  {
    "id": "382",
    "name": "csv.py",
    "path": "01_core_ai_analysis/data_processing/csv.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 27,
    "size": 627,
    "docstring": "",
    "keywords": [
      "data_processing"
    ],
    "functions": [
      "load_excel",
      "check_missing_data"
    ],
    "classes": [],
    "imports": [
      "pandas"
    ],
    "preview": "import pandas as pd\n\n\ndef load_excel(file_path):\n    try:\n        df = pd.read_excel(file_path)\n        print(\"Excel file loaded successfully.\")\n        return df\n    except Exception as e:\n        print(f\"Error loading Excel file: {e}\")\n        return None\n\n\ndef check_missing_data(df):\n    if df is not None:\n        print(\"Missing data in each column:\")\n        print(df.isnull().sum())\n    else:\n        print(\"DataFrame is empty or not loaded.\")\n",
    "last_modified": "2025-05-04T22:47:13.345048"
  },
  {
    "id": "383",
    "name": "text.py",
    "path": "01_core_ai_analysis/data_processing/text.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 59,
    "size": 2238,
    "docstring": "",
    "keywords": [
      "opencv",
      "analysis"
    ],
    "functions": [
      "extract_text_from_video",
      "process_videos_in_folder"
    ],
    "classes": [],
    "imports": [
      "os",
      "cv2",
      "pytesseract"
    ],
    "preview": "import os\n\nimport cv2\nimport pytesseract\n\n# If you encounter issues with pytesseract finding Tesseract-OCR, specify the path as shown below\n# pytesseract.pytesseract.tesseract_cmd = '/path/to/tesseract'  # Update\n# this path if necessary\n\n\ndef extract_text_from_video(video_path, output_file):\n    \"\"\"\n    Extracts text from the given video file using OCR and saves the text to an output file.\n\n    Args:\n    video_path (str): The path to the video file.\n    output_file (file): An open file object to write the extracted text.\n    \"\"\"\n    cap = cv2.VideoCapture(video_path)\n    frameRate = cap.get(5)  # frame rate",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "384",
    "name": "dthumb2.py",
    "path": "01_core_ai_analysis/data_processing/dthumb2.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 58,
    "size": 1913,
    "docstring": "",
    "keywords": [
      "data_processing",
      "web_tools"
    ],
    "functions": [
      "fetch_video_details"
    ],
    "classes": [],
    "imports": [
      "os",
      "pandas",
      "requests",
      "googleapiclient.discovery"
    ],
    "preview": "import os\n\nimport pandas as pd  # This line is necessary to use pandas in your script\nimport requests\nfrom googleapiclient.discovery import build\n\n# Initialize YouTube API\napi_key = \"AIzaSyCyfGm1oF2syfzfsPeCRHb10tMKmC0dbCo\"\nyoutube = build(\"youtube\", \"v3\", developerKey=api_key)\n\n# Define paths\ncsv_path = \"/Users/steven/Downloads/Misc/ytube - youtube_videos.csv\"\nthumbnail_dir = \"/Users/steven/Downloads/Misc/Thumbnails/d2\"\nos.makedirs(thumbnail_dir, exist_ok=True)\n\n# Load CSV\ndf = pd.read_csv(csv_path)\n\n# Function to fetch video details and download thumbnail\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "385",
    "name": "upscaleuploadimages.py",
    "path": "01_core_ai_analysis/data_processing/upscaleuploadimages.py",
    "category": "01_core_ai_analysis",
    "type": "youtube",
    "lines": 82,
    "size": 2857,
    "docstring": "",
    "keywords": [
      "data_processing",
      "web_tools"
    ],
    "functions": [],
    "classes": [],
    "imports": [
      "base64",
      "os",
      "pandas",
      "requests"
    ],
    "preview": "import base64\nimport os\n\nimport pandas as pd\nimport requests\n\n# Set your API credentials\naccess_token = \"your_printful_api_key\"\n\n# Find your shop ID by running this: curl -X GET https://api.printify.com/v1/shops.json --header \"Authorization: Bearer YOUR_PRINTIFY_API_KEY\"\n\nshop_id = \"your_shop_Id\"\n\n# Set the URL for the API endpoints\nbase_url = \"https://api.printify.com/v1\"\nupload_url = f\"{base_url}/uploads/images.json\"\nproduct_url = f\"{base_url}/shops/{shop_id}/products.json\"\n\n# Load the CSV file\ncsv_path = \"product_information.csv\"  # Update this to your CSV file path",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "386",
    "name": "YTubeDLthumbs copy 2.py",
    "path": "01_core_ai_analysis/data_processing/YTubeDLthumbs copy 2.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 58,
    "size": 1913,
    "docstring": "",
    "keywords": [
      "data_processing",
      "web_tools"
    ],
    "functions": [
      "fetch_video_details"
    ],
    "classes": [],
    "imports": [
      "os",
      "pandas",
      "requests",
      "googleapiclient.discovery"
    ],
    "preview": "import os\n\nimport pandas as pd  # This line is necessary to use pandas in your script\nimport requests\nfrom googleapiclient.discovery import build\n\n# Initialize YouTube API\napi_key = \"AIzaSyCyfGm1oF2syfzfsPeCRHb10tMKmC0dbCo\"\nyoutube = build(\"youtube\", \"v3\", developerKey=api_key)\n\n# Define paths\ncsv_path = \"/Users/steven/Downloads/Misc/ytube - youtube_videos.csv\"\nthumbnail_dir = \"/Users/steven/Downloads/Misc/Thumbnails/d2\"\nos.makedirs(thumbnail_dir, exist_ok=True)\n\n# Load CSV\ndf = pd.read_csv(csv_path)\n\n# Function to fetch video details and download thumbnail\n",
    "last_modified": "2025-09-13T05:54:07.361191"
  },
  {
    "id": "387",
    "name": "YTdownloadthumb copy 2.py",
    "path": "01_core_ai_analysis/data_processing/YTdownloadthumb copy 2.py",
    "category": "01_core_ai_analysis",
    "type": "youtube",
    "lines": 58,
    "size": 1881,
    "docstring": "",
    "keywords": [
      "data_processing",
      "web_tools"
    ],
    "functions": [
      "fetch_video_details"
    ],
    "classes": [],
    "imports": [
      "os",
      "pandas",
      "requests",
      "googleapiclient.discovery"
    ],
    "preview": "import os\n\nimport pandas as pd  # This line is necessary to use pandas in your script\nimport requests\nfrom googleapiclient.discovery import build\n\n# Initialize YouTube API\napi_key = \"AIzaSyCyfGm1oF2syfzfsPeCRHb10tMKmC0dbCo\"\nyoutube = build(\"youtube\", \"v3\", developerKey=api_key)\n\n# Define paths\ncsv_path = \"/Users/steven/etsy-automation/ytube - youtube_videos.csv\"\nthumbnail_dir = \"/Users/steven/Downloads/Misc/Thumbnails\"\nos.makedirs(thumbnail_dir, exist_ok=True)\n\n# Load CSV\ndf = pd.read_csv(csv_path)\n\n# Function to fetch video details and download thumbnail\n",
    "last_modified": "2025-08-06T13:57:29.941892"
  },
  {
    "id": "388",
    "name": "upscaler.py",
    "path": "01_core_ai_analysis/data_processing/upscaler.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 168,
    "size": 5941,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "get_user_choice",
      "apply_dpi",
      "resize_image",
      "upscale_image",
      "process_batch",
      "process_images",
      "write_log",
      "main"
    ],
    "classes": [],
    "imports": [
      "csv",
      "os",
      "time",
      "datetime",
      "PIL",
      "tqdm"
    ],
    "preview": "import csv\nimport os\nimport time\nfrom datetime import datetime\n\nfrom PIL import Image, UnidentifiedImageError\nfrom tqdm import tqdm\n\n# \ud83d\ude80 Constants\nTARGET_DPI = 300\nUPSCALE_MULTIPLIER = 2  # How much to enlarge small images\nBATCH_SIZE = 50\nPAUSE_DURATION = 3  # Just for dramatic effect \ud83c\udfad\nSIZE_THRESHOLD_MB = 9  # The Holy Grail of size rules\n\n# \ud83d\udcdc Log Data\nlog_data = []\n\n\n# \ud83c\udfc6 Ask the user for the processing mode",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "389",
    "name": "2mentest.py",
    "path": "01_core_ai_analysis/data_processing/2mentest.py",
    "category": "01_core_ai_analysis",
    "type": "testing",
    "lines": 39,
    "size": 1194,
    "docstring": "",
    "keywords": [],
    "functions": [
      "extract_sitemap_data",
      "main"
    ],
    "classes": [],
    "imports": [
      "random",
      "xml.etree.ElementTree"
    ],
    "preview": "import random\nimport xml.etree.ElementTree as ET\n\n\ndef extract_sitemap_data(xml_file_path, num_urls=200):\n    tree = ET.parse(xml_file_path)\n    root = tree.getroot()\n\n    namespaces = {\n        \"ns\": \"http://www.sitemaps.org/schemas/sitemap/0.9\",\n        \"image\": \"http://www.google.com/schemas/sitemap-image/1.1\",\n    }\n\n    all_products = []\n    for url in root.findall(\"ns:url\", namespaces):\n        loc = url.find(\"ns:loc\", namespaces).text\n        image = url.find(\"image:image\", namespaces)\n        if image is not None:\n            image_loc = image.find(\"image:loc\", namespaces).text\n            image_title = image.find(\"image:title\", namespaces).text",
    "last_modified": "2025-05-06T04:35:15.041354"
  },
  {
    "id": "390",
    "name": "ythumb copy.py",
    "path": "01_core_ai_analysis/data_processing/ythumb copy.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 47,
    "size": 1512,
    "docstring": "",
    "keywords": [
      "data_processing",
      "youtube",
      "web_tools"
    ],
    "functions": [
      "download_thumbnail"
    ],
    "classes": [],
    "imports": [
      "os",
      "pandas",
      "requests"
    ],
    "preview": "import os\n\nimport pandas as pd\nimport requests\n\n# YTUBE API: AIzaSyCyfGm1oF2syfzfsPeCRHb10tMKmC0dbCo\n# Path to the CSV file\ncsv_path = \"/Users/steven/Downloads/Misc/ytube - youtube_videos.csv\"\n\n# Load the CSV file containing your YouTube video data\ndf = pd.read_csv(csv_path)\n\n# Directory where you want to save the thumbnails\nthumbnail_dir = \"/Users/steven/Downloads/Misc/Thumbnails\"\nos.makedirs(thumbnail_dir, exist_ok=True)\n\n\ndef download_thumbnail(url, video_id):\n    # Construct the URL for the video's max resolution thumbnail\n    thumbnail_url = f\"https://img.youtube.com/vi/{video_id}/maxresdefault.jpg\"",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "391",
    "name": "song-info.py",
    "path": "01_core_ai_analysis/data_processing/song-info.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 34,
    "size": 891,
    "docstring": "",
    "keywords": [
      "data_processing"
    ],
    "functions": [],
    "classes": [],
    "imports": [
      "re",
      "pandas",
      "ace_tools"
    ],
    "preview": "import re\n\nimport pandas as pd\n\n# Load HTML content from file\nfile_path = \"/Users/steven/Music/suno/1.html\"\nwith open(file_path, \"r\", encoding=\"utf-8\") as file:\n    html_content = file.read()\n\n# Adjusted regex pattern to extract src, title, song_href, style_href, and style\npattern = r'src=\"([^\"]+)\".*?title=\"([^\"]+)\".*?href=\"([^\"]+)\".*?href=\"([^\"]+)\">([^<]+)'\n\n# Extract matches\nmatches = re.findall(pattern, html_content)\n\n# Prepare results for output\nresults = []\nfor match in matches:\n    results.append(\n        {",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "392",
    "name": "extract_webm_paths.py",
    "path": "01_core_ai_analysis/data_processing/extract_webm_paths.py",
    "category": "01_core_ai_analysis",
    "type": "web_tools",
    "lines": 23,
    "size": 672,
    "docstring": "",
    "keywords": [
      "data_processing"
    ],
    "functions": [],
    "classes": [],
    "imports": [
      "pandas"
    ],
    "preview": "import pandas as pd\n\n# Path to your input CSV file\ncsv_path = \"/Users/steven/vids-12-05-00:16.csv\"\n\n# Output paths for the new CSV and TXT files\noutput_csv = \"/Users/steven/webm_files.csv\"\noutput_txt = \"/Users/steven/webm_files.txt\"\n\n# Load the CSV file\ndata = pd.read_csv(csv_path)\n\n# Filter rows where the file path ends with `.webm`\nwebm_files = data[data[\"Original Path\"].str.endswith(\".webm\", na=False)]\n\n# Save to a new CSV file\nwebm_files[[\"Original Path\"]].to_csv(output_csv, index=False)\n\n# Save to a TXT file\nwebm_files[\"Original Path\"].to_csv(output_txt, index=False, header=False)",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "393",
    "name": "htmlouts.py",
    "path": "01_core_ai_analysis/data_processing/htmlouts.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 116,
    "size": 3751,
    "docstring": "",
    "keywords": [
      "data_processing",
      "analysis"
    ],
    "functions": [
      "group_files_by_title",
      "generate_html_and_csv",
      "main"
    ],
    "classes": [],
    "imports": [
      "os",
      "collections",
      "pandas"
    ],
    "preview": "import os\nfrom collections import defaultdict\n\nimport pandas as pd\n\n\ndef group_files_by_title(directory):\n    \"\"\"\n    Groups files by base title (ignoring version-specific suffixes).\n    \"\"\"\n    file_groups = defaultdict(list)\n    for root, _, files in os.walk(directory):\n        for file in files:\n            if file.endswith((\".mp3\", \".txt\")):\n                base_title = (\n                    os.path.splitext(file)[0]\n                    .split(\"_analysis\")[0]\n                    .split(\"_lyrics\")[0]\n                    .split(\"_song\")[0]\n                    .strip()",
    "last_modified": "2025-09-13T05:54:54.780461"
  },
  {
    "id": "394",
    "name": "sorty.py",
    "path": "01_core_ai_analysis/data_processing/sorty.py",
    "category": "01_core_ai_analysis",
    "type": "organization",
    "lines": 69,
    "size": 1859,
    "docstring": "",
    "keywords": [
      "data_processing"
    ],
    "functions": [
      "categorize_platform"
    ],
    "classes": [],
    "imports": [
      "os",
      "pandas"
    ],
    "preview": "import os\n\nimport pandas as pd\n\n\n# \ud83d\ude80 Your Custom Sorting Logic \u2013 Because Machines Should Think for You\ndef categorize_platform(filename):\n    \"\"\"Determines if an image belongs on Etsy or TikTok based on filename magic.\"\"\"\n    etsy_keywords = [\n        \"vintage\",\n        \"handmade\",\n        \"rustic\",\n        \"cozy\",\n        \"aesthetic\",\n        \"custom\",\n        \"wedding\",\n        \"gift\",\n        \"doormat\",\n        \"candle\",\n    ]",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "395",
    "name": "deepseek_python_20250608130553.py",
    "path": "01_core_ai_analysis/data_processing/deepseek_python_20250608130553.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 481,
    "size": 18382,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "__init__",
      "get_system_info",
      "analyze",
      "analyze_file",
      "analyze_path_relationships",
      "get_file_metrics",
      "ast_analysis",
      "run_analysis_tools",
      "complexity_analysis",
      "update_summary"
    ],
    "classes": [
      "AdvancedPythonAnalyzer"
    ],
    "imports": [
      "ast",
      "csv",
      "datetime",
      "json",
      "os",
      "platform",
      "subprocess",
      "sys",
      "collections",
      "matplotlib.pyplot"
    ],
    "preview": "import ast\nimport csv\nimport datetime\nimport json\nimport os\nimport platform\nimport subprocess\nimport sys\nfrom collections import defaultdict\n\nimport matplotlib.pyplot as plt\nimport networkx as nx\nimport radon\nfrom pylint import epylint as lint\nfrom radon.complexity import cc_visit\nfrom radon.metrics import mi_visit\n\n\nclass AdvancedPythonAnalyzer:\n    def __init__(self, directory):",
    "last_modified": "2025-09-13T05:53:27.850984"
  },
  {
    "id": "396",
    "name": "9mb-up.py",
    "path": "01_core_ai_analysis/data_processing/9mb-up.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 172,
    "size": 5516,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "setup_logging",
      "get_user_choice",
      "resize_image",
      "upscale_image",
      "process_batch",
      "process_images",
      "write_log",
      "main"
    ],
    "classes": [],
    "imports": [
      "csv",
      "logging",
      "os",
      "time",
      "datetime",
      "PIL",
      "tqdm"
    ],
    "preview": "#!/usr/bin/env python3\nimport csv\nimport logging\nimport os\nimport time\nfrom datetime import datetime\n\nfrom PIL import Image, UnidentifiedImageError\nfrom tqdm import tqdm\n\n# \ud83d\ude80 Constants\nTARGET_DPI = 300\nUPSCALE_MULTIPLIER = 2  # How much to enlarge small images\nBATCH_SIZE = 50\nPAUSE_DURATION = 3  # Seconds between batches\nSIZE_THRESHOLD_MB = 9  # Max image size threshold\nMAX_IMAGE_SIZE_MB = 9  # Holy grail max size\n\n\n# Configure logging",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "397",
    "name": "autodownloaderUI.py",
    "path": "01_core_ai_analysis/data_processing/autodownloaderUI.py",
    "category": "01_core_ai_analysis",
    "type": "youtube",
    "lines": 371,
    "size": 14813,
    "docstring": "",
    "keywords": [
      "organization"
    ],
    "functions": [
      "cleanDatabase",
      "deleteClipsForFilter",
      "__init__",
      "closeEvent",
      "openFinishedVids",
      "openClipBin",
      "addNewFTPUser",
      "deleteFTPUser",
      "updateAccountInfo",
      "populateRemoveUserList"
    ],
    "classes": [
      "PassiveDownloaderWindow"
    ],
    "imports": [
      "os",
      "pickle",
      "sys",
      "threading",
      "time",
      "autodownloader",
      "database",
      "scriptwrapper",
      "server",
      "settings"
    ],
    "preview": "import os\nimport pickle\nimport sys\nfrom threading import Thread\nfrom time import sleep\n\nimport autodownloader\nimport database\nimport scriptwrapper\nimport server\nimport settings\nimport tiktok\nfrom filtercreator import FilterCreationWindow\nfrom PyQt5 import QtCore, QtGui, QtWidgets, uic\nfrom PyQt5.QtCore import *\nfrom PyQt5.QtCore import QDir, QObject, QPoint, QRect, Qt, QUrl, pyqtSignal\nfrom PyQt5.QtGui import QIcon\nfrom PyQt5.QtMultimedia import (\n    QAbstractVideoBuffer,\n    QAbstractVideoSurface,",
    "last_modified": "2025-09-13T05:53:31.800705"
  },
  {
    "id": "398",
    "name": "quick_demo.py",
    "path": "01_core_ai_analysis/data_processing/quick_demo.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 113,
    "size": 3745,
    "docstring": "quick_demo.ipynb",
    "keywords": [],
    "functions": [
      "on_change"
    ],
    "classes": [],
    "imports": [
      "glob",
      "ipywidgets",
      "matplotlib.pyplot",
      "os",
      "sys",
      "base64",
      "IPython.display"
    ],
    "preview": "# -*- coding: utf-8 -*-\n\"\"\"quick_demo.ipynb\n\nAutomatically generated by Colab.\n\nOriginal file is located at\n    https://colab.research.google.com/github/Winfredy/SadTalker/blob/main/quick_demo.ipynb\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Winfredy/SadTalker/blob/main/quick_demo.ipynb)\n\n### SadTalker\uff1aLearning Realistic 3D Motion Coefficients for Stylized Audio-Driven Single Image Talking Face Animation\n\n[arxiv](https://arxiv.org/abs/2211.12194) | [project](https://sadtalker.github.io) | [Github](https://github.com/Winfredy/SadTalker)\n\nWenxuan Zhang, Xiaodong Cun, Xuan Wang, Yong Zhang, Xi Shen, Yu Guo, Ying Shan, Fei Wang.\n\nXi'an Jiaotong University, Tencent AI Lab, Ant Group\n\nCVPR 2023\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "399",
    "name": "1500-cutout-orn-process.py",
    "path": "01_core_ai_analysis/data_processing/1500-cutout-orn-process.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 71,
    "size": 2445,
    "docstring": "",
    "keywords": [
      "opencv",
      "analysis"
    ],
    "functions": [
      "apply_circular_mask",
      "process_images_in_directory"
    ],
    "classes": [],
    "imports": [
      "os",
      "cv2",
      "numpy"
    ],
    "preview": "import os\n\nimport cv2\nimport numpy as np\n\n\ndef apply_circular_mask(image_path, output_path):\n    \"\"\"\n    Applies a circular mask to the input image and saves it with transparency.\n\n    Args:\n        image_path (str): Path to the input image file.\n        output_path (str): Path to save the masked image.\n    \"\"\"\n    # Load the image\n    image = cv2.imread(image_path, cv2.IMREAD_UNCHANGED)\n\n    # If the image is not loaded correctly, skip it\n    if image is None:\n        print(f\"Failed to load image: {image_path}\")",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "400",
    "name": "xlsx.py",
    "path": "01_core_ai_analysis/data_processing/xlsx.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 54,
    "size": 1776,
    "docstring": "",
    "keywords": [
      "data_processing",
      "web_tools"
    ],
    "functions": [
      "sanitize_title"
    ],
    "classes": [],
    "imports": [
      "pathlib",
      "pandas",
      "requests"
    ],
    "preview": "from pathlib import Path\n\nimport pandas as pd\nimport requests\n\n# Other necessary code ...\n\n\n# Function to sanitize the title\ndef sanitize_title(title):\n    return title.replace(\" \", \"_\").replace(\"/\", \"_\").replace(\"|\", \"\").replace(\",\", \"\")\n\n\n# Read the original CSV file\ncsv_file = Path(\"/Users/steven/Downloads/NeAt/Misc/reformatted_mydesigns - Sheet1.csv\")\ndf = pd.read_csv(csv_file)\n\n# Directory where images will be downloaded\nbase_dir = Path(\"/Users/steven/csv2/\")\nbase_dir.mkdir(exist_ok=True)",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "401",
    "name": "process.py",
    "path": "01_core_ai_analysis/data_processing/process.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 85,
    "size": 2241,
    "docstring": "process",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "configure_loggers",
      "handle_exception",
      "remove_old_log_files",
      "process"
    ],
    "classes": [],
    "imports": [
      "datetime",
      "logging",
      "os",
      "sys",
      "time",
      "ytdl.downloadupload",
      "ytdl.oshelper",
      "ytdl.ytdlconfiguration"
    ],
    "preview": "\"process\"\n\nimport datetime\nimport logging\nimport os\nimport sys\nimport time\n\nfrom ytdl.downloadupload import Downloadupload\nfrom ytdl.oshelper import absolute_files, mkdir, remove\nfrom ytdl.ytdlconfiguration import Ytdlconfiguration\n\n\ndef configure_loggers(config):\n    \"Configure logger\"\n\n    logging.getLogger(\"\").handlers = []\n\n    mkdir(config.log_folder)\n    logs_file_name = os.path.join(config.log_folder, str(datetime.date.today()) + \"process.log\")",
    "last_modified": "2025-09-13T05:54:15.204254"
  },
  {
    "id": "402",
    "name": "mamba_reset.py",
    "path": "01_core_ai_analysis/data_processing/mamba_reset.py",
    "category": "01_core_ai_analysis",
    "type": "setup",
    "lines": 437,
    "size": 16095,
    "docstring": "Complete Mamba/Miniforge Reset & Setup Script\nRemoves old conda/anaconda installations and sets up fresh Miniforge with Mamba.",
    "keywords": [
      "youtube",
      "organization"
    ],
    "functions": [
      "print_colored",
      "print_step",
      "run_command",
      "backup_important_files",
      "find_conda_installations",
      "remove_conda_installations",
      "clean_shell_configs",
      "download_miniforge",
      "install_miniforge",
      "initialize_mamba"
    ],
    "classes": [
      "Colors"
    ],
    "imports": [
      "os",
      "sys",
      "shutil",
      "subprocess",
      "platform",
      "pathlib",
      "datetime"
    ],
    "preview": "#!/usr/bin/env python3\n\"\"\"\nComplete Mamba/Miniforge Reset & Setup Script\nRemoves old conda/anaconda installations and sets up fresh Miniforge with Mamba.\n\"\"\"\n\nimport os\nimport sys\nimport shutil\nimport subprocess\nimport platform\nfrom pathlib import Path\nfrom datetime import datetime\n\nclass Colors:\n    HEADER = '\\033[95m'\n    BLUE = '\\033[94m'\n    CYAN = '\\033[96m'\n    GREEN = '\\033[92m'\n    YELLOW = '\\033[93m'",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "403",
    "name": "yt-playlist.py",
    "path": "01_core_ai_analysis/data_processing/yt-playlist.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 33,
    "size": 991,
    "docstring": "",
    "keywords": [
      "data_processing"
    ],
    "functions": [],
    "classes": [],
    "imports": [
      "pandas",
      "pytube"
    ],
    "preview": "import pandas as pd\nfrom pytube import Playlist\n\n# Replace 'YOUR_PLAYLIST_URL' with the URL of your YouTube playlist\nplaylist_url = (\n    \"https://www.youtube.com/playlist?list=PLfudK7D_bQIgAVsQUK5WtfVe3_kz9cXjA\"\n)\nplaylist = Playlist(playlist_url)\n\n# Dictionary to store video information\nvideos_info = {\n    \"Title\": [],\n    \"Video URL\": [],\n    \"Length (seconds)\": [],\n    \"Views\": [],\n    \"Thumbnail URL\": [],\n    \"Description\": [],\n}\n\n# Loop through all videos in the playlist",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "404",
    "name": "bubblespider_scraper.py",
    "path": "01_core_ai_analysis/data_processing/bubblespider_scraper.py",
    "category": "01_core_ai_analysis",
    "type": "web_tools",
    "lines": 34,
    "size": 1244,
    "docstring": "",
    "keywords": [],
    "functions": [
      "extract_product_info_to_csv"
    ],
    "classes": [],
    "imports": [
      "csv",
      "re"
    ],
    "preview": "import csv\nimport re\n\n\ndef extract_product_info_to_csv(input_file_path, output_csv_path):\n    with open(input_file_path, \"r\", encoding=\"utf-8\") as file:\n        content = file.read()\n\n    # Regular expression to match product titles and image URLs\n    pattern = re.compile(\n        r\"(.*?)\\n(https://images-na\\.ssl-images-amazon\\.com/images/W/MEDIAX_792452-T2/images/I/[^\\s]+\\.jpg)\",\n        re.DOTALL,\n    )\n\n    # Write the extracted data to a CSV file\n    with open(output_csv_path, \"w\", newline=\"\", encoding=\"utf-8\") as csvfile:\n        csvwriter = csv.writer(csvfile)\n        for match in pattern.finditer(content):\n            title = match.group(1).strip()\n            urls = re.findall(",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "405",
    "name": "miniforge-reset.py",
    "path": "01_core_ai_analysis/data_processing/miniforge-reset.py",
    "category": "01_core_ai_analysis",
    "type": "setup",
    "lines": 437,
    "size": 16095,
    "docstring": "Complete Mamba/Miniforge Reset & Setup Script\nRemoves old conda/anaconda installations and sets up fresh Miniforge with Mamba.",
    "keywords": [
      "youtube",
      "organization"
    ],
    "functions": [
      "print_colored",
      "print_step",
      "run_command",
      "backup_important_files",
      "find_conda_installations",
      "remove_conda_installations",
      "clean_shell_configs",
      "download_miniforge",
      "install_miniforge",
      "initialize_mamba"
    ],
    "classes": [
      "Colors"
    ],
    "imports": [
      "os",
      "sys",
      "shutil",
      "subprocess",
      "platform",
      "pathlib",
      "datetime"
    ],
    "preview": "#!/usr/bin/env python3\n\"\"\"\nComplete Mamba/Miniforge Reset & Setup Script\nRemoves old conda/anaconda installations and sets up fresh Miniforge with Mamba.\n\"\"\"\n\nimport os\nimport sys\nimport shutil\nimport subprocess\nimport platform\nfrom pathlib import Path\nfrom datetime import datetime\n\nclass Colors:\n    HEADER = '\\033[95m'\n    BLUE = '\\033[94m'\n    CYAN = '\\033[96m'\n    GREEN = '\\033[92m'\n    YELLOW = '\\033[93m'",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "406",
    "name": "eucjpprober.py",
    "path": "01_core_ai_analysis/data_processing/eucjpprober.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 103,
    "size": 3934,
    "docstring": "",
    "keywords": [],
    "functions": [
      "__init__",
      "reset",
      "charset_name",
      "language",
      "feed",
      "get_confidence"
    ],
    "classes": [
      "EUCJPProber"
    ],
    "imports": [
      "typing",
      "chardistribution",
      "codingstatemachine",
      "enums",
      "jpcntx",
      "mbcharsetprober",
      "mbcssm"
    ],
    "preview": "######################## BEGIN LICENSE BLOCK ########################\n# The Original Code is mozilla.org code.\n#\n# The Initial Developer of the Original Code is\n# Netscape Communications Corporation.\n# Portions created by the Initial Developer are Copyright (C) 1998\n# the Initial Developer. All Rights Reserved.\n#\n# Contributor(s):\n#   Mark Pilgrim - port to Python\n#\n# This library is free software; you can redistribute it and/or\n# modify it under the terms of the GNU Lesser General Public\n# License as published by the Free Software Foundation; either\n# version 2.1 of the License, or (at your option) any later version.\n#\n# This library is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU\n# Lesser General Public License for more details.",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "407",
    "name": "magickbg.py",
    "path": "01_core_ai_analysis/data_processing/magickbg.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 36,
    "size": 982,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "remove_background",
      "process_directory"
    ],
    "classes": [],
    "imports": [
      "os",
      "subprocess"
    ],
    "preview": "import os\nimport subprocess\n\n\ndef remove_background(input_path, output_path):\n    command = [\n        \"magick\",\n        input_path,\n        \"-fuzz\",\n        \"20%\",\n        \"-transparent\",\n        \"white\",\n        output_path,\n    ]\n    subprocess.run(command)\n\n\ndef process_directory(input_dir):\n    output_dir = os.path.join(input_dir, \"output\")\n    if not os.path.exists(output_dir):",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "408",
    "name": "idt copy.py",
    "path": "01_core_ai_analysis/data_processing/idt copy.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 34,
    "size": 1067,
    "docstring": "",
    "keywords": [
      "data_processing"
    ],
    "functions": [],
    "classes": [],
    "imports": [
      "os",
      "pandas"
    ],
    "preview": "import os\n\nimport pandas as pd\n\n# Load your YouTube video data\ncsv_path = \"/Users/steven/Downloads/Misc/ytube - youtube_videos.csv\"\ndf = pd.read_csv(csv_path)\n\n# Directory containing the downloaded thumbnails\nthumbnail_dir = \"/Users/steven/Downloads/Misc/Thumbnails/idT\"\n\n# Add a new column for the thumbnail path if it doesn't exist\nif \"Thumbnail Path\" not in df.columns:\n    df[\"Thumbnail Path\"] = \"\"\n\n# Iterate through the thumbnails in the directory\nfor filename in os.listdir(thumbnail_dir):\n    if filename.endswith(\".jpg\") or filename.endswith(\".png\"):\n        # Extract the video ID from the filename\n        video_id = filename.split(\".\")[0]",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "409",
    "name": "generate_song_csv (1).py",
    "path": "01_core_ai_analysis/data_processing/generate_song_csv (1).py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 94,
    "size": 3021,
    "docstring": "",
    "keywords": [
      "data_processing"
    ],
    "functions": [
      "prompt_for_files",
      "predefined_file_paths",
      "extract_song_details",
      "main"
    ],
    "classes": [],
    "imports": [
      "os",
      "pandas",
      "bs4"
    ],
    "preview": "import os\n\nimport pandas as pd\nfrom bs4 import BeautifulSoup\n\n\n# Option A: Prompt for file paths\ndef prompt_for_files():\n    print(\"Enter the file paths for the HTML files (comma-separated):\")\n    file_paths = input().split(\",\")\n    return [file.strip() for file in file_paths]\n\n\n# Option B: Predefined file paths\ndef predefined_file_paths():\n    return [\n        \"/Users/steven/Music/suno/1.html\",\n        \"/Users/steven/Music/suno/2.html\",\n        \"/Users/steven/Music/suno/3.html\",\n        \"/Users/steven/Music/suno/4.html\",",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "410",
    "name": "multi_platform_product_recommender.py",
    "path": "01_core_ai_analysis/data_processing/multi_platform_product_recommender.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 281,
    "size": 9315,
    "docstring": "",
    "keywords": [
      "opencv",
      "analysis"
    ],
    "functions": [
      "iminfo",
      "ocr_text",
      "keywords_from",
      "pick_product",
      "seo_and_tags",
      "process_dir",
      "main"
    ],
    "classes": [],
    "imports": [
      "csv",
      "os",
      "time",
      "datetime",
      "cv2",
      "numpy",
      "pytesseract",
      "PIL"
    ],
    "preview": "# filename: multi_platform_product_recommender.py\nimport csv\nimport os\nimport time\nfrom datetime import datetime\n\ntry:\n    import cv2\n    import numpy as np\n    import pytesseract\n    from PIL import Image, UnidentifiedImageError\nexcept Exception as e:\n    raise SystemExit(\"Please `pip install pillow opencv-python pytesseract numpy`\") from e\n\npytesseract.pytesseract.tesseract_cmd = \"/usr/local/bin/tesseract\"  # adjust if needed\n\nCSV_FIELDS = [\n    \"Platform\",\n    \"Folder\",\n    \"File\",",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "411",
    "name": "generate-info.py",
    "path": "01_core_ai_analysis/data_processing/generate-info.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 94,
    "size": 3021,
    "docstring": "",
    "keywords": [
      "data_processing"
    ],
    "functions": [
      "prompt_for_files",
      "predefined_file_paths",
      "extract_song_details",
      "main"
    ],
    "classes": [],
    "imports": [
      "os",
      "pandas",
      "bs4"
    ],
    "preview": "import os\n\nimport pandas as pd\nfrom bs4 import BeautifulSoup\n\n\n# Option A: Prompt for file paths\ndef prompt_for_files():\n    print(\"Enter the file paths for the HTML files (comma-separated):\")\n    file_paths = input().split(\",\")\n    return [file.strip() for file in file_paths]\n\n\n# Option B: Predefined file paths\ndef predefined_file_paths():\n    return [\n        \"/Users/steven/Music/suno/1.html\",\n        \"/Users/steven/Music/suno/2.html\",\n        \"/Users/steven/Music/suno/3.html\",\n        \"/Users/steven/Music/suno/4.html\",",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "412",
    "name": "myd-csv-download.py",
    "path": "01_core_ai_analysis/data_processing/myd-csv-download.py",
    "category": "01_core_ai_analysis",
    "type": "youtube",
    "lines": 97,
    "size": 3180,
    "docstring": "",
    "keywords": [
      "data_processing",
      "analysis",
      "youtube",
      "web_tools"
    ],
    "functions": [
      "ensure_directories",
      "download_image",
      "process_images",
      "write_log_to_csv",
      "main"
    ],
    "classes": [],
    "imports": [
      "os",
      "datetime",
      "pandas",
      "requests"
    ],
    "preview": "import os\nfrom datetime import datetime\n\nimport pandas as pd\nimport requests\n\n\n# Ensure that CSV and output directories exist based on the user prompt\ndef ensure_directories(base_dir):\n    output_dir = os.path.join(base_dir, \"processed_images\")\n    os.makedirs(output_dir, exist_ok=True)\n    return output_dir\n\n\n# Function to download an image\ndef download_image(url, filename):\n    try:\n        response = requests.get(url, timeout=10)\n        if response.status_code == 200:\n            with open(filename, \"wb\") as f:",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "413",
    "name": "deepseek_python_20250608130226.py",
    "path": "01_core_ai_analysis/data_processing/deepseek_python_20250608130226.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 497,
    "size": 17786,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "__init__",
      "get_system_info",
      "analyze",
      "analyze_file",
      "analyze_path_relationships",
      "get_file_metrics",
      "ast_analysis",
      "run_analysis_tools",
      "complexity_analysis",
      "update_summary"
    ],
    "classes": [
      "AdvancedPythonAnalyzer"
    ],
    "imports": [
      "ast",
      "csv",
      "datetime",
      "json",
      "os",
      "platform",
      "subprocess",
      "sys",
      "collections",
      "matplotlib.pyplot"
    ],
    "preview": "import ast\nimport csv\nimport datetime\nimport json\nimport os\nimport platform\nimport subprocess\nimport sys\nfrom collections import defaultdict\n\nimport matplotlib.pyplot as plt\nimport networkx as nx\nimport radon\nfrom pylint import epylint as lint\nfrom radon.complexity import cc_visit\nfrom radon.metrics import mi_visit\n\n\nclass AdvancedPythonAnalyzer:\n    def __init__(self, directory):",
    "last_modified": "2025-09-13T05:53:27.589738"
  },
  {
    "id": "414",
    "name": "big5prober.py",
    "path": "01_core_ai_analysis/data_processing/big5prober.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 48,
    "size": 1763,
    "docstring": "",
    "keywords": [],
    "functions": [
      "__init__",
      "charset_name",
      "language"
    ],
    "classes": [
      "Big5Prober"
    ],
    "imports": [
      "chardistribution",
      "codingstatemachine",
      "mbcharsetprober",
      "mbcssm"
    ],
    "preview": "######################## BEGIN LICENSE BLOCK ########################\n# The Original Code is Mozilla Communicator client code.\n#\n# The Initial Developer of the Original Code is\n# Netscape Communications Corporation.\n# Portions created by the Initial Developer are Copyright (C) 1998\n# the Initial Developer. All Rights Reserved.\n#\n# Contributor(s):\n#   Mark Pilgrim - port to Python\n#\n# This library is free software; you can redistribute it and/or\n# modify it under the terms of the GNU Lesser General Public\n# License as published by the Free Software Foundation; either\n# version 2.1 of the License, or (at your option) any later version.\n#\n# This library is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU\n# Lesser General Public License for more details.",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "415",
    "name": "sort-suno-regx.py",
    "path": "01_core_ai_analysis/data_processing/sort-suno-regx.py",
    "category": "01_core_ai_analysis",
    "type": "organization",
    "lines": 55,
    "size": 2013,
    "docstring": "",
    "keywords": [
      "data_processing"
    ],
    "functions": [],
    "classes": [],
    "imports": [
      "re",
      "pandas"
    ],
    "preview": "import re\n\nimport pandas as pd\n\n# Define the adjusted regex pattern\npattern = r'src=\"([^\"]+)\".*?title=\"([^\"]+)\".*?href=\"([^\"]+)\".*?href=\"([^\"]+)\">([^<]+)'\n\n# Prompt the user for HTML file paths\nprint(\"Enter the paths to your HTML files, separated by commas:\")\nfile_input = input(\"> \")\n\n# Convert the input into a list of file paths\nhtml_files = [path.strip() for path in file_input.split(\",\")]\n\n# Initialize a list to store the extracted results\nresults = []\n\n# Process each file to extract matches\nfor html_file in html_files:\n    try:",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "416",
    "name": "pods2.py",
    "path": "01_core_ai_analysis/data_processing/pods2.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 96,
    "size": 3263,
    "docstring": "",
    "keywords": [],
    "functions": [
      "extract_sections",
      "compare_sections",
      "merge_files"
    ],
    "classes": [],
    "imports": [
      "os",
      "re",
      "difflib"
    ],
    "preview": "import os\nimport re\nfrom difflib import unified_diff\n\n# Files to merge and analyze\nfiles_to_merge = [\n    \"/Users/steven/Documents/podcast/14-16-45-Podcast_Production_Assistance copy 2.md\",\n    \"/Users/steven/Documents/podcast/ChatGPT-Project_2025_Imagery_Design.html\",\n    \"/Users/steven/Documents/podcast/14-16-45-Podcast_Production_Assistance copy.md\",\n    \"/Users/steven/Documents/podcast/Content Plan & Strategy.md\",\n    \"/Users/steven/Documents/podcast/JusticeThomas.md\",\n    \"/Users/steven/Documents/podcast/podcast-palyerzs.md\",\n    \"/Users/steven/Documents/podcast/Podcast-Trump.md\",\n    \"/Users/steven/Documents/podcast/Transition from Donald Trump to The Messiah of Mar-a-Lago.md\",\n]\n\n# Output file path\noutput_file = \"/Users/steven/Documents/podcast/merged_unique_output.md\"\n\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "417",
    "name": "similar.py",
    "path": "01_core_ai_analysis/data_processing/similar.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 20,
    "size": 817,
    "docstring": "",
    "keywords": [],
    "functions": [
      "merge_py_files"
    ],
    "classes": [],
    "imports": [
      "os",
      "pathlib"
    ],
    "preview": "import os\nfrom pathlib import Path\n\ndef merge_py_files(file_paths, output_path):\n    merged_content = []\n    for file_path in file_paths:\n        with open(file_path, 'r', encoding='utf-8') as f:\n            merged_content.extend(f.readlines())\n    merged_content = list(dict.fromkeys(merged_content))  # Remove duplicate lines\n    with open(output_path, 'w', encoding='utf-8') as out:\n        out.writelines(merged_content)\n    print(f\"Merged {file_paths} into {output_path}\")\n\n# Example: Merge analyze*.py files\nfiles_to_merge = [\n    \"/Users/steven/Documents/python/sphinx-docs/analyze 1.py\",\n    \"/Users/steven/Documents/python/sphinx-docs/analyze 2.py\",\n    \"/Users/steven/Documents/python/sphinx-docs/analyze 6.py\"\n]\nmerge_py_files(files_to_merge, \"/Users/steven/Documents/python/sphinx-docs/analyze_merged.py\")",
    "last_modified": "2025-10-08T06:50:53"
  },
  {
    "id": "418",
    "name": "subject_tracker (1).py",
    "path": "01_core_ai_analysis/data_processing/subject_tracker (1).py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 209,
    "size": 7148,
    "docstring": "Subject-aware reframing using face/pose detection for intelligent cropping",
    "keywords": [
      "opencv",
      "analysis"
    ],
    "functions": [
      "generate_smooth_path",
      "__init__",
      "track_segment",
      "_extract_frame",
      "_detect_faces",
      "__init__",
      "init_tracker",
      "update"
    ],
    "classes": [
      "SubjectTracker",
      "MotionTracker"
    ],
    "imports": [
      "__future__",
      "os",
      "subprocess",
      "tempfile",
      "typing",
      "cv2",
      "numpy"
    ],
    "preview": "\"\"\"\nSubject-aware reframing using face/pose detection for intelligent cropping\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nimport subprocess\nimport tempfile\nfrom typing import Any, Dict, List, Optional, Tuple\n\nimport cv2\nimport numpy as np\n\n\nclass SubjectTracker:\n    def __init__(self):\n        # Initialize face detection\n        try:\n            self.face_cascade = cv2.CascadeClassifier(",
    "last_modified": "2025-09-13T05:55:10.405594"
  },
  {
    "id": "419",
    "name": "johabprober.py",
    "path": "01_core_ai_analysis/data_processing/johabprober.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 48,
    "size": 1752,
    "docstring": "",
    "keywords": [],
    "functions": [
      "__init__",
      "charset_name",
      "language"
    ],
    "classes": [
      "JOHABProber"
    ],
    "imports": [
      "chardistribution",
      "codingstatemachine",
      "mbcharsetprober",
      "mbcssm"
    ],
    "preview": "######################## BEGIN LICENSE BLOCK ########################\n# The Original Code is mozilla.org code.\n#\n# The Initial Developer of the Original Code is\n# Netscape Communications Corporation.\n# Portions created by the Initial Developer are Copyright (C) 1998\n# the Initial Developer. All Rights Reserved.\n#\n# Contributor(s):\n#   Mark Pilgrim - port to Python\n#\n# This library is free software; you can redistribute it and/or\n# modify it under the terms of the GNU Lesser General Public\n# License as published by the Free Software Foundation; either\n# version 2.1 of the License, or (at your option) any later version.\n#\n# This library is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU\n# Lesser General Public License for more details.",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "420",
    "name": "my_csv_tools.py",
    "path": "01_core_ai_analysis/data_processing/my_csv_tools.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 27,
    "size": 627,
    "docstring": "",
    "keywords": [
      "data_processing"
    ],
    "functions": [
      "load_excel",
      "check_missing_data"
    ],
    "classes": [],
    "imports": [
      "pandas"
    ],
    "preview": "import pandas as pd\n\n\ndef load_excel(file_path):\n    try:\n        df = pd.read_excel(file_path)\n        print(\"Excel file loaded successfully.\")\n        return df\n    except Exception as e:\n        print(f\"Error loading Excel file: {e}\")\n        return None\n\n\ndef check_missing_data(df):\n    if df is not None:\n        print(\"Missing data in each column:\")\n        print(df.isnull().sum())\n    else:\n        print(\"DataFrame is empty or not loaded.\")\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "421",
    "name": "GmailBot.py",
    "path": "01_core_ai_analysis/data_processing/GmailBot.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 995,
    "size": 37149,
    "docstring": "",
    "keywords": [
      "data_processing",
      "analysis",
      "youtube",
      "web_tools"
    ],
    "functions": [
      "send_delayed_keys",
      "randomize",
      "check_proxy",
      "load_proxy",
      "download_driver",
      "get_driver",
      "quit_driver",
      "click_whatever",
      "check_username_taken",
      "create_database"
    ],
    "classes": [
      "bcolors"
    ],
    "imports": [
      "json",
      "logging",
      "os",
      "platform",
      "random",
      "re",
      "sqlite3",
      "string",
      "sys",
      "time"
    ],
    "preview": "import json\nimport logging\nimport os\nimport platform\nimport random\nimport re\nimport sqlite3\nimport string\nimport sys\nimport time\nimport zipfile\nfrom contextlib import closing\nfrom datetime import datetime\nfrom random import choice, choices, randint, uniform\n\nimport pandas as pd\nimport requests\nimport xlrd\nfrom fake_headers import Headers, browsers\nfrom selenium import webdriver",
    "last_modified": "2025-09-13T05:54:13.699296"
  },
  {
    "id": "422",
    "name": "finetune.py",
    "path": "01_core_ai_analysis/data_processing/finetune.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 368,
    "size": 12752,
    "docstring": "",
    "keywords": [],
    "functions": [
      "load_model",
      "finetune",
      "inference",
      "save_model",
      "check_if_huggingface_model_exists",
      "format_samples_sft",
      "format_samples_dpo"
    ],
    "classes": [],
    "imports": [
      "argparse",
      "os",
      "pathlib",
      "unsloth",
      "typing",
      "torch",
      "datasets",
      "huggingface_hub",
      "huggingface_hub.utils",
      "transformers"
    ],
    "preview": "import argparse\nimport os\nfrom pathlib import Path\n\nfrom unsloth import PatchDPOTrainer\n\nPatchDPOTrainer()\n\nfrom typing import Any, List, Literal, Optional  # noqa: E402\n\nimport torch  # noqa\nfrom datasets import concatenate_datasets, load_dataset  # noqa: E402\nfrom huggingface_hub import HfApi  # noqa: E402\nfrom huggingface_hub.utils import RepositoryNotFoundError  # noqa: E402\nfrom transformers import TextStreamer, TrainingArguments  # noqa: E402\nfrom trl import DPOConfig, DPOTrainer, SFTTrainer  # noqa: E402\nfrom unsloth import FastLanguageModel, is_bfloat16_supported  # noqa: E402\nfrom unsloth.chat_templates import get_chat_template  # noqa: E402\n\nalpaca_template = \"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.",
    "last_modified": "2025-09-13T05:53:42.427738"
  },
  {
    "id": "423",
    "name": "download_yt-videos.py",
    "path": "01_core_ai_analysis/data_processing/download_yt-videos.py",
    "category": "01_core_ai_analysis",
    "type": "youtube",
    "lines": 40,
    "size": 894,
    "docstring": "",
    "keywords": [
      "data_processing"
    ],
    "functions": [],
    "classes": [],
    "imports": [
      "os",
      "subprocess",
      "pandas"
    ],
    "preview": "import os\nimport subprocess\n\nimport pandas as pd\n\n# Path to your CSV file\ncsv_file_path = \"/Users/steven/Movies/SEO/rebrandy-vidiq-2025-04-29.csv\"\n\n# Load CSV data\ndf = pd.read_csv(csv_file_path)\n\n# Create output directory\noutput_dir = \"/Users/steven/Movies/SEO\"\nos.makedirs(output_dir, exist_ok=True)\n\n# Column in CSV with URLs (adjust if needed)\nurl_column = \"URL\"\n\n# yt-dlp format string for ~480-600p range\nformat_str = \"bestvideo[height<=600]+bestaudio/best[height<=600]\"",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "424",
    "name": "csv2.py",
    "path": "01_core_ai_analysis/data_processing/csv2.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 67,
    "size": 2403,
    "docstring": "",
    "keywords": [
      "data_processing",
      "web_tools"
    ],
    "functions": [],
    "classes": [],
    "imports": [
      "pathlib",
      "pandas",
      "requests"
    ],
    "preview": "from pathlib import Path\n\nimport pandas as pd\nimport requests\n\n# Corrected CSV file path\ncsv_file = Path(\"/Users/steven/Downloads/NeAt/Misc/reformatted_mydesigns - Sheet1.csv\")\n\ndf = pd.read_csv(csv_file)\n\n# Base directory to save images and info\nbase_dir = Path(\"/Users/steven/csv2/\")\nbase_dir.mkdir(exist_ok=True)\n\n# Iterate through each row of the CSV and process the images and info\nfor index, row in df.iterrows():\n    title = row[\"TITLE\"]\n    description = row[\"DESCRIPTION\"]\n    tags = row[\"TAGS\"]\n    language = row.get(\"LANGUAGE\", \"EN\")  # Assuming 'EN' as default if not specified",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "425",
    "name": "create_csv_from_json-leonardo.py",
    "path": "01_core_ai_analysis/data_processing/create_csv_from_json-leonardo.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 75,
    "size": 2203,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "process_json_to_csv",
      "combine_json_to_csv"
    ],
    "classes": [],
    "imports": [
      "csv",
      "gzip",
      "json",
      "os"
    ],
    "preview": "import csv\nimport gzip\nimport json\nimport os\n\n# Configuration\nOUTPUT_DIR = \"/Users/steven/Pictures/leodowns\"\nCSV_FILE = os.path.join(OUTPUT_DIR, \"leonardo.csv\")\n\n# Headers for CSV\nHEADERS = [\n    \"id\",\n    \"prompt\",\n    \"negativePrompt\",\n    \"motionStrength\",\n    \"createdAt\",\n    \"image_url\",\n    \"motion_url\",\n    \"local_image_path\",\n    \"local_motion_path\",",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "426",
    "name": "organize_files.py",
    "path": "01_core_ai_analysis/data_processing/organize_files.py",
    "category": "01_core_ai_analysis",
    "type": "organization",
    "lines": 107,
    "size": 3312,
    "docstring": "",
    "keywords": [
      "analysis",
      "organization"
    ],
    "functions": [
      "get_creation_date",
      "custom_tags",
      "organize_files",
      "tag_files",
      "create_database",
      "insert_metadata"
    ],
    "classes": [],
    "imports": [
      "os",
      "shutil",
      "sqlite3",
      "datetime"
    ],
    "preview": "import os\nimport shutil\nimport sqlite3\nfrom datetime import datetime\n\n\ndef get_creation_date(filepath):\n    \"\"\"Get the creation date of the file.\"\"\"\n    return datetime.fromtimestamp(os.path.getctime(filepath)).strftime(\"%Y-%m-%d %H:%M:%S\")\n\n\ndef custom_tags(filename, filepath):\n    \"\"\"Determine custom tags based on file content or filename patterns.\"\"\"\n    custom_tag = None\n    if filename.endswith(\".py\"):\n        with open(filepath, \"r\") as file:\n            content = file.read()\n            if \"import pandas\" in content:\n                custom_tag = \"python_data_analysis\"\n            elif \"import tensorflow\" in content:",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "427",
    "name": "backupcsv.py",
    "path": "01_core_ai_analysis/data_processing/backupcsv.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 69,
    "size": 2237,
    "docstring": "",
    "keywords": [
      "data_processing",
      "analysis"
    ],
    "functions": [
      "generate_dry_run_csv",
      "is_excluded"
    ],
    "classes": [],
    "imports": [
      "os",
      "re",
      "subprocess",
      "pandas"
    ],
    "preview": "import os\nimport re\nimport subprocess\n\nimport pandas as pd\n\n\ndef generate_dry_run_csv(directories, csv_path):\n    rows = []\n\n    # Regex patterns for exclusions\n    excluded_patterns = [\n        r\"^\\..*\",  # Hidden files and directories\n        r\".*/venv/.*\",  # venv directories\n        r\".*/\\.venv/.*\",  # .venv directories\n        r\".*/my_global_venv/.*\",  # my_global_venv directories\n        r\".*/simplegallery/.*\",\n        r\".*/avatararts/.*\",\n        r\".*/github/.*\",\n        r\".*/Documents/gitHub/.*\",  # Specific gitHub directory",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "428",
    "name": "re.py",
    "path": "01_core_ai_analysis/data_processing/re.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 52,
    "size": 1731,
    "docstring": "",
    "keywords": [
      "data_processing",
      "web_tools"
    ],
    "functions": [
      "sanitize_title"
    ],
    "classes": [],
    "imports": [
      "pathlib",
      "pandas",
      "requests"
    ],
    "preview": "from pathlib import Path\n\nimport pandas as pd\nimport requests\n\n\ndef sanitize_title(title):\n    # Sanitize the title to create a valid file name\n    return title.replace(\" \", \"_\").replace(\"/\", \"_\").replace(\"|\", \"\").replace(\",\", \"\")\n\n\n# Read the original CSV file\ncsv_file = Path(\"/Users/steven/Downloads/NeAt/Misc/reformatted_mydesigns - Sheet1.csv\")\ndf = pd.read_csv(csv_file)\n\n# Directory where images will be downloaded\nbase_dir = Path(\"/Users/steven/csv2/\")\nbase_dir.mkdir(exist_ok=True)\n\n# Process each row in the DataFrame",
    "last_modified": "2025-05-04T22:47:12"
  },
  {
    "id": "429",
    "name": "YTubeDLthumbs copy.py",
    "path": "01_core_ai_analysis/data_processing/YTubeDLthumbs copy.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 58,
    "size": 1913,
    "docstring": "",
    "keywords": [
      "data_processing",
      "web_tools"
    ],
    "functions": [
      "fetch_video_details"
    ],
    "classes": [],
    "imports": [
      "os",
      "pandas",
      "requests",
      "googleapiclient.discovery"
    ],
    "preview": "import os\n\nimport pandas as pd  # This line is necessary to use pandas in your script\nimport requests\nfrom googleapiclient.discovery import build\n\n# Initialize YouTube API\napi_key = \"AIzaSyCyfGm1oF2syfzfsPeCRHb10tMKmC0dbCo\"\nyoutube = build(\"youtube\", \"v3\", developerKey=api_key)\n\n# Define paths\ncsv_path = \"/Users/steven/Downloads/Misc/ytube - youtube_videos.csv\"\nthumbnail_dir = \"/Users/steven/Downloads/Misc/Thumbnails/d2\"\nos.makedirs(thumbnail_dir, exist_ok=True)\n\n# Load CSV\ndf = pd.read_csv(csv_path)\n\n# Function to fetch video details and download thumbnail\n",
    "last_modified": "2025-09-13T05:54:13.128583"
  },
  {
    "id": "430",
    "name": "test_environment.py",
    "path": "01_core_ai_analysis/data_processing/test_environment.py",
    "category": "01_core_ai_analysis",
    "type": "testing",
    "lines": 16,
    "size": 405,
    "docstring": "",
    "keywords": [
      "data_processing"
    ],
    "functions": [],
    "classes": [],
    "imports": [
      "sys",
      "librosa",
      "pandas",
      "plotly.express",
      "sklearn.linear_model"
    ],
    "preview": "# test_environment.py\nimport sys\n\nimport librosa\nimport pandas as pd\nimport plotly.express as px\nfrom sklearn.linear_model import LinearRegression\n\nprint(f\"Python version: {sys.version}\")\nprint(f\"Pandas version: {pd.__version__}\")\nprint(f\"Librosa version: {librosa.__version__}\")\n\n# Test audio analysis\ny, sr = librosa.load(librosa.ex(\"trumpet\"))\nprint(f\"Audio sample loaded: {len(y)} samples at {sr}Hz\")\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "431",
    "name": "dblsort.py",
    "path": "01_core_ai_analysis/data_processing/dblsort.py",
    "category": "01_core_ai_analysis",
    "type": "organization",
    "lines": 25,
    "size": 1140,
    "docstring": "",
    "keywords": [
      "data_processing"
    ],
    "functions": [],
    "classes": [],
    "imports": [
      "pandas",
      "ace_tools"
    ],
    "preview": "import pandas as pd\n\n# Load the CSV file\nfile_path = \"/Users/steven/Documents/Python/fdupes/detailed_duplicate_report.csv\"\nduplicate_report = pd.read_csv(file_path)\n\n# Sort by Duplicate Count in descending order\nsorted_by_duplicates = duplicate_report.sort_values(by=\"Duplicate Count\", ascending=False)\n\n# Filter to show only duplicates (where Duplicate Count > 1)\nonly_duplicates = sorted_by_duplicates[sorted_by_duplicates[\"Duplicate Count\"] > 1]\n\n# Sort by File Size (assuming sizes are in KB and removing the KB text for sorting)\nduplicate_report[\"File Size\"] = duplicate_report[\"File Size\"].str.replace(\" KB\", \"\").astype(float)\nsorted_by_size = duplicate_report.sort_values(by=\"File Size\", ascending=False)\n\n# Display the sorted and filtered dataframes to the user\nimport ace_tools as tools\n\ntools.display_dataframe_to_user(name=\"Sorted by Duplicate Count\", dataframe=sorted_by_duplicates)",
    "last_modified": "2025-09-13T05:54:29.167088"
  },
  {
    "id": "432",
    "name": "ClipHandler.py",
    "path": "01_core_ai_analysis/data_processing/ClipHandler.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 194,
    "size": 8089,
    "docstring": "",
    "keywords": [
      "youtube",
      "web_tools"
    ],
    "functions": [
      "__init__",
      "get_clips",
      "is_ingame_clip",
      "get_game_clips",
      "is_required_length",
      "handle_response_data",
      "download_clip",
      "filter_func"
    ],
    "classes": [
      "ClipHandler"
    ],
    "imports": [
      "logging",
      "os",
      "pathlib",
      "numpy",
      "requests",
      "streamlink",
      "tensorflow",
      "moviepy.editor",
      "src",
      "src.APIHandler"
    ],
    "preview": "import logging\nimport os\nfrom pathlib import Path\n\nimport numpy as np\nimport requests\nimport streamlink\nimport tensorflow as tf\nfrom moviepy.editor import VideoFileClip\nfrom src import utils\nfrom src.APIHandler import APIHandler\nfrom src.Clip import Clip\nfrom src.MetadataHandler import MetadataHandler\n\nimport config\n\n\nclass ClipHandler:\n    clips = []\n    retries = 3",
    "last_modified": "2025-09-13T05:53:45.547250"
  },
  {
    "id": "433",
    "name": "reportbot.py",
    "path": "01_core_ai_analysis/data_processing/reportbot.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 197,
    "size": 5134,
    "docstring": "",
    "keywords": [
      "video_processing"
    ],
    "functions": [
      "clear_screen",
      "chunks",
      "profile_attack_process",
      "video_attack_process",
      "video_attack",
      "profile_attack",
      "main"
    ],
    "classes": [],
    "imports": [
      "os",
      "os",
      "sys",
      "libs.check_modules",
      "os",
      "sys",
      "multiprocessing",
      "os",
      "time",
      "colorama"
    ],
    "preview": "import os\n\n\ndef clear_screen():\n    os.system(\"cls\")\n\n\nclear_screen()\n\nfrom os import _exit\nfrom sys import exit\n\nfrom libs.check_modules import check_modules\n\ncheck_modules()\n\nimport os\nimport sys\nfrom multiprocessing import Process\nfrom os import path",
    "last_modified": "2025-09-13T05:53:30.261669"
  },
  {
    "id": "434",
    "name": "organizer.py",
    "path": "01_core_ai_analysis/data_processing/organizer.py",
    "category": "01_core_ai_analysis",
    "type": "organization",
    "lines": 51,
    "size": 1506,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "process_files",
      "process_audio"
    ],
    "classes": [],
    "imports": [
      "argparse",
      "pathlib",
      "utils"
    ],
    "preview": "#!/usr/bin/env python3\nimport argparse\nfrom pathlib import Path\n\nfrom utils import EXCLUSIONS, SETTINGS, FileOrganizer\n\n\ndef process_files(file_type: str, directories: List[Path]):\n    processor = {\n        \"audio\": process_audio,\n        \"video\": process_video,\n        \"image\": process_image,\n        \"documents\": process_docs,\n        \"other\": process_other,\n    }[file_type]\n\n    results = []\n    for directory in directories:\n        for path in directory.rglob(\"*\"):\n            if path.is_file() and not FileOrganizer.should_exclude(path):",
    "last_modified": "2025-09-06T12:24:11.731941"
  },
  {
    "id": "435",
    "name": "winterm.py",
    "path": "01_core_ai_analysis/data_processing/winterm.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 200,
    "size": 7095,
    "docstring": "",
    "keywords": [],
    "functions": [
      "enable_vt_processing",
      "__init__",
      "get_attrs",
      "set_attrs",
      "reset_all",
      "fore",
      "back",
      "style",
      "set_console",
      "get_position"
    ],
    "classes": [
      "WinColor",
      "WinStyle",
      "WinTerm"
    ],
    "imports": [
      "msvcrt"
    ],
    "preview": "# Copyright Jonathan Hartley 2013. BSD 3-Clause license, see LICENSE file.\ntry:\n    from msvcrt import get_osfhandle\nexcept ImportError:\n\n    def get_osfhandle(_):\n        raise OSError(\"This isn't windows!\")\n\n\nfrom . import win32\n\n\n# from wincon.h\nclass WinColor(object):\n    BLACK = 0\n    BLUE = 1\n    GREEN = 2\n    CYAN = 3\n    RED = 4\n    MAGENTA = 5",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "436",
    "name": "sorts.py",
    "path": "01_core_ai_analysis/data_processing/sorts.py",
    "category": "01_core_ai_analysis",
    "type": "organization",
    "lines": 287,
    "size": 9050,
    "docstring": "",
    "keywords": [
      "data_processing",
      "analysis"
    ],
    "functions": [
      "is_excluded",
      "compute_md5",
      "normalize_file_size",
      "get_creation_date",
      "get_image_metadata",
      "format_file_size",
      "generate_detailed_duplicate_report",
      "prompt_for_csv_files"
    ],
    "classes": [],
    "imports": [
      "csv",
      "hashlib",
      "os",
      "re",
      "collections",
      "datetime",
      "pandas",
      "PIL"
    ],
    "preview": "import csv\nimport hashlib\nimport os\nimport re\nfrom collections import defaultdict\nfrom datetime import datetime\n\nimport pandas as pd\nfrom PIL import Image\n\n\ndef is_excluded(path, patterns):\n    \"\"\"\n    Check if a given path matches any of the exclusion patterns.\n\n    Parameters:\n    path (str): The path to check.\n    patterns (list): A list of regex patterns for exclusion.\n\n    Returns:",
    "last_modified": "2025-09-13T05:54:29.675982"
  },
  {
    "id": "437",
    "name": "check-python.py",
    "path": "01_core_ai_analysis/data_processing/check-python.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 57,
    "size": 1661,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "check_with_pylint",
      "check_with_flake8",
      "process_directory"
    ],
    "classes": [],
    "imports": [
      "os",
      "subprocess"
    ],
    "preview": "import os\nimport subprocess\n\n\ndef check_with_pylint(file_path):\n    \"\"\"\n    Run pylint on the given Python file to check for errors and style issues.\n\n    :param file_path: Path to the Python file.\n    :return: None\n    \"\"\"\n    result = subprocess.run([\"pylint\", file_path], capture_output=True, text=True)\n    if result.returncode != 0:\n        print(f\"Pylint issues in {file_path}:\")\n        print(result.stdout)\n    else:\n        print(f\"No pylint issues found in {file_path}\")\n\n\ndef check_with_flake8(file_path):",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "438",
    "name": "unpacking.py",
    "path": "01_core_ai_analysis/data_processing/unpacking.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 257,
    "size": 8820,
    "docstring": "Utilities related archives.",
    "keywords": [],
    "functions": [
      "current_umask",
      "split_leading_dir",
      "has_leading_dir",
      "is_within_directory",
      "set_extracted_file_to_default_mode_plus_executable",
      "zip_item_is_executable",
      "unzip_file",
      "untar_file",
      "unpack_file"
    ],
    "classes": [],
    "imports": [
      "logging",
      "os",
      "shutil",
      "stat",
      "tarfile",
      "zipfile",
      "typing",
      "zipfile",
      "pip._internal.exceptions",
      "pip._internal.utils.filetypes"
    ],
    "preview": "\"\"\"Utilities related archives.\"\"\"\n\nimport logging\nimport os\nimport shutil\nimport stat\nimport tarfile\nimport zipfile\nfrom typing import Iterable, List, Optional\nfrom zipfile import ZipInfo\n\nfrom pip._internal.exceptions import InstallationError\nfrom pip._internal.utils.filetypes import (\n    BZ2_EXTENSIONS,\n    TAR_EXTENSIONS,\n    XZ_EXTENSIONS,\n    ZIP_EXTENSIONS,\n)\nfrom pip._internal.utils.misc import ensure_dir\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "439",
    "name": "autofix.py",
    "path": "01_core_ai_analysis/data_processing/autofix.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 89,
    "size": 2796,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "create_backup",
      "apply_autopep8",
      "run_pylint",
      "process_directory"
    ],
    "classes": [],
    "imports": [
      "os",
      "shutil",
      "subprocess",
      "datetime"
    ],
    "preview": "import os\nimport shutil\nimport subprocess\nfrom datetime import datetime\n\n\ndef create_backup(file_path, backup_dir):\n    \"\"\"\n    Create a backup of the given file in the specified backup directory.\n\n    :param file_path: Path to the original file.\n    :param backup_dir: Path to the backup directory.\n    :return: None\n    \"\"\"\n    os.makedirs(backup_dir, exist_ok=True)\n    file_name = os.path.basename(file_path)\n    backup_path = os.path.join(backup_dir, file_name)\n    shutil.copy(file_path, backup_path)\n    print(f\"Backup created for {file_path} at {backup_path}\")\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "440",
    "name": "merger.py",
    "path": "01_core_ai_analysis/data_processing/merger.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 21,
    "size": 818,
    "docstring": "",
    "keywords": [],
    "functions": [
      "merge_py_files"
    ],
    "classes": [],
    "imports": [
      "os",
      "pathlib"
    ],
    "preview": "import os\nfrom pathlib import Path\n\ndef merge_py_files(file_paths, output_path):\n    merged_content = []\n    for file_path in file_paths:\n        with open(file_path, 'r', encoding='utf-8') as f:\n            merged_content.extend(f.readlines())\n    merged_content = list(dict.fromkeys(merged_content))  # Remove duplicate lines\n    with open(output_path, 'w', encoding='utf-8') as out:\n        out.writelines(merged_content)\n    print(f\"Merged {file_paths} into {output_path}\")\n\n# Example: Merge analyze*.py files\nfiles_to_merge = [\n    \"/Users/steven/Documents/python/sphinx-docs/analyze 1.py\",\n    \"/Users/steven/Documents/python/sphinx-docs/analyze 2.py\",\n    \"/Users/steven/Documents/python/sphinx-docs/analyze 6.py\"\n]\nmerge_py_files(files_to_merge, \"/Users/steven/Documents/python/sphinx-docs/analyze_merged.py\")",
    "last_modified": "2025-10-08T06:39:52.518525"
  },
  {
    "id": "441",
    "name": "csv_from_json2.py",
    "path": "01_core_ai_analysis/data_processing/csv_from_json2.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 72,
    "size": 2339,
    "docstring": "",
    "keywords": [
      "data_processing"
    ],
    "functions": [],
    "classes": [],
    "imports": [
      "json",
      "os",
      "glob",
      "pandas",
      "natsort"
    ],
    "preview": "import json\nimport os\nfrom glob import glob\n\nimport pandas as pd\nfrom natsort import natsorted\n\n# Set your root directory containing the JSON files\nROOT_DIR = \"/Users/steven/Documents/Conversation_JSONs/\"  # Adjust as needed\n\n# Find all JSON files recursively and sort them naturally\njson_files = glob(os.path.join(ROOT_DIR, \"**/*.json\"), recursive=True)\njson_files = natsorted(json_files)\n\n# Define the headers for the CSV\nHEADERS = [\"Title\", \"Node ID\", \"Parent\", \"Children\", \"Author Role\", \"Content\", \"Status\"]\n\ndata_list = []\n\nfor file in json_files:",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "442",
    "name": "pods.py",
    "path": "01_core_ai_analysis/data_processing/pods.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 84,
    "size": 3213,
    "docstring": "",
    "keywords": [],
    "functions": [
      "extract_sections",
      "get_unique_sections",
      "merge_files"
    ],
    "classes": [],
    "imports": [
      "os",
      "re",
      "difflib"
    ],
    "preview": "import os\nimport re\nfrom difflib import unified_diff\n\n# List of files to merge\nfiles_to_merge = [\n    \"/Users/steven/Documents/podcast/14-16-45-Podcast_Production_Assistance copy 2.md\",\n    \"/Users/steven/Documents/podcast/ChatGPT-Project_2025_Imagery_Design.html\",\n    \"/Users/steven/Documents/podcast/14-16-45-Podcast_Production_Assistance copy.md\",\n    \"/Users/steven/Documents/podcast/Content Plan & Strategy.md\",\n    \"/Users/steven/Documents/podcast/JusticeThomas.md\",\n    \"/Users/steven/Documents/podcast/podcast-palyerzs.md\",\n    \"/Users/steven/Documents/podcast/Podcast-Trump.md\",\n    \"/Users/steven/Documents/podcast/Transition from Donald Trump to The Messiah of Mar-a-Lago.md\",\n]\n\n# Output file\noutput_file = \"/Users/steven/Documents/podcast/merged_unique_output.md\"\n\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "443",
    "name": "analyze.py.py",
    "path": "01_core_ai_analysis/analyze.py_consolidated/analyze.py.py",
    "category": "01_core_ai_analysis",
    "type": "analysis",
    "lines": 57,
    "size": 1994,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis",
      "transcription"
    ],
    "functions": [
      "transcribe_audio",
      "analyze_text"
    ],
    "classes": [],
    "imports": [
      "openai",
      "os",
      "dotenv",
      "sys"
    ],
    "preview": "from openai import OpenAI\n\nclient = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\nimport os\n\nfrom dotenv import load_dotenv\n\n# Load environment variables from .env\nload_dotenv()\n\n\n# Function to transcribe audio using OpenAI Whisper\ndef transcribe_audio(file_path):\n    with open(file_path, \"rb\") as audio_file:\n        transcript = client.audio.transcribe(\"whisper-1\", audio_file)\n        return transcript.text\n\n\n# Function to analyze the transcript using GPT\ndef analyze_text(text):",
    "last_modified": "2025-05-04T22:47:11"
  },
  {
    "id": "444",
    "name": "analyze.py_02.py",
    "path": "01_core_ai_analysis/analyze.py_consolidated/analyze.py_02.py",
    "category": "01_core_ai_analysis",
    "type": "analysis",
    "lines": 42,
    "size": 1099,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis"
    ],
    "functions": [
      "analyze_text"
    ],
    "classes": [],
    "imports": [
      "openai",
      "os",
      "dotenv",
      "sys"
    ],
    "preview": "from openai import OpenAI\n\nclient = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\nimport os\n\nfrom dotenv import load_dotenv\n\n# Load environment variables from .env\nload_dotenv()\n\n\ndef analyze_text(text):\n    response = client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",  # Ensure you're using a chat model like gpt-3.5-turbo\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": \"You are a helpful assistant that analyzes song lyrics.\",\n            },\n            {",
    "last_modified": "2025-05-04T22:47:11"
  },
  {
    "id": "445",
    "name": "analyzer.py_02.py",
    "path": "01_core_ai_analysis/analyzer.py_consolidated/analyzer.py_02.py",
    "category": "01_core_ai_analysis",
    "type": "analysis",
    "lines": 42,
    "size": 1080,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis"
    ],
    "functions": [
      "analyze_text"
    ],
    "classes": [],
    "imports": [
      "os",
      "openai",
      "dotenv",
      "sys"
    ],
    "preview": "import os\n\nimport openai\nfrom dotenv import load_dotenv\n\n# Load environment variables from .env\nload_dotenv()\n\nopenai.api_key = os.getenv(\"OPENAI_API_KEY\")\n\n\ndef analyze_text(text):\n    response = openai.ChatCompletion.create(\n        model=\"gpt-3.5-turbo\",  # Ensure you're using a chat model like gpt-3.5-turbo\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": \"You are a helpful assistant that analyzes song lyrics.\",\n            },\n            {",
    "last_modified": "2025-05-06T04:35:14"
  },
  {
    "id": "446",
    "name": "analyzer.py.py",
    "path": "01_core_ai_analysis/analyzer.py_consolidated/analyzer.py.py",
    "category": "01_core_ai_analysis",
    "type": "analysis",
    "lines": 60,
    "size": 3430,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis"
    ],
    "functions": [
      "analyze_text"
    ],
    "classes": [],
    "imports": [
      "shared.config",
      "openai",
      "os"
    ],
    "preview": "from shared.config import *\nfrom openai import OpenAI\n\nclient = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\nimport os\n\n# Load environment variables from .env\nenv_path = os.path.expanduser(\"~/.env\")\n# Update the path if needed\nload_dotenv(dotenv_path=env_path)\n\n# Error checking for openai API key\nif not openai.api_key:\n    raise EnvironmentError(\"openai API key not found. Please check your .env file.\")\n\n\ndef analyze_text(text):\n    response = client.chat.completions.create(\n        model=\"gpt-4o\",\n        messages=[",
    "last_modified": "2025-10-09T05:27:15.570663"
  },
  {
    "id": "447",
    "name": "analyzer.py_03.py",
    "path": "01_core_ai_analysis/analyzer.py_consolidated/analyzer.py_03.py",
    "category": "01_core_ai_analysis",
    "type": "analysis",
    "lines": 778,
    "size": 31333,
    "docstring": "This script is intended for use as a GYP_GENERATOR. It takes as input (by way of\nthe generator flag config_path) the path of a json file that dictates the files\nand targets to search for. The following keys are supported:\nfiles: list of paths (relative) of the files to search for.\ntest_targets: unqualified target names to search for. Any target in this list\nthat depends upon a file in |files| is output regardless of the type of target\nor chain of dependencies.\nadditional_compile_targets: Unqualified targets to search for in addition to\ntest_targets. Targets in the combined list that depend upon a file in |files|\nare not necessarily output. For example, if the target is of type none then the\ntarget is not output (but one of the descendants of the target will be).\n\nThe following is output:\nerror: only supplied if there is an error.\ncompile_targets: minimal set of targets that directly or indirectly (for\n  targets of type none) depend on the files in |files| and is one of the\n  supplied targets or a target that one of the supplied targets depends on.\n  The expectation is this set of targets is passed into a build step. This list\n  always contains the output of test_targets as well.\ntest_targets: set of targets from the supplied |test_targets| that either\n  directly or indirectly depend upon a file in |files|. This list if useful\n  if additional processing needs to be done for certain targets after the\n  build, such as running tests.\nstatus: outputs one of three values: none of the supplied files were found,\n  one of the include files changed so that it should be assumed everything\n  changed (in this case test_targets and compile_targets are not output) or at\n  least one file was found.\ninvalid_targets: list of supplied targets that were not found.\n\nExample:\nConsider a graph like the following:\n  A     D\n / B   C\nA depends upon both B and C, A is of type none and B and C are executables.\nD is an executable, has no dependencies and nothing depends on it.\nIf |additional_compile_targets| = [\"A\"], |test_targets| = [\"B\", \"C\"] and\nfiles = [\"b.cc\", \"d.cc\"] (B depends upon b.cc and D depends upon d.cc), then\nthe following is output:\n|compile_targets| = [\"B\"] B must built as it depends upon the changed file b.cc\nand the supplied target A depends upon it. A is not output as a build_target\nas it is of type none with no rules and actions.\n|test_targets| = [\"B\"] B directly depends upon the change file b.cc.\n\nEven though the file d.cc, which D depends upon, has changed D is not output\nas it was not supplied by way of |additional_compile_targets| or |test_targets|.\n\nIf the generator flag analyzer_output_path is specified, output is written\nthere. Otherwise output is written to stdout.\n\nIn Gyp the \"all\" target is shorthand for the root targets in the files passed\nto gyp. For example, if file \"a.gyp\" contains targets \"a1\" and\n\"a2\", and file \"b.gyp\" contains targets \"b1\" and \"b2\" and \"a2\" has a dependency\non \"b2\" and gyp is supplied \"a.gyp\" then \"all\" consists of \"a1\" and \"a2\".\nNotice that \"b1\" and \"b2\" are not in the \"all\" target as \"b.gyp\" was not\ndirectly supplied to gyp. OTOH if both \"a.gyp\" and \"b.gyp\" are supplied to gyp\nthen the \"all\" target includes \"b1\" and \"b2\".",
    "keywords": [],
    "functions": [
      "_ToGypPath",
      "_ResolveParent",
      "_AddSources",
      "_ExtractSourcesFromAction",
      "_ToLocalPath",
      "_ExtractSources",
      "_WasBuildFileModified",
      "_GetOrCreateTargetByName",
      "_DoesTargetTypeRequireBuild",
      "_GenerateTargets"
    ],
    "classes": [
      "Target",
      "Config",
      "TargetCalculator"
    ],
    "imports": [
      "json",
      "os",
      "posixpath",
      "gyp.common"
    ],
    "preview": "# Copyright (c) 2014 Google Inc. All rights reserved.\n# Use of this source code is governed by a BSD-style license that can be\n# found in the LICENSE file.\n\n\"\"\"\nThis script is intended for use as a GYP_GENERATOR. It takes as input (by way of\nthe generator flag config_path) the path of a json file that dictates the files\nand targets to search for. The following keys are supported:\nfiles: list of paths (relative) of the files to search for.\ntest_targets: unqualified target names to search for. Any target in this list\nthat depends upon a file in |files| is output regardless of the type of target\nor chain of dependencies.\nadditional_compile_targets: Unqualified targets to search for in addition to\ntest_targets. Targets in the combined list that depend upon a file in |files|\nare not necessarily output. For example, if the target is of type none then the\ntarget is not output (but one of the descendants of the target will be).\n\nThe following is output:\nerror: only supplied if there is an error.\ncompile_targets: minimal set of targets that directly or indirectly (for",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "448",
    "name": "main.py_03.py",
    "path": "01_core_ai_analysis/main.py_consolidated/main.py_03.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 41,
    "size": 1399,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "os",
      "slugify",
      "config",
      "utils.textgenerator",
      "utils.videogenerator"
    ],
    "preview": "import os\n\nfrom slugify import slugify\n\nimport config\nfrom utils.textgenerator import generate_text_list\nfrom utils.videogenerator import VideoGenerator\n\nif __name__ == \"__main__\":\n    vg = VideoGenerator(\n        video_folder=config.VIDEO,\n        music_folder=config.MUSIC,\n        duration=config.DURATION,\n        size=config.SIZE,\n    )\n    with open(\"prompts.txt\", \"r\") as file:\n        prompts = file.readlines()\n        prompts = [line.strip() for line in prompts]\n\n    for prompt in prompts:",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "449",
    "name": "main.py.py",
    "path": "01_core_ai_analysis/main.py_consolidated/main.py.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 19,
    "size": 558,
    "docstring": "",
    "keywords": [],
    "functions": [
      "main"
    ],
    "classes": [],
    "imports": [
      "csv",
      "generate_speech"
    ],
    "preview": "import csv\n\nfrom generate_speech import generate_speech\n\n\ndef main():\n    csv_path = \"quiz329.csv\"\n\n    with open(csv_path, newline=\"\", encoding=\"utf-8\") as csvfile:\n        reader = csv.DictReader(csvfile)\n        for i, row in enumerate(reader):\n            question_text = row[\"Question\"]  # Assuming 'Question' is the column name\n            output_path = f\"/Users/steven/Documents/quiz-talk/quiz329/question/question_{i+1}.mp3\"\n            generate_speech(question_text, voice=\"shimmer\", output_path=output_path)\n\n\nif __name__ == \"__main__\":\n    main()\n",
    "last_modified": "2025-09-13T05:53:51.047536"
  },
  {
    "id": "450",
    "name": "main.py_02.py",
    "path": "01_core_ai_analysis/main.py_consolidated/main.py_02.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 80,
    "size": 3116,
    "docstring": "",
    "keywords": [],
    "functions": [
      "str_to_bool"
    ],
    "classes": [],
    "imports": [
      "argparse",
      "twitchtube.video"
    ],
    "preview": "import argparse\n\nfrom twitchtube.video import make_video\n\nparser = argparse.ArgumentParser(description=\"\")\n\n\ndef str_to_bool(value):\n    if isinstance(value, bool):\n        return value\n    if value.lower() in (\"yes\", \"true\", \"t\", \"y\", \"1\"):\n        return True\n    elif value.lower() in (\"no\", \"false\", \"f\", \"n\", \"0\"):\n        return False\n    else:\n        raise argparse.ArgumentTypeError(\"Boolean value expected.\")\n\n\nparser.add_argument(\"--data\", type=str, help=\"\")\nparser.add_argument(\"--blacklist\", type=str, help=\"\")",
    "last_modified": "2025-03-28T18:37:10.706376"
  },
  {
    "id": "451",
    "name": "final_video.py.py",
    "path": "01_core_ai_analysis/final_video.py_consolidated/final_video.py.py",
    "category": "01_core_ai_analysis",
    "type": "youtube",
    "lines": 58,
    "size": 1810,
    "docstring": "",
    "keywords": [],
    "functions": [
      "make_final_video"
    ],
    "classes": [],
    "imports": [
      "moviepy.editor",
      "utils.console"
    ],
    "preview": "from moviepy.editor import (\n    AudioFileClip,\n    CompositeAudioClip,\n    CompositeVideoClip,\n    ImageClip,\n    VideoFileClip,\n    concatenate_audioclips,\n    concatenate_videoclips,\n)\n\nfrom utils.console import print_step\n\nW, H = 1080, 1920\n\n\ndef make_final_video(number_of_clips):\n    print_step(\"Creating the final video \ud83c\udfa5\")\n    VideoFileClip.reW = lambda clip: clip.resize(width=W)\n    VideoFileClip.reH = lambda clip: clip.resize(width=H)\n",
    "last_modified": "2025-09-13T05:53:59.421604"
  },
  {
    "id": "452",
    "name": "final_video.py_02.py",
    "path": "01_core_ai_analysis/final_video.py_consolidated/final_video.py_02.py",
    "category": "01_core_ai_analysis",
    "type": "youtube",
    "lines": 87,
    "size": 3056,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "createBackground",
      "finalize"
    ],
    "classes": [],
    "imports": [
      "os",
      "random",
      "mutagen.mp3",
      "moviepy.editor",
      "moviepy.video.tools.subtitles",
      "sub"
    ],
    "preview": "import os\nimport random\n\nimport mutagen.mp3 as MP3\nfrom moviepy.editor import (\n    AudioFileClip,\n    CompositeVideoClip,\n    TextClip,\n    VideoFileClip,\n    concatenate_videoclips,\n)\nfrom moviepy.video.tools.subtitles import SubtitlesClip\nfrom sub import *\n\n\ndef createBackground(\n    audio_path, videoPath\n):  # the path to the mp3 file, the path to the video FILES (folder)\n    print(\"Creating background for the video...Gathering Audio and Video files\")\n    audio = MP3.MP3(audio_path)",
    "last_modified": "2025-09-13T05:53:29.595240"
  },
  {
    "id": "453",
    "name": "config.py.py",
    "path": "01_core_ai_analysis/config.py_consolidated/config.py.py",
    "category": "01_core_ai_analysis",
    "type": "analysis",
    "lines": 33,
    "size": 987,
    "docstring": "Configuration settings for the Transcription Analyzer",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [],
    "preview": "\"\"\"\nConfiguration settings for the Transcription Analyzer\n\"\"\"\n\n# Whisper model options (smaller = faster, less accurate)\n# Options: \"tiny\", \"base\", \"small\", \"medium\", \"large\"\nWHISPER_MODEL = \"base\"\n\n# OpenAI model for analysis\nOPENAI_MODEL = \"gpt-4o\"\n\n# Analysis settings\nANALYSIS_TEMPERATURE = 0.3\nANALYSIS_MAX_TOKENS = 2000\n\n# File processing settings\nSUPPORTED_AUDIO_FORMATS = ['.mp3', '.mp4', '.MP3', '.MP4']\nAUDIO_QUALITY = \"medium\"  # Options: \"low\", \"medium\", \"high\"\n\n# Output settings",
    "last_modified": "2025-10-09T05:16:03.900549"
  },
  {
    "id": "454",
    "name": "config.py_03.py",
    "path": "01_core_ai_analysis/config.py_consolidated/config.py_03.py",
    "category": "01_core_ai_analysis",
    "type": "setup",
    "lines": 22,
    "size": 744,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [],
    "preview": "# MODEL SETTINGS\nMODEL = \"text-davinci-003\"\nAPI_PARAM = {\n    \"engine\": MODEL,\n    \"max_tokens\": 512,\n    \"temperature\": 0.77,\n    \"top_p\": 1,\n    \"frequency_penalty\": 0.28,\n    \"presence_penalty\": 0.13,\n}\n# VIDEO SETTINGS\nCHANNEL_NAME = \"historyfactstv\"\nDURATION = 8\nSIZE = (1080, 1920)\nFPS = 30\n# FOLDERS\nVIDEO = \"video\"\nMUSIC = \"music\"\n\nVID_TO_GENRATE = 12  # How many videos generate for each request",
    "last_modified": "2025-03-28T18:36:56"
  },
  {
    "id": "455",
    "name": "config.py_02.py",
    "path": "01_core_ai_analysis/config.py_consolidated/config.py_02.py",
    "category": "01_core_ai_analysis",
    "type": "setup",
    "lines": 18,
    "size": 359,
    "docstring": "Shared configuration for all Python projects",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "os",
      "dotenv"
    ],
    "preview": "\"\"\"\nShared configuration for all Python projects\n\"\"\"\nimport os\nfrom dotenv import load_dotenv\n\n# Load environment variables\nload_dotenv(os.path.expanduser(\"~/.env\"))\n\n# API Keys\nOPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\nYOUTUBE_API_KEY = os.getenv(\"YOUTUBE_API_KEY\")\n\n# Common settings\nDEFAULT_MODEL = \"gpt-4o\"\nDEFAULT_TEMPERATURE = 0.3\nLOG_LEVEL = \"INFO\"\n",
    "last_modified": "2025-10-09T05:27:15.568467"
  },
  {
    "id": "456",
    "name": "settings.py_02.py",
    "path": "01_core_ai_analysis/settings.py_consolidated/settings.py_02.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 73,
    "size": 2607,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "generateConfigFile",
      "loadValues"
    ],
    "classes": [],
    "imports": [
      "configparser",
      "os",
      "sys"
    ],
    "preview": "import configparser\nimport os\nfrom sys import platform\n\ncurrentPath = os.path.dirname(os.path.realpath(__file__))\n\n\naddress = \"127.0.0.1\"\nFTP_PORT = 2121\nHTTP_PORT = 8000\n\nFTP_USER = \"Tom\"\nFTP_PASSWORD = \"password\"\n\nautoLogin = False\n\nblock_size = 262144\n\nconfig = configparser.ConfigParser()\n",
    "last_modified": "2025-09-13T05:53:31.587377"
  },
  {
    "id": "457",
    "name": "settings.py.py",
    "path": "01_core_ai_analysis/settings.py_consolidated/settings.py.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 126,
    "size": 4000,
    "docstring": "",
    "keywords": [],
    "functions": [
      "OPENAI_MAX_TOKEN_WINDOW",
      "load_settings",
      "export"
    ],
    "classes": [
      "Settings"
    ],
    "imports": [
      "loguru",
      "pydantic_settings",
      "zenml.client",
      "zenml.exceptions"
    ],
    "preview": "from loguru import logger\nfrom pydantic_settings import BaseSettings, SettingsConfigDict\nfrom zenml.client import Client\nfrom zenml.exceptions import EntityExistsError\n\n\nclass Settings(BaseSettings):\n    model_config = SettingsConfigDict(env_file=\".env\", env_file_encoding=\"utf-8\")\n\n    # --- Required settings even when working locally. ---\n\n    # OpenAI API\n    OPENAI_MODEL_ID: str = \"gpt-4o-mini\"\n    OPENAI_API_KEY: str | None = None\n\n    # Huggingface API\n    HUGGINGFACE_ACCESS_TOKEN: str | None = None\n\n    # Comet ML (during training)\n    COMET_API_KEY: str | None = None",
    "last_modified": "2025-05-04T22:47:11.573896"
  },
  {
    "id": "458",
    "name": "setup.py.py",
    "path": "01_core_ai_analysis/setup.py_consolidated/setup.py.py",
    "category": "01_core_ai_analysis",
    "type": "analysis",
    "lines": 105,
    "size": 3527,
    "docstring": "Setup script for the Transcription Analyzer",
    "keywords": [],
    "functions": [
      "check_python_version",
      "install_requirements",
      "check_ffmpeg",
      "setup_env_file",
      "main"
    ],
    "classes": [],
    "imports": [
      "os",
      "sys",
      "subprocess",
      "pathlib"
    ],
    "preview": "#!/usr/bin/env python3\n\"\"\"\nSetup script for the Transcription Analyzer\n\"\"\"\n\nimport os\nimport sys\nimport subprocess\nfrom pathlib import Path\n\ndef check_python_version():\n    \"\"\"Check if Python version is 3.8 or higher.\"\"\"\n    if sys.version_info < (3, 8):\n        print(\"\u274c Python 3.8 or higher is required\")\n        print(f\"Current version: {sys.version}\")\n        return False\n    print(f\"\u2705 Python version: {sys.version.split()[0]}\")\n    return True\n\ndef install_requirements():",
    "last_modified": "2025-10-09T05:14:33.307201"
  },
  {
    "id": "459",
    "name": "setup.py_02.py",
    "path": "01_core_ai_analysis/setup.py_consolidated/setup.py_02.py",
    "category": "01_core_ai_analysis",
    "type": "setup",
    "lines": 27,
    "size": 614,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "setuptools"
    ],
    "preview": "from setuptools import setup\n\nsetup(\n    name=\"deepseek-python-analyzer\",\n    version=\"0.1.0\",\n    py_modules=[\"deepseek_python\"],\n    install_requires=[\n        \"matplotlib\",\n        \"networkx\",\n        \"radon\",\n        \"pylint\",\n        \"flake8\",\n        \"mypy\",\n        \"jsonschema\",\n    ],\n    entry_points={\n        \"console_scripts\": [\n            \"deepseek-python=deepseek_python:main\",\n        ],\n    },",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "460",
    "name": "categorize_html.py",
    "path": "01_core_ai_analysis/ai_generation/categorize_html.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 130,
    "size": 3829,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "categorize_html",
      "generate_html",
      "save_html"
    ],
    "classes": [],
    "imports": [
      "os",
      "bs4"
    ],
    "preview": "import os\n\nfrom bs4 import BeautifulSoup\n\n# Directory containing HTML files\nHTML_DIR = \"/Users/steven/Documents/HTML/html\"\n\n# Define categories and keywords (you can customize these)\nCATEGORIES = {\n    \"Art\": [\n        \"illustration\",\n        \"painting\",\n        \"artistic\",\n        \"graphic design\",\n        \"creative artwork\",\n    ],\n    \"Technology\": [\n        \"API\",\n        \"automation\",\n        \"script\",",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "461",
    "name": "test_google_gallery_logic.py",
    "path": "01_core_ai_analysis/ai_generation/test_google_gallery_logic.py",
    "category": "01_core_ai_analysis",
    "type": "testing",
    "lines": 65,
    "size": 2378,
    "docstring": "",
    "keywords": [
      "testing"
    ],
    "functions": [
      "test_create_thumbnails",
      "test_generate_images_data"
    ],
    "classes": [
      "GoogleGalleryTestCase"
    ],
    "imports": [
      "os",
      "unittest",
      "unittest",
      "simplegallery.test.helpers",
      "simplegallery.logic.variants.google_gallery_logic",
      "testfixtures"
    ],
    "preview": "import os\nimport unittest\nfrom unittest import mock\n\nimport simplegallery.test.helpers as helpers\nfrom simplegallery.logic.variants.google_gallery_logic import GoogleGalleryLogic\nfrom testfixtures import TempDirectory\n\n\nclass GoogleGalleryTestCase(unittest.TestCase):\n\n    remote_link = \"https://photos.app.goo.gl/cevaz94hQiF8Z5p67\"\n\n    @mock.patch(\"builtins.input\", side_effect=[\"\", \"\", \"\", \"\"])\n    def test_create_thumbnails(self, input):\n        with TempDirectory() as tempdir:\n            # Init files gallery logic\n            gallery_config = helpers.init_gallery_and_read_gallery_config(\n                tempdir.path, self.remote_link\n            )",
    "last_modified": "2025-09-13T05:53:52.853674"
  },
  {
    "id": "462",
    "name": "testFonts.py",
    "path": "01_core_ai_analysis/ai_generation/testFonts.py",
    "category": "01_core_ai_analysis",
    "type": "testing",
    "lines": 106,
    "size": 2840,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "create_text_slide",
      "create_intro",
      "create_transition",
      "create_outro"
    ],
    "classes": [],
    "imports": [
      "sys",
      "modules.clipEditor",
      "modules.configHandler",
      "PIL"
    ],
    "preview": "import sys\n\nfrom modules.clipEditor import Slide\nfrom modules.configHandler import *\nfrom PIL import Image, ImageDraw, ImageFont\n\n\ndef create_text_slide(slide):\n    # Create and save a text slide based on the Slide object that is passed\n    assert type(slide) == Slide\n\n    if slide.customBg:\n        image = Image.open(f\"./res/{slide.bgName}\")\n        if image.size != slide.size:\n            image = image.resize(slide.size)\n    else:\n        image = Image.new(\"RGB\", slide.size, color=slide.bgColor)\n    drawImage = ImageDraw.Draw(image)\n\n    # Taken from https://stackoverflow.com/a/61891053",
    "last_modified": "2025-03-28T18:37:01.441426"
  },
  {
    "id": "463",
    "name": "VideoEdit.py",
    "path": "01_core_ai_analysis/ai_generation/VideoEdit.py",
    "category": "01_core_ai_analysis",
    "type": "youtube",
    "lines": 72,
    "size": 2572,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "__init__",
      "create_dir",
      "create_movie"
    ],
    "classes": [
      "VideoEditor"
    ],
    "imports": [
      "os",
      "moviepy.editor",
      "PIL"
    ],
    "preview": "# This clas will hold the class that edits the images into video\n# TODO: Once complete, remove global import and only import required\n\nimport os  # used for some file grabbing\n\nfrom moviepy.editor import *  # movie editor\nfrom PIL import Image  # Image library\n\n\nclass VideoEditor:\n    def __init__(self, num_replies, video_name):\n        self.num_replies = num_replies\n        self.video_name = video_name\n        self.image_path = \"../images/\"\n        self.audio_path = \"../audio/\"\n        self.save_path = \"../edited_videos/\"\n\n        self.create_dir()  # Creates the edited videos dir if it doesnt exist\n\n        print(self.num_replies)",
    "last_modified": "2025-09-13T05:53:51.715010"
  },
  {
    "id": "464",
    "name": "show.py",
    "path": "01_core_ai_analysis/ai_generation/show.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 183,
    "size": 6352,
    "docstring": "",
    "keywords": [],
    "functions": [
      "search_packages_info",
      "print_results",
      "add_options",
      "run",
      "_get_requiring_packages"
    ],
    "classes": [
      "ShowCommand",
      "_PackageInfo"
    ],
    "imports": [
      "logging",
      "optparse",
      "typing",
      "pip._internal.cli.base_command",
      "pip._internal.cli.status_codes",
      "pip._internal.metadata",
      "pip._internal.utils.misc",
      "pip._vendor.packaging.utils"
    ],
    "preview": "import logging\nfrom optparse import Values\nfrom typing import Generator, Iterable, Iterator, List, NamedTuple, Optional\n\nfrom pip._internal.cli.base_command import Command\nfrom pip._internal.cli.status_codes import ERROR, SUCCESS\nfrom pip._internal.metadata import BaseDistribution, get_default_environment\nfrom pip._internal.utils.misc import write_output\nfrom pip._vendor.packaging.utils import canonicalize_name\n\nlogger = logging.getLogger(__name__)\n\n\nclass ShowCommand(Command):\n    \"\"\"\n    Show information about one or more installed packages.\n\n    The output is in RFC-compliant mail header format.\n    \"\"\"\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "465",
    "name": "run.py",
    "path": "01_core_ai_analysis/ai_generation/run.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 44,
    "size": 1586,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "create_endpoint"
    ],
    "classes": [],
    "imports": [
      "loguru",
      "llm_engineering.model.utils",
      "llm_engineering.settings",
      "config",
      "sagemaker_huggingface",
      "sagemaker.enums",
      "sagemaker.huggingface"
    ],
    "preview": "from loguru import logger\n\ntry:\n    from sagemaker.enums import EndpointType\n    from sagemaker.huggingface import get_huggingface_llm_image_uri\nexcept ModuleNotFoundError:\n    logger.warning(\n        \"Couldn't load SageMaker imports. Run 'poetry install --with aws' to support AWS.\"\n    )\n\nfrom llm_engineering.model.utils import ResourceManager\nfrom llm_engineering.settings import settings\n\nfrom .config import hugging_face_deploy_config, model_resource_config\nfrom .sagemaker_huggingface import DeploymentService, SagemakerHuggingfaceStrategy\n\n\ndef create_endpoint(endpoint_type=EndpointType.INFERENCE_COMPONENT_BASED) -> None:\n    assert settings.AWS_ARN_ROLE is not None, \"AWS_ARN_ROLE is not set in the .env file.\"\n",
    "last_modified": "2025-09-13T05:53:41.967624"
  },
  {
    "id": "466",
    "name": "onedrive_gallery_logic 2.py",
    "path": "01_core_ai_analysis/ai_generation/onedrive_gallery_logic 2.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 108,
    "size": 3893,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "parse_photo_link",
      "create_thumbnails",
      "generate_images_data"
    ],
    "classes": [
      "OnedriveGalleryLogic"
    ],
    "imports": [
      "time",
      "pkg_resources",
      "simplegallery.common",
      "simplegallery.media",
      "selenium",
      "selenium.webdriver.firefox.options",
      "simplegallery.logic.base_gallery_logic"
    ],
    "preview": "import time\n\nimport pkg_resources\nimport simplegallery.common as spg_common\nimport simplegallery.media as spg_media\nfrom selenium import webdriver\nfrom selenium.webdriver.firefox.options import Options\nfrom simplegallery.logic.base_gallery_logic import BaseGalleryLogic\n\n\ndef parse_photo_link(photo_url):\n    \"\"\"\n    Extracts the base URL (URL without query parameters) and the photo name from a Onedrive photo URL\n    :param photo_url: photo URL\n    :return: base URL and photo name\n    \"\"\"\n    base_url = photo_url.split(\"?\")[0]\n    name = base_url.split(\"/\")[-1]\n\n    return base_url, name",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "467",
    "name": "media.py",
    "path": "01_core_ai_analysis/ai_generation/media.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 257,
    "size": 8299,
    "docstring": "",
    "keywords": [
      "opencv",
      "analysis",
      "web_tools"
    ],
    "functions": [
      "rotate_image_by_orientation",
      "get_thumbnail_size",
      "create_image_thumbnail",
      "create_video_thumbnail",
      "create_thumbnail",
      "get_remote_image_size",
      "get_image_size",
      "get_video_size",
      "get_image_description",
      "parse_exif_datetime"
    ],
    "classes": [],
    "imports": [
      "os",
      "io",
      "cv2",
      "requests",
      "PIL",
      "datetime",
      "simplegallery.common"
    ],
    "preview": "import os\nfrom io import BytesIO\n\nimport cv2\nimport requests\nfrom PIL import ExifTags, Image, ImageFile\n\nImageFile.LOAD_TRUNCATED_IMAGES = True\nfrom datetime import datetime\n\nimport simplegallery.common as spg_common\n\n# Mapping of the string representation if an Exif tag to its id\nEXIF_TAG_MAP = {ExifTags.TAGS[tag]: tag for tag in ExifTags.TAGS}\n\n\ndef rotate_image_by_orientation(image):\n    \"\"\"\n    Rotates an image according to it's Orientation EXIF Tag\n    :param im: Image",
    "last_modified": "2025-09-13T05:53:52.709810"
  },
  {
    "id": "468",
    "name": "MetadataHandler.py",
    "path": "01_core_ai_analysis/ai_generation/MetadataHandler.py",
    "category": "01_core_ai_analysis",
    "type": "analysis",
    "lines": 199,
    "size": 8243,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "__init__",
      "create_metadata",
      "get_metadata_config",
      "get_metadata",
      "refresh_metadata",
      "get_youtube_description",
      "get_youtube_title",
      "get_number_in_series",
      "create_youtube_description",
      "create_youtube_title"
    ],
    "classes": [
      "MetadataHandler"
    ],
    "imports": [
      "logging",
      "os",
      "random",
      "datetime",
      "jinja2",
      "moviepy.editor",
      "moviepy.video.fx.resize",
      "moviepy.video.fx.rotate",
      "src",
      "src.APIHandler"
    ],
    "preview": "import logging\nimport os\nimport random\nfrom datetime import time, timedelta\n\nfrom jinja2 import Environment, FileSystemLoader\nfrom moviepy.editor import CompositeVideoClip, ImageClip, TextClip, VideoFileClip\nfrom moviepy.video.fx.resize import resize\nfrom moviepy.video.fx.rotate import rotate\nfrom src import Clip, utils\nfrom src.APIHandler import APIHandler\n\nimport config\n\n\nclass MetadataHandler:\n    def __init__(self, game: str, asset_path: str, output_path: str):\n        self.output_path = output_path\n        self.game = game\n        self.asset_path = asset_path",
    "last_modified": "2025-09-13T05:53:45.651344"
  },
  {
    "id": "469",
    "name": "impo.py",
    "path": "01_core_ai_analysis/ai_generation/impo.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 49,
    "size": 1529,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "create_thumbnail",
      "create_contact_sheet",
      "save_contact_sheet"
    ],
    "classes": [],
    "imports": [
      "os",
      "PIL"
    ],
    "preview": "import os\n\nfrom PIL import Image\n\n\ndef create_thumbnail(image_path, thumbnail_size=(100, 100)):\n    with Image.open(image_path) as img:\n        img.thumbnail(thumbnail_size)\n        return img\n\n\ndef create_contact_sheet(directory, thumbnail_size=(100, 100), sheet_size=(500, 500)):\n    thumbnails = []\n    for subdir, dirs, files in os.walk(directory):\n        for file in files:\n            if file.lower().endswith((\"png\", \"jpg\", \"jpeg\", \"gif\", \"bmp\")):\n                image_path = os.path.join(subdir, file)\n                thumbnail = create_thumbnail(image_path, thumbnail_size)\n                thumbnails.append(thumbnail)\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "470",
    "name": "process_leonardo_20250102104751.py",
    "path": "01_core_ai_analysis/ai_generation/process_leonardo_20250102104751.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 52,
    "size": 1578,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "csv",
      "json"
    ],
    "preview": "import csv\nimport json\n\n# File paths\ninput_file_path = \"/Users/steven/Pictures/leonardo.json\"  # Path to the input JSON file\noutput_file_path = (\n    \"/Users/steven/Pictures/leonardo_generations_full.csv\"  # Path to the output CSV file\n)\n\n# Headers for the CSV file\nheaders = [\n    \"id\",\n    \"prompt\",\n    \"negativePrompt\",\n    \"url\",\n    \"motionMP4URL\",\n    \"motionStrength\",\n    \"height\",\n    \"width\",\n    \"duration\",",
    "last_modified": "2025-09-13T05:53:49.665378"
  },
  {
    "id": "471",
    "name": "polly_main.py",
    "path": "01_core_ai_analysis/ai_generation/polly_main.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 64,
    "size": 1788,
    "docstring": "",
    "keywords": [],
    "functions": [
      "polly",
      "tts_task_resp",
      "transcript_generator",
      "get_word_by_transcript"
    ],
    "classes": [],
    "imports": [
      "json",
      "boto3",
      "botocore.config",
      "utilities.const"
    ],
    "preview": "import json\n\nimport boto3\nfrom botocore.config import Config\nfrom utilities.const import AWS_ACCESS_KEY, AWS_SEC_KEY, S3_BUCKET, get_current_date\n\nmy_config = Config(\n    region_name=\"ap-south-1\",\n    signature_version=\"v4\",\n    retries={\"max_attempts\": 3, \"mode\": \"standard\"},\n)\n\nclient = boto3.client(\n    \"polly\",\n    config=my_config,\n    aws_access_key_id=AWS_ACCESS_KEY,\n    aws_secret_access_key=AWS_SEC_KEY,\n)\n\n",
    "last_modified": "2025-09-13T05:53:28.648046"
  },
  {
    "id": "472",
    "name": "alphabet-html-gall.py",
    "path": "01_core_ai_analysis/ai_generation/alphabet-html-gall.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 125,
    "size": 4184,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "generate_grouped_gallery"
    ],
    "classes": [],
    "imports": [
      "os"
    ],
    "preview": "import os\n\n\ndef generate_grouped_gallery(folder_path, output_html, group_size=4):\n    \"\"\"\n    Generates an HTML gallery grouped alphabetically by sets of images and MP4s.\n    \"\"\"\n    # Get all valid files\n    valid_extensions = (\".jpg\", \".jpeg\", \".png\", \".mp4\")\n    all_files = [f for f in os.listdir(folder_path) if f.lower().endswith(valid_extensions)]\n\n    # Sort files alphabetically\n    all_files.sort()\n\n    # Group files into sets of the specified size\n    grouped_files = [all_files[i : i + group_size] for i in range(0, len(all_files), group_size)]\n\n    # Start the HTML structure\n    html_content = \"\"\"\n    <!DOCTYPE html>",
    "last_modified": "2025-09-13T05:54:15.432728"
  },
  {
    "id": "473",
    "name": "contacts.py",
    "path": "01_core_ai_analysis/ai_generation/contacts.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 50,
    "size": 1836,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "create_contact_sheet"
    ],
    "classes": [],
    "imports": [
      "os",
      "PIL"
    ],
    "preview": "import os\n\nfrom PIL import Image\n\n\ndef create_contact_sheet(source_folder, output_size=(2000, 2000), images_per_row=2):\n    # List all the image files in the source folder with the correct extension\n    image_files = [\n        os.path.join(source_folder, f)\n        for f in os.listdir(source_folder)\n        if f.endswith((\".png\", \".jpg\", \".jpeg\"))\n    ]\n\n    # Create a new blank white image to serve as the contact sheet\n    contact_sheet = Image.new(\"RGB\", output_size, \"white\")\n\n    # Define the new width and height for each image\n    new_width = output_size[0] // images_per_row\n    # Calculate the height while maintaining aspect ratio\n    new_height = int(new_width / (16 / 9))",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "474",
    "name": "gallery_init_20241204123524.py",
    "path": "01_core_ai_analysis/ai_generation/gallery_init_20241204123524.py",
    "category": "01_core_ai_analysis",
    "type": "setup",
    "lines": 292,
    "size": 9791,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "parse_args",
      "check_if_gallery_creation_possible",
      "check_if_gallery_already_exists",
      "create_gallery_folder_structure",
      "create_gallery_json",
      "main"
    ],
    "classes": [],
    "imports": [
      "argparse",
      "glob",
      "importlib.resources",
      "json",
      "os",
      "shutil",
      "sys",
      "distutils.dir_util",
      "simplegallery.common",
      "simplegallery.logic.gallery_logic"
    ],
    "preview": "import argparse\nimport glob\nimport importlib.resources\nimport json\nimport os\nimport shutil\nimport sys\nfrom distutils.dir_util import copy_tree\n\nimport simplegallery.common as spg_common\nimport simplegallery.logic.gallery_logic as gallery_logic\n\n\ndef parse_args():\n    \"\"\"\n    Configures the argument parser\n    :return: Parsed arguments\n    \"\"\"\n\n    description = \"\"\"Initializes a new Simple Photo Gallery in the specified folder (default is the current folder).",
    "last_modified": "2025-09-13T05:55:18.370327"
  },
  {
    "id": "475",
    "name": "mklabels.py",
    "path": "01_core_ai_analysis/ai_generation/mklabels.py",
    "category": "01_core_ai_analysis",
    "type": "web_tools",
    "lines": 63,
    "size": 1293,
    "docstring": "webencodings.mklabels\n~~~~~~~~~~~~~~~~~~~~~\n\nRegenarate the webencodings.labels module.\n\n:copyright: Copyright 2012 by Simon Sapin\n:license: BSD, see LICENSE for details.",
    "keywords": [
      "testing",
      "analysis"
    ],
    "functions": [
      "assert_lower",
      "generate"
    ],
    "classes": [],
    "imports": [
      "json",
      "urllib",
      "urllib.request"
    ],
    "preview": "\"\"\"\n\nwebencodings.mklabels\n~~~~~~~~~~~~~~~~~~~~~\n\nRegenarate the webencodings.labels module.\n\n:copyright: Copyright 2012 by Simon Sapin\n:license: BSD, see LICENSE for details.\n\n\"\"\"\n\nimport json\n\ntry:\n    from urllib import urlopen\nexcept ImportError:\n    from urllib.request import urlopen\n\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "476",
    "name": "playlist_20221230180403.py",
    "path": "01_core_ai_analysis/ai_generation/playlist_20221230180403.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 212,
    "size": 6700,
    "docstring": "",
    "keywords": [
      "youtube",
      "analysis",
      "web_tools"
    ],
    "functions": [
      "generate_token",
      "get_youtube_url",
      "download_mp3",
      "last_fm_artist_info",
      "set_metadata",
      "write_tracks",
      "write_playlist",
      "split_spotify_uri",
      "get_os",
      "get_folder"
    ],
    "classes": [],
    "imports": [
      "argparse",
      "json",
      "os",
      "re",
      "subprocess",
      "urllib.parse",
      "urllib.request",
      "secret",
      "spotipy",
      "spotipy.oauth2"
    ],
    "preview": "import argparse\nimport json\nimport os\nimport re\nimport subprocess\nimport urllib.parse\nimport urllib.request\n\nimport secret\nimport spotipy\nimport spotipy.oauth2 as oauth2\nfrom requests.utils import quote\n\n\ndef generate_token():\n    credentials = oauth2.SpotifyClientCredentials(\n        client_id=secret.SPOTIFY_CLIENT_ID, client_secret=secret.SPOTIFY_CLIENT_SECRET\n    )\n    token = credentials.get_access_token()\n    return token",
    "last_modified": "2025-09-13T05:54:01.977236"
  },
  {
    "id": "477",
    "name": "videogenerator.py",
    "path": "01_core_ai_analysis/ai_generation/videogenerator.py",
    "category": "01_core_ai_analysis",
    "type": "youtube",
    "lines": 101,
    "size": 3278,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "__init__",
      "_pick_random_file",
      "make_audio",
      "make_video",
      "header",
      "content",
      "footer",
      "_make_main_clip",
      "generate_video"
    ],
    "classes": [
      "VideoGenerator"
    ],
    "imports": [
      "os",
      "random",
      "typing",
      "moviepy.editor"
    ],
    "preview": "import os\nimport random\nfrom typing import Tuple\n\nimport moviepy.editor as me\n\n\nclass VideoGenerator:\n\n    def __init__(\n        self,\n        video_folder: str = \"video\",\n        music_folder: str = \"music\",\n        duration: int = 8,\n        size: Tuple[int, int] = (1080, 1920),\n        channel_name: str = \"historyfactstv\",\n    ) -> None:\n        self.video_folder = video_folder\n        self.music_folder = music_folder\n        self.video_list = os.listdir(video_folder)",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "478",
    "name": "metadata.py",
    "path": "01_core_ai_analysis/ai_generation/metadata.py",
    "category": "01_core_ai_analysis",
    "type": "analysis",
    "lines": 35,
    "size": 1407,
    "docstring": "Metadata generation logic for source distributions.",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "generate_metadata"
    ],
    "classes": [],
    "imports": [
      "os",
      "pip._internal.build_env",
      "pip._internal.exceptions",
      "pip._internal.utils.subprocess",
      "pip._internal.utils.temp_dir",
      "pip._vendor.pyproject_hooks"
    ],
    "preview": "\"\"\"Metadata generation logic for source distributions.\"\"\"\n\nimport os\n\nfrom pip._internal.build_env import BuildEnvironment\nfrom pip._internal.exceptions import InstallationSubprocessError, MetadataGenerationFailed\nfrom pip._internal.utils.subprocess import runner_with_spinner_message\nfrom pip._internal.utils.temp_dir import TempDirectory\nfrom pip._vendor.pyproject_hooks import BuildBackendHookCaller\n\n\ndef generate_metadata(\n    build_env: BuildEnvironment, backend: BuildBackendHookCaller, details: str\n) -> str:\n    \"\"\"Generate metadata using mechanisms described in PEP 517.\n\n    Returns the generated metadata directory.\n    \"\"\"\n    metadata_tmpdir = TempDirectory(kind=\"modern-metadata\", globally_managed=True)\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "479",
    "name": "batch_upload.py",
    "path": "01_core_ai_analysis/ai_generation/batch_upload.py",
    "category": "01_core_ai_analysis",
    "type": "youtube",
    "lines": 62,
    "size": 2127,
    "docstring": "",
    "keywords": [
      "analysis",
      "youtube"
    ],
    "functions": [
      "authenticate",
      "upload_video",
      "generate_title_description",
      "main"
    ],
    "classes": [],
    "imports": [
      "os",
      "google_auth_oauthlib.flow",
      "googleapiclient.discovery",
      "googleapiclient.errors",
      "googleapiclient.http"
    ],
    "preview": "import os\n\nimport google_auth_oauthlib.flow\nimport googleapiclient.discovery\nimport googleapiclient.errors\nimport googleapiclient.http\n\n\ndef authenticate():\n    os.environ[\"OAUTHLIB_INSECURE_TRANSPORT\"] = \"1\"\n    api_service_name = \"youtube\"\n    api_version = \"v3\"\n    client_secrets_file = \"client_secrets.json\"\n    flow = google_auth_oauthlib.flow.InstalledAppFlow.from_client_secrets_file(\n        client_secrets_file, [\"https://www.googleapis.com/auth/youtube.upload\"]\n    )\n    credentials = flow.run_local_server()\n    youtube = googleapiclient.discovery.build(\n        api_service_name, api_version, credentials=credentials\n    )",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "480",
    "name": "gallery_init_20241204123444.py",
    "path": "01_core_ai_analysis/ai_generation/gallery_init_20241204123444.py",
    "category": "01_core_ai_analysis",
    "type": "setup",
    "lines": 292,
    "size": 9791,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "parse_args",
      "check_if_gallery_creation_possible",
      "check_if_gallery_already_exists",
      "create_gallery_folder_structure",
      "create_gallery_json",
      "main"
    ],
    "classes": [],
    "imports": [
      "argparse",
      "glob",
      "importlib.resources",
      "json",
      "os",
      "shutil",
      "sys",
      "distutils.dir_util",
      "simplegallery.common",
      "simplegallery.logic.gallery_logic"
    ],
    "preview": "import argparse\nimport glob\nimport importlib.resources\nimport json\nimport os\nimport shutil\nimport sys\nfrom distutils.dir_util import copy_tree\n\nimport simplegallery.common as spg_common\nimport simplegallery.logic.gallery_logic as gallery_logic\n\n\ndef parse_args():\n    \"\"\"\n    Configures the argument parser\n    :return: Parsed arguments\n    \"\"\"\n\n    description = \"\"\"Initializes a new Simple Photo Gallery in the specified folder (default is the current folder).",
    "last_modified": "2025-09-13T05:55:18.122361"
  },
  {
    "id": "481",
    "name": "analyzer 1.py",
    "path": "01_core_ai_analysis/ai_generation/analyzer 1.py",
    "category": "01_core_ai_analysis",
    "type": "analysis",
    "lines": 812,
    "size": 31839,
    "docstring": "This script is intended for use as a GYP_GENERATOR. It takes as input (by way of\nthe generator flag config_path) the path of a json file that dictates the files\nand targets to search for. The following keys are supported:\nfiles: list of paths (relative) of the files to search for.\ntest_targets: unqualified target names to search for. Any target in this list\nthat depends upon a file in |files| is output regardless of the type of target\nor chain of dependencies.\nadditional_compile_targets: Unqualified targets to search for in addition to\ntest_targets. Targets in the combined list that depend upon a file in |files|\nare not necessarily output. For example, if the target is of type none then the\ntarget is not output (but one of the descendants of the target will be).\n\nThe following is output:\nerror: only supplied if there is an error.\ncompile_targets: minimal set of targets that directly or indirectly (for\n  targets of type none) depend on the files in |files| and is one of the\n  supplied targets or a target that one of the supplied targets depends on.\n  The expectation is this set of targets is passed into a build step. This list\n  always contains the output of test_targets as well.\ntest_targets: set of targets from the supplied |test_targets| that either\n  directly or indirectly depend upon a file in |files|. This list if useful\n  if additional processing needs to be done for certain targets after the\n  build, such as running tests.\nstatus: outputs one of three values: none of the supplied files were found,\n  one of the include files changed so that it should be assumed everything\n  changed (in this case test_targets and compile_targets are not output) or at\n  least one file was found.\ninvalid_targets: list of supplied targets that were not found.\n\nExample:\nConsider a graph like the following:\n  A     D\n / B   C\nA depends upon both B and C, A is of type none and B and C are executables.\nD is an executable, has no dependencies and nothing depends on it.\nIf |additional_compile_targets| = [\"A\"], |test_targets| = [\"B\", \"C\"] and\nfiles = [\"b.cc\", \"d.cc\"] (B depends upon b.cc and D depends upon d.cc), then\nthe following is output:\n|compile_targets| = [\"B\"] B must built as it depends upon the changed file b.cc\nand the supplied target A depends upon it. A is not output as a build_target\nas it is of type none with no rules and actions.\n|test_targets| = [\"B\"] B directly depends upon the change file b.cc.\n\nEven though the file d.cc, which D depends upon, has changed D is not output\nas it was not supplied by way of |additional_compile_targets| or |test_targets|.\n\nIf the generator flag analyzer_output_path is specified, output is written\nthere. Otherwise output is written to stdout.\n\nIn Gyp the \"all\" target is shorthand for the root targets in the files passed\nto gyp. For example, if file \"a.gyp\" contains targets \"a1\" and\n\"a2\", and file \"b.gyp\" contains targets \"b1\" and \"b2\" and \"a2\" has a dependency\non \"b2\" and gyp is supplied \"a.gyp\" then \"all\" consists of \"a1\" and \"a2\".\nNotice that \"b1\" and \"b2\" are not in the \"all\" target as \"b.gyp\" was not\ndirectly supplied to gyp. OTOH if both \"a.gyp\" and \"b.gyp\" are supplied to gyp\nthen the \"all\" target includes \"b1\" and \"b2\".",
    "keywords": [],
    "functions": [
      "_ToGypPath",
      "_ResolveParent",
      "_AddSources",
      "_ExtractSourcesFromAction",
      "_ToLocalPath",
      "_ExtractSources",
      "_WasBuildFileModified",
      "_GetOrCreateTargetByName",
      "_DoesTargetTypeRequireBuild",
      "_GenerateTargets"
    ],
    "classes": [
      "Target",
      "Config",
      "TargetCalculator"
    ],
    "imports": [
      "json",
      "os",
      "posixpath",
      "gyp.common"
    ],
    "preview": "# Copyright (c) 2014 Google Inc. All rights reserved.\n# Use of this source code is governed by a BSD-style license that can be\n# found in the LICENSE file.\n\n\"\"\"\nThis script is intended for use as a GYP_GENERATOR. It takes as input (by way of\nthe generator flag config_path) the path of a json file that dictates the files\nand targets to search for. The following keys are supported:\nfiles: list of paths (relative) of the files to search for.\ntest_targets: unqualified target names to search for. Any target in this list\nthat depends upon a file in |files| is output regardless of the type of target\nor chain of dependencies.\nadditional_compile_targets: Unqualified targets to search for in addition to\ntest_targets. Targets in the combined list that depend upon a file in |files|\nare not necessarily output. For example, if the target is of type none then the\ntarget is not output (but one of the descendants of the target will be).\n\nThe following is output:\nerror: only supplied if there is an error.\ncompile_targets: minimal set of targets that directly or indirectly (for",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "482",
    "name": "metadata_editable.py",
    "path": "01_core_ai_analysis/ai_generation/metadata_editable.py",
    "category": "01_core_ai_analysis",
    "type": "analysis",
    "lines": 35,
    "size": 1437,
    "docstring": "Metadata generation logic for source distributions.",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "generate_editable_metadata"
    ],
    "classes": [],
    "imports": [
      "os",
      "pip._internal.build_env",
      "pip._internal.exceptions",
      "pip._internal.utils.subprocess",
      "pip._internal.utils.temp_dir",
      "pip._vendor.pyproject_hooks"
    ],
    "preview": "\"\"\"Metadata generation logic for source distributions.\"\"\"\n\nimport os\n\nfrom pip._internal.build_env import BuildEnvironment\nfrom pip._internal.exceptions import InstallationSubprocessError, MetadataGenerationFailed\nfrom pip._internal.utils.subprocess import runner_with_spinner_message\nfrom pip._internal.utils.temp_dir import TempDirectory\nfrom pip._vendor.pyproject_hooks import BuildBackendHookCaller\n\n\ndef generate_editable_metadata(\n    build_env: BuildEnvironment, backend: BuildBackendHookCaller, details: str\n) -> str:\n    \"\"\"Generate metadata using mechanisms described in PEP 660.\n\n    Returns the generated metadata directory.\n    \"\"\"\n    metadata_tmpdir = TempDirectory(kind=\"modern-metadata\", globally_managed=True)\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "483",
    "name": "generate_album_pages.py",
    "path": "01_core_ai_analysis/ai_generation/generate_album_pages.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 90,
    "size": 2534,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "read_text_file",
      "generate_album_html"
    ],
    "classes": [],
    "imports": [
      "os"
    ],
    "preview": "import os\n\nTEMPLATE = \"\"\"\n<!DOCTYPE html>\n<html lang=\"en\">\n<head>\n    <meta charset=\"UTF-8\">\n    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n    <title>{album_name}</title>\n    <link rel=\"stylesheet\" href=\"../css/styles.css\">\n</head>\n<body>\n    <div class=\"album-container\">\n        <div class=\"album\">\n            <img src=\"../albums/{folder_name}/album_cover.jpg\" alt=\"{album_name} Cover\">\n            <h3>{album_name}</h3>\n            <audio controls>\n                <source src=\"../albums/{folder_name}/{mp3_file}\" type=\"audio/mpeg\">\n                Your browser does not support the audio element.\n            </audio>",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "484",
    "name": "sort.py",
    "path": "01_core_ai_analysis/ai_generation/sort.py",
    "category": "01_core_ai_analysis",
    "type": "organization",
    "lines": 173,
    "size": 6182,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "get_creation_date",
      "get_image_metadata",
      "get_video_metadata",
      "custom_tags",
      "contains_web_project_files",
      "generate_dry_run_csv"
    ],
    "classes": [],
    "imports": [
      "csv",
      "os",
      "shutil",
      "datetime",
      "ffmpeg",
      "PIL"
    ],
    "preview": "import csv\nimport os\nimport shutil\nfrom datetime import datetime\n\nimport ffmpeg\nfrom PIL import Image\n\n\ndef get_creation_date(filepath):\n    \"\"\"Get the creation date of the file.\"\"\"\n    return datetime.fromtimestamp(os.path.getctime(filepath)).strftime(\"%Y-%m-%d %H:%M:%S\")\n\n\ndef get_image_metadata(filepath):\n    \"\"\"Extract metadata from an image file.\"\"\"\n    with Image.open(filepath) as img:\n        return img.size, os.path.getsize(filepath)\n\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "485",
    "name": "lazy 2.py",
    "path": "01_core_ai_analysis/ai_generation/lazy 2.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 161,
    "size": 6963,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "parse_album_data",
      "generate_html",
      "main"
    ],
    "classes": [],
    "imports": [
      "os"
    ],
    "preview": "import os\n\n\ndef parse_album_data(paths_file):\n    albums = {}\n    with open(paths_file, \"r\", encoding=\"utf-8\") as f:\n        for line in f:\n            line = line.strip()\n            if not line:\n                continue\n            # Process only if the line is in the mp4 folder structure\n            if \"/mp4/\" in line:\n                # Extract album folder name by splitting at \"/mp4/\" then taking first folder\n                parts = line.split(\"/mp4/\")[1].split(\"/\")\n                if not parts:\n                    continue\n                album_key = parts[0]\n                if album_key not in albums:\n                    albums[album_key] = {\n                        \"mp3\": None,",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "486",
    "name": "base_gallery_logic.py",
    "path": "01_core_ai_analysis/ai_generation/base_gallery_logic.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 54,
    "size": 1937,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "__init__",
      "create_thumbnails",
      "generate_images_data",
      "create_images_data_file"
    ],
    "classes": [
      "BaseGalleryLogic"
    ],
    "imports": [
      "json",
      "os",
      "collections"
    ],
    "preview": "import json\nimport os\nfrom collections import OrderedDict\n\n\nclass BaseGalleryLogic:\n    \"\"\"\n    Base class for defining a gallery logic. Derived classes should implement the methods create_thumbnails and\n    generate_images_data to define the specific logic.\n    \"\"\"\n\n    def __init__(self, gallery_config):\n        \"\"\"\n        Initializes the gallery logic\n        :param gallery_config: Gallery config dictionary as read from the gallery.json\n        \"\"\"\n        self.gallery_config = gallery_config\n\n    def create_thumbnails(self, force=False):\n        \"\"\"",
    "last_modified": "2025-05-04T23:28:22"
  },
  {
    "id": "487",
    "name": "oshelper.py",
    "path": "01_core_ai_analysis/ai_generation/oshelper.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 125,
    "size": 3252,
    "docstring": "OS helper",
    "keywords": [],
    "functions": [
      "absolute_files",
      "absolute_dirs",
      "try_create_lock_file",
      "file_exists",
      "lock_file_exists",
      "try_delete_lock_file",
      "get_track_file",
      "get_album_art_file",
      "get_track_info_file",
      "isdir"
    ],
    "classes": [],
    "imports": [
      "errno",
      "logging",
      "os",
      "shutil"
    ],
    "preview": "\"OS helper\"\n\nimport errno\nimport logging\nimport os\nfrom shutil import copy2, rmtree\n\nDEFAULT_FILE_NAME = \"\"\nLOCK_FILE_NAME = \"CANT_TOUCH_THIS\"\n\n\ndef absolute_files(path):\n    \"Returns the files at this directory as absolute filepaths\"\n    if os.path.isdir(path):\n        files = [os.path.join(path, f) for f in os.listdir(path)]\n        return [f for f in files if os.path.isfile(f)]\n    return []\n\n\ndef absolute_dirs(path):",
    "last_modified": "2025-05-04T23:27:55"
  },
  {
    "id": "488",
    "name": "process_leonardo_20250102110149.py",
    "path": "01_core_ai_analysis/ai_generation/process_leonardo_20250102110149.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 52,
    "size": 1578,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "csv",
      "json"
    ],
    "preview": "import csv\nimport json\n\n# File paths\ninput_file_path = \"/Users/steven/Pictures/leonardo.json\"  # Path to the input JSON file\noutput_file_path = (\n    \"/Users/steven/Pictures/leonardo_generations_full.csv\"  # Path to the output CSV file\n)\n\n# Headers for the CSV file\nheaders = [\n    \"id\",\n    \"prompt\",\n    \"negativePrompt\",\n    \"url\",\n    \"motionMP4URL\",\n    \"motionStrength\",\n    \"height\",\n    \"width\",\n    \"duration\",",
    "last_modified": "2025-09-13T05:53:49.760131"
  },
  {
    "id": "489",
    "name": "clipEditor.py",
    "path": "01_core_ai_analysis/ai_generation/clipEditor.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 221,
    "size": 7185,
    "docstring": "",
    "keywords": [
      "opencv",
      "analysis"
    ],
    "functions": [
      "get_max_dimensions",
      "get_video_dimension",
      "get_max_fps",
      "create_text_slide",
      "create_intro",
      "create_transition",
      "create_outro",
      "create_video",
      "__init__"
    ],
    "classes": [
      "Slide"
    ],
    "imports": [
      "random",
      "modules.configHandler",
      "moviepy.editor",
      "PIL",
      "cv2",
      "cv2",
      "cv2"
    ],
    "preview": "#!/usr/bin/env python\nimport random\n\nfrom modules.configHandler import *\nfrom moviepy.editor import *\nfrom PIL import Image, ImageDraw, ImageFont\n\n\nclass Slide:\n\n    text: str  # Text Displayed in the Slide\n    size: tuple  # Size of the Slide as as a tuple (width,height)\n    bgColor: tuple  # Color of the background  as a tuple (R,G,B)\n    txtColor: tuple  # Color of the text as a tuple (R,G,B)\n    fileName: str  # File name of the Slide file , without extension\n    fontName: str  # Name of the Font file used for the text without the extension,\n    #   must be in the /res folder and be a ttf file\n    textRatio: float  # Font size of the text in the Slide\n    customBg = bool  # {True|False} Use Custom Background\n    bgName: (",
    "last_modified": "2025-09-13T05:53:44.981305"
  },
  {
    "id": "490",
    "name": "conf.py",
    "path": "01_core_ai_analysis/ai_generation/conf.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 155,
    "size": 4818,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "os",
      "sys",
      "savify"
    ],
    "preview": "#!/usr/bin/env python\n#\n# savify documentation build configuration file, created by\n# sphinx-quickstart on Fri Jun  9 13:47:02 2017.\n#\n# This file is execfile()d with the current directory set to its\n# containing dir.\n#\n# Note that not all possible configuration values are present in this\n# autogenerated file.\n#\n# All configuration values have a default; values that are commented out\n# serve to show the default.\n\n# If extensions (or modules to document with autodoc) are in another\n# directory, add these directories to sys.path here. If the directory is\n# relative to the documentation root, use os.path.abspath to make it\n# absolute, like shown here.\n#\nimport os",
    "last_modified": "2025-05-04T23:27:53.601941"
  },
  {
    "id": "491",
    "name": "database.py",
    "path": "01_core_ai_analysis/ai_generation/database.py",
    "category": "01_core_ai_analysis",
    "type": "analysis",
    "lines": 71,
    "size": 2454,
    "docstring": "MIT License\n\nCopyright (c) 2021-2022 MShawon\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "create_database",
      "update_database"
    ],
    "classes": [],
    "imports": [
      "os",
      "shutil",
      "sqlite3",
      "contextlib",
      "datetime"
    ],
    "preview": "\"\"\"\nMIT License\n\nCopyright (c) 2021-2022 MShawon\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,",
    "last_modified": "2025-05-04T23:27:45"
  },
  {
    "id": "492",
    "name": "client.py",
    "path": "01_core_ai_analysis/ai_generation/client.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 263,
    "size": 8174,
    "docstring": "",
    "keywords": [
      "testing",
      "youtube",
      "web_tools"
    ],
    "functions": [
      "requestGames",
      "requestClips",
      "testFTPConnection",
      "requestClipsWithoutClips",
      "uploadFile",
      "VideoGeneratorRenderStatus",
      "exportVideo",
      "requestFinishedVideoList",
      "downloadFinishedVideo"
    ],
    "classes": [],
    "imports": [
      "ftplib",
      "sys",
      "traceback",
      "clientUI",
      "requests",
      "scriptwrapper",
      "settings",
      "time"
    ],
    "preview": "import ftplib\nimport sys\nimport traceback\n\nimport clientUI\nimport requests\nimport scriptwrapper\nimport settings\n\nsettings.generateConfigFile()\nhttpaddress = \"%s:%s\" % (settings.address, settings.HTTP_PORT)\nmax_progress = None\ncurrent_progress = None\nrender_message = None\nmusic_categories = [\"None\"]\nmainMenuWindow = None\n\nfrom time import sleep\n\n",
    "last_modified": "2025-09-13T05:53:31.063273"
  },
  {
    "id": "493",
    "name": "process_leonardo.py",
    "path": "01_core_ai_analysis/ai_generation/process_leonardo.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 137,
    "size": 4831,
    "docstring": "",
    "keywords": [
      "youtube",
      "web_tools"
    ],
    "functions": [
      "download_image",
      "save_metadata_to_file",
      "fetch_and_download_all_generations"
    ],
    "classes": [],
    "imports": [
      "gzip",
      "json",
      "os",
      "concurrent.futures",
      "requests",
      "tqdm"
    ],
    "preview": "import gzip\nimport json\nimport os\nfrom concurrent.futures import ThreadPoolExecutor\n\nimport requests\nfrom tqdm import tqdm\n\n# Configuration\nBASE_URL = (\n    \"https://cloud.leonardo.ai/api/rest/v1/generations/user/f7bb8476-e3f0-4f1f-9a06-4600866cc49c\"\n)\nAUTH_TOKEN = \"Bearer 93043291-957d-4ec1-8c79-ee734abcb6e3\"\nOUTPUT_DIR = \"/Users/steven/Pictures/leodowns\"\nMAX_RECORDS_PER_FILE = 500  # Limit records per JSON file\nMAX_WORKERS = 5  # For parallel downloads\nINCLUDE_IMAGES = True  # Set False to skip image downloads\n\nHEADERS = {\n    \"accept\": \"application/json\",",
    "last_modified": "2025-09-13T05:53:50.665952"
  },
  {
    "id": "494",
    "name": "toc-merge.py",
    "path": "01_core_ai_analysis/ai_generation/toc-merge.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 71,
    "size": 1856,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "create_toc_pdf",
      "merge_pdfs",
      "add_bookmarks"
    ],
    "classes": [],
    "imports": [
      "pypdf",
      "pypdf",
      "PyPDF2",
      "reportlab.lib.pagesizes",
      "reportlab.pdfgen"
    ],
    "preview": "from pypdf import PdfReader as PdfReader_pypdf\nfrom pypdf import PdfWriter as PdfWriter_pypdf\nfrom PyPDF2 import PdfReader, PdfWriter\nfrom reportlab.lib.pagesizes import letter\nfrom reportlab.pdfgen import canvas\n\n\ndef create_toc_pdf(toc_items, output_path):\n    c = canvas.Canvas(output_path, pagesize=letter)\n    width, height = letter\n    c.setFont(\"Helvetica\", 12)\n\n    y_position = height - 40\n    for item in toc_items:\n        text = f\"{item['title']} ... {item['page']}\"\n        c.drawString(40, y_position, text)\n        y_position -= 20\n\n    c.save()\n",
    "last_modified": "2025-05-04T22:47:13.332114"
  },
  {
    "id": "495",
    "name": "keywords.py",
    "path": "01_core_ai_analysis/ai_generation/keywords.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 30,
    "size": 903,
    "docstring": "",
    "keywords": [],
    "functions": [
      "get_keywords"
    ],
    "classes": [],
    "imports": [
      "multi_rake"
    ],
    "preview": "from multi_rake import Rake\n\n\ndef get_keywords(text):\n\n    rake = Rake()\n\n    keywords = rake.apply(text)\n\n    return keywords\n\n\nif __name__ == \"__main__\":\n\n    text = (\n        \"Compatibility of systems of linear constraints over the set of \"\n        \"natural numbers. Criteria of compatibility of a system of linear \"\n        \"Diophantine equations, strict inequations, and nonstrict inequations \"\n        \"are considered. Upper bounds for components of a minimal set of \"\n        \"solutions and algorithms of construction of minimal generating sets \"",
    "last_modified": "2025-03-28T18:35:46.756178"
  },
  {
    "id": "496",
    "name": "prompt_templates.py",
    "path": "01_core_ai_analysis/ai_generation/prompt_templates.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 55,
    "size": 1993,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "separator",
      "create_template",
      "create_template"
    ],
    "classes": [
      "QueryExpansionTemplate",
      "SelfQueryTemplate"
    ],
    "imports": [
      "langchain.prompts",
      "base"
    ],
    "preview": "from langchain.prompts import PromptTemplate\n\nfrom .base import PromptTemplateFactory\n\n\nclass QueryExpansionTemplate(PromptTemplateFactory):\n    prompt: str = \"\"\"You are an AI language model assistant. Your task is to generate {expand_to_n}\n    different versions of the given user question to retrieve relevant documents from a vector\n    database. By generating multiple perspectives on the user question, your goal is to help\n    the user overcome some of the limitations of the distance-based similarity search.\n    Provide these alternative questions seperated by '{separator}'.\n    Original question: {question}\"\"\"\n\n    @property\n    def separator(self) -> str:\n        return \"#next-question#\"\n\n    def create_template(self, expand_to_n: int) -> PromptTemplate:\n        return PromptTemplate(\n            template=self.prompt,",
    "last_modified": "2025-05-04T22:47:11.562551"
  },
  {
    "id": "497",
    "name": "backup-clone-move.py",
    "path": "01_core_ai_analysis/ai_generation/backup-clone-move.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 34,
    "size": 1245,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "os",
      "shutil"
    ],
    "preview": "import os\nimport shutil\n\n# Path to the CSV file\ncsv_path = \"/Users/steven/image_list.csv\"\n\n# The root directory where you want to backup the files\nbackup_root = \"/Volumes/iMac/backup\"\n\n# The root directory where you want to move the files\ndestination_root = \"/Volumes/iMac/ogMac images\"\n\n# Open and read the CSV file line by line\nwith open(csv_path, \"r\") as file:\n    for line in file:\n        file_path = line.strip()\n        if file_path:\n            # For backup: Generate the backup path and copy the file\n            backup_path = os.path.join(\n                backup_root,",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "498",
    "name": "duplicate-appscript.py",
    "path": "01_core_ai_analysis/ai_generation/duplicate-appscript.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 48,
    "size": 1510,
    "docstring": "",
    "keywords": [],
    "functions": [
      "hash_file",
      "find_duplicates"
    ],
    "classes": [],
    "imports": [
      "hashlib",
      "os",
      "collections",
      "sys"
    ],
    "preview": "import hashlib\nimport os\nfrom collections import defaultdict\n\n\ndef hash_file(file_path):\n    \"\"\"Generate MD5 hash for a file.\"\"\"\n    hasher = hashlib.md5()\n    try:\n        with open(file_path, \"rb\") as f:\n            buf = f.read(65536)  # Read in chunks to avoid large memory usage\n            while len(buf) > 0:\n                hasher.update(buf)\n                buf = f.read(65536)\n        return hasher.hexdigest()\n    except IOError:\n        print(f\"Error opening or reading file: {file_path}\")\n        return None\n\n",
    "last_modified": "2025-03-28T18:37:05"
  },
  {
    "id": "499",
    "name": "retriever.py",
    "path": "01_core_ai_analysis/ai_generation/retriever.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 118,
    "size": 4051,
    "docstring": "",
    "keywords": [],
    "functions": [
      "__init__",
      "search",
      "_search",
      "rerank",
      "_search_data_category"
    ],
    "classes": [
      "ContextRetriever"
    ],
    "imports": [
      "concurrent.futures",
      "opik",
      "llm_engineering.application",
      "llm_engineering.application.preprocessing.dispatchers",
      "llm_engineering.domain.embedded_chunks",
      "llm_engineering.domain.queries",
      "loguru",
      "qdrant_client.models",
      "query_expanison",
      "reranking"
    ],
    "preview": "import concurrent.futures\n\nimport opik\nfrom llm_engineering.application import utils\nfrom llm_engineering.application.preprocessing.dispatchers import EmbeddingDispatcher\nfrom llm_engineering.domain.embedded_chunks import (\n    EmbeddedArticleChunk,\n    EmbeddedChunk,\n    EmbeddedPostChunk,\n    EmbeddedRepositoryChunk,\n)\nfrom llm_engineering.domain.queries import EmbeddedQuery, Query\nfrom loguru import logger\nfrom qdrant_client.models import FieldCondition, Filter, MatchValue\n\nfrom .query_expanison import QueryExpansion\nfrom .reranking import Reranker\nfrom .self_query import SelfQuery\n\n",
    "last_modified": "2025-09-13T05:53:41.805315"
  },
  {
    "id": "500",
    "name": "2025-vid.py",
    "path": "01_core_ai_analysis/ai_generation/2025-vid.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 219,
    "size": 6894,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "get_creation_date",
      "get_video_metadata",
      "format_file_size",
      "format_duration",
      "generate_dry_run_csv",
      "write_csv",
      "get_unique_file_path",
      "save_last_directory",
      "load_last_directory"
    ],
    "classes": [],
    "imports": [
      "csv",
      "os",
      "re",
      "datetime",
      "mutagen.easymp4",
      "mutagen.mp4"
    ],
    "preview": "import csv\nimport os\nimport re\nfrom datetime import datetime\n\nfrom mutagen.easymp4 import EasyMP4\nfrom mutagen.mp4 import MP4\n\nLAST_DIRECTORY_FILE = \"vids.txt\"\n\n\n# Function to get the creation date of a file\ndef get_creation_date(filepath):\n    try:\n        return datetime.fromtimestamp(os.path.getctime(filepath)).strftime(\"%m-%d-%y\")\n    except Exception as e:\n        print(f\"Error getting creation date for {filepath}: {e}\")\n        return \"Unknown\"\n\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "501",
    "name": "test_onedrive_gallery_logic.py",
    "path": "01_core_ai_analysis/ai_generation/test_onedrive_gallery_logic.py",
    "category": "01_core_ai_analysis",
    "type": "testing",
    "lines": 65,
    "size": 2147,
    "docstring": "",
    "keywords": [
      "testing"
    ],
    "functions": [
      "test_create_thumbnails",
      "test_generate_images_data"
    ],
    "classes": [
      "OnedriveGalleryTestCase"
    ],
    "imports": [
      "os",
      "unittest",
      "unittest",
      "simplegallery.test.helpers",
      "simplegallery.logic.variants.onedrive_gallery_logic",
      "testfixtures"
    ],
    "preview": "import os\nimport unittest\nfrom unittest import mock\n\nimport simplegallery.test.helpers as helpers\nfrom simplegallery.logic.variants.onedrive_gallery_logic import OnedriveGalleryLogic\nfrom testfixtures import TempDirectory\n\n\nclass OnedriveGalleryTestCase(unittest.TestCase):\n\n    remote_link = \"https://1drv.ms/u/s!AkD5kF--59kUf28rkcQAphGO668?e=ELiTW6\"\n\n    @mock.patch(\"builtins.input\", side_effect=[\"\", \"\", \"\", \"\"])\n    def test_create_thumbnails(self, input):\n        with TempDirectory() as tempdir:\n            # Init files gallery logic\n            gallery_config = helpers.init_gallery_and_read_gallery_config(\n                tempdir.path, self.remote_link\n            )",
    "last_modified": "2025-09-13T05:53:52.882942"
  },
  {
    "id": "502",
    "name": "thumbnail.py",
    "path": "01_core_ai_analysis/ai_generation/thumbnail.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 51,
    "size": 1481,
    "docstring": "",
    "keywords": [
      "opencv",
      "analysis"
    ],
    "functions": [
      "create_thumbnails",
      "create_thumbnail_frame"
    ],
    "classes": [],
    "imports": [
      "random",
      "cv2",
      "PIL",
      "utils"
    ],
    "preview": "import random\n\nimport cv2\nfrom PIL import Image, ImageDraw, ImageFont\n\nfrom .utils import resource_path\n\n\ndef create_thumbnails(video_filepath: str, options: dict = {}):\n    vidcap = cv2.VideoCapture(video_filepath)\n    total_frames = int(vidcap.get(cv2.CAP_PROP_FRAME_COUNT))\n\n    amount = options[\"amount\"]\n    font_size = options[\"font_size\"]\n    title = options[\"title\"]\n\n    results = []\n    for _ in range(amount):\n        random_frame = random.randint(total_frames // 10, total_frames * 9 // 10)\n        results.append(create_thumbnail_frame(vidcap, random_frame, title, font_size))",
    "last_modified": "2025-05-04T22:47:12"
  },
  {
    "id": "503",
    "name": "pydoc.py",
    "path": "01_core_ai_analysis/ai_generation/pydoc.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 19,
    "size": 595,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "generate_docs"
    ],
    "classes": [],
    "imports": [
      "os",
      "subprocess"
    ],
    "preview": "import os\nimport subprocess\n\n\ndef generate_docs(directory):\n    # Walk through the directory and its subdirectories\n    for root, dirs, files in os.walk(directory):\n        for file in files:\n            if file.endswith(\".py\"):\n                file_path = os.path.join(root, file)\n                print(f\"Generating documentation for {file_path}\")\n                # Generate documentation using pydoc\n                subprocess.run([\"python3\", \"-m\", \"pydoc\", \"-w\", file_path])\n\n\n# Specify the directory to generate docs for\ndirectory = \"/Users/steven/Documents/Python\"\ngenerate_docs(directory)\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "504",
    "name": "docs.py",
    "path": "01_core_ai_analysis/ai_generation/docs.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 205,
    "size": 6494,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "get_creation_date",
      "format_file_size",
      "generate_dry_run_csv",
      "write_csv",
      "get_unique_file_path",
      "save_last_directory",
      "load_last_directory"
    ],
    "classes": [],
    "imports": [
      "csv",
      "os",
      "re",
      "datetime",
      "config"
    ],
    "preview": "import csv\nimport os\nimport re\nfrom datetime import datetime\n\nimport config  # Import the configuration\n\n# Constants\nLAST_DIRECTORY_FILE = \"docs.txt\"\n\n# Function to get the creation date of a file\n\n\ndef get_creation_date(filepath):\n    try:\n        return datetime.fromtimestamp(os.path.getctime(filepath)).strftime(\"%m-%d-%y\")\n    except Exception as e:\n        print(f\"Error getting creation date for {filepath}: {e}\")\n        return \"Unknown\"\n",
    "last_modified": "2025-09-13T05:53:48.416059"
  },
  {
    "id": "505",
    "name": "other.py",
    "path": "01_core_ai_analysis/ai_generation/other.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 235,
    "size": 7330,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "get_creation_date",
      "format_file_size",
      "generate_dry_run_csv",
      "write_csv",
      "get_unique_file_path",
      "save_last_directory",
      "load_last_directory"
    ],
    "classes": [],
    "imports": [
      "csv",
      "os",
      "re",
      "datetime",
      "config"
    ],
    "preview": "import csv\nimport os\nimport re\nfrom datetime import datetime\n\nimport config  # Import the configuration\n\n# Constants\nLAST_DIRECTORY_FILE = \"other.txt\"\n\n# Function to get the creation date of a file\n\n\ndef get_creation_date(filepath):\n    try:\n        return datetime.fromtimestamp(os.path.getctime(filepath)).strftime(\"%m-%d-%y\")\n    except Exception as e:\n        print(f\"Error getting creation date for {filepath}: {e}\")\n        return \"Unknown\"\n",
    "last_modified": "2025-09-13T05:53:48.616079"
  },
  {
    "id": "506",
    "name": "archives.py",
    "path": "01_core_ai_analysis/ai_generation/archives.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 33,
    "size": 1100,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "create_tar_gz"
    ],
    "classes": [],
    "imports": [
      "os",
      "tarfile"
    ],
    "preview": "import os\nimport tarfile\n\n# List of directories to include in the tar.gz archive\ndirectories = [\n    \"/Users/steven/Pictures/other-07-25-05:32.csv\",\n    \"/Users/steven/Pictures/audio-07-25-05:31.csv\",\n    \"/Users/steven/Pictures/docs-07-25-05:31.csv\",\n    \"/Users/steven/Pictures/image_data-07-25-05:32.csv\",\n    \"/Users/steven/Pictures/vids-07-25-05:34.csv\",\n]\n\n# Output tar.gz file\noutput_filename = \"/Users/steven/Pictures/data_archive.tar.gz\"\n\n\n# Function to create tar.gz archive\ndef create_tar_gz(output_filename, directories):\n    with tarfile.open(output_filename, \"w:gz\") as tar:\n        for dir_path in directories:",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "507",
    "name": "video_text_test.py",
    "path": "01_core_ai_analysis/ai_generation/video_text_test.py",
    "category": "01_core_ai_analysis",
    "type": "youtube",
    "lines": 9,
    "size": 208,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "clips"
    ],
    "preview": "from clips import *\n\nclips = []\nclips.append(create_comment_clip(author=\"adsf\", content=\"Why is wrong with this?\"))\n\nvideo = concatenate_videoclips(clips)\n\nvideo.write_videofile(\"media/test_out.mp4\", fps=24)\n",
    "last_modified": "2025-03-28T18:35:48"
  },
  {
    "id": "508",
    "name": "playlist_20221230180809.py",
    "path": "01_core_ai_analysis/ai_generation/playlist_20221230180809.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 212,
    "size": 6700,
    "docstring": "",
    "keywords": [
      "youtube",
      "analysis",
      "web_tools"
    ],
    "functions": [
      "generate_token",
      "get_youtube_url",
      "download_mp3",
      "last_fm_artist_info",
      "set_metadata",
      "write_tracks",
      "write_playlist",
      "split_spotify_uri",
      "get_os",
      "get_folder"
    ],
    "classes": [],
    "imports": [
      "argparse",
      "json",
      "os",
      "re",
      "subprocess",
      "urllib.parse",
      "urllib.request",
      "secret",
      "spotipy",
      "spotipy.oauth2"
    ],
    "preview": "import argparse\nimport json\nimport os\nimport re\nimport subprocess\nimport urllib.parse\nimport urllib.request\n\nimport secret\nimport spotipy\nimport spotipy.oauth2 as oauth2\nfrom requests.utils import quote\n\n\ndef generate_token():\n    credentials = oauth2.SpotifyClientCredentials(\n        client_id=secret.SPOTIFY_CLIENT_ID, client_secret=secret.SPOTIFY_CLIENT_SECRET\n    )\n    token = credentials.get_access_token()\n    return token",
    "last_modified": "2025-09-13T05:54:02.085648"
  },
  {
    "id": "509",
    "name": "process_leonardo_20250102110441.py",
    "path": "01_core_ai_analysis/ai_generation/process_leonardo_20250102110441.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 137,
    "size": 4831,
    "docstring": "",
    "keywords": [
      "youtube",
      "web_tools"
    ],
    "functions": [
      "download_image",
      "save_metadata_to_file",
      "fetch_and_download_all_generations"
    ],
    "classes": [],
    "imports": [
      "gzip",
      "json",
      "os",
      "concurrent.futures",
      "requests",
      "tqdm"
    ],
    "preview": "import gzip\nimport json\nimport os\nfrom concurrent.futures import ThreadPoolExecutor\n\nimport requests\nfrom tqdm import tqdm\n\n# Configuration\nBASE_URL = (\n    \"https://cloud.leonardo.ai/api/rest/v1/generations/user/f7bb8476-e3f0-4f1f-9a06-4600866cc49c\"\n)\nAUTH_TOKEN = \"Bearer 93043291-957d-4ec1-8c79-ee734abcb6e3\"\nOUTPUT_DIR = \"/Users/steven/Pictures/leodowns\"\nMAX_RECORDS_PER_FILE = 500  # Limit records per JSON file\nMAX_WORKERS = 5  # For parallel downloads\nINCLUDE_IMAGES = True  # Set False to skip image downloads\n\nHEADERS = {\n    \"accept\": \"application/json\",",
    "last_modified": "2025-09-13T05:53:49.891924"
  },
  {
    "id": "510",
    "name": "generate_album_pages 2.py",
    "path": "01_core_ai_analysis/ai_generation/generate_album_pages 2.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 78,
    "size": 2806,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "os"
    ],
    "preview": "import os\n\n# Directory paths\nalbums_dir = \"/Users/steven/Music/nocTurneMeLoDieS/assests/all\"\noutput_dir = albums_dir  # Output HTML in the root music_site directory\ncss_path = \"/css/styles.css\"\n\n# Loop through each album folder\nfor album in os.listdir(albums_dir):\n    album_path = os.path.join(albums_dir, album)\n\n    # Check if it's a directory and contains the required files\n    if os.path.isdir(album_path):\n        cover_img = os.path.join(album_path, f\"{album}.png\")\n        audio_file = os.path.join(album_path, f\"{album}.mp3\")\n        lyrics_file = os.path.join(album_path, f\"{album}_transcript.txt\")\n        analysis_file = os.path.join(album_path, f\"{album}_analysis.txt\")\n\n        # Check if required files exist\n        if (",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "511",
    "name": "gallery_init.py",
    "path": "01_core_ai_analysis/ai_generation/gallery_init.py",
    "category": "01_core_ai_analysis",
    "type": "setup",
    "lines": 292,
    "size": 9791,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "parse_args",
      "check_if_gallery_creation_possible",
      "check_if_gallery_already_exists",
      "create_gallery_folder_structure",
      "create_gallery_json",
      "main"
    ],
    "classes": [],
    "imports": [
      "argparse",
      "glob",
      "importlib.resources",
      "json",
      "os",
      "shutil",
      "sys",
      "distutils.dir_util",
      "simplegallery.common",
      "simplegallery.logic.gallery_logic"
    ],
    "preview": "import argparse\nimport glob\nimport importlib.resources\nimport json\nimport os\nimport shutil\nimport sys\nfrom distutils.dir_util import copy_tree\n\nimport simplegallery.common as spg_common\nimport simplegallery.logic.gallery_logic as gallery_logic\n\n\ndef parse_args():\n    \"\"\"\n    Configures the argument parser\n    :return: Parsed arguments\n    \"\"\"\n\n    description = \"\"\"Initializes a new Simple Photo Gallery in the specified folder (default is the current folder).",
    "last_modified": "2025-09-13T05:53:52.418972"
  },
  {
    "id": "512",
    "name": "img.py",
    "path": "01_core_ai_analysis/ai_generation/img.py",
    "category": "01_core_ai_analysis",
    "type": "image_processing",
    "lines": 217,
    "size": 7032,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "get_creation_date",
      "get_image_metadata",
      "format_file_size",
      "generate_csv",
      "write_csv",
      "get_unique_file_path",
      "save_last_directory",
      "load_last_directory"
    ],
    "classes": [],
    "imports": [
      "csv",
      "os",
      "re",
      "datetime",
      "PIL"
    ],
    "preview": "import csv\nimport os\nimport re\nfrom datetime import datetime\n\nfrom PIL import Image\n\n# Constants\nLAST_DIRECTORY_FILE = \"image_data.txt\"\n\n\n# Function to get the creation date of a file\ndef get_creation_date(filepath):\n    try:\n        return datetime.fromtimestamp(os.path.getctime(filepath)).strftime(\"%m-%d-%y\")\n    except Exception as e:\n        print(f\"Error getting creation date for {filepath}: {e}\")\n        return \"Unknown\"\n\n",
    "last_modified": "2025-09-13T05:53:48.506154"
  },
  {
    "id": "513",
    "name": "process_leonardo_20250102110258.py",
    "path": "01_core_ai_analysis/ai_generation/process_leonardo_20250102110258.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 137,
    "size": 4839,
    "docstring": "",
    "keywords": [
      "youtube",
      "web_tools"
    ],
    "functions": [
      "download_image",
      "save_metadata_to_file",
      "fetch_and_download_all_generations"
    ],
    "classes": [],
    "imports": [
      "gzip",
      "json",
      "os",
      "concurrent.futures",
      "requests",
      "tqdm"
    ],
    "preview": "import gzip\nimport json\nimport os\nfrom concurrent.futures import ThreadPoolExecutor\n\nimport requests\nfrom tqdm import tqdm\n\n# Configuration\nBASE_URL = (\n    \"https://cloud.leonardo.ai/api/rest/v1/generations/user/f7bb8476-e3f0-4f1f-9a06-4600866cc49c\"\n)\nAUTH_TOKEN = \"Bearer 93043291-957d-4ec1-8c79-ee734abcb6e3\"\nOUTPUT_DIR = \"/Users/steven/Downloads/leonardo_images\"\nMAX_RECORDS_PER_FILE = 500  # Limit records per JSON file\nMAX_WORKERS = 5  # For parallel downloads\nINCLUDE_IMAGES = True  # Set False to skip image downloads\n\nHEADERS = {\n    \"accept\": \"application/json\",",
    "last_modified": "2025-09-13T05:53:49.825950"
  },
  {
    "id": "514",
    "name": "pics.py",
    "path": "01_core_ai_analysis/ai_generation/pics.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 217,
    "size": 6519,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "get_creation_date",
      "get_image_metadata",
      "format_file_size",
      "generate_csv",
      "generate_dry_run_csv",
      "write_csv",
      "get_unique_file_path",
      "save_last_directory",
      "load_last_directory"
    ],
    "classes": [],
    "imports": [
      "csv",
      "os",
      "datetime",
      "PIL"
    ],
    "preview": "import csv\nimport os\nfrom datetime import datetime\n\nfrom PIL import Image\n\nLAST_DIRECTORY_FILE = \"images.txt\"\n\n# Function to get the creation date of a file\n\n\ndef get_creation_date(filepath):\n    try:\n        return datetime.fromtimestamp(os.path.getctime(filepath)).strftime(\"%m-%d-%y\")\n    except Exception as e:\n        print(f\"Error getting creation date for {filepath}: {e}\")\n        return \"Unknown\"\n\n\n# Function to extract metadata from an image file using PIL",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "515",
    "name": "generate_songs_csv 1.py",
    "path": "01_core_ai_analysis/ai_generation/generate_songs_csv 1.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 64,
    "size": 2088,
    "docstring": "",
    "keywords": [],
    "functions": [
      "get_matching_files"
    ],
    "classes": [],
    "imports": [
      "csv",
      "os"
    ],
    "preview": "import csv\nimport os\n\n# Define the directories\nmp3_dir = \"/Users/steven/Music/suno/mp3\"\ntxt_dir = \"/Users/steven/Music/suno/txt\"\ncsv_output = \"/Users/steven/Music/suno/music_project/songs_data.csv\"\n\n# Collect the list of MP3 and text files\nmp3_files = [f for f in os.listdir(mp3_dir) if f.endswith(\".mp3\")]\ntxt_files = [f for f in os.listdir(txt_dir) if f.endswith(\".txt\")]\n\n\n# Function to match song with corresponding text files\ndef get_matching_files(song_title, txt_files):\n    analysis_file = None\n    transcript_file = None\n    base_title = song_title.replace(\".mp3\", \"\")\n    for txt in txt_files:\n        if base_title in txt:",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "516",
    "name": "voice.py",
    "path": "01_core_ai_analysis/ai_generation/voice.py",
    "category": "01_core_ai_analysis",
    "type": "transcription",
    "lines": 43,
    "size": 1433,
    "docstring": "Synthesizes speech from the input string of text or ssml.\nMake sure to be working in a virtual environment.\n\nNote: ssml must be well-formed according to:\n    https://www.w3.org/TR/speech-synthesis/",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "generateVoice"
    ],
    "classes": [],
    "imports": [
      "os",
      "google.cloud"
    ],
    "preview": "\"\"\"Synthesizes speech from the input string of text or ssml.\nMake sure to be working in a virtual environment.\n\nNote: ssml must be well-formed according to:\n    https://www.w3.org/TR/speech-synthesis/\n\"\"\"\n\nimport os\n\nfrom google.cloud import texttospeech\n\nos.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"./credentials/key-gg-tts.json\"\n\n\ndef generateVoice(text, output):\n    # Instantiates a client\n    client = texttospeech.TextToSpeechClient()\n\n    # Set the text input to be synthesized\n    synthesis_input = texttospeech.SynthesisInput(text=text)",
    "last_modified": "2025-09-13T05:53:29.724622"
  },
  {
    "id": "517",
    "name": "command_context.py",
    "path": "01_core_ai_analysis/ai_generation/command_context.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 28,
    "size": 774,
    "docstring": "",
    "keywords": [],
    "functions": [
      "__init__",
      "main_context",
      "enter_context"
    ],
    "classes": [
      "CommandContextMixIn"
    ],
    "imports": [
      "contextlib",
      "typing"
    ],
    "preview": "from contextlib import ExitStack, contextmanager\nfrom typing import ContextManager, Generator, TypeVar\n\n_T = TypeVar(\"_T\", covariant=True)\n\n\nclass CommandContextMixIn:\n    def __init__(self) -> None:\n        super().__init__()\n        self._in_main_context = False\n        self._main_context = ExitStack()\n\n    @contextmanager\n    def main_context(self) -> Generator[None, None, None]:\n        assert not self._in_main_context\n\n        self._in_main_context = True\n        try:\n            with self._main_context:\n                yield",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "518",
    "name": "jobs.py",
    "path": "01_core_ai_analysis/ai_generation/jobs.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 291,
    "size": 8457,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "get_unique_filename",
      "create_excerpt",
      "generate_job_cards",
      "write_html_job_cards",
      "generate_table_header",
      "generate_table_body",
      "write_html_table",
      "main"
    ],
    "classes": [],
    "imports": [
      "csv",
      "html",
      "os"
    ],
    "preview": "import csv\nimport html\nimport os\n\n\ndef get_unique_filename(filepath):\n    \"\"\"\n    If the filepath already exists, append _1, _2, ... until a unique filename is found.\n    Returns the unique filepath.\n    \"\"\"\n    base, ext = os.path.splitext(filepath)\n    counter = 1\n    unique_path = filepath\n    while os.path.exists(unique_path):\n        unique_path = f\"{base}_{counter}{ext}\"\n        counter += 1\n    return unique_path\n\n\n# Prompt the user for input CSV file path and output base name",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "519",
    "name": "vids.py",
    "path": "01_core_ai_analysis/ai_generation/vids.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 239,
    "size": 7360,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "get_creation_date",
      "get_video_metadata",
      "format_file_size",
      "format_duration",
      "generate_dry_run_csv",
      "write_csv",
      "get_unique_file_path",
      "save_last_directory",
      "load_last_directory"
    ],
    "classes": [],
    "imports": [
      "csv",
      "os",
      "re",
      "datetime",
      "mutagen.easymp4",
      "mutagen.mp4",
      "config"
    ],
    "preview": "import csv\nimport os\nimport re\nfrom datetime import datetime\n\nfrom mutagen.easymp4 import EasyMP4\nfrom mutagen.mp4 import MP4\n\nimport config  # Import the configuration\n\nLAST_DIRECTORY_FILE = \"vids.txt\"\n\n# Function to get the creation date of a file\n\n\ndef get_creation_date(filepath):\n    try:\n        return datetime.fromtimestamp(os.path.getctime(filepath)).strftime(\"%m-%d-%y\")\n    except Exception as e:\n        print(f\"Error getting creation date for {filepath}: {e}\")",
    "last_modified": "2025-09-13T05:53:48.717566"
  },
  {
    "id": "520",
    "name": "TextToSpeech.py",
    "path": "01_core_ai_analysis/ai_generation/TextToSpeech.py",
    "category": "01_core_ai_analysis",
    "type": "transcription",
    "lines": 45,
    "size": 1420,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "__init__",
      "create_dir",
      "create_tts"
    ],
    "classes": [
      "TextToSpeech"
    ],
    "imports": [
      "os",
      "sys",
      "gtts"
    ],
    "preview": "# TextToSpeech.py\n# Called in run.py after RedditScrape.py has scraped all of the content\n# Creates an audio file for each scraped post\n#\n#\n\n\nimport os\nimport sys\n\n# including the text to speech API: subject to change\nfrom gtts import gTTS\n\nsys.path.append(\"../\")\n\n\nclass TextToSpeech:\n\n    def __init__(self):\n        self.audio_path = \"../audio/\"",
    "last_modified": "2025-05-04T23:28:21.785924"
  },
  {
    "id": "521",
    "name": "img 2.py",
    "path": "01_core_ai_analysis/ai_generation/img 2.py",
    "category": "01_core_ai_analysis",
    "type": "image_processing",
    "lines": 217,
    "size": 7032,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "get_creation_date",
      "get_image_metadata",
      "format_file_size",
      "generate_csv",
      "write_csv",
      "get_unique_file_path",
      "save_last_directory",
      "load_last_directory"
    ],
    "classes": [],
    "imports": [
      "csv",
      "os",
      "re",
      "datetime",
      "PIL"
    ],
    "preview": "import csv\nimport os\nimport re\nfrom datetime import datetime\n\nfrom PIL import Image\n\n# Constants\nLAST_DIRECTORY_FILE = \"image_data.txt\"\n\n\n# Function to get the creation date of a file\ndef get_creation_date(filepath):\n    try:\n        return datetime.fromtimestamp(os.path.getctime(filepath)).strftime(\"%m-%d-%y\")\n    except Exception as e:\n        print(f\"Error getting creation date for {filepath}: {e}\")\n        return \"Unknown\"\n\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "522",
    "name": "spinners.py",
    "path": "01_core_ai_analysis/ai_generation/spinners.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 160,
    "size": 5118,
    "docstring": "",
    "keywords": [],
    "functions": [
      "open_spinner",
      "hidden_cursor",
      "spin",
      "finish",
      "__init__",
      "_write",
      "spin",
      "finish",
      "__init__",
      "_update"
    ],
    "classes": [
      "SpinnerInterface",
      "InteractiveSpinner",
      "NonInteractiveSpinner",
      "RateLimiter"
    ],
    "imports": [
      "contextlib",
      "itertools",
      "logging",
      "sys",
      "time",
      "typing",
      "pip._internal.utils.compat",
      "pip._internal.utils.logging"
    ],
    "preview": "import contextlib\nimport itertools\nimport logging\nimport sys\nimport time\nfrom typing import IO, Generator, Optional\n\nfrom pip._internal.utils.compat import WINDOWS\nfrom pip._internal.utils.logging import get_indentation\n\nlogger = logging.getLogger(__name__)\n\n\nclass SpinnerInterface:\n    def spin(self) -> None:\n        raise NotImplementedError()\n\n    def finish(self, final_status: str) -> None:\n        raise NotImplementedError()\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "523",
    "name": "autocompletion.py",
    "path": "01_core_ai_analysis/ai_generation/autocompletion.py",
    "category": "01_core_ai_analysis",
    "type": "setup",
    "lines": 163,
    "size": 6591,
    "docstring": "Logic that powers autocompletion installed by ``pip completion``.",
    "keywords": [],
    "functions": [
      "autocomplete",
      "get_path_completion_type",
      "auto_complete_paths"
    ],
    "classes": [],
    "imports": [
      "optparse",
      "os",
      "sys",
      "itertools",
      "typing",
      "pip._internal.cli.main_parser",
      "pip._internal.commands",
      "pip._internal.metadata"
    ],
    "preview": "\"\"\"Logic that powers autocompletion installed by ``pip completion``.\"\"\"\n\nimport optparse\nimport os\nimport sys\nfrom itertools import chain\nfrom typing import Any, Iterable, List, Optional\n\nfrom pip._internal.cli.main_parser import create_main_parser\nfrom pip._internal.commands import commands_dict, create_command\nfrom pip._internal.metadata import get_default_environment\n\n\ndef autocomplete() -> None:\n    \"\"\"Entry Point for completion of main and subcommand options.\"\"\"\n    # Don't complete if user hasn't sourced bash_completion file.\n    if \"PIP_AUTO_COMPLETE\" not in os.environ:\n        return\n    cwords = os.environ[\"COMP_WORDS\"].split()[1:]\n    cword = int(os.environ[\"COMP_CWORD\"])",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "524",
    "name": "contact-sheet.py",
    "path": "01_core_ai_analysis/ai_generation/contact-sheet.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 36,
    "size": 1125,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "create_contact_sheet"
    ],
    "classes": [],
    "imports": [
      "os",
      "PIL"
    ],
    "preview": "import os\n\nfrom PIL import Image\n\n\ndef create_contact_sheet(\n    directory, image_size=(600, 600), grid_size=(5, 5), sheet_size=(3000, 3000)\n):\n    contact_sheet = Image.new(\"RGB\", sheet_size)\n    x_offset, y_offset = 0, 0\n    count = 0\n\n    for subdir, dirs, files in os.walk(directory):\n        for file in files:\n            if file.lower().endswith((\"png\", \"jpg\", \"jpeg\", \"gif\", \"bmp\")):\n                image_path = os.path.join(subdir, file)\n                with Image.open(image_path) as img:\n                    img = img.resize(image_size)\n                    contact_sheet.paste(img, (x_offset, y_offset))\n                    x_offset += image_size[0]",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "525",
    "name": "setuptools_build.py",
    "path": "01_core_ai_analysis/ai_generation/setuptools_build.py",
    "category": "01_core_ai_analysis",
    "type": "setup",
    "lines": 147,
    "size": 4435,
    "docstring": "",
    "keywords": [],
    "functions": [
      "make_setuptools_shim_args",
      "make_setuptools_bdist_wheel_args",
      "make_setuptools_clean_args",
      "make_setuptools_develop_args",
      "make_setuptools_egg_info_args"
    ],
    "classes": [],
    "imports": [
      "sys",
      "textwrap",
      "typing"
    ],
    "preview": "import sys\nimport textwrap\nfrom typing import List, Optional, Sequence\n\n# Shim to wrap setup.py invocation with setuptools\n# Note that __file__ is handled via two {!r} *and* %r, to ensure that paths on\n# Windows are correctly handled (it should be \"C:\\\\Users\" not \"C:\\Users\").\n_SETUPTOOLS_SHIM = textwrap.dedent(\n    \"\"\"\n    exec(compile('''\n    # This is <pip-setuptools-caller> -- a caller that pip uses to run setup.py\n    #\n    # - It imports setuptools before invoking setup.py, to enable projects that directly\n    #   import from `distutils.core` to work with newer packaging standards.\n    # - It provides a clear error message when setuptools is not installed.\n    # - It sets `sys.argv[0]` to the underlying `setup.py`, when invoking `setup.py` so\n    #   setuptools doesn't think the script is `-c`. This avoids the following warning:\n    #     manifest_maker: standard file '-c' not found\".\n    # - It generates a shim setup.py, for handling setup.cfg-only projects.\n    import os, sys, tokenize",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "526",
    "name": "pydoc_20250504224712.py",
    "path": "01_core_ai_analysis/ai_generation/pydoc_20250504224712.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 19,
    "size": 595,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "generate_docs"
    ],
    "classes": [],
    "imports": [
      "os",
      "subprocess"
    ],
    "preview": "import os\nimport subprocess\n\n\ndef generate_docs(directory):\n    # Walk through the directory and its subdirectories\n    for root, dirs, files in os.walk(directory):\n        for file in files:\n            if file.endswith(\".py\"):\n                file_path = os.path.join(root, file)\n                print(f\"Generating documentation for {file_path}\")\n                # Generate documentation using pydoc\n                subprocess.run([\"python3\", \"-m\", \"pydoc\", \"-w\", file_path])\n\n\n# Specify the directory to generate docs for\ndirectory = \"/Users/steven/Documents/Python\"\ngenerate_docs(directory)\n",
    "last_modified": "2025-05-06T19:59:20.309260"
  },
  {
    "id": "527",
    "name": "process_leonardo_20250102110442.py",
    "path": "01_core_ai_analysis/ai_generation/process_leonardo_20250102110442.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 137,
    "size": 4831,
    "docstring": "",
    "keywords": [
      "youtube",
      "web_tools"
    ],
    "functions": [
      "download_image",
      "save_metadata_to_file",
      "fetch_and_download_all_generations"
    ],
    "classes": [],
    "imports": [
      "gzip",
      "json",
      "os",
      "concurrent.futures",
      "requests",
      "tqdm"
    ],
    "preview": "import gzip\nimport json\nimport os\nfrom concurrent.futures import ThreadPoolExecutor\n\nimport requests\nfrom tqdm import tqdm\n\n# Configuration\nBASE_URL = (\n    \"https://cloud.leonardo.ai/api/rest/v1/generations/user/f7bb8476-e3f0-4f1f-9a06-4600866cc49c\"\n)\nAUTH_TOKEN = \"Bearer 93043291-957d-4ec1-8c79-ee734abcb6e3\"\nOUTPUT_DIR = \"/Users/steven/Pictures/leodowns\"\nMAX_RECORDS_PER_FILE = 500  # Limit records per JSON file\nMAX_WORKERS = 5  # For parallel downloads\nINCLUDE_IMAGES = True  # Set False to skip image downloads\n\nHEADERS = {\n    \"accept\": \"application/json\",",
    "last_modified": "2025-09-13T05:53:49.959583"
  },
  {
    "id": "528",
    "name": "reranking.py",
    "path": "01_core_ai_analysis/ai_generation/reranking.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 32,
    "size": 1053,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "__init__",
      "generate"
    ],
    "classes": [
      "Reranker"
    ],
    "imports": [
      "opik",
      "llm_engineering.application.networks",
      "llm_engineering.domain.embedded_chunks",
      "llm_engineering.domain.queries",
      "base"
    ],
    "preview": "import opik\nfrom llm_engineering.application.networks import CrossEncoderModelSingleton\nfrom llm_engineering.domain.embedded_chunks import EmbeddedChunk\nfrom llm_engineering.domain.queries import Query\n\nfrom .base import RAGStep\n\n\nclass Reranker(RAGStep):\n    def __init__(self, mock: bool = False) -> None:\n        super().__init__(mock=mock)\n\n        self._model = CrossEncoderModelSingleton()\n\n    @opik.track(name=\"Reranker.generate\")\n    def generate(\n        self, query: Query, chunks: list[EmbeddedChunk], keep_top_k: int\n    ) -> list[EmbeddedChunk]:\n        if self._mock:\n            return chunks",
    "last_modified": "2025-05-06T04:35:14.982898"
  },
  {
    "id": "529",
    "name": "generate_album_pages 1.py",
    "path": "01_core_ai_analysis/ai_generation/generate_album_pages 1.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 90,
    "size": 2534,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "read_text_file",
      "generate_album_html"
    ],
    "classes": [],
    "imports": [
      "os"
    ],
    "preview": "import os\n\nTEMPLATE = \"\"\"\n<!DOCTYPE html>\n<html lang=\"en\">\n<head>\n    <meta charset=\"UTF-8\">\n    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n    <title>{album_name}</title>\n    <link rel=\"stylesheet\" href=\"../css/styles.css\">\n</head>\n<body>\n    <div class=\"album-container\">\n        <div class=\"album\">\n            <img src=\"../albums/{folder_name}/album_cover.jpg\" alt=\"{album_name} Cover\">\n            <h3>{album_name}</h3>\n            <audio controls>\n                <source src=\"../albums/{folder_name}/{mp3_file}\" type=\"audio/mpeg\">\n                Your browser does not support the audio element.\n            </audio>",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "530",
    "name": "google_gallery_logic.py",
    "path": "01_core_ai_analysis/ai_generation/google_gallery_logic.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 101,
    "size": 3724,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "parse_photo_link",
      "create_thumbnails",
      "generate_images_data"
    ],
    "classes": [
      "GoogleGalleryLogic"
    ],
    "imports": [
      "time",
      "pkg_resources",
      "simplegallery.common",
      "simplegallery.media",
      "selenium",
      "selenium.webdriver.firefox.options",
      "simplegallery.logic.base_gallery_logic"
    ],
    "preview": "import time\n\nimport pkg_resources\nimport simplegallery.common as spg_common\nimport simplegallery.media as spg_media\nfrom selenium import webdriver\nfrom selenium.webdriver.firefox.options import Options\nfrom simplegallery.logic.base_gallery_logic import BaseGalleryLogic\n\n\ndef parse_photo_link(photo_url):\n    \"\"\"\n    Extracts the base URL (URL without query parameters) and the photo name from a Onedrive photo URL\n    :param photo_url: photo URL\n    :return: base URL and photo name\n    \"\"\"\n    base_url = photo_url.split(\"=\")[0]\n    name = base_url.split(\"/\")[-1]\n\n    return base_url, name",
    "last_modified": "2025-09-13T05:53:52.572855"
  },
  {
    "id": "531",
    "name": "MakeCompilation.py",
    "path": "01_core_ai_analysis/ai_generation/MakeCompilation.py",
    "category": "01_core_ai_analysis",
    "type": "youtube",
    "lines": 70,
    "size": 1821,
    "docstring": "2. Compile video out of clips",
    "keywords": [
      "testing",
      "analysis"
    ],
    "functions": [
      "select_videos",
      "create_video",
      "create_description",
      "test_mod"
    ],
    "classes": [],
    "imports": [
      "os",
      "random",
      "shutil",
      "moviepy",
      "moviepy.editor"
    ],
    "preview": "\"\"\"\n2. Compile video out of clips\n\"\"\"\n\nimport os\nimport random\nimport shutil\n\nimport moviepy\nfrom moviepy.editor import VideoFileClip, concatenate_videoclips\n\nOUTPUT_DIR = r\"directory where the compiled video will be saved\"\nDAILY_TRENDING_DIR = r\"directory location for downloaded tiktok videos\"\nTOP_TEN_VIDS_DIR = r\"directory location for top 10 videos\"\n\n\ndef select_videos(daily_trending_videos, top_videos):\n    os.chdir(daily_trending_videos)\n    trending_list = os.listdir(daily_trending_videos)\n    daily_vids = []",
    "last_modified": "2025-05-04T23:28:21"
  },
  {
    "id": "532",
    "name": "playlist.py",
    "path": "01_core_ai_analysis/ai_generation/playlist.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 214,
    "size": 6717,
    "docstring": "",
    "keywords": [
      "youtube",
      "analysis",
      "web_tools"
    ],
    "functions": [
      "generate_token",
      "get_youtube_url",
      "download_mp3",
      "last_fm_artist_info",
      "set_metadata",
      "write_tracks",
      "write_playlist",
      "split_spotify_uri",
      "get_os",
      "get_folder"
    ],
    "classes": [],
    "imports": [
      "argparse",
      "json",
      "os",
      "re",
      "subprocess",
      "urllib.parse",
      "urllib.request",
      "secret",
      "spotipy",
      "spotipy.oauth2"
    ],
    "preview": "import argparse\nimport json\nimport os\nimport re\nimport subprocess\nimport urllib.parse\nimport urllib.request\n\nimport secret\nimport spotipy\nimport spotipy.oauth2 as oauth2\nfrom requests.utils import quote\n\n\ndef generate_token():\n    credentials = oauth2.SpotifyClientCredentials(\n        client_id=secret.SPOTIFY_CLIENT_ID, client_secret=secret.SPOTIFY_CLIENT_SECRET\n    )\n    token = credentials.get_access_token()\n    return token",
    "last_modified": "2025-09-13T05:54:02.203892"
  },
  {
    "id": "533",
    "name": "conda_optimizer.py",
    "path": "01_core_ai_analysis/ai_generation/conda_optimizer.py",
    "category": "01_core_ai_analysis",
    "type": "organization",
    "lines": 315,
    "size": 10899,
    "docstring": "Conda Environment Optimizer\nProvides recommendations for cleaning up and optimizing conda environments.",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "load_report",
      "find_similar_environments",
      "find_unused_environments",
      "analyze_large_packages",
      "calculate_potential_savings",
      "generate_recommendations",
      "format_size",
      "generate_cleanup_script",
      "main"
    ],
    "classes": [],
    "imports": [
      "json",
      "pathlib",
      "collections"
    ],
    "preview": "#!/usr/bin/env python3\n\"\"\"\nConda Environment Optimizer\nProvides recommendations for cleaning up and optimizing conda environments.\n\"\"\"\n\nimport json\nfrom pathlib import Path\nfrom collections import defaultdict\n\ndef load_report(filename=\"conda_env_report.json\"):\n    \"\"\"Load the analysis report.\"\"\"\n    with open(filename) as f:\n        return json.load(f)\n\ndef find_similar_environments(environments):\n    \"\"\"Find environments with high package overlap.\"\"\"\n    existing_envs = [e for e in environments if e[\"exists\"]]\n    similar_pairs = []\n    ",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "534",
    "name": "lazy 1.py",
    "path": "01_core_ai_analysis/ai_generation/lazy 1.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 161,
    "size": 6963,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "parse_album_data",
      "generate_html",
      "main"
    ],
    "classes": [],
    "imports": [
      "os"
    ],
    "preview": "import os\n\n\ndef parse_album_data(paths_file):\n    albums = {}\n    with open(paths_file, \"r\", encoding=\"utf-8\") as f:\n        for line in f:\n            line = line.strip()\n            if not line:\n                continue\n            # Process only if the line is in the mp4 folder structure\n            if \"/mp4/\" in line:\n                # Extract album folder name by splitting at \"/mp4/\" then taking first folder\n                parts = line.split(\"/mp4/\")[1].split(\"/\")\n                if not parts:\n                    continue\n                album_key = parts[0]\n                if album_key not in albums:\n                    albums[album_key] = {\n                        \"mp3\": None,",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "535",
    "name": "generate_album-pages.py",
    "path": "01_core_ai_analysis/ai_generation/generate_album-pages.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 90,
    "size": 2534,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "read_text_file",
      "generate_album_html"
    ],
    "classes": [],
    "imports": [
      "os"
    ],
    "preview": "import os\n\nTEMPLATE = \"\"\"\n<!DOCTYPE html>\n<html lang=\"en\">\n<head>\n    <meta charset=\"UTF-8\">\n    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n    <title>{album_name}</title>\n    <link rel=\"stylesheet\" href=\"../css/styles.css\">\n</head>\n<body>\n    <div class=\"album-container\">\n        <div class=\"album\">\n            <img src=\"../albums/{folder_name}/album_cover.jpg\" alt=\"{album_name} Cover\">\n            <h3>{album_name}</h3>\n            <audio controls>\n                <source src=\"../albums/{folder_name}/{mp3_file}\" type=\"audio/mpeg\">\n                Your browser does not support the audio element.\n            </audio>",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "536",
    "name": "regexopt.py",
    "path": "01_core_ai_analysis/ai_generation/regexopt.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 106,
    "size": 3225,
    "docstring": "pygments.regexopt\n~~~~~~~~~~~~~~~~~\n\nAn algorithm that generates optimized regexes for matching long lists of\nliteral strings.\n\n:copyright: Copyright 2006-2023 by the Pygments team, see AUTHORS.\n:license: BSD, see LICENSE for details.",
    "keywords": [],
    "functions": [
      "make_charset",
      "regex_opt_inner",
      "regex_opt"
    ],
    "classes": [],
    "imports": [
      "re",
      "itertools",
      "operator",
      "os.path",
      "re"
    ],
    "preview": "\"\"\"\npygments.regexopt\n~~~~~~~~~~~~~~~~~\n\nAn algorithm that generates optimized regexes for matching long lists of\nliteral strings.\n\n:copyright: Copyright 2006-2023 by the Pygments team, see AUTHORS.\n:license: BSD, see LICENSE for details.\n\"\"\"\n\nimport re\nfrom itertools import groupby\nfrom operator import itemgetter\nfrom os.path import commonprefix\nfrom re import escape\n\nCS_ESCAPE = re.compile(r\"[\\[\\^\\\\\\-\\]]\")\nFIRST_ELEMENT = itemgetter(0)\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "537",
    "name": "api_login.py",
    "path": "01_core_ai_analysis/ai_generation/api_login.py",
    "category": "01_core_ai_analysis",
    "type": "web_tools",
    "lines": 405,
    "size": 21643,
    "docstring": "",
    "keywords": [
      "analysis",
      "web_tools"
    ],
    "functions": [
      "sync_device_features",
      "sync_launcher",
      "set_contact_point_prefill",
      "get_prefill_candidates",
      "get_account_family",
      "get_zr_token_result",
      "banyan",
      "igtv_browse_feed",
      "creatives_ar_class",
      "pre_login_flow"
    ],
    "classes": [],
    "imports": [
      "json",
      "os",
      "random",
      "time",
      "traceback",
      "requests",
      "requests.utils"
    ],
    "preview": "import json\nimport os\nimport random\nimport time\nimport traceback\n\nimport requests\nimport requests.utils\n\nfrom . import config, devices\n\n# ====== SYNC METHODS ====== #\n\n\ndef sync_device_features(self, login=None):\n    data = {\n        \"id\": self.uuid,\n        \"server_config_retrieval\": \"1\",\n        \"experiments\": config.LOGIN_EXPERIMENTS,\n    }",
    "last_modified": "2025-09-13T05:54:56.206413"
  },
  {
    "id": "538",
    "name": "app.py",
    "path": "01_core_ai_analysis/ai_generation/app.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 140,
    "size": 4413,
    "docstring": "",
    "keywords": [
      "analysis",
      "youtube",
      "organization"
    ],
    "functions": [
      "safe_filename",
      "main",
      "valid_post",
      "get_valid_posts",
      "download_assets",
      "create_directories",
      "create_directory",
      "clean_temp_dir"
    ],
    "classes": [
      "ttsvibelounge"
    ],
    "imports": [
      "argparse",
      "logging",
      "os",
      "re",
      "pathlib",
      "praw",
      "settings",
      "video",
      "yaml",
      "config"
    ],
    "preview": "import argparse\nimport logging\nimport os\nimport re\nfrom pathlib import Path\n\nimport praw\nimport settings\nimport video\nimport yaml\n\nimport config\n\nlogging.basicConfig(\n    format=\"%(asctime)s %(levelname)-8s %(message)s\",\n    level=logging.INFO,\n    datefmt=\"%Y-%m-%d %H:%M:%S\",\n    handlers=[logging.FileHandler(\"debug.log\"), logging.StreamHandler()],\n)\n",
    "last_modified": "2025-09-13T05:53:29.187021"
  },
  {
    "id": "539",
    "name": "freeze.py",
    "path": "01_core_ai_analysis/ai_generation/freeze.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 106,
    "size": 3126,
    "docstring": "",
    "keywords": [],
    "functions": [
      "_should_suppress_build_backends",
      "_dev_pkgs",
      "add_options",
      "run"
    ],
    "classes": [
      "FreezeCommand"
    ],
    "imports": [
      "sys",
      "optparse",
      "typing",
      "pip._internal.cli",
      "pip._internal.cli.base_command",
      "pip._internal.cli.status_codes",
      "pip._internal.operations.freeze",
      "pip._internal.utils.compat"
    ],
    "preview": "import sys\nfrom optparse import Values\nfrom typing import AbstractSet, List\n\nfrom pip._internal.cli import cmdoptions\nfrom pip._internal.cli.base_command import Command\nfrom pip._internal.cli.status_codes import SUCCESS\nfrom pip._internal.operations.freeze import freeze\nfrom pip._internal.utils.compat import stdlib_pkgs\n\n\ndef _should_suppress_build_backends() -> bool:\n    return sys.version_info < (3, 12)\n\n\ndef _dev_pkgs() -> AbstractSet[str]:\n    pkgs = {\"pip\"}\n\n    if _should_suppress_build_backends():\n        pkgs |= {\"setuptools\", \"distribute\", \"wheel\"}",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "540",
    "name": "sora-song.py",
    "path": "01_core_ai_analysis/ai_generation/sora-song.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 118,
    "size": 4452,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "get_music_directory",
      "scan_music_directory",
      "generate_storyboard_csv"
    ],
    "classes": [],
    "imports": [
      "csv",
      "os"
    ],
    "preview": "import csv\nimport os\n\n\ndef get_music_directory():\n    \"\"\"Prompt the user to input the directory where MP3 files are stored.\"\"\"\n    music_directory = input(\n        \"Enter the path to your music directory (e.g., /Users/steven/Music/NocTurnE-meLoDieS/mp3/): \"\n    ).strip()\n\n    # Validate if the directory exists\n    if not os.path.isdir(music_directory):\n        print(\n            f\"Error: The directory '{music_directory}' does not exist. Please enter a valid directory.\"\n        )\n        return get_music_directory()\n\n    return music_directory\n\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "541",
    "name": "file_utils.py",
    "path": "01_core_ai_analysis/ai_generation/file_utils.py",
    "category": "01_core_ai_analysis",
    "type": "organization",
    "lines": 28,
    "size": 832,
    "docstring": "",
    "keywords": [],
    "functions": [
      "get_creation_date",
      "format_size",
      "should_exclude"
    ],
    "classes": [
      "FileOrganizer"
    ],
    "imports": [
      "logging",
      "re",
      "datetime",
      "pathlib"
    ],
    "preview": "import logging\nimport re\nfrom datetime import datetime\nfrom pathlib import Path\n\n\nclass FileOrganizer:\n    @staticmethod\n    def get_creation_date(path: Path) -> str:\n        try:\n            return datetime.fromtimestamp(path.stat().st_ctime).strftime(\"%m-%d-%y\")\n        except Exception as e:\n            logging.error(f\"Creation date error: {str(e)}\")\n            return \"Unknown\"\n\n    @staticmethod\n    def format_size(size: int) -> str:\n        for unit in [\"B\", \"KB\", \"MB\", \"GB\", \"TB\"]:\n            if size < 1024:\n                return f\"{size:.2f} {unit}\"",
    "last_modified": "2025-05-04T22:47:12.611806"
  },
  {
    "id": "542",
    "name": "img-origin-date.py",
    "path": "01_core_ai_analysis/ai_generation/img-origin-date.py",
    "category": "01_core_ai_analysis",
    "type": "image_processing",
    "lines": 130,
    "size": 3978,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "get_creation_date",
      "get_image_metadata",
      "format_file_size",
      "generate_csv",
      "write_csv"
    ],
    "classes": [],
    "imports": [
      "csv",
      "os",
      "datetime",
      "PIL"
    ],
    "preview": "import csv\nimport os\nfrom datetime import datetime\n\nfrom PIL import Image\n\n\n# Function to get the creation date of a file\ndef get_creation_date(filepath):\n    try:\n        return datetime.fromtimestamp(os.path.getctime(filepath)).strftime(\"%y-%m-%d\")\n    except Exception as e:\n        print(f\"Error getting creation date for {filepath}: {e}\")\n        return \"Unknown\"\n\n\n# Function to extract metadata from an image file using PIL\ndef get_image_metadata(filepath):\n    try:\n        with Image.open(filepath) as img:",
    "last_modified": "2025-05-04T22:47:13.347944"
  },
  {
    "id": "543",
    "name": "docs2.py",
    "path": "01_core_ai_analysis/ai_generation/docs2.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 215,
    "size": 7169,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "generate_dry_run_csv",
      "write_csv",
      "format_file_size",
      "get_creation_date",
      "save_last_directory",
      "load_last_directory",
      "get_unique_file_path"
    ],
    "classes": [],
    "imports": [
      "csv",
      "logging",
      "os",
      "re",
      "pathlib",
      "datetime"
    ],
    "preview": "import csv\nimport logging\nimport os\nimport re\nfrom pathlib import Path\n\nlogger = logging.getLogger(__name__)\nlogging.basicConfig(level=logging.INFO)\n\n# Pre-compile all patterns for performance and clarity\nEXCLUDED_REGEXES = [\n    re.compile(pattern)\n    for pattern in [\n        r\"^\\..*\",  # Hidden files/directories at the root\n        r\".*\\/venv\\/.*\",  # venv directories\n        r\".*\\/\\.venv\\/.*\",  # .venv directories\n        r\".*\\/lib\\/.*\",  # library directories\n        r\".*\\/\\.lib\\/.*\",  # .lib directories\n        r\".*\\/my_global_venv\\/.*\",\n        r\".*\\/simplegallery\\/.*\",",
    "last_modified": "2025-09-06T12:24:11.724261"
  },
  {
    "id": "544",
    "name": "audio.py",
    "path": "01_core_ai_analysis/ai_generation/audio.py",
    "category": "01_core_ai_analysis",
    "type": "transcription",
    "lines": 239,
    "size": 7359,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "get_creation_date",
      "get_audio_metadata",
      "format_file_size",
      "format_duration",
      "generate_dry_run_csv",
      "write_csv",
      "get_unique_file_path",
      "save_last_directory",
      "load_last_directory"
    ],
    "classes": [],
    "imports": [
      "csv",
      "os",
      "re",
      "datetime",
      "mutagen.easyid3",
      "mutagen.mp3",
      "config"
    ],
    "preview": "import csv\nimport os\nimport re\nfrom datetime import datetime\n\nfrom mutagen.easyid3 import EasyID3\nfrom mutagen.mp3 import MP3\n\nimport config  # Import the configuration\n\n# Constants\nLAST_DIRECTORY_FILE = \"audio.txt\"\n\n# Function to get the creation date of a file\n\n\ndef get_creation_date(filepath):\n    try:\n        return datetime.fromtimestamp(os.path.getctime(filepath)).strftime(\"%m-%d-%y\")\n    except Exception as e:",
    "last_modified": "2025-09-13T05:53:48.331970"
  },
  {
    "id": "545",
    "name": "generate_prompts.py",
    "path": "01_core_ai_analysis/ai_generation/generate_prompts.py",
    "category": "01_core_ai_analysis",
    "type": "image_processing",
    "lines": 154,
    "size": 6462,
    "docstring": "Generate detailed, descriptive image prompts from a timestamped transcript.\n\nInput:\n  A transcript text file with lines like:\n    [HH:MM:SS] some narrative text...\nOutput:\n  - <base>_prompts.jsonl  (one JSON per prompt item)\n  - <base>_prompts.md     (nicely formatted, ready to paste into your generator)\nDesign:\n  For each line, we infer mood, setting, and symbolism, then produce a trio:\n    - Transition image (connective visual)\n    - Main image (cinematic narrative focus)\n    - Filler/typography (graphic/overlay for pacing)\nThe script uses lightweight heuristics and word-shape detection to avoid\nheavy dependencies while still yielding rich, cinematic prompts.",
    "keywords": [],
    "functions": [
      "parse_transcript_lines",
      "pick_tags",
      "enrich_prompt",
      "make_triplet",
      "write_jsonl_md",
      "main"
    ],
    "classes": [],
    "imports": [
      "argparse",
      "sys",
      "json",
      "re",
      "pathlib"
    ],
    "preview": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\nGenerate detailed, descriptive image prompts from a timestamped transcript.\n\nInput:\n  A transcript text file with lines like:\n    [HH:MM:SS] some narrative text...\nOutput:\n  - <base>_prompts.jsonl  (one JSON per prompt item)\n  - <base>_prompts.md     (nicely formatted, ready to paste into your generator)\nDesign:\n  For each line, we infer mood, setting, and symbolism, then produce a trio:\n    - Transition image (connective visual)\n    - Main image (cinematic narrative focus)\n    - Filler/typography (graphic/overlay for pacing)\nThe script uses lightweight heuristics and word-shape detection to avoid\nheavy dependencies while still yielding rich, cinematic prompts.\n\"\"\"\nimport argparse, sys, json, re",
    "last_modified": "2025-09-15T18:25:10"
  },
  {
    "id": "546",
    "name": "input_handler.py",
    "path": "01_core_ai_analysis/ai_generation/input_handler.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 270,
    "size": 7546,
    "docstring": "",
    "keywords": [],
    "functions": [
      "get_inputs",
      "get_time_period",
      "get_directory",
      "get_languages",
      "check_inputs",
      "select_clips"
    ],
    "classes": [],
    "imports": [
      "os",
      "tkinter",
      "tkinter.filedialog",
      "cmd_logs",
      "configHandler",
      "twitchClips"
    ],
    "preview": "# Local imports\n# Imports\nimport os\nfrom tkinter import Tk\nfrom tkinter.filedialog import askdirectory\n\nfrom .cmd_logs import *\nfrom .configHandler import *\nfrom .twitchClips import *\n\navailable_langs = [\n    \"eng\",\n    \"id\",\n    \"ca\",\n    \"da\",\n    \"de\",\n    \"es\",\n    \"fr\",\n    \"hu\",\n    \"nl\",",
    "last_modified": "2025-09-13T05:53:45.139003"
  },
  {
    "id": "547",
    "name": "gallery_init_20241204123453.py",
    "path": "01_core_ai_analysis/ai_generation/gallery_init_20241204123453.py",
    "category": "01_core_ai_analysis",
    "type": "setup",
    "lines": 292,
    "size": 9797,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "parse_args",
      "check_if_gallery_creation_possible",
      "check_if_gallery_already_exists",
      "create_gallery_folder_structure",
      "create_gallery_json",
      "main"
    ],
    "classes": [],
    "imports": [
      "argparse",
      "glob",
      "importlib.resources",
      "json",
      "os",
      "shutil",
      "sys",
      "distutils.dir_util",
      "simplegallery.common",
      "simplegallery.logic.gallery_logic"
    ],
    "preview": "import argparse\nimport glob\nimport importlib.resources\nimport json\nimport os\nimport shutil\nimport sys\nfrom distutils.dir_util import copy_tree\n\nimport simplegallery.common as spg_common\nimport simplegallery.logic.gallery_logic as gallery_logic\n\n\ndef parse_args():\n    \"\"\"\n    Configures the argument parser\n    :return: Parsed arguments\n    \"\"\"\n\n    description = \"\"\"Initializes a new Simple Photo Gallery in the specified folder (default is the current folder).",
    "last_modified": "2025-09-13T05:55:18.237910"
  },
  {
    "id": "548",
    "name": "ansi.py",
    "path": "01_core_ai_analysis/ai_generation/ansi.py",
    "category": "01_core_ai_analysis",
    "type": "web_tools",
    "lines": 111,
    "size": 2297,
    "docstring": "This module generates ANSI character codes to printing colors to terminals.\nSee: http://en.wikipedia.org/wiki/ANSI_escape_code",
    "keywords": [],
    "functions": [
      "code_to_chars",
      "set_title",
      "clear_screen",
      "clear_line",
      "__init__",
      "UP",
      "DOWN",
      "FORWARD",
      "BACK",
      "POS"
    ],
    "classes": [
      "AnsiCodes",
      "AnsiCursor",
      "AnsiFore",
      "AnsiBack",
      "AnsiStyle"
    ],
    "imports": [],
    "preview": "# Copyright Jonathan Hartley 2013. BSD 3-Clause license, see LICENSE file.\n\"\"\"\nThis module generates ANSI character codes to printing colors to terminals.\nSee: http://en.wikipedia.org/wiki/ANSI_escape_code\n\"\"\"\n\nCSI = \"\\033[\"\nOSC = \"\\033]\"\nBEL = \"\\a\"\n\n\ndef code_to_chars(code):\n    return CSI + str(code) + \"m\"\n\n\ndef set_title(title):\n    return OSC + \"2;\" + title + BEL\n\n\ndef clear_screen(mode=2):",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "549",
    "name": "contact-sheet2.py",
    "path": "01_core_ai_analysis/ai_generation/contact-sheet2.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 49,
    "size": 1529,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "create_thumbnail",
      "create_contact_sheet",
      "save_contact_sheet"
    ],
    "classes": [],
    "imports": [
      "os",
      "PIL"
    ],
    "preview": "import os\n\nfrom PIL import Image\n\n\ndef create_thumbnail(image_path, thumbnail_size=(100, 100)):\n    with Image.open(image_path) as img:\n        img.thumbnail(thumbnail_size)\n        return img\n\n\ndef create_contact_sheet(directory, thumbnail_size=(100, 100), sheet_size=(500, 500)):\n    thumbnails = []\n    for subdir, dirs, files in os.walk(directory):\n        for file in files:\n            if file.lower().endswith((\"png\", \"jpg\", \"jpeg\", \"gif\", \"bmp\")):\n                image_path = os.path.join(subdir, file)\n                thumbnail = create_thumbnail(image_path, thumbnail_size)\n                thumbnails.append(thumbnail)\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "550",
    "name": "website.py",
    "path": "01_core_ai_analysis/ai_generation/website.py",
    "category": "01_core_ai_analysis",
    "type": "web_tools",
    "lines": 188,
    "size": 5246,
    "docstring": "MIT License\n\nCopyright (c) 2021-2022 MShawon\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "create_graph_data",
      "create_dropdown_data",
      "shutdown_server",
      "start_server",
      "home",
      "update",
      "graph",
      "shutdown"
    ],
    "classes": [],
    "imports": [
      "calendar",
      "sqlite3",
      "warnings",
      "contextlib",
      "datetime",
      "flask"
    ],
    "preview": "\"\"\"\nMIT License\n\nCopyright (c) 2021-2022 MShawon\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,",
    "last_modified": "2025-05-04T23:27:45"
  },
  {
    "id": "551",
    "name": "tocpdf.py",
    "path": "01_core_ai_analysis/ai_generation/tocpdf.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 27,
    "size": 642,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "create_toc_pdf"
    ],
    "classes": [],
    "imports": [
      "reportlab.lib.pagesizes",
      "reportlab.pdfgen"
    ],
    "preview": "from reportlab.lib.pagesizes import letter\nfrom reportlab.pdfgen import canvas\n\n\ndef create_toc_pdf(toc_items, output_path):\n    c = canvas.Canvas(output_path, pagesize=letter)\n    width, height = letter\n    c.setFont(\"Helvetica\", 12)\n\n    y_position = height - 40\n    for item in toc_items:\n        text = f\"{item['title']} ... {item['page']}\"\n        c.drawString(40, y_position, text)\n        y_position -= 20\n\n    c.save()\n\n\ntoc_items = [\n    {\"title\": \"Introduction\", \"page\": 1},",
    "last_modified": "2025-05-04T22:47:13.332373"
  },
  {
    "id": "552",
    "name": "progress_bars.py",
    "path": "01_core_ai_analysis/ai_generation/progress_bars.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 68,
    "size": 1967,
    "docstring": "",
    "keywords": [],
    "functions": [
      "_rich_progress_bar",
      "get_download_progress_renderer"
    ],
    "classes": [],
    "imports": [
      "functools",
      "typing",
      "pip._internal.utils.logging",
      "pip._vendor.rich.progress"
    ],
    "preview": "import functools\nfrom typing import Callable, Generator, Iterable, Iterator, Optional, Tuple\n\nfrom pip._internal.utils.logging import get_indentation\nfrom pip._vendor.rich.progress import (\n    BarColumn,\n    DownloadColumn,\n    FileSizeColumn,\n    Progress,\n    ProgressColumn,\n    SpinnerColumn,\n    TextColumn,\n    TimeElapsedColumn,\n    TimeRemainingColumn,\n    TransferSpeedColumn,\n)\n\nDownloadProgressRenderer = Callable[[Iterable[bytes]], Iterator[bytes]]\n\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "553",
    "name": "dallecsv.py",
    "path": "01_core_ai_analysis/ai_generation/dallecsv.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 42,
    "size": 995,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "csv"
    ],
    "preview": "import csv\n\n# Path to the input text file\ninput_file_path = \"/Users/steven/Pictures/DaLL-E/dalle/dalle.txt\"\n\n# Path to the output CSV file\noutput_csv_path = \"/Users/steven/Pictures/DaLL-E/dalle/dalle_output.csv\"\n\n# Read the content of the input file\nwith open(input_file_path, \"r\") as file:\n    lines = file.readlines()\n\n# Prepare the data for CSV\ndata = []\nurl = None\ninfo = None\n\nfor line in lines:\n    line = line.strip()\n    if line.startswith(\"https://\"):",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "554",
    "name": "onedrive_gallery_logic.py",
    "path": "01_core_ai_analysis/ai_generation/onedrive_gallery_logic.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 101,
    "size": 3762,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "parse_photo_link",
      "create_thumbnails",
      "generate_images_data"
    ],
    "classes": [
      "OnedriveGalleryLogic"
    ],
    "imports": [
      "time",
      "pkg_resources",
      "simplegallery.common",
      "simplegallery.media",
      "selenium",
      "selenium.webdriver.firefox.options",
      "simplegallery.logic.base_gallery_logic"
    ],
    "preview": "import time\n\nimport pkg_resources\nimport simplegallery.common as spg_common\nimport simplegallery.media as spg_media\nfrom selenium import webdriver\nfrom selenium.webdriver.firefox.options import Options\nfrom simplegallery.logic.base_gallery_logic import BaseGalleryLogic\n\n\ndef parse_photo_link(photo_url):\n    \"\"\"\n    Extracts the base URL (URL without query parameters) and the photo name from a Onedrive photo URL\n    :param photo_url: photo URL\n    :return: base URL and photo name\n    \"\"\"\n    base_url = photo_url.split(\"?\")[0]\n    name = base_url.split(\"/\")[-1]\n\n    return base_url, name",
    "last_modified": "2025-09-13T05:53:52.615382"
  },
  {
    "id": "555",
    "name": "lazy.py",
    "path": "01_core_ai_analysis/ai_generation/lazy.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 161,
    "size": 6963,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "parse_album_data",
      "generate_html",
      "main"
    ],
    "classes": [],
    "imports": [
      "os"
    ],
    "preview": "import os\n\n\ndef parse_album_data(paths_file):\n    albums = {}\n    with open(paths_file, \"r\", encoding=\"utf-8\") as f:\n        for line in f:\n            line = line.strip()\n            if not line:\n                continue\n            # Process only if the line is in the mp4 folder structure\n            if \"/mp4/\" in line:\n                # Extract album folder name by splitting at \"/mp4/\" then taking first folder\n                parts = line.split(\"/mp4/\")[1].split(\"/\")\n                if not parts:\n                    continue\n                album_key = parts[0]\n                if album_key not in albums:\n                    albums[album_key] = {\n                        \"mp3\": None,",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "556",
    "name": "compatibility_tags.py",
    "path": "01_core_ai_analysis/ai_generation/compatibility_tags.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 165,
    "size": 5376,
    "docstring": "Generate and work with PEP 425 Compatibility Tags.",
    "keywords": [],
    "functions": [
      "version_info_to_nodot",
      "_mac_platforms",
      "_custom_manylinux_platforms",
      "_get_custom_platforms",
      "_expand_allowed_platforms",
      "_get_python_version",
      "_get_custom_interpreter",
      "get_supported"
    ],
    "classes": [],
    "imports": [
      "re",
      "typing",
      "pip._vendor.packaging.tags"
    ],
    "preview": "\"\"\"Generate and work with PEP 425 Compatibility Tags.\"\"\"\n\nimport re\nfrom typing import List, Optional, Tuple\n\nfrom pip._vendor.packaging.tags import (\n    PythonVersion,\n    Tag,\n    compatible_tags,\n    cpython_tags,\n    generic_tags,\n    interpreter_name,\n    interpreter_version,\n    mac_platforms,\n)\n\n_osx_arch_pat = re.compile(r\"(.+)_(\\d+)_(\\d+)_(.+)\")\n\n\ndef version_info_to_nodot(version_info: Tuple[int, ...]) -> str:",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "557",
    "name": "test_file_gallery_logic.py",
    "path": "01_core_ai_analysis/ai_generation/test_file_gallery_logic.py",
    "category": "01_core_ai_analysis",
    "type": "testing",
    "lines": 114,
    "size": 4710,
    "docstring": "",
    "keywords": [
      "testing"
    ],
    "functions": [
      "test_create_thumbnails",
      "test_generate_images_data"
    ],
    "classes": [
      "FileGalleryLogicTestCase"
    ],
    "imports": [
      "os",
      "unittest",
      "unittest",
      "simplegallery.media",
      "simplegallery.test.helpers",
      "simplegallery.logic.variants.files_gallery_logic",
      "testfixtures"
    ],
    "preview": "import os\nimport unittest\nfrom unittest import mock\n\nimport simplegallery.media as spg_media\nimport simplegallery.test.helpers as helpers\nfrom simplegallery.logic.variants.files_gallery_logic import FilesGalleryLogic\nfrom testfixtures import TempDirectory\n\n\nclass FileGalleryLogicTestCase(unittest.TestCase):\n    @mock.patch(\"builtins.input\", side_effect=[\"\", \"\", \"\", \"\"])\n    def test_create_thumbnails(self, input):\n        with TempDirectory() as tempdir:\n            helpers.create_mock_image(os.path.join(tempdir.path, \"photo.jpg\"), 1000, 500)\n            helpers.create_mock_image(os.path.join(tempdir.path, \"photo2.gif\"), 1000, 500)\n            helpers.create_mock_image(os.path.join(tempdir.path, \"photo3.png\"), 1000, 500)\n\n            thumbnail_path = os.path.join(\n                tempdir.path, \"public\", \"images\", \"thumbnails\", \"photo.jpg\"",
    "last_modified": "2025-09-13T05:53:52.828780"
  },
  {
    "id": "558",
    "name": "files_gallery_logic.py",
    "path": "01_core_ai_analysis/ai_generation/files_gallery_logic.py",
    "category": "01_core_ai_analysis",
    "type": "organization",
    "lines": 140,
    "size": 5667,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "check_correct_thumbnail_size",
      "get_thumbnail_name",
      "create_thumbnails",
      "format_image_date",
      "generate_images_data"
    ],
    "classes": [
      "FilesGalleryLogic"
    ],
    "imports": [
      "glob",
      "os",
      "datetime",
      "simplegallery.common",
      "simplegallery.media",
      "simplegallery.logic.base_gallery_logic"
    ],
    "preview": "import glob\nimport os\nfrom datetime import datetime\n\nimport simplegallery.common as spg_common\nimport simplegallery.media as spg_media\nfrom simplegallery.logic.base_gallery_logic import BaseGalleryLogic\n\n\ndef check_correct_thumbnail_size(thumbnail_path, expected_height):\n    \"\"\"\n    Check if a thumbnail has the correct height\n    :param thumbnail_path: Path to the thumbnail file\n    :param expected_height: Expected height of the thumbnail in pixels\n    :return: True if the height of the thumbnail equals the expected height, False otherwise\n    \"\"\"\n    return expected_height == spg_media.get_image_size(thumbnail_path)[1]\n\n\ndef get_thumbnail_name(thumbnails_path, photo_name):",
    "last_modified": "2025-09-13T05:53:52.534901"
  },
  {
    "id": "559",
    "name": "metadata_legacy.py",
    "path": "01_core_ai_analysis/ai_generation/metadata_legacy.py",
    "category": "01_core_ai_analysis",
    "type": "analysis",
    "lines": 72,
    "size": 2175,
    "docstring": "Metadata generation logic for legacy source distributions.",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "_find_egg_info",
      "generate_metadata"
    ],
    "classes": [],
    "imports": [
      "logging",
      "os",
      "pip._internal.build_env",
      "pip._internal.cli.spinners",
      "pip._internal.exceptions",
      "pip._internal.utils.setuptools_build",
      "pip._internal.utils.subprocess",
      "pip._internal.utils.temp_dir"
    ],
    "preview": "\"\"\"Metadata generation logic for legacy source distributions.\"\"\"\n\nimport logging\nimport os\n\nfrom pip._internal.build_env import BuildEnvironment\nfrom pip._internal.cli.spinners import open_spinner\nfrom pip._internal.exceptions import (\n    InstallationError,\n    InstallationSubprocessError,\n    MetadataGenerationFailed,\n)\nfrom pip._internal.utils.setuptools_build import make_setuptools_egg_info_args\nfrom pip._internal.utils.subprocess import call_subprocess\nfrom pip._internal.utils.temp_dir import TempDirectory\n\nlogger = logging.getLogger(__name__)\n\n\ndef _find_egg_info(directory: str) -> str:",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "560",
    "name": "pipgen.py",
    "path": "01_core_ai_analysis/ai_generation/pipgen.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 11,
    "size": 282,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "subprocess"
    ],
    "preview": "# generate_requirements.py\nimport subprocess\n\n# Path to your project directory\nproject_path = \"/Users/steven/Documents/Python\"\n\n# Step 1: Generate requirements.txt using pipreqs\nsubprocess.run([\"pipreqs\", project_path, \"--force\"])\n\nprint(\"requirements.txt generated successfully.\")\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "561",
    "name": "test_gallery_build.py",
    "path": "01_core_ai_analysis/ai_generation/test_gallery_build.py",
    "category": "01_core_ai_analysis",
    "type": "testing",
    "lines": 109,
    "size": 4020,
    "docstring": "",
    "keywords": [
      "testing",
      "analysis"
    ],
    "functions": [
      "create_mock_image",
      "test_nonexisting_gallery_config",
      "test_thumbnails_generation",
      "test_images_data_generation",
      "test_index_html"
    ],
    "classes": [
      "SPGBuildTestCase"
    ],
    "imports": [
      "json",
      "os",
      "sys",
      "unittest",
      "unittest",
      "simplegallery.gallery_build",
      "simplegallery.gallery_init",
      "simplegallery.media",
      "PIL",
      "testfixtures"
    ],
    "preview": "import json\nimport os\nimport sys\nimport unittest\nfrom unittest import mock\n\nimport simplegallery.gallery_build as gallery_build\nimport simplegallery.gallery_init as gallery_init\nimport simplegallery.media as spg_media\nfrom PIL import Image\nfrom testfixtures import TempDirectory\n\n\ndef create_mock_image(path, width, height):\n    img = Image.new(\"RGB\", (width, height), color=\"red\")\n    img.save(path)\n    img.close()\n\n\nclass SPGBuildTestCase(unittest.TestCase):",
    "last_modified": "2025-09-13T05:53:52.938426"
  },
  {
    "id": "562",
    "name": "gallery_init_20241204123258.py",
    "path": "01_core_ai_analysis/ai_generation/gallery_init_20241204123258.py",
    "category": "01_core_ai_analysis",
    "type": "setup",
    "lines": 292,
    "size": 9785,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "parse_args",
      "check_if_gallery_creation_possible",
      "check_if_gallery_already_exists",
      "create_gallery_folder_structure",
      "create_gallery_json",
      "main"
    ],
    "classes": [],
    "imports": [
      "argparse",
      "glob",
      "importlib.resources",
      "json",
      "os",
      "shutil",
      "sys",
      "distutils.dir_util",
      "simplegallery.common",
      "simplegallery.logic.gallery_logic"
    ],
    "preview": "import argparse\nimport glob\nimport importlib.resources\nimport json\nimport os\nimport shutil\nimport sys\nfrom distutils.dir_util import copy_tree\n\nimport simplegallery.common as spg_common\nimport simplegallery.logic.gallery_logic as gallery_logic\n\n\ndef parse_args():\n    \"\"\"\n    Configures the argument parser\n    :return: Parsed arguments\n    \"\"\"\n\n    description = \"\"\"Initializes a new Simple Photo Gallery in the specified folder (default is the current folder).",
    "last_modified": "2025-09-13T05:55:18.006675"
  },
  {
    "id": "563",
    "name": "helpers.py",
    "path": "01_core_ai_analysis/ai_generation/helpers.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 73,
    "size": 2558,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "init_gallery_and_read_gallery_config",
      "check_image_data",
      "create_mock_image"
    ],
    "classes": [],
    "imports": [
      "json",
      "os",
      "sys",
      "simplegallery.gallery_init",
      "PIL"
    ],
    "preview": "import json\nimport os\nimport sys\n\nimport simplegallery.gallery_init as gallery_init\nfrom PIL import Image\n\n\ndef init_gallery_and_read_gallery_config(path, remote_link=\"\"):\n    \"\"\"\n    Initializes a new gallery and reads the data from the gallery.json file\n    :param path: path to the folder where the gallery will be created\n    :param remote_link: optional remote link to initialize a remote gallery\n    :return: gallery config dict\n    \"\"\"\n\n    sys.argv = [\"gallery_init\", remote_link, \"-p\", path]\n    gallery_init.main()\n\n    with open(os.path.join(path, \"gallery.json\"), \"r\") as json_in:",
    "last_modified": "2025-09-13T05:53:52.742558"
  },
  {
    "id": "564",
    "name": "csv-html-gen.py",
    "path": "01_core_ai_analysis/ai_generation/csv-html-gen.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 277,
    "size": 8279,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "create_excerpt",
      "generate_job_cards",
      "write_html_job_cards",
      "generate_table_header",
      "generate_table_body",
      "write_html_table",
      "main"
    ],
    "classes": [],
    "imports": [
      "csv",
      "html",
      "os"
    ],
    "preview": "import csv\nimport html\nimport os\n\n# File paths \u2013 adjust as needed\ninput_csv = \"/Users/steven/Documents/QuantumForgeLabs/data.csv\"\noutput_html_job = \"/Users/steven/Documents/QuantumForgeLabs/data.html\"\noutput_html_table = \"/Users/steven/Documents/QuantumForgeLabs/linkd-scrape.html\"\n\n#########################################\n# PART 1: Generate job card HTML output #\n#########################################\njob_html_header = \"\"\"<!DOCTYPE html>\n<html lang=\"en\">\n<head>\n    <meta charset=\"UTF-8\" />\n    <title>LinkedIn Jobs Scraped - Job Cards</title>\n    <style>\n        body {\n            font-family: Arial, sans-serif;",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "565",
    "name": "job-template.py",
    "path": "01_core_ai_analysis/ai_generation/job-template.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 154,
    "size": 4610,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "create_excerpt"
    ],
    "classes": [],
    "imports": [
      "csv",
      "html"
    ],
    "preview": "import csv\nimport html\n\n# Set the input/output paths; update these as necessary.\ninput_csv = \"/Users/steven/Documents/QuantumForgeLabs/data.csv\"\noutput_html = \"/Users/steven/Documents/QuantumForgeLabs/data.html\"\n\n# This is a template for an individual job listing; you can modify the styling or fields.\njob_template = \"\"\"\n<div class=\"job-listing\">\n  <div class=\"job-header\">\n    <div class=\"job-title\">\n      <a href=\"{apply_url}\" target=\"_blank\">{job_title}</a>\n    </div>\n    <div class=\"job-posted\">\n      Posted: <strong>{posted_time}</strong>\n    </div>\n  </div>\n  <div class=\"job-details\">\n    <div class=\"company\">",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "566",
    "name": "alphabet.py",
    "path": "01_core_ai_analysis/ai_generation/alphabet.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 125,
    "size": 4184,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "generate_grouped_gallery"
    ],
    "classes": [],
    "imports": [
      "os"
    ],
    "preview": "import os\n\n\ndef generate_grouped_gallery(folder_path, output_html, group_size=4):\n    \"\"\"\n    Generates an HTML gallery grouped alphabetically by sets of images and MP4s.\n    \"\"\"\n    # Get all valid files\n    valid_extensions = (\".jpg\", \".jpeg\", \".png\", \".mp4\")\n    all_files = [f for f in os.listdir(folder_path) if f.lower().endswith(valid_extensions)]\n\n    # Sort files alphabetically\n    all_files.sort()\n\n    # Group files into sets of the specified size\n    grouped_files = [all_files[i : i + group_size] for i in range(0, len(all_files), group_size)]\n\n    # Start the HTML structure\n    html_content = \"\"\"\n    <!DOCTYPE html>",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "567",
    "name": "dupes.py",
    "path": "01_core_ai_analysis/ai_generation/dupes.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 163,
    "size": 5472,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "is_excluded",
      "scan_directory",
      "compute_md5",
      "generate_detailed_duplicate_report",
      "prompt_for_directories"
    ],
    "classes": [],
    "imports": [
      "csv",
      "hashlib",
      "os",
      "re",
      "collections"
    ],
    "preview": "import csv\nimport hashlib\nimport os\nimport re\nfrom collections import defaultdict\n\n\ndef is_excluded(path, patterns):\n    \"\"\"\n    Check if a given path matches any of the exclusion patterns.\n\n    Parameters:\n    path (str): The path to check.\n    patterns (list): A list of regex patterns for exclusion.\n\n    Returns:\n    bool: True if path matches any pattern, False otherwise.\n    \"\"\"\n    for pattern in patterns:\n        if re.search(pattern, path):",
    "last_modified": "2025-09-13T05:54:29.371317"
  },
  {
    "id": "568",
    "name": "base.py",
    "path": "01_core_ai_analysis/ai_generation/base.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 22,
    "size": 510,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "create_template",
      "__init__",
      "generate"
    ],
    "classes": [
      "PromptTemplateFactory",
      "RAGStep"
    ],
    "imports": [
      "abc",
      "typing",
      "langchain.prompts",
      "llm_engineering.domain.queries",
      "pydantic"
    ],
    "preview": "from abc import ABC, abstractmethod\nfrom typing import Any\n\nfrom langchain.prompts import PromptTemplate\nfrom llm_engineering.domain.queries import Query\nfrom pydantic import BaseModel\n\n\nclass PromptTemplateFactory(ABC, BaseModel):\n    @abstractmethod\n    def create_template(self) -> PromptTemplate:\n        pass\n\n\nclass RAGStep(ABC):\n    def __init__(self, mock: bool = False) -> None:\n        self._mock = mock\n\n    @abstractmethod\n    def generate(self, query: Query, *args, **kwargs) -> Any:",
    "last_modified": "2025-05-06T04:35:14.981687"
  },
  {
    "id": "569",
    "name": "gallery_build.py",
    "path": "01_core_ai_analysis/ai_generation/gallery_build.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 163,
    "size": 5602,
    "docstring": "",
    "keywords": [],
    "functions": [
      "parse_args",
      "build_html",
      "main"
    ],
    "classes": [],
    "imports": [
      "argparse",
      "json",
      "os",
      "sys",
      "collections",
      "jinja2",
      "simplegallery.common",
      "simplegallery.logic.gallery_logic"
    ],
    "preview": "import argparse\nimport json\nimport os\nimport sys\nfrom collections import OrderedDict\n\nimport jinja2\nimport simplegallery.common as spg_common\nfrom simplegallery.logic.gallery_logic import get_gallery_logic\n\n\ndef parse_args():\n    \"\"\"\n    Configures the argument parser\n    :return: Parsed arguments\n    \"\"\"\n\n    description = \"\"\"Generated all files needed to display the gallery (thumbnails, image descriptions and HTML page).\n                    For detailed documentation please refer to https://github.com/haltakov/simple-photo-gallery.\"\"\"\n",
    "last_modified": "2025-09-13T05:53:52.319507"
  },
  {
    "id": "570",
    "name": "img (1).py",
    "path": "01_core_ai_analysis/ai_generation/img (1).py",
    "category": "01_core_ai_analysis",
    "type": "image_processing",
    "lines": 217,
    "size": 7032,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "get_creation_date",
      "get_image_metadata",
      "format_file_size",
      "generate_csv",
      "write_csv",
      "get_unique_file_path",
      "save_last_directory",
      "load_last_directory"
    ],
    "classes": [],
    "imports": [
      "csv",
      "os",
      "re",
      "datetime",
      "PIL"
    ],
    "preview": "import csv\nimport os\nimport re\nfrom datetime import datetime\n\nfrom PIL import Image\n\n# Constants\nLAST_DIRECTORY_FILE = \"image_data.txt\"\n\n\n# Function to get the creation date of a file\ndef get_creation_date(filepath):\n    try:\n        return datetime.fromtimestamp(os.path.getctime(filepath)).strftime(\"%m-%d-%y\")\n    except Exception as e:\n        print(f\"Error getting creation date for {filepath}: {e}\")\n        return \"Unknown\"\n\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "571",
    "name": "gallery_init_20241204120021.py",
    "path": "01_core_ai_analysis/ai_generation/gallery_init_20241204120021.py",
    "category": "01_core_ai_analysis",
    "type": "setup",
    "lines": 292,
    "size": 9791,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "parse_args",
      "check_if_gallery_creation_possible",
      "check_if_gallery_already_exists",
      "create_gallery_folder_structure",
      "create_gallery_json",
      "main"
    ],
    "classes": [],
    "imports": [
      "argparse",
      "glob",
      "importlib.resources",
      "json",
      "os",
      "shutil",
      "sys",
      "distutils.dir_util",
      "simplegallery.common",
      "simplegallery.logic.gallery_logic"
    ],
    "preview": "import argparse\nimport glob\nimport importlib.resources\nimport json\nimport os\nimport shutil\nimport sys\nfrom distutils.dir_util import copy_tree\n\nimport simplegallery.common as spg_common\nimport simplegallery.logic.gallery_logic as gallery_logic\n\n\ndef parse_args():\n    \"\"\"\n    Configures the argument parser\n    :return: Parsed arguments\n    \"\"\"\n\n    description = \"\"\"Initializes a new Simple Photo Gallery in the specified folder (default is the current folder).",
    "last_modified": "2025-09-13T05:55:17.894305"
  },
  {
    "id": "572",
    "name": "tag.py",
    "path": "01_core_ai_analysis/ai_generation/tag.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 147,
    "size": 5372,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "get_creation_date",
      "get_image_metadata",
      "get_video_metadata",
      "custom_tags",
      "contains_web_project_files",
      "generate_dry_run_csv"
    ],
    "classes": [],
    "imports": [
      "csv",
      "os",
      "datetime",
      "ffmpeg",
      "PIL"
    ],
    "preview": "import csv\nimport os\nfrom datetime import datetime\n\nimport ffmpeg\nfrom PIL import Image\n\n\ndef get_creation_date(filepath):\n    \"\"\"Get the creation date of the file.\"\"\"\n    return datetime.fromtimestamp(os.path.getctime(filepath)).strftime(\"%Y-%m-%d %H:%M:%S\")\n\n\ndef get_image_metadata(filepath):\n    \"\"\"Extract metadata from an image file.\"\"\"\n    with Image.open(filepath) as img:\n        return img.size, os.path.getsize(filepath)\n\n\ndef get_video_metadata(filepath):",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "573",
    "name": "glibc.py",
    "path": "01_core_ai_analysis/ai_generation/glibc.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 89,
    "size": 3113,
    "docstring": "",
    "keywords": [],
    "functions": [
      "glibc_version_string",
      "glibc_version_string_confstr",
      "glibc_version_string_ctypes",
      "libc_ver"
    ],
    "classes": [],
    "imports": [
      "os",
      "sys",
      "typing",
      "ctypes"
    ],
    "preview": "import os\nimport sys\nfrom typing import Optional, Tuple\n\n\ndef glibc_version_string() -> Optional[str]:\n    \"Returns glibc version string, or None if not using glibc.\"\n    return glibc_version_string_confstr() or glibc_version_string_ctypes()\n\n\ndef glibc_version_string_confstr() -> Optional[str]:\n    \"Primary implementation of glibc_version_string using os.confstr.\"\n    # os.confstr is quite a bit faster than ctypes.DLL. It's also less likely\n    # to be broken or missing. This strategy is used in the standard library\n    # platform module:\n    # https://github.com/python/cpython/blob/fcf1d003bf4f0100c9d0921ff3d70e1127ca1b71/Lib/platform.py#L175-L183\n    if sys.platform == \"win32\":\n        return None\n    try:\n        gnu_libc_version = os.confstr(\"CS_GNU_LIBC_VERSION\")",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "574",
    "name": "pydoc_20250506195920.py",
    "path": "01_core_ai_analysis/ai_generation/pydoc_20250506195920.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 19,
    "size": 600,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "generate_docs"
    ],
    "classes": [],
    "imports": [
      "os",
      "subprocess"
    ],
    "preview": "import os\nimport subprocess\n\n\ndef generate_docs(directory):\n    # Walk through the directory and its subdirectories\n    for root, dirs, files in os.walk(directory):\n        for file in files:\n            if file.endswith(\".py\"):\n                file_path = os.path.join(root, file)\n                print(f\"Generating documentation for {file_path}\")\n                # Generate documentation using pydoc\n                subprocess.run([\"python3\", \"-m\", \"pydoc\", \"-w\", file_path])\n\n\n# Specify the directory to generate docs for\ndirectory = \"/Users/steven/Documents/python/docs\"\ngenerate_docs(directory)\n",
    "last_modified": "2025-05-06T19:59:20.357197"
  },
  {
    "id": "575",
    "name": "generate_album_html-pages.py",
    "path": "01_core_ai_analysis/ai_generation/generate_album_html-pages.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 132,
    "size": 3517,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "generate_album_html",
      "main"
    ],
    "classes": [],
    "imports": [
      "os"
    ],
    "preview": "import os\n\n# Define the base directory\nbase_dir = \"/Users/steven/Music/nocTurneMeLoDieS/mp4\"\noutput_file = \"/Users/steven/Music/nocTurneMeLoDieS/mp4/album-cover-html/index.html\"\n\n\n# Function to generate HTML for a single album\ndef generate_album_html(album_name):\n    album_path = os.path.join(base_dir, album_name)\n    cover_img = os.path.join(album_path, f\"{album_name}.png\")\n    audio_file = os.path.join(album_path, f\"{album_name}.mp3\")\n\n    # Use placeholder image if no cover image is found\n    cover_img_url = cover_img if os.path.exists(cover_img) else \"https://via.placeholder.com/150\"\n\n    # Generate HTML for the album\n    album_html = f\"\"\"\n        <div class=\"album\">\n            <img src=\"{cover_img_url}\" alt=\"{album_name} Cover\">",
    "last_modified": "2025-09-13T05:53:28.987258"
  },
  {
    "id": "576",
    "name": "build_tracker.py",
    "path": "01_core_ai_analysis/ai_generation/build_tracker.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 140,
    "size": 4832,
    "docstring": "",
    "keywords": [
      "organization"
    ],
    "functions": [
      "update_env_context_manager",
      "get_build_tracker",
      "__init__",
      "__enter__",
      "__exit__",
      "_entry_path",
      "add",
      "remove",
      "cleanup",
      "track"
    ],
    "classes": [
      "TrackerId",
      "BuildTracker"
    ],
    "imports": [
      "contextlib",
      "hashlib",
      "logging",
      "os",
      "types",
      "typing",
      "pip._internal.models.link",
      "pip._internal.req.req_install",
      "pip._internal.utils.temp_dir"
    ],
    "preview": "import contextlib\nimport hashlib\nimport logging\nimport os\nfrom types import TracebackType\nfrom typing import Dict, Generator, Optional, Set, Type, Union\n\nfrom pip._internal.models.link import Link\nfrom pip._internal.req.req_install import InstallRequirement\nfrom pip._internal.utils.temp_dir import TempDirectory\n\nlogger = logging.getLogger(__name__)\n\n\n@contextlib.contextmanager\ndef update_env_context_manager(**changes: str) -> Generator[None, None, None]:\n    target = os.environ\n\n    # Save values from the target and change them.\n    non_existent_marker = object()",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "577",
    "name": "filtercreator.py",
    "path": "01_core_ai_analysis/ai_generation/filtercreator.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 163,
    "size": 5913,
    "docstring": "",
    "keywords": [],
    "functions": [
      "__init__",
      "__init__",
      "changeCategory",
      "updateDisplay",
      "attemptCreateFilter"
    ],
    "classes": [
      "Filter",
      "FilterCreationWindow"
    ],
    "imports": [
      "os",
      "database",
      "scriptwrapper",
      "PyQt5",
      "PyQt5.QtCore",
      "PyQt5.QtCore",
      "PyQt5.QtMultimedia",
      "PyQt5.QtWidgets"
    ],
    "preview": "import os\n\nimport database\nimport scriptwrapper\nfrom PyQt5 import QtCore, QtGui, QtWidgets, uic\nfrom PyQt5.QtCore import *\nfrom PyQt5.QtCore import QDir, QObject, QPoint, QRect, Qt, QUrl, pyqtSignal\nfrom PyQt5.QtMultimedia import (\n    QAbstractVideoBuffer,\n    QAbstractVideoSurface,\n    QMediaContent,\n    QMediaPlayer,\n    QMediaPlaylist,\n    QVideoFrame,\n    QVideoSurfaceFormat,\n)\nfrom PyQt5.QtWidgets import *\n\ncurrent_path = os.path.dirname(os.path.realpath(__file__))\n",
    "last_modified": "2025-09-13T05:53:32.100018"
  },
  {
    "id": "578",
    "name": "img-prompt.py",
    "path": "01_core_ai_analysis/ai_generation/img-prompt.py",
    "category": "01_core_ai_analysis",
    "type": "image_processing",
    "lines": 198,
    "size": 6318,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "get_creation_date",
      "get_image_metadata",
      "get_prompt",
      "generate_csv",
      "format_file_size",
      "write_csv",
      "get_unique_file_path",
      "save_last_directory",
      "load_last_directory"
    ],
    "classes": [],
    "imports": [
      "csv",
      "os",
      "re",
      "datetime",
      "PIL"
    ],
    "preview": "import csv\nimport os\nimport re\nfrom datetime import datetime\n\nfrom PIL import Image, UnidentifiedImageError\n\n# Constants\nLAST_DIRECTORY_FILE = \"image_data.txt\"\n\n\n# Function to get the creation date of a file\ndef get_creation_date(filepath):\n    try:\n        return datetime.fromtimestamp(os.path.getctime(filepath)).strftime(\"%m-%d-%y\")\n    except Exception as e:\n        print(f\"Error getting creation date for {filepath}: {e}\")\n        return \"Unknown\"\n\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "579",
    "name": "create_photo_grid.py",
    "path": "01_core_ai_analysis/ai_generation/create_photo_grid.py",
    "category": "01_core_ai_analysis",
    "type": "image_processing",
    "lines": 65,
    "size": 2029,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "run_applescript",
      "create_photo_grid"
    ],
    "classes": [],
    "imports": [
      "math",
      "os",
      "osascript"
    ],
    "preview": "import math\nimport os\n\nimport osascript\n\n\ndef run_applescript(script, js_code):\n    \"\"\"Execute AppleScript from Python\"\"\"\n    full_script = script.format(js_code=js_code)\n    osascript.run(full_script)\n\n\ndef create_photo_grid(folder_path, grid_width, grid_height):\n    # Calculate the number of images and cell size\n    image_paths = [\n        os.path.join(folder_path, f)\n        for f in os.listdir(folder_path)\n        if f.endswith((\".png\", \".jpg\", \".jpeg\"))\n    ]\n    num_images = min(len(image_paths), grid_width * grid_height)",
    "last_modified": "2025-05-04T22:47:11.428050"
  },
  {
    "id": "580",
    "name": "ClipCompilationCreator.py",
    "path": "01_core_ai_analysis/ai_generation/ClipCompilationCreator.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 146,
    "size": 5867,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "__init__",
      "create_compilation",
      "write_clips",
      "composite_clips",
      "generate_clip_text",
      "load_clips",
      "ffmpeg_compress_audio"
    ],
    "classes": [
      "ClipCompilationCreator"
    ],
    "imports": [
      "logging",
      "os",
      "moviepy.editor",
      "moviepy.tools",
      "src",
      "src.MetadataHandler",
      "config"
    ],
    "preview": "import logging\nimport os\n\nfrom moviepy.editor import (\n    CompositeVideoClip,\n    ImageClip,\n    TextClip,\n    VideoFileClip,\n    afx,\n    concatenate_videoclips,\n)\nfrom moviepy.tools import subprocess_call\nfrom src import utils\nfrom src.MetadataHandler import MetadataHandler\n\nimport config\n\n\nclass ClipCompilationCreator:\n    def __init__(self, game: str, asset_path: str, output_path: str):",
    "last_modified": "2025-09-13T05:53:45.443818"
  },
  {
    "id": "581",
    "name": "run.py_02.py",
    "path": "02_media_processing/run.py_consolidated/run.py_02.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 114,
    "size": 3200,
    "docstring": "",
    "keywords": [
      "web_tools"
    ],
    "functions": [
      "scrape"
    ],
    "classes": [],
    "imports": [
      "os",
      "time",
      "selenium",
      "selenium",
      "selenium.webdriver.common.action_chains",
      "selenium.webdriver.common.by",
      "selenium.webdriver.common.keys",
      "selenium.webdriver.support",
      "selenium.webdriver.support.ui",
      "webdriver_manager.chrome"
    ],
    "preview": "import os\nimport time\n\nimport selenium\nfrom selenium import webdriver\nfrom selenium.webdriver.common.action_chains import ActionChains\nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.common.keys import Keys\nfrom selenium.webdriver.support import expected_conditions as EC\nfrom selenium.webdriver.support.ui import WebDriverWait\nfrom webdriver_manager.chrome import ChromeDriverManager as CM\n\n# Complete these 2 fields ==================\nUSERNAME = \"your instagram username\"\nPASSWORD = \"your instagram password\"\n# ==========================================\n\nTIMEOUT = 15\n\n",
    "last_modified": "2025-09-13T05:53:38.691563"
  },
  {
    "id": "582",
    "name": "run.py.py",
    "path": "02_media_processing/run.py_consolidated/run.py.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 92,
    "size": 3313,
    "docstring": "",
    "keywords": [],
    "functions": [
      "main"
    ],
    "classes": [],
    "imports": [
      "argparse",
      "ImageCreator",
      "RedditScrape",
      "TextToSpeech",
      "VideoEdit"
    ],
    "preview": "# This file will be the main driver function and run the entire progam\n# This will import the Reddit Scraping Class and the Video Editing Class\n\nimport argparse  # Used to handle command line arguments\n\nfrom ImageCreator import ImageCreator  # Generates images of posts\nfrom RedditScrape import (\n    RedditScrape,\n)  # Importing reddit scraping class to acquire posts and authors\nfrom TextToSpeech import TextToSpeech  # Importing tts class to make mp3 of posts\nfrom VideoEdit import VideoEditor  # Edits all the tts mp3 and Images into a mp4 video\n\n\ndef main() -> int:\n\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"file\", help=\"file path of input file\")\n    args = parser.parse_args()\n\n    \"\"\" input_metadata holds the meta data from each entry in the input file",
    "last_modified": "2025-09-13T05:53:51.779488"
  },
  {
    "id": "583",
    "name": "upload.py_02.py",
    "path": "02_media_processing/upload.py_consolidated/upload.py_02.py",
    "category": "02_media_processing",
    "type": "youtube",
    "lines": 127,
    "size": 4493,
    "docstring": "",
    "keywords": [],
    "functions": [
      "initialize_upload",
      "resumable_upload",
      "print_progress"
    ],
    "classes": [],
    "imports": [
      "random",
      "sys",
      "time",
      "googleapiclient.discovery",
      "googleapiclient.errors",
      "googleapiclient.http",
      "constants",
      "presets"
    ],
    "preview": "import random\nimport sys\nimport time\n\nfrom googleapiclient.discovery import Resource\nfrom googleapiclient.errors import HttpError\nfrom googleapiclient.http import MediaFileUpload\n\nfrom .constants import MAX_RETRIES, RETRIABLE_EXCEPTIONS, RETRIABLE_STATUS_CODES\nfrom .presets import PresetOptions\n\n\ndef initialize_upload(youtube: Resource, options: PresetOptions):\n    body_status = {\"selfDeclaredMadeForKids\": False}\n    if options.publish_at == \"Now\":\n        body_status[\"privacyStatus\"] = \"public\"\n    else:\n        body_status[\"privacyStatus\"] = \"private\"\n        body_status[\"publishAt\"] = options.publish_at.isoformat()\n",
    "last_modified": "2025-09-13T05:53:47.358678"
  },
  {
    "id": "584",
    "name": "upload.py.py",
    "path": "02_media_processing/upload.py_consolidated/upload.py.py",
    "category": "02_media_processing",
    "type": "youtube",
    "lines": 105,
    "size": 2496,
    "docstring": "",
    "keywords": [
      "image_processing",
      "youtube",
      "web_tools"
    ],
    "functions": [
      "upload_image",
      "upscale_image",
      "get_upscaled_image",
      "main"
    ],
    "classes": [],
    "imports": [
      "json",
      "time",
      "requests"
    ],
    "preview": "import json\nimport time\n\nimport requests\n\n\ndef upload_image(api_key, image_file_path):\n headers = {\n \"accept\": \"application/json\",\n \"content-type\": \"application/json\",\n \"authorization\": f\"Bearer {api_key}\"\n }\n \n # Get a presigned URL for uploading an image\n url = \"https://cloud.leonardo.ai/api/rest/v1/init-image\"\n\n payload = {\"extension\": \"jpg\"}\n response = requests.post(url, json=payload, headers=headers)\n\n if response.status_code != 200:",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "585",
    "name": "exceptions.py.py",
    "path": "02_media_processing/exceptions.py_consolidated/exceptions.py.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 68,
    "size": 1963,
    "docstring": "",
    "keywords": [],
    "functions": [
      "__init__",
      "__str__",
      "__init__",
      "__str__",
      "__init__",
      "__str__",
      "__init__",
      "__str__",
      "__init__",
      "__str__"
    ],
    "classes": [
      "SavifyError",
      "FFmpegNotInstalledError",
      "SpotifyApiCredentialsNotSetError",
      "UrlNotSupportedError",
      "YoutubeDlExtractionError",
      "InternetConnectionError"
    ],
    "imports": [],
    "preview": "class SavifyError(Exception):\n    def __init__(self, message=\"Savify ran into an error!\"):\n        self.message = message\n        super().__init__(self.message)\n\n    def __str__(self):\n        return self.message\n\n\nclass FFmpegNotInstalledError(SavifyError):\n    def __init__(\n        self,\n        message=\"FFmpeg must be installed to use Savify! [https://ffmpeg.org/download.html]\",\n    ):\n        self.message = message\n        super().__init__(self.message)\n\n    def __str__(self):\n        return self.message\n",
    "last_modified": "2025-05-04T23:27:53.632816"
  },
  {
    "id": "586",
    "name": "exceptions.py_02.py",
    "path": "02_media_processing/exceptions.py_consolidated/exceptions.py_02.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 15,
    "size": 362,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [
      "TwitchTubeError",
      "InvalidCategory",
      "VideoPathAlreadyExists",
      "NoClipsFound"
    ],
    "imports": [],
    "preview": "class TwitchTubeError(Exception):\n    \"\"\"General error class for TwitchTube.\"\"\"\n\n\nclass InvalidCategory(TwitchTubeError):\n    \"\"\"Error for when the specified category is invalid\"\"\"\n\n\nclass VideoPathAlreadyExists(TwitchTubeError):\n    \"\"\"Error for when a path already exists.\"\"\"\n\n\nclass NoClipsFound(TwitchTubeError):\n    \"\"\"Error for when no clips are found.\"\"\"\n",
    "last_modified": "2025-03-28T18:37:10.665090"
  },
  {
    "id": "587",
    "name": "organize.py.py",
    "path": "02_media_processing/organize.py_consolidated/organize.py.py",
    "category": "02_media_processing",
    "type": "organization",
    "lines": 35,
    "size": 1256,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "os",
      "shutil",
      "datetime"
    ],
    "preview": "import os\nimport shutil\nfrom datetime import datetime\n\n# Set the source directory where your images are stored\nsource_dir = \"/path/to/your/images\"\n\n# Loop through each file in the source directory\nfor filename in os.listdir(source_dir):\n    # Check for common image file extensions, case-insensitively\n    if filename.lower().endswith((\".png\", \".jpg\", \".jpeg\", \".gif\", \".bmp\")):\n        try:\n            # Get the full path to the file\n            file_path = os.path.join(source_dir, filename)\n\n            # Get the creation time and convert it to a year\n            creation_time = os.path.getctime(file_path)\n            year = datetime.fromtimestamp(creation_time).strftime(\"%Y\")\n\n            # Define the destination directory based on the year",
    "last_modified": "2025-03-28T18:37:05"
  },
  {
    "id": "588",
    "name": "organize.py_02.py",
    "path": "02_media_processing/organize.py_consolidated/organize.py_02.py",
    "category": "02_media_processing",
    "type": "organization",
    "lines": 29,
    "size": 1063,
    "docstring": "",
    "keywords": [],
    "functions": [
      "copy_files_with_structure"
    ],
    "classes": [],
    "imports": [
      "csv",
      "os",
      "shutil"
    ],
    "preview": "import csv\nimport os\nimport shutil\n\n\ndef copy_files_with_structure(csv_file_path, destination_base_path):\n    with open(csv_file_path, \"r\") as csv_file:\n        reader = csv.DictReader(csv_file)\n        for row in reader:\n            original_path = row[\"Original Path\"]\n            destination_path = os.path.join(destination_base_path, original_path.lstrip(os.sep))\n\n            # Create the destination directory if it doesn't exist\n            destination_dir = os.path.dirname(destination_path)\n            if not os.path.exists(destination_dir):\n                os.makedirs(destination_dir)\n\n            # Copy the file to the destination\n            shutil.copy2(original_path, destination_path)\n            print(f\"Copied {original_path} to {destination_path}\")",
    "last_modified": "2025-09-13T05:55:10.835843"
  },
  {
    "id": "589",
    "name": "watch_user_likers_stories.py",
    "path": "02_media_processing/format_conversion/watch_user_likers_stories.py",
    "category": "02_media_processing",
    "type": "organization",
    "lines": 84,
    "size": 2539,
    "docstring": "Watch user likers stories!\nThis script could be very useful to attract someone's\naudience to your account.\n\nIf you will not specify the user_id, the script will use\nyour likers as targets.\n\nDependencies:\n    pip install -U instabot\n\nNotes:\n    You can change file and add there your comments.",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "os",
      "random",
      "sys",
      "time",
      "instabot"
    ],
    "preview": "\"\"\"\nWatch user likers stories!\nThis script could be very useful to attract someone's\naudience to your account.\n\nIf you will not specify the user_id, the script will use\nyour likers as targets.\n\nDependencies:\n    pip install -U instabot\n\nNotes:\n    You can change file and add there your comments.\n\"\"\"\n\nimport os\nimport random\nimport sys\nimport time\n",
    "last_modified": "2025-09-13T05:54:55.835828"
  },
  {
    "id": "590",
    "name": "bot_comment.py",
    "path": "02_media_processing/format_conversion/bot_comment.py",
    "category": "02_media_processing",
    "type": "organization",
    "lines": 141,
    "size": 4898,
    "docstring": "Bot functions to generate and post a comments.\n\nInstructions to file with comments:\n    one line - one comment.\n\nExample:\n    lol\n    kek",
    "keywords": [],
    "functions": [
      "comment",
      "reply_to_comment",
      "comment_medias",
      "comment_hashtag",
      "comment_user",
      "comment_users",
      "comment_geotag",
      "is_commented"
    ],
    "classes": [],
    "imports": [
      "tqdm",
      "datetime"
    ],
    "preview": "\"\"\"\nBot functions to generate and post a comments.\n\nInstructions to file with comments:\n    one line - one comment.\n\nExample:\n    lol\n    kek\n\n\"\"\"\n\nfrom tqdm import tqdm\n\n\ndef comment(self, media_id, comment_text):\n    if self.is_commented(media_id):\n        return True\n    if not self.reached_limit(\"comments\"):\n        if self.blocked_actions[\"comments\"]:",
    "last_modified": "2025-09-13T05:54:57.203849"
  },
  {
    "id": "591",
    "name": "envs.py",
    "path": "02_media_processing/format_conversion/envs.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 26,
    "size": 689,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "json",
      "os",
      "dotenv"
    ],
    "preview": "import json\nimport os\n\nfrom dotenv import load_dotenv\n\n# Load .env variables\nload_dotenv(dotenv_path=os.path.expanduser(\"~/.env\"))\n\n# Retrieve API Key & Shop Data\nAPI_TOKEN = os.getenv(\"PRINTIFY_API_KEY\")\nSHOP_DATA_RAW = os.getenv(\"PRINTIFY_SHOPS\")\n\n# Ensure environment variable is loaded\nif SHOP_DATA_RAW is None:\n    print(\"\u274c ERROR: PRINTIFY_SHOPS not found in environment!\")\n    exit(1)\n\n# Convert JSON string to a dictionary\nSHOP_DATA = json.loads(SHOP_DATA_RAW)\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "592",
    "name": "subredditSentiment.py",
    "path": "02_media_processing/format_conversion/subredditSentiment.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 50,
    "size": 1637,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "math",
      "praw",
      "textblob"
    ],
    "preview": "import math\n\nimport praw\nfrom textblob import TextBlob\n\nreddit = praw.Reddit(\n    client_id=\"uifJuvRl06uxTg\",\n    client_secret=\"Bf4eHDQhORvpJBw7syIvzat8gyA\",\n    user_agent=\"subSentiment\",\n)\n\n# opens file with subreddit names\nwith open(\"sb.txt\") as f:\n\n    for line in f:\n        subreddit = reddit.subreddit(line.strip())\n        # write web agent to get converter for datetime to epoch on a daily basis for updates\n        day_start = 1510635601\n        day_end = 1510721999\n",
    "last_modified": "2025-09-13T05:53:51.271538"
  },
  {
    "id": "593",
    "name": "formatter.py",
    "path": "02_media_processing/format_conversion/formatter.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 125,
    "size": 4158,
    "docstring": "pygments.formatter\n~~~~~~~~~~~~~~~~~~\n\nBase formatter class.\n\n:copyright: Copyright 2006-2023 by the Pygments team, see AUTHORS.\n:license: BSD, see LICENSE for details.",
    "keywords": [],
    "functions": [
      "_lookup_style",
      "__init__",
      "get_style_defs",
      "format"
    ],
    "classes": [
      "Formatter"
    ],
    "imports": [
      "codecs",
      "pip._vendor.pygments.styles",
      "pip._vendor.pygments.util"
    ],
    "preview": "\"\"\"\npygments.formatter\n~~~~~~~~~~~~~~~~~~\n\nBase formatter class.\n\n:copyright: Copyright 2006-2023 by the Pygments team, see AUTHORS.\n:license: BSD, see LICENSE for details.\n\"\"\"\n\nimport codecs\n\nfrom pip._vendor.pygments.styles import get_style_by_name\nfrom pip._vendor.pygments.util import get_bool_opt\n\n__all__ = [\"Formatter\"]\n\n\ndef _lookup_style(style):\n    if isinstance(style, str):",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "594",
    "name": "egg_link.py",
    "path": "02_media_processing/format_conversion/egg_link.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 78,
    "size": 2450,
    "docstring": "",
    "keywords": [],
    "functions": [
      "_egg_link_names",
      "egg_link_path_from_sys_path",
      "egg_link_path_from_location"
    ],
    "classes": [],
    "imports": [
      "os",
      "re",
      "sys",
      "typing",
      "pip._internal.locations",
      "pip._internal.utils.virtualenv"
    ],
    "preview": "import os\nimport re\nimport sys\nfrom typing import List, Optional\n\nfrom pip._internal.locations import site_packages, user_site\nfrom pip._internal.utils.virtualenv import running_under_virtualenv, virtualenv_no_global\n\n__all__ = [\n    \"egg_link_path_from_sys_path\",\n    \"egg_link_path_from_location\",\n]\n\n\ndef _egg_link_names(raw_name: str) -> List[str]:\n    \"\"\"\n    Convert a Name metadata value to a .egg-link name, by applying\n    the same substitution as pkg_resources's safe_name function.\n    Note: we cannot use canonicalize_name because it has a different logic.\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "595",
    "name": "my_token.py",
    "path": "02_media_processing/format_conversion/my_token.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 201,
    "size": 4840,
    "docstring": "pygments.token\n~~~~~~~~~~~~~~\n\nBasic token types and the standard tokens.\n\n:copyright: Copyright 2006-2023 by the Pygments team, see AUTHORS.\n:license: BSD, see LICENSE for details.",
    "keywords": [],
    "functions": [
      "is_token_subtype",
      "string_to_tokentype",
      "split",
      "__init__",
      "__contains__",
      "__getattr__",
      "__repr__",
      "__copy__",
      "__deepcopy__"
    ],
    "classes": [
      "_TokenType"
    ],
    "imports": [],
    "preview": "\"\"\"\npygments.token\n~~~~~~~~~~~~~~\n\nBasic token types and the standard tokens.\n\n:copyright: Copyright 2006-2023 by the Pygments team, see AUTHORS.\n:license: BSD, see LICENSE for details.\n\"\"\"\n\n\nclass _TokenType(tuple):\n    parent = None\n\n    def split(self):\n        buf = []\n        node = self\n        while node is not None:\n            buf.append(node)\n            node = node.parent",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "596",
    "name": "link-html.py",
    "path": "02_media_processing/format_conversion/link-html.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 61,
    "size": 3065,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "pathlib",
      "markdown"
    ],
    "preview": "# file: generate_blog_from_markdown.py\n\nfrom pathlib import Path\n\nimport markdown\n\n# List of input markdown files\nmd_files = [\n    \"/Users/steven/Documents/DeepSeek/LinkedIn_SEO_and_Brand_Optimiz_2025-03-30_19_10_36.md\",\n    \"/Users/steven/Documents/DeepSeek/LinkedIn SEO and Brand Optimization Strategy_2025-03-30T18-35-10.md\",\n    \"/Users/steven/Documents/DeepSeek/LinkedIn_SEO_and_Brand_Optimiz_2025-03-30_18_38_16.md\",\n    \"/Users/steven/Documents/DeepSeek/LinkedIn SEO and Brand Optimization Strategy_2025-03-30T18-44-58.md\",\n    \"/Users/steven/Documents/DeepSeek/LinkedIn_SEO_and_Brand_Optimiz_2025-03-30_18_45_32.md\",\n    \"/Users/steven/Documents/DeepSeek/LinkedIn SEO and Brand Optimization Strategy_2025-03-30T18-46-35.md\",\n    \"/Users/steven/Documents/DeepSeek/LinkedIn SEO and Brand Optimization Strategy_2025-03-30T18-46-37.md\",\n    \"/Users/steven/Documents/DeepSeek/LinkedIn SEO and Brand Optimization Strategy_2025-03-30T18-47-49.md\",\n    \"/Users/steven/Documents/DeepSeek/LinkedIn SEO and Brand Optimization Strategy_2025-03-30T18-47-52.md\",\n    \"/Users/steven/Documents/DeepSeek/LinkedIn_SEO_and_Brand_Optimiz_2025-03-30_18_48_14.md\",\n    \"/Users/steven/Documents/DeepSeek/LinkedIn SEO and Brand Optimization Strategy_2025-03-30T18-48-31.md\",\n    \"/Users/steven/Documents/DeepSeek/LinkedIn_SEO_and_Brand_Optimiz_2025-03-30_18_52_16.md\",",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "597",
    "name": "data_warehouse.py",
    "path": "02_media_processing/format_conversion/data_warehouse.py",
    "category": "02_media_processing",
    "type": "analysis",
    "lines": 106,
    "size": 3062,
    "docstring": "",
    "keywords": [],
    "functions": [
      "main",
      "__export",
      "__export_data_category",
      "__import",
      "__import_data_category"
    ],
    "classes": [],
    "imports": [
      "json",
      "pathlib",
      "click",
      "llm_engineering.domain.base.nosql",
      "llm_engineering.domain.documents",
      "loguru"
    ],
    "preview": "import json\nfrom pathlib import Path\n\nimport click\nfrom llm_engineering.domain.base.nosql import NoSQLBaseDocument\nfrom llm_engineering.domain.documents import (\n    ArticleDocument,\n    PostDocument,\n    RepositoryDocument,\n    UserDocument,\n)\nfrom loguru import logger\n\n\n@click.command()\n@click.option(\n    \"--export-raw-data\",\n    is_flag=True,\n    default=False,\n    help=\"Whether to export your data warehouse to a JSON file.\",",
    "last_modified": "2025-09-13T05:53:42.569473"
  },
  {
    "id": "598",
    "name": "direct_url_helpers.py",
    "path": "02_media_processing/format_conversion/direct_url_helpers.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 86,
    "size": 3184,
    "docstring": "",
    "keywords": [],
    "functions": [
      "direct_url_as_pep440_direct_reference",
      "direct_url_for_editable",
      "direct_url_from_link"
    ],
    "classes": [],
    "imports": [
      "typing",
      "pip._internal.models.direct_url",
      "pip._internal.models.link",
      "pip._internal.utils.urls",
      "pip._internal.vcs"
    ],
    "preview": "from typing import Optional\n\nfrom pip._internal.models.direct_url import ArchiveInfo, DirectUrl, DirInfo, VcsInfo\nfrom pip._internal.models.link import Link\nfrom pip._internal.utils.urls import path_to_url\nfrom pip._internal.vcs import vcs\n\n\ndef direct_url_as_pep440_direct_reference(direct_url: DirectUrl, name: str) -> str:\n    \"\"\"Convert a DirectUrl to a pip requirement string.\"\"\"\n    direct_url.validate()  # if invalid, this is a pip bug\n    requirement = name + \" @ \"\n    fragments = []\n    if isinstance(direct_url.info, VcsInfo):\n        requirement += \"{}+{}@{}\".format(\n            direct_url.info.vcs, direct_url.url, direct_url.info.commit_id\n        )\n    elif isinstance(direct_url.info, ArchiveInfo):\n        requirement += direct_url.url\n        if direct_url.info.hash:",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "599",
    "name": "bot_unlike.py",
    "path": "02_media_processing/format_conversion/bot_unlike.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 61,
    "size": 1865,
    "docstring": "",
    "keywords": [],
    "functions": [
      "unlike",
      "unlike_comment",
      "unlike_media_comments",
      "unlike_medias",
      "unlike_user"
    ],
    "classes": [],
    "imports": [
      "tqdm"
    ],
    "preview": "from tqdm import tqdm\n\n\ndef unlike(self, media_id):\n    if not self.reached_limit(\"unlikes\"):\n        self.delay(\"unlike\")\n        if self.api.unlike(media_id):\n            self.total[\"unlikes\"] += 1\n            return True\n    else:\n        self.logger.info(\"Out of unlikes for today.\")\n    return False\n\n\ndef unlike_comment(self, comment_id):\n    if self.api.unlike_comment(comment_id):\n        return True\n    return False\n\n",
    "last_modified": "2025-09-13T05:54:58.099631"
  },
  {
    "id": "600",
    "name": "rtf.py",
    "path": "02_media_processing/format_conversion/rtf.py",
    "category": "02_media_processing",
    "type": "organization",
    "lines": 147,
    "size": 4932,
    "docstring": "pygments.formatters.rtf\n~~~~~~~~~~~~~~~~~~~~~~~\n\nA formatter that generates RTF files.\n\n:copyright: Copyright 2006-2023 by the Pygments team, see AUTHORS.\n:license: BSD, see LICENSE for details.",
    "keywords": [],
    "functions": [
      "__init__",
      "_escape",
      "_escape_text",
      "format_unencoded"
    ],
    "classes": [
      "RtfFormatter"
    ],
    "imports": [
      "pip._vendor.pygments.formatter",
      "pip._vendor.pygments.util"
    ],
    "preview": "\"\"\"\npygments.formatters.rtf\n~~~~~~~~~~~~~~~~~~~~~~~\n\nA formatter that generates RTF files.\n\n:copyright: Copyright 2006-2023 by the Pygments team, see AUTHORS.\n:license: BSD, see LICENSE for details.\n\"\"\"\n\nfrom pip._vendor.pygments.formatter import Formatter\nfrom pip._vendor.pygments.util import get_int_opt, surrogatepair\n\n__all__ = [\"RtfFormatter\"]\n\n\nclass RtfFormatter(Formatter):\n    \"\"\"\n    Format tokens as RTF markup. This formatter automatically outputs full RTF\n    documents with color information and other useful stuff. Perfect for Copy and",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "601",
    "name": "sources.py",
    "path": "02_media_processing/format_conversion/sources.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 285,
    "size": 8687,
    "docstring": "",
    "keywords": [],
    "functions": [
      "_is_html_file",
      "build_source",
      "link",
      "page_candidates",
      "file_links",
      "__init__",
      "_scan_directory",
      "page_candidates",
      "project_name_to_urls",
      "__init__"
    ],
    "classes": [
      "LinkSource",
      "_FlatDirectoryToUrls",
      "_FlatDirectorySource",
      "_LocalFileSource",
      "_RemoteFileSource",
      "_IndexDirectorySource"
    ],
    "imports": [
      "logging",
      "mimetypes",
      "os",
      "collections",
      "typing",
      "pip._internal.models.candidate",
      "pip._internal.models.link",
      "pip._internal.utils.urls",
      "pip._internal.vcs",
      "pip._vendor.packaging.utils"
    ],
    "preview": "import logging\nimport mimetypes\nimport os\nfrom collections import defaultdict\nfrom typing import Callable, Dict, Iterable, List, Optional, Tuple\n\nfrom pip._internal.models.candidate import InstallationCandidate\nfrom pip._internal.models.link import Link\nfrom pip._internal.utils.urls import path_to_url, url_to_path\nfrom pip._internal.vcs import is_url\nfrom pip._vendor.packaging.utils import (\n    InvalidSdistFilename,\n    InvalidVersion,\n    InvalidWheelFilename,\n    canonicalize_name,\n    parse_sdist_filename,\n    parse_wheel_filename,\n)\n\nlogger = logging.getLogger(__name__)",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "602",
    "name": "json-to-csv.py",
    "path": "02_media_processing/format_conversion/json-to-csv.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 62,
    "size": 2149,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "csv",
      "json",
      "os"
    ],
    "preview": "import csv\nimport json\nimport os\n\n# Prompt for the JSON input file path with a default value\ndefault_json_path = \"/Users/steven/Downloads/AI_Workflow_Automation_Summary.json\"\njson_file_path = (\n    input(f\"Enter JSON file path (default: {default_json_path}): \") or default_json_path\n)\n\n# Validate that the JSON file exists\nif not os.path.exists(json_file_path):\n    print(f\"Error: The file {json_file_path} does not exist.\")\n    exit(1)\n\n# Prompt for the CSV output file path with a default value\ndefault_csv_path = \"/Users/steven/Documents/output.csv\"\ncsv_file_path = (\n    input(f\"Enter CSV output file path (default: {default_csv_path}): \") or default_csv_path\n)",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "603",
    "name": "svg.py",
    "path": "02_media_processing/format_conversion/svg.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 200,
    "size": 7337,
    "docstring": "pygments.formatters.svg\n~~~~~~~~~~~~~~~~~~~~~~~\n\nFormatter for SVG output.\n\n:copyright: Copyright 2006-2023 by the Pygments team, see AUTHORS.\n:license: BSD, see LICENSE for details.",
    "keywords": [],
    "functions": [
      "escape_html",
      "__init__",
      "format_unencoded",
      "_get_style"
    ],
    "classes": [
      "SvgFormatter"
    ],
    "imports": [
      "pip._vendor.pygments.formatter",
      "pip._vendor.pygments.token",
      "pip._vendor.pygments.util"
    ],
    "preview": "\"\"\"\npygments.formatters.svg\n~~~~~~~~~~~~~~~~~~~~~~~\n\nFormatter for SVG output.\n\n:copyright: Copyright 2006-2023 by the Pygments team, see AUTHORS.\n:license: BSD, see LICENSE for details.\n\"\"\"\n\nfrom pip._vendor.pygments.formatter import Formatter\nfrom pip._vendor.pygments.token import Comment\nfrom pip._vendor.pygments.util import get_bool_opt, get_int_opt\n\n__all__ = [\"SvgFormatter\"]\n\n\ndef escape_html(text):\n    \"\"\"Escape &, <, > as well as single and double quotes for HTML.\"\"\"\n    return (",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "604",
    "name": "packaging.py",
    "path": "02_media_processing/format_conversion/packaging.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 56,
    "size": 2102,
    "docstring": "",
    "keywords": [],
    "functions": [
      "check_requires_python",
      "get_requirement",
      "safe_extra"
    ],
    "classes": [],
    "imports": [
      "functools",
      "logging",
      "re",
      "typing",
      "pip._vendor.packaging",
      "pip._vendor.packaging.requirements"
    ],
    "preview": "import functools\nimport logging\nimport re\nfrom typing import NewType, Optional, Tuple, cast\n\nfrom pip._vendor.packaging import specifiers, version\nfrom pip._vendor.packaging.requirements import Requirement\n\nNormalizedExtra = NewType(\"NormalizedExtra\", str)\n\nlogger = logging.getLogger(__name__)\n\n\ndef check_requires_python(requires_python: Optional[str], version_info: Tuple[int, ...]) -> bool:\n    \"\"\"\n    Check if the given Python version matches a \"Requires-Python\" specifier.\n\n    :param version_info: A 3-tuple of ints representing a Python\n        major-minor-micro version to check (e.g. `sys.version_info[:3]`).\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "605",
    "name": "sagemaker.py",
    "path": "02_media_processing/format_conversion/sagemaker.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 76,
    "size": 2570,
    "docstring": "",
    "keywords": [],
    "functions": [
      "run_finetuning_on_sagemaker"
    ],
    "classes": [],
    "imports": [
      "pathlib",
      "huggingface_hub",
      "loguru",
      "llm_engineering.settings",
      "sagemaker.huggingface"
    ],
    "preview": "from pathlib import Path\n\nfrom huggingface_hub import HfApi\nfrom loguru import logger\n\ntry:\n    from sagemaker.huggingface import HuggingFace\nexcept ModuleNotFoundError:\n    logger.warning(\n        \"Couldn't load SageMaker imports. Run 'poetry install --with aws' to support AWS.\"\n    )\n\nfrom llm_engineering.settings import settings\n\nfinetuning_dir = Path(__file__).resolve().parent\nfinetuning_requirements_path = finetuning_dir / \"requirements.txt\"\n\n\ndef run_finetuning_on_sagemaker(\n    finetuning_type: str = \"sft\",",
    "last_modified": "2025-09-13T05:53:42.465710"
  },
  {
    "id": "606",
    "name": "message_users.py",
    "path": "02_media_processing/format_conversion/message_users.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 92,
    "size": 2813,
    "docstring": "instabot example\n\nWorkflow:\n1) Ask Message type\n2) Load messages CSV (if needed)\n3) Send message to each users",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "csv",
      "os",
      "sys",
      "time",
      "tqdm",
      "instabot"
    ],
    "preview": "\"\"\"\ninstabot example\n\nWorkflow:\n1) Ask Message type\n2) Load messages CSV (if needed)\n3) Send message to each users\n\"\"\"\n\nimport csv\nimport os\nimport sys\nimport time\n\nfrom tqdm import tqdm\n\nsys.path.append(os.path.join(sys.path[0], \"../\"))\nfrom instabot import Bot  # noqa: E402\n\ninstaUsers = [\"R1B4Z01D\", \"KoanMedia\"]",
    "last_modified": "2025-03-28T18:35:47"
  },
  {
    "id": "607",
    "name": "search.py",
    "path": "02_media_processing/format_conversion/search.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 173,
    "size": 5672,
    "docstring": "",
    "keywords": [],
    "functions": [
      "transform_hits",
      "print_dist_installation_info",
      "print_results",
      "highest_version",
      "add_options",
      "run",
      "search"
    ],
    "classes": [
      "SearchCommand",
      "TransformedHit"
    ],
    "imports": [
      "logging",
      "shutil",
      "sys",
      "textwrap",
      "xmlrpc.client",
      "collections",
      "optparse",
      "typing",
      "pip._internal.cli.base_command",
      "pip._internal.cli.req_command"
    ],
    "preview": "import logging\nimport shutil\nimport sys\nimport textwrap\nimport xmlrpc.client\nfrom collections import OrderedDict\nfrom optparse import Values\nfrom typing import TYPE_CHECKING, Dict, List, Optional\n\nfrom pip._internal.cli.base_command import Command\nfrom pip._internal.cli.req_command import SessionCommandMixin\nfrom pip._internal.cli.status_codes import NO_MATCHES_FOUND, SUCCESS\nfrom pip._internal.exceptions import CommandError\nfrom pip._internal.metadata import get_default_environment\nfrom pip._internal.models.index import PyPI\nfrom pip._internal.network.xmlrpc import PipXmlrpcTransport\nfrom pip._internal.utils.logging import indent_log\nfrom pip._internal.utils.misc import write_output\nfrom pip._vendor.packaging.version import parse as parse_version\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "608",
    "name": "filter.py",
    "path": "02_media_processing/format_conversion/filter.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 78,
    "size": 1940,
    "docstring": "pygments.filter\n~~~~~~~~~~~~~~~\n\nModule that implements the default filter.\n\n:copyright: Copyright 2006-2023 by the Pygments team, see AUTHORS.\n:license: BSD, see LICENSE for details.",
    "keywords": [],
    "functions": [
      "apply_filters",
      "simplefilter",
      "_apply",
      "__init__",
      "filter",
      "__init__",
      "filter"
    ],
    "classes": [
      "Filter",
      "FunctionFilter"
    ],
    "imports": [],
    "preview": "\"\"\"\npygments.filter\n~~~~~~~~~~~~~~~\n\nModule that implements the default filter.\n\n:copyright: Copyright 2006-2023 by the Pygments team, see AUTHORS.\n:license: BSD, see LICENSE for details.\n\"\"\"\n\n\ndef apply_filters(stream, filters, lexer=None):\n    \"\"\"\n    Use this method to apply an iterable of filters to\n    a stream. If lexer is given it's forwarded to the\n    filter, otherwise the filter receives `None`.\n    \"\"\"\n\n    def _apply(filter_, stream):\n        yield from filter_.filter(lexer, stream)",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "609",
    "name": "bot_stats.py",
    "path": "02_media_processing/format_conversion/bot_stats.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 50,
    "size": 1417,
    "docstring": "",
    "keywords": [],
    "functions": [
      "get_tsv_line",
      "get_header_line",
      "ensure_dir",
      "dump_data",
      "save_user_stats"
    ],
    "classes": [],
    "imports": [
      "datetime",
      "os"
    ],
    "preview": "import datetime\nimport os\n\n\ndef get_tsv_line(dictionary):\n    line = \"\"\n    for key in sorted(dictionary):\n        line += str(dictionary[key]) + \"\\t\"\n    return line[:-1] + \"\\n\"\n\n\ndef get_header_line(dictionary):\n    line = \"\\t\".join(sorted(dictionary))\n    return line + \"\\n\"\n\n\ndef ensure_dir(file_path):\n    directory = os.path.dirname(file_path)\n    if not os.path.exists(directory) and directory:\n        os.makedirs(directory)",
    "last_modified": "2025-03-28T18:35:47"
  },
  {
    "id": "610",
    "name": "__init__.py_02.py",
    "path": "02_media_processing/__init__.py_consolidated/__init__.py_02.py",
    "category": "02_media_processing",
    "type": "video_processing",
    "lines": 2,
    "size": 36,
    "docstring": "Unit test package for Savify.",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [],
    "preview": "\"\"\"Unit test package for Savify.\"\"\"\n",
    "last_modified": "2022-12-30T17:46:57"
  },
  {
    "id": "611",
    "name": "__init__.py.py",
    "path": "02_media_processing/__init__.py_consolidated/__init__.py.py",
    "category": "02_media_processing",
    "type": "analysis",
    "lines": 27,
    "size": 465,
    "docstring": "Savify\n\nDownload Spotify songs to mp3 with full metadata and cover art!\n\n:Copyright: 2020, Laurence Rawlings.\n:License: MIT (see /LICENSE).",
    "keywords": [],
    "functions": [
      "cli"
    ],
    "classes": [],
    "imports": [
      "savify",
      "types",
      "cli"
    ],
    "preview": "\"\"\"\nSavify\n\nDownload Spotify songs to mp3 with full metadata and cover art!\n\n:Copyright: 2020, Laurence Rawlings.\n:License: MIT (see /LICENSE).\n\"\"\"\n\nfrom .savify import *\nfrom .types import *\n\n__title__ = \"Savify\"\n__author__ = \"\"\"Laurence Rawlings\"\"\"\n__email__ = \"contact@laurencerawlings.com\"\n__version__ = \"2.3.4\"\n__license__ = \"MIT\"\n__docformat__ = \"restructuredtext en\"\n\n__all__ = [\"savify\", \"types\", \"utils\"]",
    "last_modified": "2025-05-04T23:27:53.526540"
  },
  {
    "id": "612",
    "name": "copy.py_02.py",
    "path": "02_media_processing/copy.py_consolidated/copy.py_02.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 39,
    "size": 1386,
    "docstring": "",
    "keywords": [],
    "functions": [
      "ensure_dir"
    ],
    "classes": [],
    "imports": [
      "csv",
      "os",
      "shutil"
    ],
    "preview": "import csv\nimport os\nimport shutil\n\n# Paths configuration\ncsv_file_path = \"/Users/steven/15days.csv\"\ndestination_root = \"/Volumes/iMac/15days\"\nbackup_root = os.path.join(destination_root, \"backup\")\n\n\ndef ensure_dir(directory):\n    \"\"\"Ensure the directory exists. If not, create it.\"\"\"\n    if not os.path.exists(directory):\n        os.makedirs(directory)\n\n\nwith open(csv_file_path, newline=\"\") as csvfile:\n    filereader = csv.reader(csvfile)\n    for row in filereader:\n        # Assuming each row has one column with the file path",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "613",
    "name": "copy.py.py",
    "path": "02_media_processing/copy.py_consolidated/copy.py.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 91,
    "size": 3778,
    "docstring": "",
    "keywords": [],
    "functions": [
      "move_or_copy_files"
    ],
    "classes": [],
    "imports": [
      "csv",
      "logging",
      "os",
      "shutil"
    ],
    "preview": "import csv\nimport logging\nimport os\nimport shutil\n\n# Configure logging\nlogging.basicConfig(\n    filename=\"file_operations.log\",\n    level=logging.INFO,\n    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n)\n\n# Define the paths to your CSV files\ncsv_files = {\n    \"videos\": \"/Users/steven/Documents/Organize/vids-07-11-11_34.csv\",\n    \"audio\": \"/Users/steven/Documents/Organize/audio_files-07-11-11_34.csv\",\n    \"documents\": \"/Users/steven/Documents/Organize/docs-07-11-11_34.csv\",\n    \"images\": \"/Users/steven/Documents/Organize/images-07-11-11_34.csv\",\n    \"other\": \"/Users/steven/Documents/Organize/other-07-11-11_34.csv\",\n}",
    "last_modified": "2025-09-13T05:53:54.314007"
  },
  {
    "id": "614",
    "name": "organize_albums 3.py_02.py",
    "path": "02_media_processing/organize_albums 3.py_consolidated/organize_albums 3.py_02.py",
    "category": "02_media_processing",
    "type": "organization",
    "lines": 106,
    "size": 4065,
    "docstring": "",
    "keywords": [
      "organization"
    ],
    "functions": [
      "organize_files"
    ],
    "classes": [],
    "imports": [
      "os",
      "shutil"
    ],
    "preview": "import os\nimport shutil\n\n# Define the base directory\nbase_dir = \"/Users/steven/Music/nocTurneMeLoDieS/mp3\"\n\n\ndef organize_files():\n    # Check if the base directory exists\n    if not os.path.exists(base_dir):\n        print(f\"\u274c Error: The directory '{base_dir}' does not exist.\")\n        return\n\n    # List all files in the base directory\n    try:\n        files = os.listdir(base_dir)\n    except Exception as e:\n        print(f\"\u274c Error accessing directory '{base_dir}': {e}\")\n        return\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "615",
    "name": "organize_albums 3.py.py",
    "path": "02_media_processing/organize_albums 3.py_consolidated/organize_albums 3.py.py",
    "category": "02_media_processing",
    "type": "organization",
    "lines": 62,
    "size": 2470,
    "docstring": "",
    "keywords": [
      "organization"
    ],
    "functions": [
      "organize_files"
    ],
    "classes": [],
    "imports": [
      "os",
      "shutil"
    ],
    "preview": "import os\nimport shutil\n\n# Define the base directory\nbase_dir = \"/Users/steven/Music/NocTurnE-meLoDieS/albums/\"\n\n\n# Helper function to create folders and move files\ndef organize_files():\n    # List all files in the base directory\n    files = os.listdir(base_dir)\n\n    # Process each file\n    for file in files:\n        # Skip directories\n        if os.path.isdir(os.path.join(base_dir, file)):\n            continue\n\n        # Extract the base name (album name) from the file\n        if file.endswith(\".mp3\"):",
    "last_modified": "2025-05-04T22:47:11"
  },
  {
    "id": "616",
    "name": "imgconvert_colab.py",
    "path": "02_media_processing/image_tools/imgconvert_colab.py",
    "category": "02_media_processing",
    "type": "image_processing",
    "lines": 53,
    "size": 1729,
    "docstring": "",
    "keywords": [],
    "functions": [
      "convert_and_upscale_images",
      "main"
    ],
    "classes": [],
    "imports": [
      "os",
      "PIL"
    ],
    "preview": "import os\n\nfrom PIL import Image\n\n# Function to convert WebP images to PNG and upscale by 200% with 300 DPI\n\n\ndef convert_and_upscale_images(source_directory, destination_directory):\n    # Create the destination directory if it doesn't exist\n    os.makedirs(destination_directory, exist_ok=True)\n\n    for filename in os.listdir(source_directory):\n        if filename.endswith(\".png\"):\n            source_file = os.path.join(source_directory, filename)\n            filename_no_ext = os.path.splitext(filename)[0]\n            destination_file = os.path.join(destination_directory, f\"{filename_no_ext}.png\")\n\n            # Convert WebP to PNG and upscale by 200% with 300 DPI\n            im = Image.open(source_file)\n            width, height = im.size",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "617",
    "name": "vidgenUI.py",
    "path": "02_media_processing/image_tools/vidgenUI.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 128,
    "size": 4495,
    "docstring": "",
    "keywords": [
      "testing"
    ],
    "functions": [
      "__init__",
      "populateComboBox",
      "renderBackupFromName",
      "deleteBackupFromName",
      "closeEvent",
      "testServerFTP",
      "updateScriptScreen",
      "updateRenderProgress"
    ],
    "classes": [
      "renderingScreen"
    ],
    "imports": [
      "pickle",
      "shutil",
      "sys",
      "traceback",
      "distutils.dir_util",
      "server",
      "settings",
      "vidGen",
      "PyQt5",
      "PyQt5.QtCore"
    ],
    "preview": "import pickle\nimport shutil\nimport sys\nimport traceback\nfrom distutils.dir_util import copy_tree\n\nimport server\nimport settings\nimport vidGen\nfrom PyQt5 import QtCore, QtGui, QtWidgets, uic\nfrom PyQt5.QtCore import QDir, QObject, QPoint, QRect, Qt, QUrl, pyqtSignal\nfrom PyQt5.QtGui import QIcon\nfrom PyQt5.QtWidgets import *\n\n\nclass renderingScreen(QDialog):\n    script_queue_update = pyqtSignal()\n    render_progress = pyqtSignal()\n\n    update_backups = pyqtSignal()",
    "last_modified": "2025-09-13T05:53:32.772943"
  },
  {
    "id": "618",
    "name": "scan_images_individual.py",
    "path": "02_media_processing/image_tools/scan_images_individual.py",
    "category": "02_media_processing",
    "type": "image_processing",
    "lines": 31,
    "size": 936,
    "docstring": "",
    "keywords": [],
    "functions": [
      "scan_directory",
      "main"
    ],
    "classes": [],
    "imports": [
      "os"
    ],
    "preview": "import os\n\n\ndef scan_directory(directory, file_types, min_size):\n    for root, dirs, files in os.walk(directory):\n        for file in files:\n            if file.lower().endswith(file_types):\n                file_path = os.path.join(root, file)\n                if os.path.getsize(file_path) >= min_size:\n                    yield file_path\n\n\ndef main():\n    file_types = (\".jpg\", \".jpeg\", \".png\", \".gif\", \".bmp\")\n    min_size = 1024 * 1024  # 1MB in bytes\n\n    drive = input(\"Enter the drive path to scan (e.g., /Volumes/4t): \")\n\n    # Automatically name the output file based on the drive\n    output_filename = f\"image_paths_{os.path.basename(drive)}.txt\"",
    "last_modified": "2025-05-04T22:47:11.913608"
  },
  {
    "id": "619",
    "name": "imgmove.py",
    "path": "02_media_processing/image_tools/imgmove.py",
    "category": "02_media_processing",
    "type": "image_processing",
    "lines": 56,
    "size": 2189,
    "docstring": "",
    "keywords": [],
    "functions": [
      "is_excluded_path",
      "get_creation_date",
      "is_excluded_path"
    ],
    "classes": [],
    "imports": [
      "os",
      "shutil"
    ],
    "preview": "import os\nimport shutil\n\n\ndef is_excluded_path(path, excluded_paths):\n    return any(path.startswith(excluded_path) for excluded_path in excluded_paths)\n\ndef get_creation_date(file_path):\n    try:\n        creation_time = os.path.getctime(file_path)\n    except Exception:\n        creation_time = os.path.getmtime(file_path)\n    return datetime.fromtimestamp(creation_time).strftime('%Y_%m_%d')\n    def is_excluded_path(path, excluded_paths):\n    if any(path.startswith(excluded_path) for excluded_path in excluded_paths):\n        return True\n    for part in path.split(os.sep):\n        if part.startswith('.'):\n return True\n    return False",
    "last_modified": "2025-03-28T18:37:05"
  },
  {
    "id": "620",
    "name": "youtube_dl_button 2.py",
    "path": "02_media_processing/image_tools/youtube_dl_button 2.py",
    "category": "02_media_processing",
    "type": "youtube",
    "lines": 386,
    "size": 15841,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "logging",
      "asyncio",
      "json",
      "math",
      "os",
      "shutil",
      "time",
      "datetime",
      "pyrogram",
      "translation"
    ],
    "preview": "import logging\n\nlogging.basicConfig(\n    level=logging.DEBUG, format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\n)\nlogger = logging.getLogger(__name__)\n\nimport asyncio\nimport json\nimport math\nimport os\nimport shutil\nimport time\nfrom datetime import datetime\n\n# the secret configuration specific things\nif bool(os.environ.get(\"WEBHOOK\", False)):\n    from sample_config import Config\nelse:\n    from config import Config",
    "last_modified": "2025-09-13T05:54:09.724031"
  },
  {
    "id": "621",
    "name": "convert copy.py",
    "path": "02_media_processing/image_tools/convert copy.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 24,
    "size": 995,
    "docstring": "",
    "keywords": [],
    "functions": [
      "convert_to_direct_link"
    ],
    "classes": [],
    "imports": [],
    "preview": "# Automating the conversion of Google Drive shared links to direct image URLs\n\n# Extracted URLs from the provided file\ndrive_urls = [\n    \"https://drive.google.com/file/d/1w0YfYCEbBPZtQkzlf9X9pQHrzvbX46r2/view?usp=drivesdk\",\n    \"https://drive.google.com/file/d/1P0xd0htF4_90t_zvYjOWFnFsWr3NzZHw/view?usp=drivesdk\",\n    \"https://drive.google.com/file/d/1QKGv3oypeSSj4Q2AE928CsNfXoTh00fH/view?usp=drivesdk\",\n    \"https://drive.google.com/file/d/1YhjYSTgzrOBJGg6pWF24aNXqzyyuRIRh/view?usp=drivesdk\",\n    \"https://drive.google.com/file/d/1Rmtr7TMDtvuvXZ9Ir1FVM4mUPgk5us_p/view?usp=drivesdk\",\n    # Adding only a few for demonstration; the user can add more as needed\n]\n\n# Function to convert shared link to direct link\n\n\ndef convert_to_direct_link(shared_link):\n    file_id = shared_link.split(\"/d/\")[1].split(\"/\")[0]\n    return f\"https://drive.google.com/uc?export=view&id={file_id}\"\n\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "622",
    "name": "direct_send_photo.py",
    "path": "02_media_processing/image_tools/direct_send_photo.py",
    "category": "02_media_processing",
    "type": "image_processing",
    "lines": 24,
    "size": 564,
    "docstring": "instabot example\n\nSend photo to user",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "argparse",
      "os",
      "sys",
      "instabot"
    ],
    "preview": "\"\"\"\ninstabot example\n\nSend photo to user\n\"\"\"\n\nimport argparse\nimport os\nimport sys\n\nsys.path.append(os.path.join(sys.path[0], \"../\"))\nfrom instabot import Bot  # noqa: E402\n\nparser = argparse.ArgumentParser(add_help=True)\nparser.add_argument(\"-u\", type=str, help=\"username\")\nparser.add_argument(\"-p\", type=str, help=\"password\")\nparser.add_argument(\"user\", type=str, nargs=\"*\", help=\"user\")\nparser.add_argument(\"--filepath\", required=True)\nargs = parser.parse_args()\n",
    "last_modified": "2025-03-28T18:35:47"
  },
  {
    "id": "623",
    "name": "up-down-old.py",
    "path": "02_media_processing/image_tools/up-down-old.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 124,
    "size": 5188,
    "docstring": "",
    "keywords": [],
    "functions": [
      "adjust_image_size",
      "convert_and_downscale_images_in_subfolders",
      "main"
    ],
    "classes": [],
    "imports": [
      "os",
      "PIL"
    ],
    "preview": "import os\n\nfrom PIL import Image, UnidentifiedImageError\n\n\ndef adjust_image_size(im, target_file_size, temp_file, target_dpi, upscale=False):\n    file_size = os.path.getsize(temp_file)\n\n    # Size limits: 4500x5400 max, 1024x1024 min\n    max_width, max_height = 4500, 5400\n    min_width, min_height = 1024, 1024\n\n    while (file_size > target_file_size) or (upscale and file_size < target_file_size):\n        # Downscale if image is too large\n        if file_size > target_file_size or im.size[0] > max_width or im.size[1] > max_height:\n            scale_factor = 0.9  # Downscale by 10%\n        # Upscale if image is too small\n        elif im.size[0] < min_width or im.size[1] < min_height:\n            scale_factor = 1.1  # Upscale by 10%\n        else:",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "624",
    "name": "2leomotion 2.py",
    "path": "02_media_processing/image_tools/2leomotion 2.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 90,
    "size": 3097,
    "docstring": "",
    "keywords": [
      "web_tools"
    ],
    "functions": [],
    "classes": [],
    "imports": [
      "time",
      "requests"
    ],
    "preview": "import time\n\nimport requests\n\napi_key = \"b5b99021-8e7a-42ef-8df9-4eca2c6efd3c\"\nauthorization = \"Bearer %s\" % api_key\n\nheaders = {\n    \"accept\": \"application/json\",\n    \"content-type\": \"application/json\",\n    \"authorization\": authorization,\n}\n\n# Generate an image\nurl = \"https://cloud.leonardo.ai/api/rest/v1/generations\"\n\npayload = {\n    \"height\": 960,\n    \"modelId\": \"ac614f96-1082-45bf-be9d-757f2d31c174\",\n    \"prompt\": \"A detailed photograph of a serious cyberpunk Hacker Cyborg transhumanist the past looking directly at the camera, standing straight, hands relaxed, square jaws, masculine face, dark scruff and no wrinkles, slightly buff looking, wearing a dark graphic t-shirt, detailed clothing texture realistic skin texture, black background, sharp focus, front view, waist up shot, high contrast, strong backlighting, action film dark color lut, cinematic luts\",",
    "last_modified": "2025-05-04T22:47:11"
  },
  {
    "id": "625",
    "name": "imgmp4 copy.py",
    "path": "02_media_processing/image_tools/imgmp4 copy.py",
    "category": "02_media_processing",
    "type": "image_processing",
    "lines": 58,
    "size": 1968,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "get_cover_images",
      "convert_mp3_to_mp4_with_images",
      "process_directory"
    ],
    "classes": [],
    "imports": [
      "glob",
      "os",
      "moviepy.editor",
      "PIL",
      "sys"
    ],
    "preview": "import glob\nimport os\n\nfrom moviepy.editor import AudioFileClip, ImageSequenceClip\nfrom PIL import Image\n\n\ndef get_cover_images(file_name, cover_image_directory):\n    # Check for both JPG and PNG extensions\n    images = []\n    jpg_paths = glob.glob(os.path.join(cover_image_directory, f\"{file_name}*.jpg\"))\n    png_paths = glob.glob(os.path.join(cover_image_directory, f\"{file_name}*.png\"))\n\n    images.extend(jpg_paths)\n    images.extend(png_paths)\n\n    if images:\n        return images\n    else:\n        print(f\"Cover images not found for {file_name}. Please ensure the cover images exist.\")",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "626",
    "name": "over3.py",
    "path": "02_media_processing/image_tools/over3.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 53,
    "size": 1737,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "get_cover_image",
      "convert_mp3_to_mp4",
      "process_directory"
    ],
    "classes": [],
    "imports": [
      "glob",
      "os",
      "moviepy.editor",
      "PIL",
      "sys"
    ],
    "preview": "import glob\nimport os\n\nfrom moviepy.editor import AudioFileClip, ImageClip\nfrom PIL import Image\n\n\ndef get_cover_image(file_name, cover_image_directory):\n    # Check for both JPG and PNG extensions\n    jpg_path = os.path.join(cover_image_directory, f\"{file_name}.jpg\")\n    png_path = os.path.join(cover_image_directory, f\"{file_name}.png\")\n\n    if os.path.exists(jpg_path):\n        return jpg_path\n    elif os.path.exists(png_path):\n        return png_path\n    else:\n        print(f\"Cover image not found for {file_name}. Please ensure the cover image exists.\")\n        return None\n",
    "last_modified": "2025-09-13T05:53:55.368044"
  },
  {
    "id": "627",
    "name": "test_bot_get.py",
    "path": "02_media_processing/image_tools/test_bot_get.py",
    "category": "02_media_processing",
    "type": "testing",
    "lines": 1097,
    "size": 37549,
    "docstring": "",
    "keywords": [
      "testing"
    ],
    "functions": [
      "test_get_media_owner",
      "test_get_media_info",
      "test_get_popular_medias",
      "test_get_timeline_medias",
      "test_get_timeline_users",
      "test_get_your_medias",
      "test_get_user_medias",
      "test_get_archived_medias",
      "test_search_users",
      "test_search_users_failed"
    ],
    "classes": [
      "TestBotGet"
    ],
    "imports": [
      "tempfile",
      "pytest",
      "responses",
      "instabot",
      "instabot.api.config",
      "test_bot",
      "test_variables",
      "unittest.mock",
      "mock"
    ],
    "preview": "import tempfile\n\nimport pytest\nimport responses\nfrom instabot import utils\nfrom instabot.api.config import API_URL, SIG_KEY_VERSION\n\nfrom .test_bot import TestBot\nfrom .test_variables import (\n    TEST_CAPTION_ITEM,\n    TEST_COMMENT_ITEM,\n    TEST_COMMENT_LIKER_ITEM,\n    TEST_FOLLOWER_ITEM,\n    TEST_FOLLOWING_ITEM,\n    TEST_INBOX_THREAD_ITEM,\n    TEST_LOCATION_ITEM,\n    TEST_MEDIA_LIKER,\n    TEST_MOST_RECENT_INVITER_ITEM,\n    TEST_PHOTO_ITEM,\n    TEST_SEARCH_USERNAME_ITEM,",
    "last_modified": "2025-09-13T05:54:59.155428"
  },
  {
    "id": "628",
    "name": "organize_albums 16.py",
    "path": "02_media_processing/image_tools/organize_albums 16.py",
    "category": "02_media_processing",
    "type": "organization",
    "lines": 62,
    "size": 2470,
    "docstring": "",
    "keywords": [
      "organization"
    ],
    "functions": [
      "organize_files"
    ],
    "classes": [],
    "imports": [
      "os",
      "shutil"
    ],
    "preview": "import os\nimport shutil\n\n# Define the base directory\nbase_dir = \"/Users/steven/Music/NocTurnE-meLoDieS/albums/\"\n\n\n# Helper function to create folders and move files\ndef organize_files():\n    # List all files in the base directory\n    files = os.listdir(base_dir)\n\n    # Process each file\n    for file in files:\n        # Skip directories\n        if os.path.isdir(os.path.join(base_dir, file)):\n            continue\n\n        # Extract the base name (album name) from the file\n        if file.endswith(\".mp3\"):",
    "last_modified": "2025-08-02T18:25:58.493921"
  },
  {
    "id": "629",
    "name": "config_20241213005656.py",
    "path": "02_media_processing/image_tools/config_20241213005656.py",
    "category": "02_media_processing",
    "type": "setup",
    "lines": 3,
    "size": 56,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [],
    "preview": "# config.py\nSOURCE_DIRECTORY = \"/Users/steven/Pictures\"\n",
    "last_modified": "2024-12-13T00:56:56.356783"
  },
  {
    "id": "630",
    "name": "scan_images_individual--.py",
    "path": "02_media_processing/image_tools/scan_images_individual--.py",
    "category": "02_media_processing",
    "type": "image_processing",
    "lines": 31,
    "size": 936,
    "docstring": "",
    "keywords": [],
    "functions": [
      "scan_directory",
      "main"
    ],
    "classes": [],
    "imports": [
      "os"
    ],
    "preview": "import os\n\n\ndef scan_directory(directory, file_types, min_size):\n    for root, dirs, files in os.walk(directory):\n        for file in files:\n            if file.lower().endswith(file_types):\n                file_path = os.path.join(root, file)\n                if os.path.getsize(file_path) >= min_size:\n                    yield file_path\n\n\ndef main():\n    file_types = (\".jpg\", \".jpeg\", \".png\", \".gif\", \".bmp\")\n    min_size = 1024 * 1024  # 1MB in bytes\n\n    drive = input(\"Enter the drive path to scan (e.g., /Volumes/4t): \")\n\n    # Automatically name the output file based on the drive\n    output_filename = f\"image_paths_{os.path.basename(drive)}.txt\"",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "631",
    "name": "modeline.py",
    "path": "02_media_processing/image_tools/modeline.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 47,
    "size": 983,
    "docstring": "pygments.modeline\n~~~~~~~~~~~~~~~~~\n\nA simple modeline parser (based on pymodeline).\n\n:copyright: Copyright 2006-2023 by the Pygments team, see AUTHORS.\n:license: BSD, see LICENSE for details.",
    "keywords": [],
    "functions": [
      "get_filetype_from_line",
      "get_filetype_from_buffer"
    ],
    "classes": [],
    "imports": [
      "re"
    ],
    "preview": "\"\"\"\npygments.modeline\n~~~~~~~~~~~~~~~~~\n\nA simple modeline parser (based on pymodeline).\n\n:copyright: Copyright 2006-2023 by the Pygments team, see AUTHORS.\n:license: BSD, see LICENSE for details.\n\"\"\"\n\nimport re\n\n__all__ = [\"get_filetype_from_buffer\"]\n\n\nmodeline_re = re.compile(\n    r\"\"\"\n    (?: vi | vim | ex ) (?: [<=>]? \\d* )? :\n    .* (?: ft | filetype | syn | syntax ) = ( [^:\\s]+ )\n\"\"\",",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "632",
    "name": "leonardo_script copy 2.py",
    "path": "02_media_processing/image_tools/leonardo_script copy 2.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 129,
    "size": 4439,
    "docstring": "",
    "keywords": [
      "analysis",
      "youtube",
      "web_tools"
    ],
    "functions": [
      "convert_image_to_jpeg",
      "get_presigned_url",
      "upload_image",
      "upscale_image",
      "get_upscaled_image",
      "process_images"
    ],
    "classes": [],
    "imports": [
      "json",
      "os",
      "sys",
      "time",
      "requests",
      "PIL"
    ],
    "preview": "import json\nimport os\nimport sys\nimport time\n\nimport requests\nfrom PIL import Image\n\napi_key = \"de7c9cb8-022f-42f8-8bf7-a8f9caadfaee\"\nauthorization = f\"Bearer {api_key}\"\n\nheaders = {\n    \"accept\": \"application/json\",\n    \"content-type\": \"application/json\",\n    \"authorization\": authorization,\n}\n\n# Styles to apply\nstyles = [\"GENERAL\", \"CINEMATIC\", \"2D_ART_ILLUSTRATION\", \"PHOTOREALISTIC\"]\n",
    "last_modified": "2025-09-13T05:53:50.391821"
  },
  {
    "id": "633",
    "name": "downs.py",
    "path": "02_media_processing/image_tools/downs.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 44,
    "size": 1314,
    "docstring": "",
    "keywords": [
      "youtube",
      "web_tools"
    ],
    "functions": [
      "download_and_convert_to_png"
    ],
    "classes": [],
    "imports": [
      "csv",
      "os",
      "io",
      "requests",
      "PIL"
    ],
    "preview": "import csv\nimport os\nfrom io import BytesIO\n\nimport requests\nfrom PIL import Image\n\n# Paths for input CSV and output directory\ninput_csv_path = \"/Users/steven/Downloads/Dalle-Aug2024 - Sheet1.csv\"\noutput_dir = (\n    \"/Users/steven/Downloads/output_images/\"  # You can change the output directory if needed\n)\n\n# Ensure the output directory exists\nos.makedirs(output_dir, exist_ok=True)\n\n# Read URLs from the CSV\nurls = []\nwith open(input_csv_path, newline=\"\") as csvfile:\n    reader = csv.DictReader(csvfile)",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "634",
    "name": "AskReddit_loop.py",
    "path": "02_media_processing/image_tools/AskReddit_loop.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 132,
    "size": 4743,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "random_title_msg",
      "write_to_log",
      "check_video_in_db",
      "create_submission_video"
    ],
    "classes": [],
    "imports": [
      "json",
      "time",
      "praw",
      "clips",
      "tinydb",
      "yt_upload"
    ],
    "preview": "#!./venv/bin/python\nimport json\nimport time\n\nimport praw\nfrom clips import *\nfrom tinydb import Query, TinyDB\nfrom yt_upload import upload_video\n\nwhile True:\n    try:\n        db = TinyDB(\"log/db.json\")\n        created_vids_db = db.table(\"created_videos\")\n        uploaded_vids_db = db.table(\"uploaded_vids\")\n\n        FPS = 30\n        # DURATION: int = 25\n        BACKGROUND_TRACK_VOLUME = 0.12\n        DURATION: int = 60 * 4\n        # DURATION: int = 60 * 10",
    "last_modified": "2025-09-13T05:53:51.492439"
  },
  {
    "id": "635",
    "name": "ImageCreator.py",
    "path": "02_media_processing/image_tools/ImageCreator.py",
    "category": "02_media_processing",
    "type": "image_processing",
    "lines": 97,
    "size": 3353,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "create_image_for",
      "get_text_dimensions",
      "split_string"
    ],
    "classes": [
      "ImageCreator"
    ],
    "imports": [
      "os.path",
      "PIL"
    ],
    "preview": "# ImageCreator.py\n# Called after the Reddit Post content has been scraped\n#\n# Creates an image of the text for all posts, eg: title and replies\n#\n#\n\n\n# File holds some utility functions that may be called inside of the main\n\nimport os.path  # used to create image file path\n\nfrom PIL import Image, ImageDraw, ImageFont\n\n# Path to access images\nIMAGE_PATH = \"../images/\"\nFONT_PATH = \"../fonts/\"\n\n\nclass ImageCreator:",
    "last_modified": "2025-09-13T05:53:51.650509"
  },
  {
    "id": "636",
    "name": "gallery_upload.py",
    "path": "02_media_processing/image_tools/gallery_upload.py",
    "category": "02_media_processing",
    "type": "youtube",
    "lines": 99,
    "size": 3026,
    "docstring": "",
    "keywords": [],
    "functions": [
      "parse_args",
      "main"
    ],
    "classes": [],
    "imports": [
      "argparse",
      "os",
      "sys",
      "simplegallery.common",
      "simplegallery.upload.uploader_factory"
    ],
    "preview": "import argparse\nimport os\nimport sys\n\nimport simplegallery.common as spg_common\nfrom simplegallery.upload.uploader_factory import get_uploader\n\n\ndef parse_args():\n    \"\"\"\n    Configures the argument parser\n    :return: Parsed arguments\n    \"\"\"\n\n    description = \"\"\"Uploads the gallery to a supported hosting provider. Currently supported: AWS S3 and Netlify.\n                    For detailed documentation please refer to https://github.com/haltakov/simple-photo-gallery.\"\"\"\n\n    parser = argparse.ArgumentParser(description=description)\n\n    parser.add_argument(\"hosting\", metavar=\"HOST\", default=\"\", help=\"Hosting provider: aws \")",
    "last_modified": "2025-09-13T05:53:52.460040"
  },
  {
    "id": "637",
    "name": "converts copy.py",
    "path": "02_media_processing/image_tools/converts copy.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 62,
    "size": 2268,
    "docstring": "",
    "keywords": [],
    "functions": [
      "convert_and_upscale_images",
      "main"
    ],
    "classes": [],
    "imports": [
      "os",
      "PIL"
    ],
    "preview": "import os\n\nfrom PIL import Image\n\n\ndef convert_and_upscale_images(source_directory, destination_directory):\n    # Create the destination directory if it doesn't exist\n    os.makedirs(destination_directory, exist_ok=True)\n\n    for filename in os.listdir(source_directory):\n        if filename.endswith((\".tiff\", \".png\", \".jpg\", \".jpeg\")):\n            source_file = os.path.join(source_directory, filename)\n            filename_no_ext, file_ext = os.path.splitext(filename)\n            file_ext = file_ext.lower()\n\n            if file_ext == \".tiff\":\n                destination_file = os.path.join(destination_directory, f\"{filename_no_ext}.png\")\n            elif file_ext in [\".png\", \".jpg\", \".jpeg\"]:\n                destination_file = os.path.join(destination_directory, filename)\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "638",
    "name": "organize (1).py",
    "path": "02_media_processing/image_tools/organize (1).py",
    "category": "02_media_processing",
    "type": "organization",
    "lines": 29,
    "size": 1052,
    "docstring": "",
    "keywords": [],
    "functions": [
      "copy_files_with_structure"
    ],
    "classes": [],
    "imports": [
      "csv",
      "os",
      "shutil"
    ],
    "preview": "import csv\nimport os\nimport shutil\n\n\ndef copy_files_with_structure(csv_file_path, destination_base_path):\n    with open(csv_file_path, \"r\") as csv_file:\n        reader = csv.DictReader(csv_file)\n        for row in reader:\n            original_path = row[\"Original Path\"]\n            destination_path = os.path.join(destination_base_path, original_path.lstrip(os.sep))\n\n            # Create the destination directory if it doesn't exist\n            destination_dir = os.path.dirname(destination_path)\n            if not os.path.exists(destination_dir):\n                os.makedirs(destination_dir)\n\n            # Copy the file to the destination\n            shutil.copy2(original_path, destination_path)\n            print(f\"Copied {original_path} to {destination_path}\")",
    "last_modified": "2025-09-13T05:55:10.806709"
  },
  {
    "id": "639",
    "name": "console.py",
    "path": "02_media_processing/image_tools/console.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 127,
    "size": 4063,
    "docstring": "",
    "keywords": [],
    "functions": [
      "print_markdown",
      "print_step",
      "print_table",
      "print_substep",
      "handle_input"
    ],
    "classes": [],
    "imports": [
      "re",
      "rich.columns",
      "rich.console",
      "rich.markdown",
      "rich.padding",
      "rich.panel",
      "rich.text"
    ],
    "preview": "import re\n\nfrom rich.columns import Columns\nfrom rich.console import Console\nfrom rich.markdown import Markdown\nfrom rich.padding import Padding\nfrom rich.panel import Panel\nfrom rich.text import Text\n\nconsole = Console()\n\n\ndef print_markdown(text) -> None:\n    \"\"\"Prints a rich info message. Support Markdown syntax.\"\"\"\n\n    md = Padding(Markdown(text), 2)\n    console.print(md)\n\n\ndef print_step(text) -> None:",
    "last_modified": "2025-09-13T05:54:00.117665"
  },
  {
    "id": "640",
    "name": "googlesets.py",
    "path": "02_media_processing/image_tools/googlesets.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 96,
    "size": 2569,
    "docstring": "",
    "keywords": [],
    "functions": [
      "__init__",
      "__str__",
      "html",
      "__init__",
      "get_results",
      "_maybe_raise",
      "_get_results_page",
      "_extract_results"
    ],
    "classes": [
      "GSError(Exception)",
      "GSParseError(Exception)",
      "GoogleSets(object)"
    ],
    "imports": [
      "random",
      "re",
      "urllib",
      "BeautifulSoup",
      "browser",
      "htmlentitydefs"
    ],
    "preview": "#!/usr/bin/python\n#\n# Peteris Krumins (peter@catonmat.net)\n# http://www.catonmat.net  --  good coders code, great reuse\n#\n# http://www.catonmat.net/blog/python-library-for-google-sets/\n#\n# Code is licensed under MIT license.\n#\n\nimport random\nimport re\nimport urllib\n\nfrom BeautifulSoup import BeautifulSoup\nfrom browser import Browser, BrowserError\nfrom htmlentitydefs import name2codepoint\n\n\nclass GSError(Exception):",
    "last_modified": "2025-05-04T23:28:20.724078"
  },
  {
    "id": "641",
    "name": "upscale copy 2.py",
    "path": "02_media_processing/image_tools/upscale copy 2.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 85,
    "size": 2874,
    "docstring": "",
    "keywords": [],
    "functions": [
      "upscale_and_save_image",
      "compress_image_to_size"
    ],
    "classes": [],
    "imports": [
      "os",
      "PIL"
    ],
    "preview": "import os\n\nfrom PIL import Image\n\n# Input directory (no output directory needed since we're replacing the original images)\ninput_dir = \"/Users/steven/Pictures/etsy/Snowman_Action_Scenes\"\n\n# Max file size in bytes (9 MB = 9 * 1024 * 1024)\nmax_size = 9 * 1024 * 1024\n\n\ndef upscale_and_save_image(image_path, max_size=max_size, dpi=300):\n    \"\"\"\n    Upscale the image by 2x, set the DPI, and compress to ensure the file size is <= 9MB.\n    Args:\n        image_path (str): Path to the input image.\n        max_size (int): Maximum file size in bytes.\n        dpi (int): Target DPI (default is 300).\n    \"\"\"\n    # Open the image using PIL",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "642",
    "name": "webpConvert.py",
    "path": "02_media_processing/image_tools/webpConvert.py",
    "category": "02_media_processing",
    "type": "web_tools",
    "lines": 50,
    "size": 1736,
    "docstring": "",
    "keywords": [],
    "functions": [
      "convert_and_upscale_images",
      "main"
    ],
    "classes": [],
    "imports": [
      "os",
      "PIL"
    ],
    "preview": "import os\n\nfrom PIL import Image\n\n\n# Function to convert WebP images to PNG and upscale by 200% with 300 DPI\ndef convert_and_upscale_images(source_directory, destination_directory):\n    # Create the destination directory if it doesn't exist\n    os.makedirs(destination_directory, exist_ok=True)\n\n    for filename in os.listdir(source_directory):\n        if filename.endswith(\".webp\"):\n            source_file = os.path.join(source_directory, filename)\n            filename_no_ext = os.path.splitext(filename)[0]\n            destination_file = os.path.join(destination_directory, f\"{filename_no_ext}.jpg\")\n\n            # Convert WebP to PNG and upscale by 200% with 300 DPI\n            im = Image.open(source_file)\n            width, height = im.size\n            upscale_width = width * 2",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "643",
    "name": "midj-.py",
    "path": "02_media_processing/image_tools/midj-.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 28,
    "size": 650,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "json",
      "re"
    ],
    "preview": "import json\nimport re\n\n# Load the JSONL file.\nwith open(\n    \"/Users/steven/Pictures/midjourneyDownload_2023-10-13_1697181545353/metadata.jsonl\",\n    \"r\",\n) as f:\n    jsonl_data = json.load(f)\n\n# Compile the regular expression.\nregex = re.compile(r\"https:\\/\\/([\\w-]+.){1,}\\.(png)\")\n\n# Find all URLs in the JSONL file that match the regular expression.\nmatching_urls = []\nfor jsonl_object in jsonl_data:\n    url = jsonl_object.get(\"url\")\n    if regex.match(url):\n        matching_urls.append(url)\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "644",
    "name": "repost_best_photos_from_users.py",
    "path": "02_media_processing/image_tools/repost_best_photos_from_users.py",
    "category": "02_media_processing",
    "type": "analysis",
    "lines": 114,
    "size": 3637,
    "docstring": "instabot example\nWorkflow:\nRepost best photos from users to your account\nBy default bot checks username_database.txt\nThe file should contain one username per line!",
    "keywords": [
      "organization"
    ],
    "functions": [
      "repost_best_photos",
      "sort_best_medias",
      "get_not_used_medias_from_users",
      "exists_in_posted_medias",
      "update_posted_medias",
      "repost_photo"
    ],
    "classes": [],
    "imports": [
      "argparse",
      "os",
      "random",
      "sys",
      "tqdm",
      "instabot"
    ],
    "preview": "\"\"\"\ninstabot example\nWorkflow:\nRepost best photos from users to your account\nBy default bot checks username_database.txt\nThe file should contain one username per line!\n\"\"\"\n\nimport argparse\nimport os\nimport random\nimport sys\n\nfrom tqdm import tqdm\n\nsys.path.append(os.path.join(sys.path[0], \"../\"))\nfrom instabot import Bot, utils  # noqa: E402\n\nUSERNAME_DATABASE = \"username_database.txt\"\nPOSTED_MEDIAS = \"posted_medias.txt\"",
    "last_modified": "2025-09-13T05:54:55.756914"
  },
  {
    "id": "645",
    "name": "50.py",
    "path": "02_media_processing/image_tools/50.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 38,
    "size": 1418,
    "docstring": "",
    "keywords": [],
    "functions": [
      "split_csv",
      "write_chunk"
    ],
    "classes": [],
    "imports": [
      "csv"
    ],
    "preview": "import csv\n\n\ndef split_csv(input_file_path, output_file_prefix, chunk_size=50):\n    with open(input_file_path, \"r\", encoding=\"utf-8\") as input_file:\n        reader = csv.reader(input_file)\n        header = next(reader)  # Capture the header row\n\n        file_index = 0\n        current_chunk = []\n\n        for row in reader:\n            current_chunk.append(row)\n            if len(current_chunk) == chunk_size:\n                start_index = file_index * chunk_size + 1\n                end_index = start_index + chunk_size - 1\n                write_chunk(output_file_prefix, start_index, end_index, header, current_chunk)\n                file_index += 1\n                current_chunk = []\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "646",
    "name": "imove.py",
    "path": "02_media_processing/image_tools/imove.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 28,
    "size": 1079,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "csv",
      "os",
      "shutil"
    ],
    "preview": "import csv\nimport os\nimport shutil\n\n# Define the path to the CSV file\ncsv_file_path = \"/Users/steven/Sort/image_paths.csv\"\n# Define the destination directory based on your specification\ndestination_dir = \"/Volumes/Pics/ogPro\"\n\n# Ensure the destination directory exists\nos.makedirs(destination_dir, exist_ok=True)\n\n# Open the CSV file and read its contents\nwith open(csv_file_path, newline=\"\") as csvfile:\n    reader = csv.reader(csvfile)\n    next(reader, None)  # Skip the header row if your CSV has one\n    for row in reader:\n        # Assuming the file paths are in the first column\n        file_path = row[0].strip()  # Strip to remove any leading/trailing whitespace\n        # Define the destination path for the file",
    "last_modified": "2025-05-04T22:47:11.912806"
  },
  {
    "id": "647",
    "name": "beatsaber.py",
    "path": "02_media_processing/image_tools/beatsaber.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 107,
    "size": 3495,
    "docstring": "",
    "keywords": [],
    "functions": [
      "taggify",
      "construct",
      "confirm",
      "confirm_thumbnail"
    ],
    "classes": [
      "BeatSaberPreset"
    ],
    "imports": [
      "re",
      "subprocess",
      "typing",
      "InquirerPy",
      "thumbnail",
      "utils",
      "preset"
    ],
    "preview": "import re\nimport subprocess\nfrom typing import override\n\nfrom InquirerPy import inquirer\n\nfrom ..thumbnail import create_thumbnails\nfrom ..utils import get_local_path\nfrom .preset import Preset\n\nDESCRIPTION_TEMPLATE = \"\"\"\\\n\ud83c\udfb5 __TITLE__\n\nDecided to post some VR content for fun. \n\nModding/Recording Guides:\nhttps://www.youtube.com/watch?v=DGRi8A0p1cY\nhttps://www.youtube.com/watch?v=bFK3XH_K9Jg\\\n\"\"\"\n",
    "last_modified": "2025-05-04T22:47:12"
  },
  {
    "id": "648",
    "name": "rename_file.py",
    "path": "02_media_processing/image_tools/rename_file.py",
    "category": "02_media_processing",
    "type": "organization",
    "lines": 138,
    "size": 5177,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "logging",
      "os",
      "time",
      "pyrogram",
      "translation",
      "hachoir.metadata",
      "hachoir.parser",
      "helper_funcs.chat_base",
      "helper_funcs.display_progress",
      "PIL"
    ],
    "preview": "import logging\n\nlogging.basicConfig(\n    level=logging.DEBUG, format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\n)\nlogger = logging.getLogger(__name__)\n\nimport os\nimport time\n\n# the secret configuration specific things\nif bool(os.environ.get(\"WEBHOOK\", False)):\n    from sample_config import Config\nelse:\n    from config import Config\n\n# the Strings used for this \"thing\"\nimport pyrogram\nfrom translation import Translation\n",
    "last_modified": "2025-09-13T05:53:44.294462"
  },
  {
    "id": "649",
    "name": "imgupscale 2.py",
    "path": "02_media_processing/image_tools/imgupscale 2.py",
    "category": "02_media_processing",
    "type": "image_processing",
    "lines": 69,
    "size": 2829,
    "docstring": "",
    "keywords": [],
    "functions": [
      "convert_and_upscale_images",
      "main"
    ],
    "classes": [],
    "imports": [
      "os",
      "PIL"
    ],
    "preview": "import os\n\nfrom PIL import Image, UnidentifiedImageError\n\n\n# Function to convert and upscale PNG and JPEG images by 200% with 300 DPI\ndef convert_and_upscale_images(source_directory, destination_directory, max_size_mb=8):\n    # Create the destination directory if it doesn't exist\n    os.makedirs(destination_directory, exist_ok=True)\n\n    for filename in os.listdir(source_directory):\n        if filename.lower().endswith((\".png\", \".jpeg\", \".jpg\")):\n            source_file = os.path.join(source_directory, filename)\n            filename_no_ext = os.path.splitext(filename)[0]\n            ext = filename.split(\".\")[-1].lower()\n            destination_file = os.path.join(destination_directory, f\"{filename_no_ext}.{ext}\")\n\n            try:\n                # Convert and upscale PNG or JPEG\n                with Image.open(source_file) as im:",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "650",
    "name": "organize_albums 12.py",
    "path": "02_media_processing/image_tools/organize_albums 12.py",
    "category": "02_media_processing",
    "type": "organization",
    "lines": 62,
    "size": 2470,
    "docstring": "",
    "keywords": [
      "organization"
    ],
    "functions": [
      "organize_files"
    ],
    "classes": [],
    "imports": [
      "os",
      "shutil"
    ],
    "preview": "import os\nimport shutil\n\n# Define the base directory\nbase_dir = \"/Users/steven/Music/NocTurnE-meLoDieS/albums/\"\n\n\n# Helper function to create folders and move files\ndef organize_files():\n    # List all files in the base directory\n    files = os.listdir(base_dir)\n\n    # Process each file\n    for file in files:\n        # Skip directories\n        if os.path.isdir(os.path.join(base_dir, file)):\n            continue\n\n        # Extract the base name (album name) from the file\n        if file.endswith(\".mp3\"):",
    "last_modified": "2025-08-02T18:25:58.473285"
  },
  {
    "id": "651",
    "name": "scanner.py",
    "path": "02_media_processing/image_tools/scanner.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 103,
    "size": 3004,
    "docstring": "pygments.scanner\n~~~~~~~~~~~~~~~~\n\nThis library implements a regex based scanner. Some languages\nlike Pascal are easy to parse but have some keywords that\ndepend on the context. Because of this it's impossible to lex\nthat just by using a regular expression lexer like the\n`RegexLexer`.\n\nHave a look at the `DelphiLexer` to get an idea of how to use\nthis scanner.\n\n:copyright: Copyright 2006-2023 by the Pygments team, see AUTHORS.\n:license: BSD, see LICENSE for details.",
    "keywords": [
      "testing"
    ],
    "functions": [
      "__init__",
      "eos",
      "check",
      "test",
      "scan",
      "get_char",
      "__repr__"
    ],
    "classes": [
      "EndOfText",
      "Scanner"
    ],
    "imports": [
      "re"
    ],
    "preview": "\"\"\"\npygments.scanner\n~~~~~~~~~~~~~~~~\n\nThis library implements a regex based scanner. Some languages\nlike Pascal are easy to parse but have some keywords that\ndepend on the context. Because of this it's impossible to lex\nthat just by using a regular expression lexer like the\n`RegexLexer`.\n\nHave a look at the `DelphiLexer` to get an idea of how to use\nthis scanner.\n\n:copyright: Copyright 2006-2023 by the Pygments team, see AUTHORS.\n:license: BSD, see LICENSE for details.\n\"\"\"\n\nimport re\n\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "652",
    "name": "bot_filter.py",
    "path": "02_media_processing/image_tools/bot_filter.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 280,
    "size": 9465,
    "docstring": "Filter functions for media and user lists.",
    "keywords": [],
    "functions": [
      "filter_medias",
      "_filter_medias_not_liked",
      "_filter_medias_not_commented",
      "_filter_medias_nlikes",
      "_get_media_ids",
      "check_media",
      "search_stop_words_in_user",
      "search_blacklist_hashtags_in_media",
      "check_user",
      "check_not_bot"
    ],
    "classes": [],
    "imports": [],
    "preview": "\"\"\"\nFilter functions for media and user lists.\n\"\"\"\n\n\ndef filter_medias(self, media_items, filtration=True, quiet=False, is_comment=False):\n    if filtration:\n        if not quiet:\n            self.logger.info(\"Received {} medias.\".format(len(media_items)))\n        if not is_comment:\n            media_items = _filter_medias_not_liked(media_items)\n            if self.max_likes_to_like:\n                media_items = _filter_medias_nlikes(\n                    media_items, self.max_likes_to_like, self.min_likes_to_like\n                )\n        else:\n            media_items = _filter_medias_not_commented(self, media_items)\n        if not quiet:\n            msg = \"After filtration {} medias left.\"\n            self.logger.info(msg.format(len(media_items)))",
    "last_modified": "2025-09-13T05:54:57.472833"
  },
  {
    "id": "653",
    "name": "imgg-test 1.py",
    "path": "02_media_processing/image_tools/imgg-test 1.py",
    "category": "02_media_processing",
    "type": "image_processing",
    "lines": 18,
    "size": 436,
    "docstring": "",
    "keywords": [
      "web_tools"
    ],
    "functions": [],
    "classes": [],
    "imports": [
      "os",
      "requests",
      "dotenv"
    ],
    "preview": "import os\n\nimport requests\nfrom dotenv import load_dotenv\n\n# Load .env file\nload_dotenv(\"~/.env\")\npat = os.getenv(\"CLARIFAI_PAT\")\n\nauth = (os.getenv(\"IMAGGA_API_KEY\"), os.getenv(\"IMAGGA_API_SECRET\"))\nurl = \"\"\n\nresponse = requests.get(f\"https://api.imagga.com/v2/tags?image_url={url}\", auth=auth)\n\nprint(\"\ud83d\udd16 Imagga Tags:\")\nfor tag in response.json()[\"result\"][\"tags\"][:10]:\n    print(f\"- {tag['tag']['en']} ({tag['confidence']:.2f})\")\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "654",
    "name": "organize_albums 9.py",
    "path": "02_media_processing/image_tools/organize_albums 9.py",
    "category": "02_media_processing",
    "type": "organization",
    "lines": 62,
    "size": 2466,
    "docstring": "",
    "keywords": [
      "organization"
    ],
    "functions": [
      "organize_files"
    ],
    "classes": [],
    "imports": [
      "os",
      "shutil"
    ],
    "preview": "import os\nimport shutil\n\n# Define the base directory\nbase_dir = \"/Users/steven/Music/NocTurnE-meLoDieS/mp3\"\n\n\n# Helper function to create folders and move files\ndef organize_files():\n    # List all files in the base directory\n    files = os.listdir(base_dir)\n\n    # Process each file\n    for file in files:\n        # Skip directories\n        if os.path.isdir(os.path.join(base_dir, file)):\n            continue\n\n        # Extract the base name (album name) from the file\n        if file.endswith(\".mp3\"):",
    "last_modified": "2025-05-31T00:15:07.991135"
  },
  {
    "id": "655",
    "name": "society6-upload.py",
    "path": "02_media_processing/image_tools/society6-upload.py",
    "category": "02_media_processing",
    "type": "youtube",
    "lines": 39,
    "size": 1317,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "time",
      "pyautogui",
      "selenium"
    ],
    "preview": "import time\n\nimport pyautogui\nfrom selenium import webdriver\n\n# Initialize WebDriver\ndriver = webdriver.Chrome()\ndriver.get(\"https://www.society6.com/login\")\n\n# Log in to Society6\n# Note: Replace 'your_username' and 'your_password' with your actual login credentials\ndriver.find_element_by_id(\"sjchaplinski@gmail.com\").send_keys(\"sjchaplinski@gmail.com\")\ndriver.find_element_by_id(\"Zhil*0IPLma#\").send_keys(\"Zhil*0IPLma#\")\ndriver.find_element_by_id(\"login-button\").click()\ntime.sleep(5)  # Wait for the login process to complete\n\n# Navigate to the upload page\n# Note: Replace this URL with the actual URL of Society6's upload page\ndriver.get(\"https://www.society6.com/upload\")\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "656",
    "name": "imove--.py",
    "path": "02_media_processing/image_tools/imove--.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 28,
    "size": 1092,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "csv",
      "os",
      "shutil"
    ],
    "preview": "import csv\nimport os\nimport shutil\n\n# Define the path to the CSV file\ncsv_file_path = \"/Users/steven/Downloads/Misc/ogPro - Sheet1.csv\"\n# Define the destination directory based on your specification\ndestination_dir = \"/Volumes/iMac/ogImg\"\n\n# Ensure the destination directory exists\nos.makedirs(destination_dir, exist_ok=True)\n\n# Open the CSV file and read its contents\nwith open(csv_file_path, newline=\"\") as csvfile:\n    reader = csv.reader(csvfile)\n    next(reader, None)  # Skip the header row if your CSV has one\n    for row in reader:\n        # Assuming the file paths are in the first column\n        file_path = row[0].strip()  # Strip to remove any leading/trailing whitespace\n        # Define the destination path for the file",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "657",
    "name": "friends_last_post_likes_and_interact_with_user_based_on_hashtahs.py",
    "path": "02_media_processing/image_tools/friends_last_post_likes_and_interact_with_user_based_on_hashtahs.py",
    "category": "02_media_processing",
    "type": "analysis",
    "lines": 236,
    "size": 6550,
    "docstring": "Based in @jeremycjang and @boldestfortune\nThis config is meant to run with docker-compose inside a folder call z_{user}\n(Added to gitignore)\nFolder content:\n  - data.yaml\n  - docker-compose.yaml\n  - start.py (Containing this script)\n\nContent files examples (comments between parenthesis)\n\n::data.yaml::\nusername: user              # (instagram user)\npassword: password          # (instagram password)\nfriends_interaction: True   # (if True will like friendlist posts,\nFalse will avoid create friends session)\ndo_comments: True           # (if True will comment on user interaction)\ndo_follow: True             # (if True will follow on user interaction)\nuser_interact: True         # (if True will interact with user posts)\ndo_unfollow: True           # (if True will execution unfollow)\nfriendlist: ['friend1', 'friend2', 'friend3', 'friend4']\nhashtags: ['interest1', 'interest2', 'interest3', 'interest4']\n\n\n::docker-compose.yaml::\nversion: '3'\nservices:\n  web:\n    command: [\"./wait-for-selenium.sh\", \"http://selenium:4444/wd/hub\", \"--\",\n    \"python\", \"start.py\"]\n    environment:\n      - PYTHONUNBUFFERED=0\n    build:\n      context: ../\n      dockerfile: docker_conf/python/Dockerfile\n    depends_on:\n      - selenium\n    volumes:\n      - ./start.py:/code/start.py\n      - ./data.yaml:/code/data.yaml\n      - ./logs:/code/logs\n  selenium:\n    image: selenium/standalone-chrome\n    shm_size: 128M\n\n::HOW TO RUN::\nInside z_{user} directory:\n  run in background:\n    docker-compose down && docker-compose up -d --build\n  run with log in terminal:\n    docker-compose down && docker-compose up -d --build && docker-compose\n    logs -f",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "os",
      "random",
      "yaml",
      "instapy"
    ],
    "preview": "\"\"\"\nBased in @jeremycjang and @boldestfortune\nThis config is meant to run with docker-compose inside a folder call z_{user}\n(Added to gitignore)\nFolder content:\n  - data.yaml\n  - docker-compose.yaml\n  - start.py (Containing this script)\n\nContent files examples (comments between parenthesis)\n\n::data.yaml::\nusername: user              # (instagram user)\npassword: password          # (instagram password)\nfriends_interaction: True   # (if True will like friendlist posts,\nFalse will avoid create friends session)\ndo_comments: True           # (if True will comment on user interaction)\ndo_follow: True             # (if True will follow on user interaction)\nuser_interact: True         # (if True will interact with user posts)\ndo_unfollow: True           # (if True will execution unfollow)",
    "last_modified": "2025-03-28T18:36:56"
  },
  {
    "id": "658",
    "name": "169-11.py",
    "path": "02_media_processing/image_tools/169-11.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 45,
    "size": 1302,
    "docstring": "",
    "keywords": [],
    "functions": [
      "upscale_and_replace_images",
      "main"
    ],
    "classes": [],
    "imports": [
      "os",
      "PIL"
    ],
    "preview": "import os\n\nfrom PIL import Image\n\n\ndef upscale_and_replace_images(directory):\n    for filename in os.listdir(directory):\n        if filename.endswith(\".png\"):\n            # Define the full path to the image\n            file_path = os.path.join(directory, filename)\n\n            # Load the image\n            with Image.open(file_path) as im:\n                width, height = im.size\n\n                # Upscale by 2x\n                upscale_width = width * 2\n                upscale_height = height * 2\n\n                # Resize the image",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "659",
    "name": "youtube_dl_button.py",
    "path": "02_media_processing/image_tools/youtube_dl_button.py",
    "category": "02_media_processing",
    "type": "youtube",
    "lines": 386,
    "size": 15841,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "logging",
      "asyncio",
      "json",
      "math",
      "os",
      "shutil",
      "time",
      "datetime",
      "pyrogram",
      "translation"
    ],
    "preview": "import logging\n\nlogging.basicConfig(\n    level=logging.DEBUG, format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\n)\nlogger = logging.getLogger(__name__)\n\nimport asyncio\nimport json\nimport math\nimport os\nimport shutil\nimport time\nfrom datetime import datetime\n\n# the secret configuration specific things\nif bool(os.environ.get(\"WEBHOOK\", False)):\n    from sample_config import Config\nelse:\n    from config import Config",
    "last_modified": "2025-09-13T05:53:44.585822"
  },
  {
    "id": "660",
    "name": "leonardo_script 2.py",
    "path": "02_media_processing/image_tools/leonardo_script 2.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 129,
    "size": 4447,
    "docstring": "",
    "keywords": [
      "analysis",
      "youtube",
      "web_tools"
    ],
    "functions": [
      "convert_image_to_jpeg",
      "get_presigned_url",
      "upload_image",
      "upscale_image",
      "get_upscaled_image",
      "process_images"
    ],
    "classes": [],
    "imports": [
      "json",
      "os",
      "sys",
      "time",
      "requests",
      "PIL"
    ],
    "preview": "import json\nimport os\nimport sys\nimport time\n\nimport requests\nfrom PIL import Image\n\napi_key = \"de7c9cb8-022f-42f8-8bf7-a8f9caadfaee\"\nauthorization = f\"Bearer {api_key}\"\n\nheaders = {\n    \"accept\": \"application/json\",\n    \"content-type\": \"application/json\",\n    \"authorization\": authorization,\n}\n\n# Styles to apply\nstyles = [\"GENERAL\", \"CINEMATIC\", \"2D ART & ILLUSTRATION\", \"CG ART & GAME ASSETS\"]\n",
    "last_modified": "2025-09-13T05:53:50.331753"
  },
  {
    "id": "661",
    "name": "midj--.py",
    "path": "02_media_processing/image_tools/midj--.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 28,
    "size": 650,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "json",
      "re"
    ],
    "preview": "import json\nimport re\n\n# Load the JSONL file.\nwith open(\n    \"/Users/steven/Pictures/midjourneyDownload_2023-10-13_1697181545353/metadata.jsonl\",\n    \"r\",\n) as f:\n    jsonl_data = json.load(f)\n\n# Compile the regular expression.\nregex = re.compile(r\"https:\\/\\/([\\w-]+.){1,}\\.(png)\")\n\n# Find all URLs in the JSONL file that match the regular expression.\nmatching_urls = []\nfor jsonl_object in jsonl_data:\n    url = jsonl_object.get(\"url\")\n    if regex.match(url):\n        matching_urls.append(url)\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "662",
    "name": "orn-up.py",
    "path": "02_media_processing/image_tools/orn-up.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 69,
    "size": 2253,
    "docstring": "",
    "keywords": [],
    "functions": [
      "upscale_and_compress_image",
      "compress_image_to_size"
    ],
    "classes": [],
    "imports": [
      "os",
      "PIL"
    ],
    "preview": "import os\n\nfrom PIL import Image\n\n# Input and output directories\ninput_dir = \"/Users/steven/Pictures/ornament/processed_output\"\noutput_dir = \"/Users/steven/Pictures/ornament/output\"\n\n# Max file size in bytes (9MB = 9 * 1024 * 1024)\nmax_size = 9 * 1024 * 1024\n\n# Target DPI\ntarget_dpi = 300\n\n# Create the output directory if it doesn't exist\nif not os.path.exists(output_dir):\n    os.makedirs(output_dir)\n\n\ndef upscale_and_compress_image(input_path, output_path, max_size, target_dpi):",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "663",
    "name": "convertupscale--.py",
    "path": "02_media_processing/image_tools/convertupscale--.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 50,
    "size": 1736,
    "docstring": "",
    "keywords": [],
    "functions": [
      "convert_and_upscale_images",
      "main"
    ],
    "classes": [],
    "imports": [
      "os",
      "PIL"
    ],
    "preview": "import os\n\nfrom PIL import Image\n\n\n# Function to convert WebP images to PNG and upscale by 200% with 300 DPI\ndef convert_and_upscale_images(source_directory, destination_directory):\n    # Create the destination directory if it doesn't exist\n    os.makedirs(destination_directory, exist_ok=True)\n\n    for filename in os.listdir(source_directory):\n        if filename.endswith(\".webp\"):\n            source_file = os.path.join(source_directory, filename)\n            filename_no_ext = os.path.splitext(filename)[0]\n            destination_file = os.path.join(destination_directory, f\"{filename_no_ext}.png\")\n\n            # Convert WebP to PNG and upscale by 200% with 300 DPI\n            im = Image.open(source_file)\n            width, height = im.size\n            upscale_width = width * 2",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "664",
    "name": "loop-upscale copy.py",
    "path": "02_media_processing/image_tools/loop-upscale copy.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 58,
    "size": 2241,
    "docstring": "",
    "keywords": [
      "web_tools"
    ],
    "functions": [],
    "classes": [],
    "imports": [
      "json",
      "os",
      "time",
      "requests"
    ],
    "preview": "import json\nimport os\nimport time\n\nimport requests\n\napi_key = \"3ca7a396-64c0-4e22-8cad-b00bc972d4b3\"\nauthorization = \"Bearer %s\" % api_key\n\nheaders = {\n    \"accept\": \"application/json\",\n    \"content-type\": \"application/json\",\n    \"authorization\": authorization,\n}\n\n# Directory containing images\ndirectory_path = \"/Users/steven/Pictures/CookiMonster/1\"\n\n# Loop through each file in the directory\nfor filename in os.listdir(directory_path):",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "665",
    "name": "scancsv.py",
    "path": "02_media_processing/image_tools/scancsv.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 36,
    "size": 930,
    "docstring": "",
    "keywords": [],
    "functions": [
      "scan_directory"
    ],
    "classes": [],
    "imports": [
      "csv",
      "os"
    ],
    "preview": "import csv\nimport os\n\n# List of drives to scan\ndrives = [\n    \"/Users/steven/Documents\",\n    \"/Users/steven/Music\",\n    \"/Users/steven/Movies/mine\",\n    \"/Users/steven/Pictures\",\n]\n\n# File types to look for\nfile_types = (\".jpg\", \".jpeg\", \".png\", \".gif\", \"webp\")\n\n\n# Function to scan a directory for image files\ndef scan_directory(directory):\n    for root, _, files in os.walk(directory):\n        for file in files:\n            if file.lower().endswith(file_types):",
    "last_modified": "2025-05-04T22:47:11.913987"
  },
  {
    "id": "666",
    "name": "img-img-upscale copy.py",
    "path": "02_media_processing/image_tools/img-img-upscale copy.py",
    "category": "02_media_processing",
    "type": "image_processing",
    "lines": 36,
    "size": 1608,
    "docstring": "",
    "keywords": [],
    "functions": [
      "convert_and_upscale_images"
    ],
    "classes": [],
    "imports": [
      "os",
      "PIL"
    ],
    "preview": "import os\n\nfrom PIL import Image\n\n\ndef convert_and_upscale_images(directory):\n    # Walk through all directories and files within the specified directory\n    for root, _, files in os.walk(directory):\n        for filename in files:\n            if filename.lower().endswith(\".jpg\") or filename.lower().endswith(\".jpg\"):\n                # Construct full file path\n                file_path = os.path.join(root, filename)\n                # Open the .jpg image\n                with Image.open(file_path) as img:\n                    # Upscale the image by 2x\n                    img = img.resize((img.width * 2, img.height * 2), Image.LANCZOS)\n                    # Set DPI to 300\n                    img.info[\"dpi\"] = (300, 300)\n                    # Convert the image mode to RGB (if not already in that mode)\n                    if img.mode != \"RGB\":",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "667",
    "name": "encoding.py",
    "path": "02_media_processing/image_tools/encoding.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 37,
    "size": 1169,
    "docstring": "",
    "keywords": [],
    "functions": [
      "auto_decode"
    ],
    "classes": [],
    "imports": [
      "codecs",
      "locale",
      "re",
      "sys",
      "typing"
    ],
    "preview": "import codecs\nimport locale\nimport re\nimport sys\nfrom typing import List, Tuple\n\nBOMS: List[Tuple[bytes, str]] = [\n    (codecs.BOM_UTF8, \"utf-8\"),\n    (codecs.BOM_UTF16, \"utf-16\"),\n    (codecs.BOM_UTF16_BE, \"utf-16-be\"),\n    (codecs.BOM_UTF16_LE, \"utf-16-le\"),\n    (codecs.BOM_UTF32, \"utf-32\"),\n    (codecs.BOM_UTF32_BE, \"utf-32-be\"),\n    (codecs.BOM_UTF32_LE, \"utf-32-le\"),\n]\n\nENCODING_RE = re.compile(rb\"coding[:=]\\s*([-\\w.]+)\")\n\n\ndef auto_decode(data: bytes) -> str:",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "668",
    "name": "organize_albums 13.py",
    "path": "02_media_processing/image_tools/organize_albums 13.py",
    "category": "02_media_processing",
    "type": "organization",
    "lines": 62,
    "size": 2470,
    "docstring": "",
    "keywords": [
      "organization"
    ],
    "functions": [
      "organize_files"
    ],
    "classes": [],
    "imports": [
      "os",
      "shutil"
    ],
    "preview": "import os\nimport shutil\n\n# Define the base directory\nbase_dir = \"/Users/steven/Music/NocTurnE-meLoDieS/albums/\"\n\n\n# Helper function to create folders and move files\ndef organize_files():\n    # List all files in the base directory\n    files = os.listdir(base_dir)\n\n    # Process each file\n    for file in files:\n        # Skip directories\n        if os.path.isdir(os.path.join(base_dir, file)):\n            continue\n\n        # Extract the base name (album name) from the file\n        if file.endswith(\".mp3\"):",
    "last_modified": "2025-05-04T22:47:11"
  },
  {
    "id": "669",
    "name": "convertupscale.py",
    "path": "02_media_processing/image_tools/convertupscale.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 50,
    "size": 1736,
    "docstring": "",
    "keywords": [],
    "functions": [
      "convert_and_upscale_images",
      "main"
    ],
    "classes": [],
    "imports": [
      "os",
      "PIL"
    ],
    "preview": "import os\n\nfrom PIL import Image\n\n\n# Function to convert WebP images to PNG and upscale by 200% with 300 DPI\ndef convert_and_upscale_images(source_directory, destination_directory):\n    # Create the destination directory if it doesn't exist\n    os.makedirs(destination_directory, exist_ok=True)\n\n    for filename in os.listdir(source_directory):\n        if filename.endswith(\".webp\"):\n            source_file = os.path.join(source_directory, filename)\n            filename_no_ext = os.path.splitext(filename)[0]\n            destination_file = os.path.join(destination_directory, f\"{filename_no_ext}.png\")\n\n            # Convert WebP to PNG and upscale by 200% with 300 DPI\n            im = Image.open(source_file)\n            width, height = im.size\n            upscale_width = width * 2",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "670",
    "name": "direct_url.py",
    "path": "02_media_processing/image_tools/direct_url.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 225,
    "size": 6736,
    "docstring": "PEP 610",
    "keywords": [],
    "functions": [
      "_get",
      "_get_required",
      "_exactly_one_of",
      "_filter_none",
      "__init__",
      "_from_dict",
      "_to_dict",
      "__init__",
      "hash",
      "hash"
    ],
    "classes": [
      "DirectUrlValidationError",
      "VcsInfo",
      "ArchiveInfo",
      "DirInfo",
      "DirectUrl"
    ],
    "imports": [
      "json",
      "re",
      "urllib.parse",
      "typing"
    ],
    "preview": "\"\"\"PEP 610\"\"\"\n\nimport json\nimport re\nimport urllib.parse\nfrom typing import Any, Dict, Iterable, Optional, Type, TypeVar, Union\n\n__all__ = [\n    \"DirectUrl\",\n    \"DirectUrlValidationError\",\n    \"DirInfo\",\n    \"ArchiveInfo\",\n    \"VcsInfo\",\n]\n\nT = TypeVar(\"T\")\n\nDIRECT_URL_METADATA_NAME = \"direct_url.json\"\nENV_VAR_RE = re.compile(r\"^\\$\\{[A-Za-z0-9-_]+\\}(:\\$\\{[A-Za-z0-9-_]+\\})?$\")\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "671",
    "name": "list.py",
    "path": "02_media_processing/image_tools/list.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 28,
    "size": 986,
    "docstring": "",
    "keywords": [],
    "functions": [
      "list_files_to_csv"
    ],
    "classes": [],
    "imports": [
      "csv",
      "os"
    ],
    "preview": "import csv\nimport os\n\n\ndef list_files_to_csv(startpath, output_file):\n    data = []\n    for root, dirs, files in os.walk(startpath):\n        if files:  # Ensure we write rows only for directories that contain files\n            row = [root]  # First column is the directory path\n            row.extend(os.path.join(root, f) for f in files)  # Append full paths of files\n            data.append(row)\n\n    # Find the maximum number of columns any row will have\n    max_cols = max(len(row) for row in data)\n\n    with open(output_file, \"w\", newline=\"\") as file:\n        writer = csv.writer(file)\n        writer.writerow(\n            [\"Folder\"] + [f\"IMG{i}\" for i in range(1, max_cols)]\n        )  # Create header row dynamically based on max_cols",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "672",
    "name": "readability.py",
    "path": "02_media_processing/image_tools/readability.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 34,
    "size": 718,
    "docstring": "",
    "keywords": [
      "web_tools"
    ],
    "functions": [
      "striphtml",
      "parse_url"
    ],
    "classes": [],
    "imports": [
      "re",
      "requests",
      "config"
    ],
    "preview": "import re\n\nimport requests\n\nfrom config import parse_config\n\nconfig = parse_config(\"local\")\n\nbase_url = \"https://www.readability.com/api/content/v1/parser\"\n\n\ndef striphtml(data):\n    p = re.compile(r\"<.*?>\")\n    return p.sub(\"\", data)\n\n\ndef parse_url(url):\n    url = base_url + \"?url=\" + url\n    params = {}\n    params[\"token\"] = config[\"readability_token\"]",
    "last_modified": "2025-09-13T05:53:51.210446"
  },
  {
    "id": "673",
    "name": "organize_albums 8.py",
    "path": "02_media_processing/image_tools/organize_albums 8.py",
    "category": "02_media_processing",
    "type": "organization",
    "lines": 62,
    "size": 2466,
    "docstring": "",
    "keywords": [
      "organization"
    ],
    "functions": [
      "organize_files"
    ],
    "classes": [],
    "imports": [
      "os",
      "shutil"
    ],
    "preview": "import os\nimport shutil\n\n# Define the base directory\nbase_dir = \"/Users/steven/Music/NocTurnE-meLoDieS/mp3\"\n\n\n# Helper function to create folders and move files\ndef organize_files():\n    # List all files in the base directory\n    files = os.listdir(base_dir)\n\n    # Process each file\n    for file in files:\n        # Skip directories\n        if os.path.isdir(os.path.join(base_dir, file)):\n            continue\n\n        # Extract the base name (album name) from the file\n        if file.endswith(\".mp3\"):",
    "last_modified": "2025-05-04T22:47:13.021502"
  },
  {
    "id": "674",
    "name": "get-pip.py",
    "path": "02_media_processing/image_tools/get-pip.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 33036,
    "size": 2635834,
    "docstring": "",
    "keywords": [],
    "functions": [
      "include_setuptools",
      "include_wheel",
      "determine_pip_install_arguments",
      "monkeypatch_for_cert",
      "bootstrap",
      "main",
      "cert_parse_args"
    ],
    "classes": [],
    "imports": [
      "argparse",
      "importlib",
      "os.path",
      "pkgutil",
      "shutil",
      "sys",
      "tempfile",
      "base64",
      "pip._internal.commands.install",
      "pip._internal.cli.main"
    ],
    "preview": "#!/usr/bin/env python\n#\n# Hi There!\n#\n# You may be wondering what this giant blob of binary data here is, you might\n# even be worried that we're up to something nefarious (good for you for being\n# paranoid!). This is a base85 encoding of a zip file, this zip file contains\n# an entire copy of pip (version 24.0).\n#\n# Pip is a thing that installs packages, pip itself is a package that someone\n# might want to install, especially if they're looking to run this get-pip.py\n# script. Pip has a lot of code to deal with the security of installing\n# packages, various edge cases on various platforms, and other such sort of\n# \"tribal knowledge\" that has been encoded in its code base. Because of this\n# we basically include an entire copy of pip inside this blob. We do this\n# because the alternatives are attempt to implement a \"minipip\" that probably\n# doesn't do things correctly and has weird edge cases, or compress pip itself\n# down into a single file.\n#\n# If you're wondering how this is created, it is generated using",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "675",
    "name": "convert-loop2.py",
    "path": "02_media_processing/image_tools/convert-loop2.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 163,
    "size": 5591,
    "docstring": "",
    "keywords": [
      "youtube",
      "web_tools"
    ],
    "functions": [
      "convert_image_to_jpeg",
      "get_presigned_url",
      "upload_image",
      "upscale_image",
      "get_upscaled_image"
    ],
    "classes": [],
    "imports": [
      "csv",
      "json",
      "os",
      "time",
      "datetime",
      "requests",
      "PIL"
    ],
    "preview": "import csv\nimport json\nimport os\nimport time\nfrom datetime import datetime\n\nimport requests\nfrom PIL import Image\n\napi_key = \"de7c9cb8-022f-42f8-8bf7-a8f9caadfaee\"\nauthorization = f\"Bearer {api_key}\"\n\nheaders = {\n    \"accept\": \"application/json\",\n    \"content-type\": \"application/json\",\n    \"authorization\": authorization,\n}\n\n# Directory containing images\ndirectory_path = \"/Users/steven/Pictures/TrashCaT/trashy-heartbreak\"",
    "last_modified": "2025-09-13T05:53:55.817963"
  },
  {
    "id": "676",
    "name": "bot_get.py",
    "path": "02_media_processing/image_tools/bot_get.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 513,
    "size": 14400,
    "docstring": "All methods must return media_ids that can be\npassed into e.g. like() or comment() functions.",
    "keywords": [],
    "functions": [
      "get_user_stories",
      "get_self_story_viewers",
      "get_user_reel",
      "get_media_owner",
      "get_user_tags_medias",
      "get_popular_medias",
      "get_your_medias",
      "get_archived_medias",
      "get_timeline_medias",
      "get_user_medias"
    ],
    "classes": [],
    "imports": [
      "tqdm"
    ],
    "preview": "\"\"\"\nAll methods must return media_ids that can be\npassed into e.g. like() or comment() functions.\n\"\"\"\n\nfrom tqdm import tqdm\n\n# STORY\n\n\ndef get_user_stories(self, user_id):\n    self.api.get_user_stories(user_id)\n    try:\n        if int(self.api.last_json[\"reel\"][\"media_count\"]) > 0:\n            list_image = []\n            list_video = []\n            for item in self.api.last_json[\"reel\"][\"items\"]:\n                if int(item[\"media_type\"]) == 1:  # photo\n                    img = item[\"image_versions2\"][\"candidates\"][0][\"url\"]\n                    list_image.append(img)",
    "last_modified": "2025-09-13T05:54:57.909178"
  },
  {
    "id": "677",
    "name": "style.py",
    "path": "02_media_processing/image_tools/style.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 198,
    "size": 6175,
    "docstring": "pygments.style\n~~~~~~~~~~~~~~\n\nBasic style object.\n\n:copyright: Copyright 2006-2023 by the Pygments team, see AUTHORS.\n:license: BSD, see LICENSE for details.",
    "keywords": [],
    "functions": [
      "__new__",
      "style_for_token",
      "list_styles",
      "styles_token",
      "__iter__",
      "__len__",
      "colorformat"
    ],
    "classes": [
      "StyleMeta",
      "Style"
    ],
    "imports": [
      "pip._vendor.pygments.token"
    ],
    "preview": "\"\"\"\npygments.style\n~~~~~~~~~~~~~~\n\nBasic style object.\n\n:copyright: Copyright 2006-2023 by the Pygments team, see AUTHORS.\n:license: BSD, see LICENSE for details.\n\"\"\"\n\nfrom pip._vendor.pygments.token import STANDARD_TYPES, Token\n\n# Default mapping of ansixxx to RGB colors.\n_ansimap = {\n    # dark\n    \"ansiblack\": \"000000\",\n    \"ansired\": \"7f0000\",\n    \"ansigreen\": \"007f00\",\n    \"ansiyellow\": \"7f7fe0\",\n    \"ansiblue\": \"00007f\",",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "678",
    "name": "upscale-.py",
    "path": "02_media_processing/image_tools/upscale-.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 33,
    "size": 1126,
    "docstring": "",
    "keywords": [],
    "functions": [
      "upscale_image"
    ],
    "classes": [],
    "imports": [
      "os",
      "PIL"
    ],
    "preview": "import os\n\nfrom PIL import Image\n\n\n# Function to upscale and set DPI of an image\ndef upscale_image(input_path, output_path, scale_factor=2, dpi=(300, 300)):\n    with Image.open(input_path) as img:\n        # Upscale the image\n        new_size = (img.width * scale_factor, img.height * scale_factor)\n        img_resized = img.resize(new_size, Image.ANTIALIAS)\n        # Set the DPI\n        img_resized.save(output_path, dpi=dpi)\n\n\n# Directory containing images to upscale\ninput_dir = input(\"Enter the path to the directory containing the images: \")\n# Directory to save the upscaled images\noutput_dir = input(\"Enter the path to the directory to save the upscaled images: \")\n",
    "last_modified": "2025-05-04T22:47:13.361779"
  },
  {
    "id": "679",
    "name": "youtube_dl_button 3.py",
    "path": "02_media_processing/image_tools/youtube_dl_button 3.py",
    "category": "02_media_processing",
    "type": "youtube",
    "lines": 386,
    "size": 15841,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "logging",
      "asyncio",
      "json",
      "math",
      "os",
      "shutil",
      "time",
      "datetime",
      "pyrogram",
      "translation"
    ],
    "preview": "import logging\n\nlogging.basicConfig(\n    level=logging.DEBUG, format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\n)\nlogger = logging.getLogger(__name__)\n\nimport asyncio\nimport json\nimport math\nimport os\nimport shutil\nimport time\nfrom datetime import datetime\n\n# the secret configuration specific things\nif bool(os.environ.get(\"WEBHOOK\", False)):\n    from sample_config import Config\nelse:\n    from config import Config",
    "last_modified": "2025-09-13T05:54:09.895547"
  },
  {
    "id": "680",
    "name": "UploadCompilation.py",
    "path": "02_media_processing/image_tools/UploadCompilation.py",
    "category": "02_media_processing",
    "type": "youtube",
    "lines": 47,
    "size": 1463,
    "docstring": "",
    "keywords": [
      "youtube"
    ],
    "functions": [
      "upload_compilation"
    ],
    "classes": [],
    "imports": [
      "datetime",
      "apikey",
      "google_auth_oauthlib.flow",
      "googleapiclient.discovery",
      "googleapiclient.http"
    ],
    "preview": "'''\n3. Upload video to YouTube\n'''\nimport datetime\n\nfrom apikey import apikey\nfrom google_auth_oauthlib.flow import InstalledAppFlow\nfrom googleapiclient.discovery import build\nfrom googleapiclient.http import MediaFileUpload\n\n\ndef upload_compilation(video_file, video_title, video_desc):\n    CLIENT_SECRET_FILE = 'directory to client_secret.json'\n    #https://www.googleapis.com/auth/youtube.upload\n    #https://www.googleapis.com/auth/youtube.force-ssl\n    SCOPES = ['https://www.googleapis.com/auth/youtube.upload']\n    flow = InstalledAppFlow.from_client_secrets_file(CLIENT_SECRET_FILE, SCOPES)\n    credentials = flow.run_console()\n    youtube = build('youtube', 'v3', credentials=credentials)\n",
    "last_modified": "2025-05-04T23:28:21"
  },
  {
    "id": "681",
    "name": "like_hashtags.py",
    "path": "02_media_processing/image_tools/like_hashtags.py",
    "category": "02_media_processing",
    "type": "image_processing",
    "lines": 27,
    "size": 641,
    "docstring": "instabot example\n\nWorkflow:\n    Like last images with hashtag.",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "argparse",
      "os",
      "sys",
      "instabot"
    ],
    "preview": "\"\"\"\ninstabot example\n\nWorkflow:\n    Like last images with hashtag.\n\"\"\"\n\nimport argparse\nimport os\nimport sys\n\nsys.path.append(os.path.join(sys.path[0], \"../\"))\nfrom instabot import Bot  # noqa: E402\n\nparser = argparse.ArgumentParser(add_help=True)\nparser.add_argument(\"-u\", type=str, help=\"username\")\nparser.add_argument(\"-p\", type=str, help=\"password\")\nparser.add_argument(\"-proxy\", type=str, help=\"proxy\")\nparser.add_argument(\"hashtags\", type=str, nargs=\"+\", help=\"hashtags\")\nargs = parser.parse_args()",
    "last_modified": "2025-03-28T18:35:47"
  },
  {
    "id": "682",
    "name": "csv-mydesgin.py",
    "path": "02_media_processing/image_tools/csv-mydesgin.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 31,
    "size": 956,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "csv",
      "os"
    ],
    "preview": "import csv\nimport os\n\n# Parent directory containing all the subdirectories\nparent_directory = \"/Users/steven/Pictures/V-Day CFab/tumnMuG/\"\n\n# The CSV file you want to create\ncsv_file = \"print_on_demand_data.csv\"\n\n# Define the fields for the CSV\nfields = [\"File Path\"]\n\n# List to store file details\nfile_details = []\n\n# Traverse through the parent directory and all its subdirectories\nfor subdir, dirs, files in os.walk(parent_directory):\n    for file in files:\n        if file.endswith((\"png\", \"svg\", \"dxf\", \"eps\")):\n            file_path = os.path.join(subdir, file)",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "683",
    "name": "leo-ups.py",
    "path": "02_media_processing/image_tools/leo-ups.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 178,
    "size": 5927,
    "docstring": "",
    "keywords": [
      "youtube",
      "web_tools"
    ],
    "functions": [
      "convert_image_to_jpeg",
      "get_presigned_url",
      "upload_image",
      "upscale_image",
      "get_upscaled_image",
      "initialize_csv",
      "log_to_csv"
    ],
    "classes": [],
    "imports": [
      "csv",
      "json",
      "os",
      "time",
      "datetime",
      "requests",
      "PIL"
    ],
    "preview": "import csv\nimport json\nimport os\nimport time\nfrom datetime import datetime\n\nimport requests\nfrom PIL import Image\n\napi_key = \"de7c9cb8-022f-42f8-8bf7-a8f9caadfaee\"\nauthorization = f\"Bearer {api_key}\"\n\nheaders = {\n    \"accept\": \"application/json\",\n    \"content-type\": \"application/json\",\n    \"authorization\": authorization,\n}\n\n# Directory containing images\ndirectory_path = \"/Users/steven/Pictures/TrashCaT/trashy-heartbreak\"",
    "last_modified": "2025-09-13T05:53:50.109890"
  },
  {
    "id": "684",
    "name": "pics2pdfs-100album.py",
    "path": "02_media_processing/image_tools/pics2pdfs-100album.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 57,
    "size": 2179,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "collect_image_files",
      "create_pdf_volumes",
      "main"
    ],
    "classes": [],
    "imports": [
      "os",
      "PIL"
    ],
    "preview": "import os\n\nfrom PIL import Image, UnidentifiedImageError\n\n# Increase the maximum number of pixels allowed to prevent DecompressionBombWarning.\n# Note: Be cautious with this setting to avoid processing extremely large images that could exhaust system memory.\nImage.MAX_IMAGE_PIXELS = None  # This disables the limit. Alternatively, set it to a specific limit you're comfortable with.\n\n\ndef collect_image_files(source_directory):\n    \"\"\"Collects all PNG, JPG, JPEG files from the source directory and its subdirectories.\"\"\"\n    image_files = []\n    for root, dirs, files in os.walk(source_directory):\n        for file in files:\n            if file.lower().endswith((\".png\", \".jpg\", \".jpeg\")):\n                file_path = os.path.join(root, file)\n                image_files.append(file_path)\n    return image_files\n\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "685",
    "name": "leodown_20250102111003.py",
    "path": "02_media_processing/image_tools/leodown_20250102111003.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 51,
    "size": 1326,
    "docstring": "",
    "keywords": [
      "web_tools"
    ],
    "functions": [],
    "classes": [],
    "imports": [
      "json",
      "requests"
    ],
    "preview": "import json\n\nimport requests\n\n# Base URL for API\nbase_url = (\n    \"https://cloud.leonardo.ai/api/rest/v1/generations/user/f7bb8476-e3f0-4f1f-9a06-4600866cc49c\"\n)\nheaders = {\n    \"accept\": \"application/json\",\n    \"authorization\": \"Bearer f7bb8476-e3f0-4f1f-9a06-4600866cc49c\",  # Replace with your actual token\n}\n\n# Output file\noutput_file = \"/Users/steven/Pictures/leonardo_library.json\"\n\n# Pagination parameters\noffset = 0\nlimit = 10  # Maximum number of items per request\nall_generations = []",
    "last_modified": "2025-09-13T05:53:49.617002"
  },
  {
    "id": "686",
    "name": "clips.py",
    "path": "02_media_processing/image_tools/clips.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 188,
    "size": 5843,
    "docstring": "",
    "keywords": [
      "youtube"
    ],
    "functions": [
      "get_data",
      "get_clip_data",
      "get_progress",
      "get_slug",
      "download_clip",
      "get_clips",
      "download_clips"
    ],
    "classes": [],
    "imports": [
      "datetime",
      "re",
      "urllib.request",
      "api",
      "logging",
      "utils"
    ],
    "preview": "import datetime\nimport re\nimport urllib.request\n\nfrom .api import get\nfrom .logging import Log as log\nfrom .utils import format_blacklist, is_blacklisted\n\n\ndef get_data(slug: str, oauth_token: str, client_id: str) -> dict:\n    \"\"\"\n    Gets the data from a given slug,\n    returns a JSON response from the Helix API endpoint\n    \"\"\"\n    response = get(\"data\", slug=slug, oauth_token=oauth_token, client_id=client_id)\n\n    try:\n        return response[\"data\"][0]\n    except KeyError as e:\n        log.error(f\"Ran into exception: {e}, {response}\")",
    "last_modified": "2025-09-13T05:53:56.187961"
  },
  {
    "id": "687",
    "name": "organize_albums 17.py",
    "path": "02_media_processing/image_tools/organize_albums 17.py",
    "category": "02_media_processing",
    "type": "organization",
    "lines": 62,
    "size": 2470,
    "docstring": "",
    "keywords": [
      "organization"
    ],
    "functions": [
      "organize_files"
    ],
    "classes": [],
    "imports": [
      "os",
      "shutil"
    ],
    "preview": "import os\nimport shutil\n\n# Define the base directory\nbase_dir = \"/Users/steven/Music/NocTurnE-meLoDieS/albums/\"\n\n\n# Helper function to create folders and move files\ndef organize_files():\n    # List all files in the base directory\n    files = os.listdir(base_dir)\n\n    # Process each file\n    for file in files:\n        # Skip directories\n        if os.path.isdir(os.path.join(base_dir, file)):\n            continue\n\n        # Extract the base name (album name) from the file\n        if file.endswith(\".mp3\"):",
    "last_modified": "2025-05-05T01:38:21"
  },
  {
    "id": "688",
    "name": "main_20221230233546.py",
    "path": "02_media_processing/image_tools/main_20221230233546.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 429,
    "size": 15514,
    "docstring": "",
    "keywords": [
      "transcription",
      "video_processing",
      "analysis",
      "youtube",
      "web_tools",
      "organization"
    ],
    "functions": [
      "downloadVideo",
      "scrapeVideos",
      "usedQuoteToDifferentFile",
      "getQuoteFromApi",
      "getQuoteFromTxtFile",
      "videoIntro",
      "createVideo",
      "audioClip",
      "randomBgMusic",
      "deleteTempFiles"
    ],
    "classes": [
      "bcolors"
    ],
    "imports": [
      "glob",
      "json",
      "os",
      "random",
      "subprocess",
      "gtts",
      "requests",
      "moviepy.editor",
      "moviepy.video.io.VideoFileClip",
      "mutagen.mp3"
    ],
    "preview": "import glob\nimport json\nimport os\nimport random\nimport subprocess\n\nimport gtts\nimport requests\nfrom moviepy.editor import *\nfrom moviepy.video.io.VideoFileClip import VideoFileClip\nfrom mutagen.mp3 import MP3\nfrom tqdm.auto import tqdm\n\n\n# download background video from pexels - https://www.pexels.com/api/documentation/#videos-search__parameters\ndef downloadVideo(id) -> str:\n    \"\"\"Downloads video from Pexels with the according video ID\"\"\"\n    url = \"https://www.pexels.com/video/\" + str(id) + \"/download.mp4\"\n    # Streaming, so we can iterate over the response.\n    response = requests.get(url, stream=True)",
    "last_modified": "2025-09-13T05:53:56.937195"
  },
  {
    "id": "689",
    "name": "screenshot_downloader.py",
    "path": "02_media_processing/image_tools/screenshot_downloader.py",
    "category": "02_media_processing",
    "type": "youtube",
    "lines": 54,
    "size": 1965,
    "docstring": "",
    "keywords": [
      "youtube"
    ],
    "functions": [
      "download_screenshots_of_reddit_posts"
    ],
    "classes": [],
    "imports": [
      "pathlib",
      "playwright.sync_api",
      "rich.progress",
      "utils.console"
    ],
    "preview": "from pathlib import Path\n\nfrom playwright.sync_api import sync_playwright\nfrom rich.progress import track\n\nfrom utils.console import print_step, print_substep\n\n\ndef download_screenshots_of_reddit_posts(reddit_object, screenshot_num):\n    \"\"\"Downloads screenshots of reddit posts as they are seen on the web.\n\n    Args:\n        reddit_object: The Reddit Object you received in askreddit.py\n        screenshot_num: The number of screenshots you want to download.\n    \"\"\"\n    print_step(\"Downloading Screenshots of Reddit Posts \ud83d\udcf7\")\n\n    # ! Make sure the reddit screenshots folder exists\n    Path(\"assets/png\").mkdir(parents=True, exist_ok=True)\n",
    "last_modified": "2025-09-13T05:53:59.445634"
  },
  {
    "id": "690",
    "name": "render.py",
    "path": "02_media_processing/image_tools/render.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 46,
    "size": 1411,
    "docstring": "",
    "keywords": [],
    "functions": [
      "blur",
      "render"
    ],
    "classes": [],
    "imports": [
      "moviepy.editor",
      "skimage.filters",
      "config"
    ],
    "preview": "from moviepy.editor import *\nfrom skimage.filters import gaussian\n\nimport config\n\n\ndef blur(image):\n    \"\"\"Returns a blurred (radius=2 pixels) version of the image\"\"\"\n    return gaussian(image.astype(float), sigma=25)\n\n\ndef render(\n    directory, clip_name, output_name, resolution\n):  # dir and names are strings, resolution is a tuple\n\n    blur = config.video[\"blur\"]\n\n    clip_dir = directory + clip_name\n    main_clip = VideoFileClip(clip_dir)\n",
    "last_modified": "2025-09-13T05:54:12.973310"
  },
  {
    "id": "691",
    "name": "repost_photo.py",
    "path": "02_media_processing/image_tools/repost_photo.py",
    "category": "02_media_processing",
    "type": "image_processing",
    "lines": 53,
    "size": 1382,
    "docstring": "instabot example\n\nWorkflow:\n1) Repost photo to your account",
    "keywords": [],
    "functions": [
      "exists_in_posted_medias",
      "update_posted_medias",
      "repost_photo"
    ],
    "classes": [],
    "imports": [
      "os",
      "sys",
      "instabot",
      "instabot.bot.bot_support"
    ],
    "preview": "\"\"\"\ninstabot example\n\nWorkflow:\n1) Repost photo to your account\n\"\"\"\n\nimport os\nimport sys\n\nsys.path.append(os.path.join(sys.path[0], \"../\"))\nfrom instabot import Bot  # noqa: E402\nfrom instabot.bot.bot_support import read_list_from_file  # noqa: E402\n\n\ndef exists_in_posted_medias(new_media_id, path=\"config/posted_medias.txt\"):\n    medias = read_list_from_file(path)\n    return new_media_id in medias\n\n",
    "last_modified": "2025-03-28T18:35:47"
  },
  {
    "id": "692",
    "name": "track.py",
    "path": "02_media_processing/image_tools/track.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 96,
    "size": 3405,
    "docstring": "",
    "keywords": [],
    "functions": [
      "_spotify_artist_names",
      "try_with_key_error",
      "__init__",
      "__repr__",
      "__str__"
    ],
    "classes": [
      "Track"
    ],
    "imports": [
      "typing",
      "uuid",
      "types"
    ],
    "preview": "from typing import Callable\nfrom uuid import uuid1\n\nfrom .types import Platform, Type\n\n\nclass Track:\n    def try_with_key_error(self, name: str, getter: Callable, default: str = \"\") -> None:\n        \"\"\"Wraps a try-except-assign statement.\"\"\"\n        try:\n            setattr(self, name, getter())\n        except KeyError:\n            setattr(self, name, default)\n\n    def __init__(self, spotify_data, track_type=Type.TRACK) -> None:\n        self.album_track_count = None\n        self.track_number = None\n        self.release_date = None\n        self.disc_number = None\n        self.playlist = None",
    "last_modified": "2025-09-13T05:55:15.904648"
  },
  {
    "id": "693",
    "name": "convert-loop.py",
    "path": "02_media_processing/image_tools/convert-loop.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 132,
    "size": 4531,
    "docstring": "",
    "keywords": [
      "youtube",
      "web_tools"
    ],
    "functions": [
      "convert_image_to_jpeg",
      "get_presigned_url",
      "upload_image",
      "upscale_image",
      "get_upscaled_image"
    ],
    "classes": [],
    "imports": [
      "json",
      "os",
      "time",
      "requests",
      "PIL"
    ],
    "preview": "import json\nimport os\nimport time\n\nimport requests\nfrom PIL import Image\n\napi_key = \"de7c9cb8-022f-42f8-8bf7-a8f9caadfaee\"\nauthorization = f\"Bearer {api_key}\"\n\nheaders = {\n    \"accept\": \"application/json\",\n    \"content-type\": \"application/json\",\n    \"authorization\": authorization,\n}\n\n# Directory containing images\ndirectory_path = \"/Users/steven/Music/trashCaTs/in-this-alley-where-i-hide\"\n\n# Styles to apply",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "694",
    "name": "download_your_photos.py",
    "path": "02_media_processing/image_tools/download_your_photos.py",
    "category": "02_media_processing",
    "type": "youtube",
    "lines": 19,
    "size": 275,
    "docstring": "instabot example\n\nWorkflow:\n1) Downloads your medias",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "os",
      "sys",
      "instabot"
    ],
    "preview": "\"\"\"\ninstabot example\n\nWorkflow:\n1) Downloads your medias\n\n\"\"\"\n\nimport os\nimport sys\n\nsys.path.append(os.path.join(sys.path[0], \"../\"))\nfrom instabot import Bot  # noqa: E402\n\nbot = Bot()\nbot.login()\nmedias = bot.get_total_user_medias(bot.user_id)\nbot.download_photos(medias)\n",
    "last_modified": "2025-03-28T18:35:47"
  },
  {
    "id": "695",
    "name": "motion-upload.py",
    "path": "02_media_processing/image_tools/motion-upload.py",
    "category": "02_media_processing",
    "type": "youtube",
    "lines": 58,
    "size": 1678,
    "docstring": "",
    "keywords": [
      "web_tools"
    ],
    "functions": [],
    "classes": [],
    "imports": [
      "json",
      "time",
      "requests"
    ],
    "preview": "import json\nimport time\n\nimport requests\n\napi_key = \"<YOUR_API_KEY>\"\nauthorization = \"Bearer %s\" % api_key\n\nheaders = {\n    \"accept\": \"application/json\",\n    \"content-type\": \"application/json\",\n    \"authorization\": authorization,\n}\n\n# Get a presigned URL for uploading an image\nurl = \"https://cloud.leonardo.ai/api/rest/v1/init-image\"\n\npayload = {\"extension\": \"jpg\"}\n\nresponse = requests.post(url, json=payload, headers=headers)",
    "last_modified": "2025-05-04T22:47:12.937267"
  },
  {
    "id": "696",
    "name": "imgmp4.py",
    "path": "02_media_processing/image_tools/imgmp4.py",
    "category": "02_media_processing",
    "type": "image_processing",
    "lines": 58,
    "size": 1968,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "get_cover_images",
      "convert_mp3_to_mp4_with_images",
      "process_directory"
    ],
    "classes": [],
    "imports": [
      "glob",
      "os",
      "moviepy.editor",
      "PIL",
      "sys"
    ],
    "preview": "import glob\nimport os\n\nfrom moviepy.editor import AudioFileClip, ImageSequenceClip\nfrom PIL import Image\n\n\ndef get_cover_images(file_name, cover_image_directory):\n    # Check for both JPG and PNG extensions\n    images = []\n    jpg_paths = glob.glob(os.path.join(cover_image_directory, f\"{file_name}*.jpg\"))\n    png_paths = glob.glob(os.path.join(cover_image_directory, f\"{file_name}*.png\"))\n\n    images.extend(jpg_paths)\n    images.extend(png_paths)\n\n    if images:\n        return images\n    else:\n        print(f\"Cover images not found for {file_name}. Please ensure the cover images exist.\")",
    "last_modified": "2025-09-13T05:53:49.227378"
  },
  {
    "id": "697",
    "name": "leoimg.py",
    "path": "02_media_processing/image_tools/leoimg.py",
    "category": "02_media_processing",
    "type": "image_processing",
    "lines": 38,
    "size": 1263,
    "docstring": "",
    "keywords": [
      "youtube",
      "web_tools"
    ],
    "functions": [
      "download_images"
    ],
    "classes": [],
    "imports": [
      "os",
      "requests",
      "colorama",
      "tqdm"
    ],
    "preview": "import os\n\nimport requests\nfrom colorama import Fore, Style\nfrom tqdm import tqdm\n\n\ndef download_images(source_file, destination_dir):\n    # Ensure destination directory exists\n    os.makedirs(destination_dir, exist_ok=True)\n\n    # Read URLs from the source file\n    with open(source_file, \"r\") as file:\n        urls = [line.strip() for line in file if line.strip()]\n\n    # Download each image with a progress bar\n    for url in tqdm(\n        urls,\n        desc=\"Downloading\",\n        bar_format=\"{l_bar}\" + Fore.GREEN + \"{bar}\" + Style.RESET_ALL + \"{r_bar}\",",
    "last_modified": "2025-05-04T22:47:12.933528"
  },
  {
    "id": "698",
    "name": "categories.py",
    "path": "02_media_processing/image_tools/categories.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 69,
    "size": 2061,
    "docstring": "",
    "keywords": [],
    "functions": [
      "scan_directory",
      "export_to_csv",
      "main"
    ],
    "classes": [],
    "imports": [
      "csv",
      "os"
    ],
    "preview": "import csv\nimport os\n\n\ndef scan_directory(directory):\n    categories = {\n        \"Scripts\": [\".py\", \".ipynb\"],\n        \"Data Files\": [\".csv\", \".json\", \".xls\", \".xlsx\"],\n        \"Text Files\": [\".txt\", \".md\"],\n        \"Images\": [\".jpg\", \".jpeg\", \".png\", \".gif\", \".bmp\"],\n        \"Configuration Files\": [\".ini\", \".cfg\", \".yaml\", \".yml\", \".json\"],\n        \"Other\": [],\n    }\n\n    categorized_files = []\n\n    for root, _, files in os.walk(directory):\n        for filename in files:\n            file_path = os.path.join(root, filename)\n            file_extension = os.path.splitext(filename)[1].lower()",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "699",
    "name": "leodown_20250102105151.py",
    "path": "02_media_processing/image_tools/leodown_20250102105151.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 51,
    "size": 1326,
    "docstring": "",
    "keywords": [
      "web_tools"
    ],
    "functions": [],
    "classes": [],
    "imports": [
      "json",
      "requests"
    ],
    "preview": "import json\n\nimport requests\n\n# Base URL for API\nbase_url = (\n    \"https://cloud.leonardo.ai/api/rest/v1/generations/user/f7bb8476-e3f0-4f1f-9a06-4600866cc49c\"\n)\nheaders = {\n    \"accept\": \"application/json\",\n    \"authorization\": \"Bearer 93043291-957d-4ec1-8c79-ee734abcb6e3\",  # Replace with your actual token\n}\n\n# Output file\noutput_file = \"/Users/steven/Pictures/leonardo_library.json\"\n\n# Pagination parameters\noffset = 0\nlimit = 10  # Maximum number of items per request\nall_generations = []",
    "last_modified": "2025-09-13T05:53:49.599176"
  },
  {
    "id": "700",
    "name": "YouTube Livestream Botter.py",
    "path": "02_media_processing/image_tools/YouTube Livestream Botter.py",
    "category": "02_media_processing",
    "type": "youtube",
    "lines": 171,
    "size": 7730,
    "docstring": "",
    "keywords": [
      "web_tools"
    ],
    "functions": [
      "bot",
      "__init__",
      "printservice",
      "update",
      "get_proxy",
      "FormatProxy",
      "__init__"
    ],
    "classes": [
      "main",
      "proxy"
    ],
    "imports": [
      "os",
      "platform",
      "random",
      "string",
      "threading",
      "time",
      "queue",
      "requests",
      "colorama"
    ],
    "preview": "import os\nimport platform\nimport random\nimport string\nimport threading\nimport time\nfrom queue import Queue\n\nimport requests\nfrom colorama import Fore, init\n\nintro = \"\"\"\n\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2588\u2557   \u2588\u2588\u2588\u2557      \u2588\u2588\u2588\u2588\u2588\u2588\u2557  \u2588\u2588\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2588\u2588\u2588\u2588\u2557 \n\u2588\u2588\u2554\u2550\u2550\u2550\u2550\u255d\u255a\u2550\u2550\u2588\u2588\u2554\u2550\u2550\u255d\u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2557\u2588\u2588\u2554\u2550\u2550\u2550\u2550\u255d\u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2557\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2588\u2588\u2551      \u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2557\u2588\u2588\u2554\u2550\u2550\u2550\u2588\u2588\u2557\u255a\u2550\u2550\u2588\u2588\u2554\u2550\u2550\u255d\u255a\u2550\u2550\u2588\u2588\u2554\u2550\u2550\u255d\u2588\u2588\u2554\u2550\u2550\u2550\u2550\u255d\u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2557\n\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557   \u2588\u2588\u2551   \u2588\u2588\u2588\u2588\u2588\u2588\u2554\u255d\u2588\u2588\u2588\u2588\u2588\u2557  \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2551\u2588\u2588\u2554\u2588\u2588\u2588\u2588\u2554\u2588\u2588\u2551\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2588\u2588\u2588\u2588\u2554\u255d\u2588\u2588\u2551   \u2588\u2588\u2551   \u2588\u2588\u2551      \u2588\u2588\u2551   \u2588\u2588\u2588\u2588\u2588\u2557  \u2588\u2588\u2588\u2588\u2588\u2588\u2554\u255d\n\u255a\u2550\u2550\u2550\u2550\u2588\u2588\u2551   \u2588\u2588\u2551   \u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2557\u2588\u2588\u2554\u2550\u2550\u255d  \u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2551\u2588\u2588\u2551\u255a\u2588\u2588\u2554\u255d\u2588\u2588\u2551\u255a\u2550\u2550\u2550\u2550\u255d\u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2557\u2588\u2588\u2551   \u2588\u2588\u2551   \u2588\u2588\u2551      \u2588\u2588\u2551   \u2588\u2588\u2554\u2550\u2550\u255d  \u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2557\n\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2551   \u2588\u2588\u2551   \u2588\u2588\u2551  \u2588\u2588\u2551\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2551  \u2588\u2588\u2551\u2588\u2588\u2551 \u255a\u2550\u255d \u2588\u2588\u2551      \u2588\u2588\u2588\u2588\u2588\u2588\u2554\u255d\u255a\u2588\u2588\u2588\u2588\u2588\u2588\u2554\u255d   \u2588\u2588\u2551      \u2588\u2588\u2551   \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2551  \u2588\u2588\u2551\n\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u255d   \u255a\u2550\u255d   \u255a\u2550\u255d  \u255a\u2550\u255d\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u255d\u255a\u2550\u255d  \u255a\u2550\u255d\u255a\u2550\u255d     \u255a\u2550\u255d      \u255a\u2550\u2550\u2550\u2550\u2550\u255d  \u255a\u2550\u2550\u2550\u2550\u2550\u255d    \u255a\u2550\u255d      \u255a\u2550\u255d   \u255a\u2550\u2550\u2550\u2550\u2550\u2550\u255d\u255a\u2550\u255d  \u255a\u2550\u255d\n\nhttps://github.com/KevinLage/YouTube-Livestream-Botter",
    "last_modified": "2025-05-04T23:28:20.826003"
  },
  {
    "id": "701",
    "name": "organize_albums 7.py",
    "path": "02_media_processing/image_tools/organize_albums 7.py",
    "category": "02_media_processing",
    "type": "organization",
    "lines": 62,
    "size": 2470,
    "docstring": "",
    "keywords": [
      "organization"
    ],
    "functions": [
      "organize_files"
    ],
    "classes": [],
    "imports": [
      "os",
      "shutil"
    ],
    "preview": "import os\nimport shutil\n\n# Define the base directory\nbase_dir = \"/Users/steven/Music/NocTurnE-meLoDieS/albums/\"\n\n\n# Helper function to create folders and move files\ndef organize_files():\n    # List all files in the base directory\n    files = os.listdir(base_dir)\n\n    # Process each file\n    for file in files:\n        # Skip directories\n        if os.path.isdir(os.path.join(base_dir, file)):\n            continue\n\n        # Extract the base name (album name) from the file\n        if file.endswith(\".mp3\"):",
    "last_modified": "2025-05-04T22:47:11.429396"
  },
  {
    "id": "702",
    "name": "sort.py",
    "path": "02_media_processing/image_tools/sort.py",
    "category": "02_media_processing",
    "type": "organization",
    "lines": 33,
    "size": 1090,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "os",
      "shutil"
    ],
    "preview": "import os\nimport shutil\n\n# Source directory containing your images\nsource_dir = \"/Volumes/baKs/105-mids\"\n\n# Destination directory where sorted folders will be created\ndestination_dir = \"/Volumes/baKs/105-mids\"\n\n# Create the destination directory if it doesn't exist\nos.makedirs(destination_dir, exist_ok=True)\n\n# Initialize variables\nimages_per_folder = 100\ncurrent_folder = None\nfolder_count = 0\n\n# Iterate through the source directory\nfor root, _, files in os.walk(source_dir):\n    for filename in files:",
    "last_modified": "2025-05-04T22:47:11.914841"
  },
  {
    "id": "703",
    "name": "like_hashtags_from_file.py",
    "path": "02_media_processing/image_tools/like_hashtags_from_file.py",
    "category": "02_media_processing",
    "type": "image_processing",
    "lines": 33,
    "size": 812,
    "docstring": "instabot example\n\nWorkflow:\n    Like last images with hashtags from file.",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "argparse",
      "os",
      "sys",
      "instabot"
    ],
    "preview": "\"\"\"\ninstabot example\n\nWorkflow:\n    Like last images with hashtags from file.\n\"\"\"\n\nimport argparse\nimport os\nimport sys\n\nsys.path.append(os.path.join(sys.path[0], \"../\"))\nfrom instabot import Bot  # noqa: E402\n\nparser = argparse.ArgumentParser(add_help=True)\nparser.add_argument(\"-u\", type=str, help=\"username\")\nparser.add_argument(\"-p\", type=str, help=\"password\")\nparser.add_argument(\"-proxy\", type=str, help=\"proxy\")\nparser.add_argument(\"filename\", type=str, help=\"filename\")\nargs = parser.parse_args()",
    "last_modified": "2025-03-28T18:35:47"
  },
  {
    "id": "704",
    "name": "upscale-dl.py",
    "path": "02_media_processing/image_tools/upscale-dl.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 107,
    "size": 3978,
    "docstring": "",
    "keywords": [
      "web_tools"
    ],
    "functions": [],
    "classes": [],
    "imports": [
      "json",
      "os",
      "time",
      "requests"
    ],
    "preview": "import json\nimport os\nimport time\n\nimport requests\n\napi_key = os.getenv(\"API_KEY\")\nif not api_key:\n    raise ValueError(\n        \"API key is not set. Please ensure the API_KEY environment variable is configured correctly.\"\n    )\n\nauthorization = f\"Bearer {api_key}\"\n\nheaders = {\n    \"accept\": \"application/json\",\n    \"content-type\": \"application/json\",\n    \"authorization\": authorization,\n}\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "705",
    "name": "upscale-dl 2.py",
    "path": "02_media_processing/image_tools/upscale-dl 2.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 92,
    "size": 3689,
    "docstring": "",
    "keywords": [
      "web_tools"
    ],
    "functions": [],
    "classes": [],
    "imports": [
      "json",
      "os",
      "time",
      "requests"
    ],
    "preview": "import json\nimport os\nimport time\n\nimport requests\n\napi_key = os.getenv(\"API_KEY\")\nif not api_key:\n    raise ValueError(\n        \"API key is not set. Please ensure the API_KEY environment variable is configured correctly.\"\n    )\n\nauthorization = f\"Bearer {api_key}\"\n\nheaders = {\n    \"accept\": \"application/json\",\n    \"content-type\": \"application/json\",\n    \"authorization\": authorization,\n}\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "706",
    "name": "scan_images.py",
    "path": "02_media_processing/image_tools/scan_images.py",
    "category": "02_media_processing",
    "type": "image_processing",
    "lines": 30,
    "size": 742,
    "docstring": "",
    "keywords": [],
    "functions": [
      "scan_directory"
    ],
    "classes": [],
    "imports": [
      "os"
    ],
    "preview": "import os\n\n# List of drives to scan\ndrives = [\n    \"/Users/steven/Downloads\",\n    \"/Users/steven/Documents\",\n    \"/Users/steven/Music\",\n    \"/Users/steven/Pictures\",\n]\n# File types to look for\nfile_types = (\".jpg\", \".jpeg\", \".png\")\n\n# Function to scan a directory for image files\n\n\ndef scan_directory(directory):\n    for root, dirs, files in os.walk(directory):\n        for file in files:\n            if file.lower().endswith(file_types):\n                yield os.path.join(root, file)",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "707",
    "name": "upscale 3.py",
    "path": "02_media_processing/image_tools/upscale 3.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 34,
    "size": 1127,
    "docstring": "",
    "keywords": [],
    "functions": [
      "upscale_image"
    ],
    "classes": [],
    "imports": [
      "os",
      "PIL"
    ],
    "preview": "import os\n\nfrom PIL import Image\n\n# Function to upscale and set DPI of an image\n\n\ndef upscale_image(input_path, output_path, scale_factor=2, dpi=(300, 300)):\n    with Image.open(input_path) as img:\n        # Upscale the image\n        new_size = (img.width * scale_factor, img.height * scale_factor)\n        img_resized = img.resize(new_size, Image.ANTIALIAS)\n        # Set the DPI\n        img_resized.save(output_path, dpi=dpi)\n\n\n# Directory containing images to upscale\ninput_dir = input(\"Enter the path to the directory containing the images: \")\n# Directory to save the upscaled images\noutput_dir = input(\"Enter the path to the directory to save the upscaled images: \")",
    "last_modified": "2025-05-04T22:47:13.381541"
  },
  {
    "id": "708",
    "name": "test_bot_follow.py",
    "path": "02_media_processing/image_tools/test_bot_follow.py",
    "category": "02_media_processing",
    "type": "testing",
    "lines": 530,
    "size": 21951,
    "docstring": "",
    "keywords": [
      "testing"
    ],
    "functions": [
      "reset_files",
      "test_follow",
      "test_follow_users",
      "test_follow_followers",
      "test_follow_following",
      "test_sleep_feedback_successful",
      "test_sleep_feedback_unsuccessful"
    ],
    "classes": [
      "TestBotFilter"
    ],
    "imports": [
      "pytest",
      "responses",
      "instabot.api.config",
      "test_bot",
      "test_variables",
      "unittest.mock",
      "mock"
    ],
    "preview": "import pytest\nimport responses\nfrom instabot.api.config import API_URL, SIG_KEY_VERSION\n\nfrom .test_bot import TestBot\nfrom .test_variables import (\n    TEST_FOLLOWER_ITEM,\n    TEST_FOLLOWING_ITEM,\n    TEST_SEARCH_USERNAME_ITEM,\n    TEST_USERNAME_INFO_ITEM,\n)\n\ntry:\n    from unittest.mock import patch\nexcept ImportError:\n    from mock import patch\n\n\ndef reset_files(_bot):\n    for x in _bot.followed_file.list:",
    "last_modified": "2025-09-13T05:54:58.585345"
  },
  {
    "id": "709",
    "name": "img-img.py",
    "path": "02_media_processing/image_tools/img-img.py",
    "category": "02_media_processing",
    "type": "image_processing",
    "lines": 56,
    "size": 2134,
    "docstring": "",
    "keywords": [],
    "functions": [
      "convert_and_upscale_images_in_subfolders",
      "main"
    ],
    "classes": [],
    "imports": [
      "os",
      "PIL"
    ],
    "preview": "import os\n\nfrom PIL import Image\n\n\ndef convert_and_upscale_images_in_subfolders(source_directory):\n    for root, dirs, files in os.walk(source_directory):\n        for filename in files:\n            if filename.endswith(\".png\"):\n                source_file = os.path.join(root, filename)\n                filename_no_ext = os.path.splitext(filename)[0]\n                temp_file = os.path.join(root, f\"{filename_no_ext}_temp.png\")\n\n                # Open the image and retrieve the original dimensions\n                im = Image.open(source_file)\n                width, height = im.size\n                print(f\"\ud83d\uddbc\ufe0f Processing {filename}: Original size: {width}x{height}\")\n\n                # Upscale the image by 2x\n                upscale_width = width * 2",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "710",
    "name": "Telegraph Image Downloader [RUS].py",
    "path": "02_media_processing/image_tools/Telegraph Image Downloader [RUS].py",
    "category": "02_media_processing",
    "type": "youtube",
    "lines": 446,
    "size": 19134,
    "docstring": "",
    "keywords": [
      "youtube",
      "web_tools"
    ],
    "functions": [
      "getHTML",
      "validName",
      "print_percent",
      "download",
      "main"
    ],
    "classes": [],
    "imports": [
      "threading",
      "msvcrt",
      "os",
      "pathlib",
      "re",
      "subprocess",
      "sys",
      "time",
      "traceback",
      "urllib"
    ],
    "preview": "# Telegraph Image Downloader\n# \u0410\u0432\u0442\u043e\u0440: ARTEZON (vk.com/artez0n)\n#\n# \u0412\u0435\u0440\u0441\u0438\u044f 1.2.1\n#\n# --------------------------------------------------\n# -= \u041d\u0410\u0421\u0422\u0420\u041e\u0419\u041a\u0418 =-\n# --------------------------------------------------\n# \u042f\u0437\u044b\u043a (string)\n# \u0412\u043e\u0437\u043c\u043e\u0436\u043d\u044b\u0435 \u0432\u0430\u0440\u0438\u0430\u043d\u0442\u044b: ENG, RUS, JPN, CHN, KOR \u0438 \u0434\u0440\u0443\u0433\u0438\u0435\n# language = 'RUS'\n# --------------------------------------------------\n# \u0413\u0434\u0435 \u0441\u043e\u0445\u0440\u0430\u043d\u044f\u0442\u044c \u043c\u0435\u0442\u0430\u0434\u0430\u043d\u043d\u044b\u0435 (int)\n# 0 - \u041e\u0442\u043a\u043b\u044e\u0447\u0435\u043d\u043e\n# 1 - \u0412 \u043f\u0430\u043f\u043a\u0435 \"\u0417\u0430\u0433\u0440\u0443\u0437\u043a\u0438\" (\u043f\u043e \u0443\u043c\u043e\u043b\u0447\u0430\u043d\u0438\u044e)\n# 2 - \u0420\u044f\u0434\u043e\u043c \u0441 \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u044f\u043c\u0438\nmetadataLocation = 1\n# --------------------------------------------------\n# \u041c\u0430\u043a\u0441\u0438\u043c\u0430\u043b\u044c\u043d\u043e\u0435 \u0447\u0438\u0441\u043b\u043e \u043e\u0434\u043d\u043e\u0432\u0440\u0435\u043c\u0435\u043d\u043d\u044b\u0445 \u0437\u0430\u0433\u0440\u0443\u0437\u043e\u043a (int)\n# \u041f\u043e \u0443\u043c\u043e\u043b\u0447\u0430\u043d\u0438\u044e: 10",
    "last_modified": "2025-09-13T05:55:24.813336"
  },
  {
    "id": "711",
    "name": "target_followers_of_similar_accounts_and_influencers.py",
    "path": "02_media_processing/image_tools/target_followers_of_similar_accounts_and_influencers.py",
    "category": "02_media_processing",
    "type": "analysis",
    "lines": 153,
    "size": 3981,
    "docstring": "This template is written by @Nuzzo235\n\nWhat does this quickstart script aim to do?\n- This script is targeting followers of similar accounts and influencers.\n- This is my starting point for a conservative approach: Interact with the\naudience of influencers in your niche with the help of 'Target-Lists' and\n'randomization'.\n\nNOTES:\n- For the ease of use most of the relevant data is retrieved in the upper part.",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "random",
      "instapy"
    ],
    "preview": "\"\"\"\nThis template is written by @Nuzzo235\n\nWhat does this quickstart script aim to do?\n- This script is targeting followers of similar accounts and influencers.\n- This is my starting point for a conservative approach: Interact with the\naudience of influencers in your niche with the help of 'Target-Lists' and\n'randomization'.\n\nNOTES:\n- For the ease of use most of the relevant data is retrieved in the upper part.\n\"\"\"\n\nimport random\n\nfrom instapy import InstaPy, smart_run\n\n# login credentials\ninsta_username = \"username\"\ninsta_password = \"password\"",
    "last_modified": "2025-03-28T18:36:56"
  },
  {
    "id": "712",
    "name": "vanceai-removebg.py",
    "path": "02_media_processing/image_tools/vanceai-removebg.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 71,
    "size": 2580,
    "docstring": "",
    "keywords": [
      "analysis",
      "youtube",
      "web_tools"
    ],
    "functions": [
      "remove_background",
      "download_result",
      "process_directory"
    ],
    "classes": [],
    "imports": [
      "os",
      "time",
      "requests"
    ],
    "preview": "import os\nimport time\n\nimport requests\n\n# Get the API key from the environment variable\nAPI_KEY = os.getenv(\"VANCEAI_API_KEY\")\nAPI_URL = \"https://api-service.vanceai.com/web_api/v1/removebg\"\nPROGRESS_URL = \"https://api-service.vanceai.com/web_api/v1/progress\"\n\n\ndef remove_background(input_path, output_path):\n    with open(input_path, \"rb\") as file:\n        response = requests.post(API_URL, files={\"image_file\": file}, headers={\"api_key\": API_KEY})\n\n    if response.status_code == 200:\n        result = response.json()\n        trans_id = result[\"trans_id\"]\n        download_result(trans_id, output_path)\n    else:",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "713",
    "name": "clientUI.py",
    "path": "02_media_processing/image_tools/clientUI.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 842,
    "size": 33330,
    "docstring": "",
    "keywords": [
      "video_processing",
      "opencv",
      "youtube"
    ],
    "functions": [
      "__init__",
      "attemptLogin",
      "loginSuccess",
      "__init__",
      "updateRenderProgress",
      "downloadFinishedVideo",
      "populateFinishedVideos",
      "getFinishedVideos",
      "startEditingVideo",
      "updateDownload"
    ],
    "classes": [
      "LoginWindow",
      "MainMenu",
      "ClipDownloadMenu",
      "ClipUploadMenu",
      "clipEditor"
    ],
    "imports": [
      "os",
      "pickle",
      "threading",
      "client",
      "cv2",
      "scriptwrapper",
      "settings",
      "pymediainfo",
      "PyQt5",
      "PyQt5.QtCore"
    ],
    "preview": "import os\nimport pickle\nfrom threading import Thread\n\nimport client\nimport cv2\nimport scriptwrapper\nimport settings\nfrom pymediainfo import MediaInfo\nfrom PyQt5 import QtCore, QtGui, QtWidgets, uic\nfrom PyQt5.QtCore import *\nfrom PyQt5.QtCore import QDir, QObject, QPoint, QRect, Qt, QUrl, pyqtSignal\nfrom PyQt5.QtGui import QIcon\nfrom PyQt5.QtMultimedia import (\n    QAbstractVideoBuffer,\n    QAbstractVideoSurface,\n    QMediaContent,\n    QMediaPlayer,\n    QMediaPlaylist,\n    QVideoFrame,",
    "last_modified": "2025-09-13T05:53:31.455753"
  },
  {
    "id": "714",
    "name": "download_stickers.py",
    "path": "02_media_processing/image_tools/download_stickers.py",
    "category": "02_media_processing",
    "type": "youtube",
    "lines": 117,
    "size": 3983,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "logging",
      "os",
      "time",
      "pyrogram",
      "translation",
      "helper_funcs.chat_base",
      "helper_funcs.display_progress",
      "sample_config",
      "config"
    ],
    "preview": "import logging\n\nlogging.basicConfig(\n    level=logging.DEBUG, format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\n)\nlogger = logging.getLogger(__name__)\n\nimport os\nimport time\n\n# the secret configuration specific things\nif bool(os.environ.get(\"WEBHOOK\", False)):\n    from sample_config import Config\nelse:\n    from config import Config\n\n# the Strings used for this \"thing\"\nimport pyrogram\nfrom translation import Translation\n",
    "last_modified": "2025-09-13T05:53:44.081939"
  },
  {
    "id": "715",
    "name": "up-down.py",
    "path": "02_media_processing/image_tools/up-down.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 125,
    "size": 5234,
    "docstring": "",
    "keywords": [],
    "functions": [
      "adjust_image_size",
      "convert_and_downscale_images_in_subfolders",
      "main"
    ],
    "classes": [],
    "imports": [
      "os",
      "PIL"
    ],
    "preview": "import os\n\nfrom PIL import Image, UnidentifiedImageError\n\n\ndef adjust_image_size(im, target_file_size, temp_file, target_dpi, upscale=False):\n    file_size = os.path.getsize(temp_file)\n\n    # Size limits: 4500x5400 max, 1024x1024 min\n    max_width, max_height = 4500, 5400\n    min_width, min_height = 1024, 1024\n\n    while (file_size > target_file_size) or (upscale and file_size < target_file_size):\n        # Downscale if image is too large\n        if file_size > target_file_size or im.size[0] > max_width or im.size[1] > max_height:\n            scale_factor = 0.9  # Downscale by 10%\n        # Upscale if image is too small\n        elif im.size[0] < min_width or im.size[1] < min_height:\n            scale_factor = 1.1  # Upscale by 10%\n        else:",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "716",
    "name": "convert_to_video.py",
    "path": "02_media_processing/image_tools/convert_to_video.py",
    "category": "02_media_processing",
    "type": "youtube",
    "lines": 141,
    "size": 5334,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "logging",
      "os",
      "time",
      "pyrogram",
      "translation",
      "hachoir.metadata",
      "hachoir.parser",
      "helper_funcs.chat_base",
      "helper_funcs.display_progress",
      "PIL"
    ],
    "preview": "import logging\n\nlogging.basicConfig(\n    level=logging.DEBUG, format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\n)\nlogger = logging.getLogger(__name__)\n\nimport os\nimport time\n\n# the secret configuration specific things\nif bool(os.environ.get(\"WEBHOOK\", False)):\n    from sample_config import Config\nelse:\n    from config import Config\n\n# the Strings used for this \"thing\"\nimport pyrogram\nfrom translation import Translation\n",
    "last_modified": "2025-09-13T05:53:43.785319"
  },
  {
    "id": "717",
    "name": "csvp.py",
    "path": "02_media_processing/image_tools/csvp.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 36,
    "size": 1274,
    "docstring": "",
    "keywords": [],
    "functions": [
      "compile_image_info"
    ],
    "classes": [],
    "imports": [
      "csv",
      "os"
    ],
    "preview": "import csv\nimport os\n\n\ndef compile_image_info(root_directory, master_csv_file_path):\n    with open(master_csv_file_path, mode=\"w\", newline=\"\", encoding=\"utf-8\") as master_file:\n        master_writer = csv.writer(master_file)\n        master_writer.writerow(\n            [\n                \"Subfolder\",\n                \"Product Name\",\n                \"Product Link\",\n                \"Est. Monthly Revenue\",\n                \"Reviews\",\n                \"Listing Age\",\n                \"Favorites\",\n                \"Est. Total Sales\",\n                \"Price\",\n                \"Tags\",\n            ]",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "718",
    "name": "convert.py",
    "path": "02_media_processing/image_tools/convert.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 32,
    "size": 1158,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "os",
      "PIL"
    ],
    "preview": "import os\n\nfrom PIL import Image\n\n# Your messy, no-folder, unstructured digital wasteland\ninput_dir = \"'/Users/steven/Pictures/etsy/PanoramicIndexJuice\"\n\n# Supported input formats because variety is the spice of life\ninput_formats = (\".webp\", \".tiff\", \".tif\")\n\n# Go through all files, because why not?\nfor filename in os.listdir(input_dir):\n    if filename.lower().endswith(input_formats):\n        input_path = os.path.join(input_dir, filename)\n        output_path = (\n            os.path.splitext(input_path)[0] + \".jpg\"\n        )  # Keep same name, just betray the format\n\n        # Open and mercilessly convert\n        with Image.open(input_path) as img:",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "719",
    "name": "test_bot_like.py",
    "path": "02_media_processing/image_tools/test_bot_like.py",
    "category": "02_media_processing",
    "type": "testing",
    "lines": 1270,
    "size": 46609,
    "docstring": "",
    "keywords": [
      "testing"
    ],
    "functions": [
      "test_bot_like",
      "test_bot_like_comment",
      "test_like_media_comments",
      "test_like_user",
      "test_like_users",
      "test_sleep_feedback_successful",
      "test_sleep_feedback_unsuccessful",
      "test_like_feedback",
      "test_like_medias",
      "test_like_hashtag"
    ],
    "classes": [
      "TestBotGet"
    ],
    "imports": [
      "pytest",
      "responses",
      "instabot.api.config",
      "test_bot",
      "test_variables",
      "unittest.mock",
      "mock"
    ],
    "preview": "import pytest\nimport responses\nfrom instabot.api.config import API_URL, SIG_KEY_VERSION\n\nfrom .test_bot import TestBot\nfrom .test_variables import (\n    TEST_CAPTION_ITEM,\n    TEST_COMMENT_ITEM,\n    TEST_FOLLOWER_ITEM,\n    TEST_FOLLOWING_ITEM,\n    TEST_PHOTO_ITEM,\n    TEST_SEARCH_USERNAME_ITEM,\n    TEST_TIMELINE_PHOTO_ITEM,\n    TEST_USERNAME_INFO_ITEM,\n)\n\ntry:\n    from unittest.mock import patch\nexcept ImportError:\n    from mock import patch",
    "last_modified": "2025-09-13T05:54:59.970121"
  },
  {
    "id": "720",
    "name": "listcsv.py",
    "path": "02_media_processing/image_tools/listcsv.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 23,
    "size": 597,
    "docstring": "",
    "keywords": [],
    "functions": [
      "list_files",
      "save_to_csv"
    ],
    "classes": [],
    "imports": [
      "csv",
      "os"
    ],
    "preview": "import csv\nimport os\n\n\ndef list_files(directory):\n    for root, dirs, files in os.walk(directory):\n        for file in files:\n            yield os.path.join(root, file)\n\n\ndef save_to_csv(file_list, output_csv):\n    with open(output_csv, \"w\", newline=\"\") as csvfile:\n        filewriter = csv.writer(csvfile)\n        # Write each file path as a new column in the same row\n        filewriter.writerow(file_list)\n\n\nif __name__ == \"__main__\":\n    directory = \"/Users/steven/Pictures/3dMothers\"\n    output_csv = \"file_list.csv\"",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "721",
    "name": "infinity_hashtags_liker.py",
    "path": "02_media_processing/image_tools/infinity_hashtags_liker.py",
    "category": "02_media_processing",
    "type": "image_processing",
    "lines": 32,
    "size": 723,
    "docstring": "instabot example\n\nWorkflow:\n    Like last images with hashtag.",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "argparse",
      "os",
      "sys",
      "time",
      "instabot"
    ],
    "preview": "\"\"\"\ninstabot example\n\nWorkflow:\n    Like last images with hashtag.\n\"\"\"\n\nimport argparse\nimport os\nimport sys\nimport time\n\nsys.path.append(os.path.join(sys.path[0], \"../\"))\nfrom instabot import Bot  # noqa: E402\n\nparser = argparse.ArgumentParser(add_help=True)\nparser.add_argument(\"-u\", type=str, help=\"username\")\nparser.add_argument(\"-p\", type=str, help=\"password\")\nparser.add_argument(\"-proxy\", type=str, help=\"proxy\")\nparser.add_argument(\"hashtags\", type=str, nargs=\"+\", help=\"hashtags\")",
    "last_modified": "2025-03-28T18:35:47"
  },
  {
    "id": "722",
    "name": "generate_album_html-pages_fixed 2.py",
    "path": "02_media_processing/image_tools/generate_album_html-pages_fixed 2.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 157,
    "size": 4917,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "os",
      "pathlib"
    ],
    "preview": "import os\nfrom pathlib import Path\n\nalbums_dir = Path(\"/Users/steven/Music/nocTurneMeLoDieS/Media\")\noutput_file = albums_dir / \"discography.html\"\n\nhtml_header = \"\"\"<!DOCTYPE html>\n<html lang=\"en\">\n<head>\n    <meta charset=\"UTF-8\">\n    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n    <title>Discography with MP3</title>\n    <style>\n        body { font-family: Arial, sans-serif; background-color: #f4f4f4; margin: 0; padding: 0; }\n        h1 { text-align: center; margin-top: 20px; font-size: 32px; color: #333; }\n        .grid-container {\n            display: grid;\n            grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));\n            gap: 20px;\n            padding: 20px;",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "723",
    "name": "imgconvert 2.py",
    "path": "02_media_processing/image_tools/imgconvert 2.py",
    "category": "02_media_processing",
    "type": "image_processing",
    "lines": 50,
    "size": 1736,
    "docstring": "",
    "keywords": [],
    "functions": [
      "convert_and_upscale_images",
      "main"
    ],
    "classes": [],
    "imports": [
      "os",
      "PIL"
    ],
    "preview": "import os\n\nfrom PIL import Image\n\n\n# Function to convert WebP images to PNG and upscale by 200% with 300 DPI\ndef convert_and_upscale_images(source_directory, destination_directory):\n    # Create the destination directory if it doesn't exist\n    os.makedirs(destination_directory, exist_ok=True)\n\n    for filename in os.listdir(source_directory):\n        if filename.endswith(\".webp\"):\n            source_file = os.path.join(source_directory, filename)\n            filename_no_ext = os.path.splitext(filename)[0]\n            destination_file = os.path.join(destination_directory, f\"{filename_no_ext}.png\")\n\n            # Convert WebP to PNG and upscale by 200% with 300 DPI\n            im = Image.open(source_file)\n            width, height = im.size\n            upscale_width = width * 2",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "724",
    "name": "simple_interaction_good_for_beginners.py",
    "path": "02_media_processing/image_tools/simple_interaction_good_for_beginners.py",
    "category": "02_media_processing",
    "type": "image_processing",
    "lines": 73,
    "size": 2159,
    "docstring": "This template is written by @Tachenz\n\nWhat does this quickstart script aim to do?\n- Interact with user followers, liking 3 pictures, doing 1-2 comment - and\n25% chance of follow (ratios which work the best for my account)\n\nNOTES:\n- This is used in combination with putting a 40 sec sleep delay after every\nlike the script does. It runs 24/7 at rather slower speed, but without\nproblems (so far).",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "instapy"
    ],
    "preview": "\"\"\"\nThis template is written by @Tachenz\n\nWhat does this quickstart script aim to do?\n- Interact with user followers, liking 3 pictures, doing 1-2 comment - and\n25% chance of follow (ratios which work the best for my account)\n\nNOTES:\n- This is used in combination with putting a 40 sec sleep delay after every\nlike the script does. It runs 24/7 at rather slower speed, but without\nproblems (so far).\n\"\"\"\n\nfrom instapy import InstaPy, smart_run\n\n# get a session!\nsession = InstaPy(username=\"\", password=\"\")\n\nphoto_comments = [\n    \"Nice shot! @{}\",",
    "last_modified": "2025-03-28T18:36:56"
  },
  {
    "id": "725",
    "name": "remove.py",
    "path": "02_media_processing/image_tools/remove.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 32,
    "size": 929,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "io",
      "os",
      "PIL",
      "rembg"
    ],
    "preview": "import io\nimport os\n\nfrom PIL import Image\nfrom rembg import remove\n\ninput_dir = \"/Users/steven/Pictures/orn/Dream\"\noutput_dir = \"/Users/steven/Pictures/orn/Dream/processed_output\"\n\n# Create output directory if it doesn't exist\nif not os.path.exists(output_dir):\n    os.makedirs(output_dir)\n\n# Process all images in the input directory\nfor filename in os.listdir(input_dir):\n    if filename.lower().endswith((\".png\", \".jpg\", \".jpeg\")):\n        input_path = os.path.join(input_dir, filename)\n        output_path = os.path.join(output_dir, filename)\n\n        # Open the input image",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "726",
    "name": "2leomotion.py",
    "path": "02_media_processing/image_tools/2leomotion.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 90,
    "size": 3097,
    "docstring": "",
    "keywords": [
      "web_tools"
    ],
    "functions": [],
    "classes": [],
    "imports": [
      "time",
      "requests"
    ],
    "preview": "import time\n\nimport requests\n\napi_key = \"b5b99021-8e7a-42ef-8df9-4eca2c6efd3c\"\nauthorization = \"Bearer %s\" % api_key\n\nheaders = {\n    \"accept\": \"application/json\",\n    \"content-type\": \"application/json\",\n    \"authorization\": authorization,\n}\n\n# Generate an image\nurl = \"https://cloud.leonardo.ai/api/rest/v1/generations\"\n\npayload = {\n    \"height\": 960,\n    \"modelId\": \"ac614f96-1082-45bf-be9d-757f2d31c174\",\n    \"prompt\": \"A detailed photograph of a serious cyberpunk Hacker Cyborg transhumanist the past looking directly at the camera, standing straight, hands relaxed, square jaws, masculine face, dark scruff and no wrinkles, slightly buff looking, wearing a dark graphic t-shirt, detailed clothing texture realistic skin texture, black background, sharp focus, front view, waist up shot, high contrast, strong backlighting, action film dark color lut, cinematic luts\",",
    "last_modified": "2025-08-06T14:26:12.425397"
  },
  {
    "id": "727",
    "name": "universe4 2.py",
    "path": "02_media_processing/image_tools/universe4 2.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 79,
    "size": 3098,
    "docstring": "",
    "keywords": [
      "web_tools"
    ],
    "functions": [],
    "classes": [],
    "imports": [
      "json",
      "os",
      "time",
      "requests"
    ],
    "preview": "import json\nimport os\nimport time\n\nimport requests\n\napi_key = \"2a59398a-fa31-47da-b771-64308e38bcc3\"\nauthorization = \"Bearer %s\" % api_key\n\nheaders = {\n    \"accept\": \"application/json\",\n    \"content-type\": \"application/json\",\n    \"authorization\": authorization,\n}\n\n# Directory containing images\ndirectory_path = \"/Users/steven/Pictures/Bcovers/\"\n\n# Styles to apply\n# Update the styles list according to your needs",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "728",
    "name": "png-jpg.py",
    "path": "02_media_processing/image_tools/png-jpg.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 42,
    "size": 1267,
    "docstring": "",
    "keywords": [],
    "functions": [
      "convert_images_to_jpeg",
      "main"
    ],
    "classes": [],
    "imports": [
      "os",
      "PIL"
    ],
    "preview": "import os\n\nfrom PIL import Image\n\n\ndef convert_images_to_jpeg(source_directory):\n    for root, dirs, files in os.walk(source_directory):\n        for filename in files:\n            if filename.endswith(\".png\"):\n                source_file = os.path.join(root, filename)\n                filename_no_ext = os.path.splitext(filename)[0]\n                jpeg_file = os.path.join(root, f\"{filename_no_ext}.jpg\")\n\n                # Open the PNG image\n                im = Image.open(source_file)\n\n                # Convert the image to RGB mode (JPEG doesn't support transparency)\n                im = im.convert(\"RGB\")\n\n                # Save the image as JPEG",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "729",
    "name": "download_photos_by_user.py",
    "path": "02_media_processing/image_tools/download_photos_by_user.py",
    "category": "02_media_processing",
    "type": "youtube",
    "lines": 27,
    "size": 546,
    "docstring": "instabot example\n\nWorkflow:\n    Download the specified user's medias",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "argparse",
      "os",
      "sys",
      "instabot"
    ],
    "preview": "\"\"\"\ninstabot example\n\nWorkflow:\n    Download the specified user's medias\n\n\"\"\"\n\nimport argparse\nimport os\nimport sys\n\nsys.path.append(os.path.join(sys.path[0], \"../\"))\nfrom instabot import Bot  # noqa: E402\n\nparser = argparse.ArgumentParser(add_help=True)\nparser.add_argument(\"username\", type=str, help=\"@username\")\nargs = parser.parse_args()\n\nif args.username[0] != \"@\":  # if first character isn't \"@\"",
    "last_modified": "2025-03-28T18:35:47"
  },
  {
    "id": "730",
    "name": "savify.py",
    "path": "02_media_processing/image_tools/savify.py",
    "category": "02_media_processing",
    "type": "video_processing",
    "lines": 393,
    "size": 12897,
    "docstring": "Main module for Savify.",
    "keywords": [
      "youtube",
      "web_tools"
    ],
    "functions": [
      "_sort_dir",
      "_progress",
      "__init__",
      "check_for_updates",
      "_parse_query",
      "download",
      "_download"
    ],
    "classes": [
      "Savify"
    ],
    "imports": [
      "time",
      "multiprocessing",
      "multiprocessing.dummy",
      "pathlib",
      "shutil",
      "shutil",
      "urllib.error",
      "requests",
      "tldextract",
      "validators"
    ],
    "preview": "\"\"\"Main module for Savify.\"\"\"\n\n__all__ = [\"Savify\"]\n\nimport time\nfrom multiprocessing import cpu_count\nfrom multiprocessing.dummy import Pool as ThreadPool\nfrom pathlib import Path\nfrom shutil import Error as ShutilError\nfrom shutil import move\nfrom urllib.error import URLError\n\nimport requests\nimport tldextract\nimport validators\nfrom ffmpy import FFmpeg, FFRuntimeError\nfrom youtube_dl import YoutubeDL\n\nfrom .exceptions import (\n    FFmpegNotInstalledError,",
    "last_modified": "2025-09-13T05:55:15.753381"
  },
  {
    "id": "731",
    "name": "upscale.py",
    "path": "02_media_processing/image_tools/upscale.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 55,
    "size": 2017,
    "docstring": "",
    "keywords": [],
    "functions": [
      "upscale_image"
    ],
    "classes": [],
    "imports": [
      "os",
      "PIL",
      "tqdm"
    ],
    "preview": "import os\n\nfrom PIL import Image\nfrom tqdm import tqdm\n\n\n# Function to upscale an image by 2x and set resolution to 300 DPI\ndef upscale_image(input_path, output_path):\n    \"\"\"\n    Upscale an image by 2x and set resolution to 300 DPI.\n\n    Args:\n        input_path (str): Path to the input image file.\n        output_path (str): Path to save the upscaled image.\n\n    Returns:\n        None\n    \"\"\"\n    image = Image.open(input_path)\n    upscaled_image = image.resize((image.width * 2, image.height * 2), Image.BICUBIC)",
    "last_modified": "2025-05-04T22:47:13.362289"
  },
  {
    "id": "732",
    "name": "motion-upload (1).py",
    "path": "02_media_processing/image_tools/motion-upload (1).py",
    "category": "02_media_processing",
    "type": "youtube",
    "lines": 57,
    "size": 1679,
    "docstring": "",
    "keywords": [
      "web_tools"
    ],
    "functions": [],
    "classes": [],
    "imports": [
      "json",
      "time",
      "requests"
    ],
    "preview": "import json\nimport time\n\nimport requests\n\napi_key = \"<YOUR_API_KEY>\"\nauthorization = \"Bearer %s\" % api_key\n\nheaders = {\n    \"accept\": \"application/json\",\n    \"content-type\": \"application/json\",\n    \"authorization\": authorization,\n}\n\n# Get a presigned URL for uploading an image\nurl = \"https://cloud.leonardo.ai/api/rest/v1/init-image\"\n\npayload = {\"extension\": \"jpg\"}\n\nresponse = requests.post(url, json=payload, headers=headers)",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "733",
    "name": "bing.py",
    "path": "02_media_processing/image_tools/bing.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 87,
    "size": 2594,
    "docstring": "",
    "keywords": [
      "web_tools"
    ],
    "functions": [
      "get_soup",
      "get_images"
    ],
    "classes": [],
    "imports": [
      "http.cookiejar",
      "json",
      "logging",
      "os",
      "re",
      "sys",
      "urllib.error",
      "urllib.parse",
      "urllib.request",
      "requests"
    ],
    "preview": "#!/usr/bin/env python3\nimport http.cookiejar\nimport json\nimport logging\nimport os\nimport re\nimport sys\nimport urllib.error\nimport urllib.parse\nimport urllib.request\n\nimport requests\nfrom bs4 import BeautifulSoup\n\nlogging.basicConfig(\n    format=\"%(asctime)s %(levelname)-8s %(message)s\",\n    level=logging.INFO,\n    datefmt=\"%Y-%m-%d %H:%M:%S\",\n    handlers=[logging.FileHandler(\"debug.log\"), logging.StreamHandler()],\n)",
    "last_modified": "2025-09-13T05:53:29.237077"
  },
  {
    "id": "734",
    "name": "thumbnail.py",
    "path": "02_media_processing/image_tools/thumbnail.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 38,
    "size": 1435,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "create_thumbnail"
    ],
    "classes": [],
    "imports": [
      "PIL"
    ],
    "preview": "from PIL import ImageDraw, ImageFont\n\n\ndef create_thumbnail(thumbnail, font_family, font_size, font_color, width, height, title):\n    font = ImageFont.truetype(font_family + \".ttf\", font_size)\n    Xaxis = width - (width * 0.2)  # 20% of the width\n    sizeLetterXaxis = font_size * 0.5  # 50% of the font size\n    XaxisLetterQty = round(\n        Xaxis / sizeLetterXaxis\n    )  # Quantity of letters that can fit in the X axis\n    MarginYaxis = height * 0.12  # 12% of the height\n    MarginXaxis = width * 0.05  # 5% of the width\n    # 1.1 rem\n    LineHeight = font_size * 1.1\n    # rgb = \"255,255,255\" transform to list\n    rgb = font_color.split(\",\")\n    rgb = (int(rgb[0]), int(rgb[1]), int(rgb[2]))\n\n    arrayTitle = []\n    for word in title.split():",
    "last_modified": "2025-09-13T05:54:00.502218"
  },
  {
    "id": "735",
    "name": "upscale copy.py",
    "path": "02_media_processing/image_tools/upscale copy.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 57,
    "size": 2021,
    "docstring": "",
    "keywords": [],
    "functions": [
      "upscale_image"
    ],
    "classes": [],
    "imports": [
      "os",
      "PIL",
      "tqdm"
    ],
    "preview": "import os\n\nfrom PIL import Image\nfrom tqdm import tqdm\n\n# Function to upscale an image by 2x and set resolution to 300 DPI\n\n\ndef upscale_image(input_path, output_path):\n    \"\"\"\n    Upscale an image by 2x and set resolution to 300 DPI.\n\n    Args:\n        input_path (str): Path to the input image file.\n        output_path (str): Path to save the upscaled image.\n\n    Returns:\n        None\n    \"\"\"\n    image = Image.open(input_path)",
    "last_modified": "2025-05-04T22:47:13.381792"
  },
  {
    "id": "736",
    "name": "bot_direct.py",
    "path": "02_media_processing/image_tools/bot_direct.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 231,
    "size": 7591,
    "docstring": "",
    "keywords": [],
    "functions": [
      "send_message",
      "send_messages",
      "send_media",
      "send_medias",
      "send_hashtag",
      "send_profile",
      "send_like",
      "send_photo",
      "_get_user_ids",
      "approve_pending_thread_requests"
    ],
    "classes": [],
    "imports": [
      "os",
      "mimetypes",
      "tqdm"
    ],
    "preview": "import os\nfrom mimetypes import guess_type\n\nfrom tqdm import tqdm\n\n\ndef send_message(self, text, user_ids, thread_id=None):\n    \"\"\"\n    :param self: bot\n    :param text: text of message\n    :param user_ids: list of user_ids for creating group or\n    one user_id for send to one person\n    :param thread_id: thread_id\n    \"\"\"\n    user_ids = _get_user_ids(self, user_ids)\n    if not isinstance(text, str) and isinstance(user_ids, (list, str)):\n        self.logger.error(\"Text must be an string, user_ids must be an list or string\")\n        return False\n\n    if self.reached_limit(\"messages\"):",
    "last_modified": "2025-09-13T05:54:57.345261"
  },
  {
    "id": "737",
    "name": "configHandler.py",
    "path": "02_media_processing/image_tools/configHandler.py",
    "category": "02_media_processing",
    "type": "setup",
    "lines": 347,
    "size": 8298,
    "docstring": "",
    "keywords": [],
    "functions": [
      "config_init",
      "get_output_title",
      "get_cmd_only",
      "get_out_path",
      "get_intro_slide",
      "get_intro_time",
      "get_intro_font_name",
      "get_intro_text_ratio",
      "get_intro_custom_bg",
      "get_intro_bg_name"
    ],
    "classes": [],
    "imports": [
      "os",
      "configparser",
      "cmd_logs"
    ],
    "preview": "#!/usr/bin/env python\nimport os\nfrom configparser import ConfigParser\n\nfrom .cmd_logs import *\n\nPATH = \"./res/config.ini\"\n\n\ndef config_init(bypass: bool = False, verbose: bool = False) -> None:\n    # Initialize Config file\n    # If bypass=True set config file to default values\n    global PATH\n    if os.path.isfile(PATH) and not bypass:\n        if verbose:\n            info(\"Config file already exists, add argument bypass=True to overwrite it\")\n        return\n    config_object = ConfigParser()\n    config_object[\"OUTPUT\"] = {\n        \"title\": \"clipsMontage\",",
    "last_modified": "2025-03-28T18:37:01"
  },
  {
    "id": "738",
    "name": "ups.py",
    "path": "02_media_processing/image_tools/ups.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 68,
    "size": 2552,
    "docstring": "",
    "keywords": [],
    "functions": [
      "convert_and_upscale_images",
      "main"
    ],
    "classes": [],
    "imports": [
      "os",
      "PIL"
    ],
    "preview": "import os\n\nfrom PIL import Image\n\n\ndef convert_and_upscale_images(source_directory, destination_directory):\n    # Create the destination directory if it doesn't exist\n    os.makedirs(destination_directory, exist_ok=True)\n\n    for root, dirs, files in os.walk(source_directory):\n        # Replicate the directory structure in the destination\n        relative_path = os.path.relpath(root, source_directory)\n        dest_dir = os.path.join(destination_directory, relative_path)\n        os.makedirs(dest_dir, exist_ok=True)\n\n        for filename in files:\n            if filename.endswith((\".tiff\", \".png\", \".jpg\", \".jpeg\")):\n                source_file = os.path.join(root, filename)\n                filename_no_ext, file_ext = os.path.splitext(filename)\n                file_ext = file_ext.lower()",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "739",
    "name": "playing_around_with_quota_supervisor.py",
    "path": "02_media_processing/image_tools/playing_around_with_quota_supervisor.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 152,
    "size": 4076,
    "docstring": "This template is written by @boldestfortune\n\nWhat does this quickstart script aim to do?\n- Just started playing around with Quota Supervisor, so I'm still tweaking\nthese settings",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "random",
      "instapy"
    ],
    "preview": "\"\"\"\nThis template is written by @boldestfortune\n\nWhat does this quickstart script aim to do?\n- Just started playing around with Quota Supervisor, so I'm still tweaking\nthese settings\n\"\"\"\n\nimport random\n\nfrom instapy import InstaPy, smart_run\n\n# get a session!\nsession = InstaPy(username=\"\", password=\"\")\n\n# let's go! :>\nwith smart_run(session):\n    # general settings\n    session.set_quota_supervisor(\n        enabled=True,",
    "last_modified": "2025-03-28T18:36:56"
  },
  {
    "id": "740",
    "name": "api_story.py",
    "path": "02_media_processing/image_tools/api_story.py",
    "category": "02_media_processing",
    "type": "web_tools",
    "lines": 92,
    "size": 2937,
    "docstring": "",
    "keywords": [
      "youtube",
      "web_tools"
    ],
    "functions": [
      "download_story",
      "upload_story_photo",
      "configure_story"
    ],
    "classes": [],
    "imports": [
      "__future__",
      "json",
      "os",
      "shutil",
      "time",
      "random",
      "requests_toolbelt",
      "api_photo"
    ],
    "preview": "from __future__ import unicode_literals\n\nimport json\nimport os\nimport shutil\nimport time\nfrom random import randint\n\nfrom requests_toolbelt import MultipartEncoder\n\nfrom . import config\nfrom .api_photo import get_image_size, stories_shaper\n\n\ndef download_story(self, filename, story_url, username):\n    path = \"stories/{}\".format(username)\n    if not os.path.exists(path):\n        os.makedirs(path)\n    fname = os.path.join(path, filename)\n    if os.path.exists(fname):  # already exists",
    "last_modified": "2025-09-13T05:54:56.636685"
  },
  {
    "id": "741",
    "name": "leo-url-downloader.py",
    "path": "02_media_processing/image_tools/leo-url-downloader.py",
    "category": "02_media_processing",
    "type": "youtube",
    "lines": 38,
    "size": 1263,
    "docstring": "",
    "keywords": [
      "youtube",
      "web_tools"
    ],
    "functions": [
      "download_images"
    ],
    "classes": [],
    "imports": [
      "os",
      "requests",
      "colorama",
      "tqdm"
    ],
    "preview": "import os\n\nimport requests\nfrom colorama import Fore, Style\nfrom tqdm import tqdm\n\n\ndef download_images(source_file, destination_dir):\n    # Ensure destination directory exists\n    os.makedirs(destination_dir, exist_ok=True)\n\n    # Read URLs from the source file\n    with open(source_file, \"r\") as file:\n        urls = [line.strip() for line in file if line.strip()]\n\n    # Download each image with a progress bar\n    for url in tqdm(\n        urls,\n        desc=\"Downloading\",\n        bar_format=\"{l_bar}\" + Fore.GREEN + \"{bar}\" + Style.RESET_ALL + \"{r_bar}\",",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "742",
    "name": "format.py",
    "path": "02_media_processing/image_tools/format.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 29,
    "size": 742,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "csv",
      "json",
      "os"
    ],
    "preview": "import csv\nimport json\nimport os\n\n# Define the input CSV file\ncsv_file = \"/Users/steven/Pictures/DaLLe/csv-test/Etsy-12-2-2024.csv\"\n\n# Generate JSON file path based on the CSV file\njson_file = os.path.splitext(csv_file)[0] + \".json\"\n\n# Read the CSV and convert to JSON\ndata = []\ntry:\n    with open(csv_file, \"r\") as f:\n        reader = csv.DictReader(f)\n        for row in reader:\n            data.append(row)\nexcept FileNotFoundError:\n    print(f\"Error: The file {csv_file} was not found.\")\n    exit(1)",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "743",
    "name": "stylish_unfollow_tips_and_like_by_tags.py",
    "path": "02_media_processing/image_tools/stylish_unfollow_tips_and_like_by_tags.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 138,
    "size": 4387,
    "docstring": "This template is written by @Nocturnal-2\n\nWhat does this quickstart script aim to do?\n- I do some unfollow and like by tags mostly\n\nNOTES:\n- I am an one month old InstaPy user, with a small following. So my numbers\nin settings are bit conservative.",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "instapy"
    ],
    "preview": "\"\"\"\nThis template is written by @Nocturnal-2\n\nWhat does this quickstart script aim to do?\n- I do some unfollow and like by tags mostly\n\nNOTES:\n- I am an one month old InstaPy user, with a small following. So my numbers\nin settings are bit conservative.\n\"\"\"\n\nfrom instapy import InstaPy, smart_run\n\n# get a session!\nsession = InstaPy(username=\"\", password=\"\")\n\n# let's go! :>\nwith smart_run(session):\n    \"\"\"Start of parameter setting\"\"\"\n    # don't like if a post already has more than 150 likes",
    "last_modified": "2025-03-28T18:36:56"
  },
  {
    "id": "744",
    "name": "botLike.py",
    "path": "02_media_processing/image_tools/botLike.py",
    "category": "02_media_processing",
    "type": "web_tools",
    "lines": 196,
    "size": 5434,
    "docstring": "Created in 12/2019\n@Author: Paulo https://github.com/alpdias",
    "keywords": [],
    "functions": [
      "functionLike",
      "botlogin",
      "findhashtag",
      "like"
    ],
    "classes": [],
    "imports": [
      "os",
      "random",
      "pathlib",
      "time",
      "art",
      "selenium",
      "selenium.webdriver.common.keys"
    ],
    "preview": "# -*- coding: utf-8 -*-\n\n\"\"\"\nCreated in 12/2019\n@Author: Paulo https://github.com/alpdias\n\"\"\"\n\n# imported libraries\nimport os\nimport random\nfrom pathlib import Path\nfrom time import sleep\n\nimport art\nfrom selenium import webdriver\nfrom selenium.webdriver.common.keys import Keys\n\n\ndef functionLike(mySystem):\n    \"\"\"",
    "last_modified": "2025-09-13T05:54:07.011516"
  },
  {
    "id": "745",
    "name": "playlist_20221230180508.py",
    "path": "02_media_processing/image_tools/playlist_20221230180508.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 167,
    "size": 6416,
    "docstring": "",
    "keywords": [
      "youtube",
      "analysis",
      "web_tools"
    ],
    "functions": [
      "generate_token",
      "get_youtube_url",
      "download_mp3",
      "last_fm_artist_info",
      "set_metadata",
      "write_tracks",
      "write_playlist",
      "split_spotify_uri",
      "get_os",
      "get_folder"
    ],
    "classes": [],
    "imports": [
      "argparse",
      "json",
      "os",
      "re",
      "subprocess",
      "urllib.parse",
      "urllib.request",
      "secret",
      "spotipy",
      "spotipy.oauth2"
    ],
    "preview": "import argparse\nimport json\nimport os\nimport re\nimport subprocess\nimport urllib.parse\nimport urllib.request\n\nimport secret\nimport spotipy\nimport spotipy.oauth2 as oauth2\nfrom requests.utils import quote\n\n\ndef generate_token():\n    credentials = oauth2.SpotifyClientCredentials(\n        client_id=secret.28b20556906f4b75874c4ae98320c81d,\n        client_secret=secret.SPOTIFY_CLIENT_SECRET)\n    token = credentials.get_access_token()\n    return token",
    "last_modified": "2025-05-04T23:28:21.387315"
  },
  {
    "id": "746",
    "name": "leocsv.py",
    "path": "02_media_processing/image_tools/leocsv.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 186,
    "size": 6195,
    "docstring": "",
    "keywords": [
      "youtube",
      "web_tools"
    ],
    "functions": [
      "convert_image_to_jpeg",
      "get_presigned_url",
      "upload_image",
      "upscale_image",
      "get_upscaled_image",
      "initialize_csv",
      "log_to_csv"
    ],
    "classes": [],
    "imports": [
      "csv",
      "json",
      "os",
      "time",
      "datetime",
      "requests",
      "PIL"
    ],
    "preview": "import csv\nimport json\nimport os\nimport time\nfrom datetime import datetime\n\nimport requests\nfrom PIL import Image\n\napi_key = \"de7c9cb8-022f-42f8-8bf7-a8f9caadfaee\"\nauthorization = f\"Bearer {api_key}\"\n\nheaders = {\n    \"accept\": \"application/json\",\n    \"content-type\": \"application/json\",\n    \"authorization\": authorization,\n}\n\n# Directory containing images\ndirectory_path = \"/Users/steven/Pictures/TrashCaT/trashy-heartbreak\"",
    "last_modified": "2025-09-13T05:53:50.187982"
  },
  {
    "id": "747",
    "name": "html-auto-img-gallery.py",
    "path": "02_media_processing/image_tools/html-auto-img-gallery.py",
    "category": "02_media_processing",
    "type": "image_processing",
    "lines": 81,
    "size": 2230,
    "docstring": "",
    "keywords": [],
    "functions": [
      "csv_to_html"
    ],
    "classes": [],
    "imports": [
      "csv"
    ],
    "preview": "import csv\n\n\ndef csv_to_html(csv_file, output_html):\n    # Start the HTML structure\n    html_content = \"\"\"\n    <!DOCTYPE html>\n    <html lang=\"en\">\n    <head>\n        <meta charset=\"UTF-8\">\n        <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n        <title>Image Gallery</title>\n        <style>\n            body {\n                font-family: Arial, sans-serif;\n                background-color: #f0f0f0;\n                padding: 20px;\n            }\n            h1 {\n                text-align: center;",
    "last_modified": "2025-09-13T05:53:55.182254"
  },
  {
    "id": "748",
    "name": "upscale-sub.py",
    "path": "02_media_processing/image_tools/upscale-sub.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 68,
    "size": 2591,
    "docstring": "",
    "keywords": [],
    "functions": [
      "convert_and_upscale_images",
      "main"
    ],
    "classes": [],
    "imports": [
      "os",
      "PIL"
    ],
    "preview": "import os\n\nfrom PIL import Image\n\n\ndef convert_and_upscale_images(source_directory, destination_directory):\n    # Create the destination directory if it doesn't exist\n    os.makedirs(destination_directory, exist_ok=True)\n\n    for root, dirs, files in os.walk(source_directory):\n        # Replicate the directory structure in the destination\n        relative_path = os.path.relpath(root, source_directory)\n        dest_dir = os.path.join(destination_directory, relative_path)\n        os.makedirs(dest_dir, exist_ok=True)\n\n        for filename in files:\n            if filename.endswith((\".tiff\", \".png\", \".jpg\", \".jpeg\")):\n                source_file = os.path.join(root, filename)\n                filename_no_ext, file_ext = os.path.splitext(filename)\n                file_ext = file_ext.lower()",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "749",
    "name": "convert_to_audio.py",
    "path": "02_media_processing/image_tools/convert_to_audio.py",
    "category": "02_media_processing",
    "type": "transcription",
    "lines": 143,
    "size": 5501,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "logging",
      "os",
      "time",
      "pyrogram",
      "translation",
      "hachoir.metadata",
      "hachoir.parser",
      "helper_funcs.chat_base",
      "helper_funcs.display_progress",
      "PIL"
    ],
    "preview": "import logging\n\nlogging.basicConfig(\n    level=logging.DEBUG, format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\n)\nlogger = logging.getLogger(__name__)\n\nimport os\nimport time\n\n# the secret configuration specific things\nif bool(os.environ.get(\"WEBHOOK\", False)):\n    from sample_config import Config\nelse:\n    from config import Config\n\n# the Strings used for this \"thing\"\nimport pyrogram\nfrom translation import Translation\n",
    "last_modified": "2025-09-13T05:53:43.718276"
  },
  {
    "id": "750",
    "name": "img2img.py",
    "path": "02_media_processing/image_tools/img2img.py",
    "category": "02_media_processing",
    "type": "image_processing",
    "lines": 47,
    "size": 1754,
    "docstring": "",
    "keywords": [],
    "functions": [
      "convert_and_upscale_images",
      "main"
    ],
    "classes": [],
    "imports": [
      "os",
      "PIL"
    ],
    "preview": "import os\n\nfrom PIL import Image\n\n\ndef convert_and_upscale_images(source_directory, destination_directory):\n    os.makedirs(destination_directory, exist_ok=True)\n\n    for filename in os.listdir(source_directory):\n        if filename.endswith(\".jpg\"):\n            source_file = os.path.join(source_directory, filename)\n            filename_no_ext = os.path.splitext(filename)[0]\n            destination_file = os.path.join(destination_directory, f\"{filename_no_ext}.jpg\")\n\n            im = Image.open(source_file)\n            width, height = im.size\n            upscale_width = width * 2\n            upscale_height = height * 2\n            im_resized = im.resize((upscale_width, upscale_height))\n",
    "last_modified": "2025-09-13T05:55:28.400073"
  },
  {
    "id": "751",
    "name": "imagenarator.py",
    "path": "02_media_processing/image_tools/imagenarator.py",
    "category": "02_media_processing",
    "type": "image_processing",
    "lines": 86,
    "size": 2942,
    "docstring": "",
    "keywords": [
      "image_processing"
    ],
    "functions": [
      "draw_multiple_line_text",
      "imagemaker"
    ],
    "classes": [],
    "imports": [
      "os",
      "re",
      "textwrap",
      "PIL",
      "rich.progress",
      "TTS.engine_wrapper"
    ],
    "preview": "import os\nimport re\nimport textwrap\n\nfrom PIL import Image, ImageDraw, ImageFont\nfrom rich.progress import track\nfrom TTS.engine_wrapper import process_text\n\n\ndef draw_multiple_line_text(\n    image, text, font, text_color, padding, wrap=50, transparent=False\n) -> None:\n    \"\"\"\n    Draw multiline text over given image\n    \"\"\"\n    draw = ImageDraw.Draw(image)\n    Fontperm = font.getsize(text)\n    image_width, image_height = image.size\n    lines = textwrap.wrap(text, width=wrap)\n    y = (image_height / 2) - (",
    "last_modified": "2025-09-13T05:54:00.322776"
  },
  {
    "id": "752",
    "name": "leodown.py",
    "path": "02_media_processing/image_tools/leodown.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 51,
    "size": 1326,
    "docstring": "",
    "keywords": [
      "web_tools"
    ],
    "functions": [],
    "classes": [],
    "imports": [
      "json",
      "requests"
    ],
    "preview": "import json\n\nimport requests\n\n# Base URL for API\nbase_url = (\n    \"https://cloud.leonardo.ai/api/rest/v1/generations/user/f7bb8476-e3f0-4f1f-9a06-4600866cc49c\"\n)\nheaders = {\n    \"accept\": \"application/json\",\n    \"authorization\": \"Bearer 93043291-957d-4ec1-8c79-ee734abcb6e3\",  # Replace with your actual token\n}\n\n# Output file\noutput_file = \"/Users/steven/Pictures/leonardo_library.json\"\n\n# Pagination parameters\noffset = 0\nlimit = 10  # Maximum number of items per request\nall_generations = []",
    "last_modified": "2025-09-13T05:53:50.205950"
  },
  {
    "id": "753",
    "name": "basic_follow-unfollow_activity.py",
    "path": "02_media_processing/image_tools/basic_follow-unfollow_activity.py",
    "category": "02_media_processing",
    "type": "organization",
    "lines": 108,
    "size": 3186,
    "docstring": "This template is written by @cormo1990\n\nWhat does this quickstart script aim to do?\n- Basic follow/unfollow activity.\n\nNOTES:\n- I don't want to automate comment and too much likes because I want to do\nthis only for post that I really like the content so at the moment I only\nuse the function follow/unfollow.\n- I use two files \"quickstart\", one for follow and one for unfollow.\n- I noticed that the most important thing is that the account from where I\nget followers has similar contents to mine in order to be sure that my\ncontent could be appreciated. After the following step, I start unfollowing\nthe user that don't followed me back.\n- At the end I clean my account unfollowing all the users followed with\nInstaPy.",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "instapy"
    ],
    "preview": "\"\"\"\nThis template is written by @cormo1990\n\nWhat does this quickstart script aim to do?\n- Basic follow/unfollow activity.\n\nNOTES:\n- I don't want to automate comment and too much likes because I want to do\nthis only for post that I really like the content so at the moment I only\nuse the function follow/unfollow.\n- I use two files \"quickstart\", one for follow and one for unfollow.\n- I noticed that the most important thing is that the account from where I\nget followers has similar contents to mine in order to be sure that my\ncontent could be appreciated. After the following step, I start unfollowing\nthe user that don't followed me back.\n- At the end I clean my account unfollowing all the users followed with\nInstaPy.\n\"\"\"\n\n# imports",
    "last_modified": "2025-09-13T05:53:49.274698"
  },
  {
    "id": "754",
    "name": "bot_story.py",
    "path": "02_media_processing/image_tools/bot_story.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 62,
    "size": 2237,
    "docstring": "",
    "keywords": [
      "youtube"
    ],
    "functions": [
      "download_stories",
      "upload_story_photo",
      "watch_users_reels"
    ],
    "classes": [],
    "imports": [],
    "preview": "def download_stories(self, username):\n    user_id = self.get_user_id_from_username(username)\n    list_image, list_video = self.get_user_stories(user_id)\n    if list_image == [] and list_video == []:\n        self.logger.error(\n            (\"Make sure that '{}' is NOT private and that \" \"posted some stories\").format(username)\n        )\n        return False\n    self.logger.info(\"Downloading stories...\")\n    for story_url in list_image:\n        filename = story_url.split(\"/\")[-1].split(\".\")[0] + \".jpg\"\n        self.api.download_story(filename, story_url, username)\n    for story_url in list_video:\n        filename = story_url.split(\"/\")[-1].split(\".\")[0] + \".mp4\"\n        self.api.download_story(filename, story_url, username)\n\n\ndef upload_story_photo(self, photo, upload_id=None):\n    self.small_delay()\n    if self.api.upload_story_photo(photo, upload_id):",
    "last_modified": "2025-09-13T05:54:57.992603"
  },
  {
    "id": "755",
    "name": "organize_albums 6.py",
    "path": "02_media_processing/image_tools/organize_albums 6.py",
    "category": "02_media_processing",
    "type": "organization",
    "lines": 62,
    "size": 2470,
    "docstring": "",
    "keywords": [
      "organization"
    ],
    "functions": [
      "organize_files"
    ],
    "classes": [],
    "imports": [
      "os",
      "shutil"
    ],
    "preview": "import os\nimport shutil\n\n# Define the base directory\nbase_dir = \"/Users/steven/Music/NocTurnE-meLoDieS/albums/\"\n\n\n# Helper function to create folders and move files\ndef organize_files():\n    # List all files in the base directory\n    files = os.listdir(base_dir)\n\n    # Process each file\n    for file in files:\n        # Skip directories\n        if os.path.isdir(os.path.join(base_dir, file)):\n            continue\n\n        # Extract the base name (album name) from the file\n        if file.endswith(\".mp3\"):",
    "last_modified": "2025-05-04T22:47:11"
  },
  {
    "id": "756",
    "name": "leo.py",
    "path": "02_media_processing/image_tools/leo.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 26,
    "size": 1313,
    "docstring": "",
    "keywords": [
      "web_tools"
    ],
    "functions": [],
    "classes": [],
    "imports": [
      "requests"
    ],
    "preview": "import requests\n\n# API URL\nurl = \"https://cloud.leonardo.ai/api/rest/v1/generations\"\n\n# Payload for the POST request\npayload = {\n    \"height\": 512,\n    \"modelId\": \"6bef9f1b-29cb-40c7-b9df-32b51c1f67d3\",\n    \"prompt\": \"Create a vibrant and lively image featuring a front-facing, centered group of the CoTTonWooDs, the adorable inhabitants of Itchy Isle. These fluffy creatures should be depicted in a range of pastel colors, each exuding unique personality and joy. Imagine them with soft, cotton-like textures, big, sparkling eyes full of wonder, and wide, beaming smiles. They should have playful tufts of hair styled in whimsical ways, resembling colorful strands of yarn. The background should be a cheerful blur of Itchy Isle's signature landmarks - macrame houses, yarn trees, and a rainbow-streaked sky. The overall atmosphere of the image should be one of happiness, friendship, and the magical essence of a carefree, enchanting world.\",\n    \"width\": 512,\n}\n\n# Headers for the POST request\nheaders = {\n    \"accept\": \"application/json\",\n    \"content-type\": \"application/json\",\n    \"authorization\": \"Bearer 7ccf0307-636e-4334-9a61-814202374698\",\n}\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "757",
    "name": "latin1prober.py",
    "path": "02_media_processing/image_tools/latin1prober.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 146,
    "size": 5356,
    "docstring": "",
    "keywords": [],
    "functions": [
      "__init__",
      "reset",
      "charset_name",
      "language",
      "feed",
      "get_confidence"
    ],
    "classes": [
      "Latin1Prober"
    ],
    "imports": [
      "typing",
      "charsetprober",
      "enums"
    ],
    "preview": "######################## BEGIN LICENSE BLOCK ########################\n# The Original Code is Mozilla Universal charset detector code.\n#\n# The Initial Developer of the Original Code is\n# Netscape Communications Corporation.\n# Portions created by the Initial Developer are Copyright (C) 2001\n# the Initial Developer. All Rights Reserved.\n#\n# Contributor(s):\n#   Mark Pilgrim - port to Python\n#   Shy Shalom - original C code\n#\n# This library is free software; you can redistribute it and/or\n# modify it under the terms of the GNU Lesser General Public\n# License as published by the Free Software Foundation; either\n# version 2.1 of the License, or (at your option) any later version.\n#\n# This library is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "758",
    "name": "like_by_tag_interact_unfollow.py",
    "path": "02_media_processing/image_tools/like_by_tag_interact_unfollow.py",
    "category": "02_media_processing",
    "type": "image_processing",
    "lines": 146,
    "size": 2994,
    "docstring": "This template is written by @timgrossmann\n\nWhat does this quickstart script aim to do?\n- This script is automatically executed every 6h on my server via cron",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "random",
      "instapy"
    ],
    "preview": "\"\"\"\nThis template is written by @timgrossmann\n\nWhat does this quickstart script aim to do?\n- This script is automatically executed every 6h on my server via cron\n\"\"\"\n\nimport random\n\nfrom instapy import InstaPy, smart_run\n\n# login credentials\ninsta_username = \"\"\ninsta_password = \"\"\n\ndont_likes = [\n    \"sex\",\n    \"nude\",\n    \"naked\",\n    \"beef\",",
    "last_modified": "2025-09-13T05:53:49.464857"
  },
  {
    "id": "759",
    "name": "ultimate.py",
    "path": "02_media_processing/image_tools/ultimate.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 144,
    "size": 4897,
    "docstring": "",
    "keywords": [
      "youtube"
    ],
    "functions": [
      "stats",
      "like_hashtags",
      "like_timeline",
      "like_followers_from_random_user_file",
      "follow_followers",
      "comment_medias",
      "unfollow_non_followers",
      "follow_users_from_hashtag_file",
      "comment_hashtag",
      "upload_pictures"
    ],
    "classes": [],
    "imports": [
      "argparse",
      "os",
      "sys",
      "threading",
      "time",
      "glob",
      "config",
      "schedule",
      "instabot"
    ],
    "preview": "# -*- coding: utf-8 -*-\n\nimport argparse\nimport os\nimport sys\nimport threading\nimport time\nfrom glob import glob\n\nimport config\n\nsys.path.append(os.path.join(sys.path[0], \"../../\"))\nimport schedule  # noqa: E402\nfrom instabot import Bot, utils  # noqa: E402\n\nbot = Bot(\n    comments_file=config.COMMENTS_FILE,\n    blacklist_file=config.BLACKLIST_FILE,\n    whitelist_file=config.WHITELIST_FILE,\n    friends_file=config.FRIENDS_FILE,",
    "last_modified": "2025-09-13T05:54:55.924954"
  },
  {
    "id": "760",
    "name": "upscale 2.py",
    "path": "02_media_processing/image_tools/upscale 2.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 76,
    "size": 1849,
    "docstring": "",
    "keywords": [
      "web_tools"
    ],
    "functions": [],
    "classes": [],
    "imports": [
      "json",
      "os",
      "time",
      "requests"
    ],
    "preview": "import json\nimport os\nimport time\n\nimport requests\n\napi_key = os.getenv(\"API_KEY\")\nif not api_key:\n    raise ValueError(\n        \"API key is not set. Please ensure the API_KEY environment variable is configured correctly.\"\n    )\n\nauthorization = f\"Bearer {api_key}\"\n\nheaders = {\n    \"accept\": \"application/json\",\n    \"content-type\": \"application/json\",\n    \"authorization\": authorization,\n}\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "761",
    "name": "charsetprober.py",
    "path": "02_media_processing/image_tools/charsetprober.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 146,
    "size": 5414,
    "docstring": "",
    "keywords": [],
    "functions": [
      "__init__",
      "reset",
      "charset_name",
      "language",
      "feed",
      "state",
      "get_confidence",
      "filter_high_byte_only",
      "filter_international_words",
      "remove_xml_tags"
    ],
    "classes": [
      "CharSetProber"
    ],
    "imports": [
      "logging",
      "re",
      "typing",
      "enums"
    ],
    "preview": "######################## BEGIN LICENSE BLOCK ########################\n# The Original Code is Mozilla Universal charset detector code.\n#\n# The Initial Developer of the Original Code is\n# Netscape Communications Corporation.\n# Portions created by the Initial Developer are Copyright (C) 2001\n# the Initial Developer. All Rights Reserved.\n#\n# Contributor(s):\n#   Mark Pilgrim - port to Python\n#   Shy Shalom - original C code\n#\n# This library is free software; you can redistribute it and/or\n# modify it under the terms of the GNU Lesser General Public\n# License as published by the Free Software Foundation; either\n# version 2.1 of the License, or (at your option) any later version.\n#\n# This library is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "762",
    "name": "upscale--.py",
    "path": "02_media_processing/image_tools/upscale--.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 55,
    "size": 2017,
    "docstring": "",
    "keywords": [],
    "functions": [
      "upscale_image"
    ],
    "classes": [],
    "imports": [
      "os",
      "PIL",
      "tqdm"
    ],
    "preview": "import os\n\nfrom PIL import Image\nfrom tqdm import tqdm\n\n\n# Function to upscale an image by 2x and set resolution to 300 DPI\ndef upscale_image(input_path, output_path):\n    \"\"\"\n    Upscale an image by 2x and set resolution to 300 DPI.\n\n    Args:\n        input_path (str): Path to the input image file.\n        output_path (str): Path to save the upscaled image.\n\n    Returns:\n        None\n    \"\"\"\n    image = Image.open(input_path)\n    upscaled_image = image.resize((image.width * 2, image.height * 2), Image.BICUBIC)",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "763",
    "name": "botComment.py",
    "path": "02_media_processing/image_tools/botComment.py",
    "category": "02_media_processing",
    "type": "web_tools",
    "lines": 224,
    "size": 6739,
    "docstring": "Created in 12/2019\n@Author: Paulo https://github.com/alpdias",
    "keywords": [],
    "functions": [
      "functionComment",
      "botlogin",
      "findhashtag",
      "typephrase",
      "likecomment"
    ],
    "classes": [],
    "imports": [
      "os",
      "random",
      "pathlib",
      "time",
      "art",
      "selenium",
      "selenium.webdriver.common.keys"
    ],
    "preview": "# -*- coding: utf-8 -*-\n\n\"\"\"\nCreated in 12/2019\n@Author: Paulo https://github.com/alpdias\n\"\"\"\n\n# imported libraries\nimport os\nimport random\nfrom pathlib import Path\nfrom time import sleep\n\nimport art\nfrom selenium import webdriver\nfrom selenium.webdriver.common.keys import Keys\n\n\ndef functionComment(mySystem):\n    \"\"\"",
    "last_modified": "2025-09-13T05:54:06.694749"
  },
  {
    "id": "764",
    "name": "img.py",
    "path": "02_media_processing/image_tools/img.py",
    "category": "02_media_processing",
    "type": "image_processing",
    "lines": 48,
    "size": 1316,
    "docstring": "",
    "keywords": [],
    "functions": [
      "is_system_path"
    ],
    "classes": [],
    "imports": [
      "datetime",
      "os"
    ],
    "preview": "import datetime\nimport os\n\n\ndef is_system_path(path):\n    system_paths = [\n        \"~/Desktop\",\n        \"/System\",\n        \"/Documents/Git\" \"/Applications\",\n        \"/Library\",\n        \"/usr\",\n        \"/bin\",\n        \"/sbin\",\n        \"/var\",\n        \"/private\",\n        \"/etc\",\n        \"/tmp\",\n        \"/.\",\n        \"/Python\",\n    ]",
    "last_modified": "2025-09-13T05:54:14.192869"
  },
  {
    "id": "765",
    "name": "names.py",
    "path": "02_media_processing/image_tools/names.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 33,
    "size": 1139,
    "docstring": "",
    "keywords": [
      "web_tools"
    ],
    "functions": [],
    "classes": [],
    "imports": [
      "csv",
      "os",
      "requests",
      "slugify"
    ],
    "preview": "import csv\nimport os\n\nimport requests\nfrom slugify import slugify  # Install with: pip install python-slugify\n\n# Read CSV\nwith open(\"/Users/steven/Pictures/etsy/cookie/combined_csv.csv\", \"r\", encoding=\"utf-8\") as csvfile:\n    reader = csv.DictReader(csvfile)\n    for row in reader:\n        # Extract data\n        file_id = row[\"ID\"]\n        title = row[\"Listing.Title\"]\n        image_url = row[\"Main file_slot_image_url\"]  # Use the relevant URL column\n\n        # Skip rows without an image URL\n        if not image_url.strip():\n            continue\n\n        # Sanitize title for filename",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "766",
    "name": "main_20221230223947.py",
    "path": "02_media_processing/image_tools/main_20221230223947.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 127,
    "size": 3754,
    "docstring": "",
    "keywords": [
      "opencv",
      "analysis",
      "web_tools"
    ],
    "functions": [
      "generate"
    ],
    "classes": [],
    "imports": [
      "os",
      "sys",
      "time",
      "urllib.request",
      "io",
      "chromedriver_autoinstaller",
      "cv2",
      "imutils",
      "requests",
      "bs4"
    ],
    "preview": "import os\nimport sys\nimport time\nimport urllib.request\nfrom io import BytesIO, StringIO\n\n# Optional\nimport chromedriver_autoinstaller\nimport cv2\nimport imutils\nimport requests\nfrom bs4 import BeautifulSoup\nfrom selenium import webdriver\nfrom wand.display import display\nfrom wand.image import Image\n\nchromedriver_autoinstaller.install()\n\nopts = webdriver.ChromeOptions()\nopts.headless = True",
    "last_modified": "2025-09-13T05:54:14.142637"
  },
  {
    "id": "767",
    "name": "web-png-upscale.py",
    "path": "02_media_processing/image_tools/web-png-upscale.py",
    "category": "02_media_processing",
    "type": "web_tools",
    "lines": 42,
    "size": 2008,
    "docstring": "",
    "keywords": [],
    "functions": [
      "convert_and_upscale_images"
    ],
    "classes": [],
    "imports": [
      "os",
      "PIL"
    ],
    "preview": "import os\n\nfrom PIL import Image\n\n\ndef convert_and_upscale_images(directory):\n    # Walk through all directories and files within the specified directory\n    for root, _, files in os.walk(directory):\n        print(f\"Checking directory: {root}\")  # Debug statement\n        for filename in files:\n            print(f\"Found file: {filename}\")  # Debug statement\n            if filename.lower().endswith(\".tiff\") or filename.lower().endswith(\".tif\"):\n                print(f\"Processing file: {filename}\")  # Debug statement\n                # Construct full file path\n                file_path = os.path.join(root, filename)\n                try:\n                    # Open the .tiff image\n                    with Image.open(file_path) as img:\n                        # Upscale the image by 2x\n                        img = img.resize((img.width * 2, img.height * 2), Image.LANCZOS)",
    "last_modified": "2025-09-13T05:55:28.646451"
  },
  {
    "id": "768",
    "name": "attack.py",
    "path": "02_media_processing/image_tools/attack.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 308,
    "size": 9831,
    "docstring": "",
    "keywords": [
      "web_tools"
    ],
    "functions": [
      "random_str",
      "report_profile_attack",
      "report_video_attack"
    ],
    "classes": [],
    "imports": [
      "pprint",
      "random",
      "string",
      "sys",
      "libs.user_agents",
      "libs.utils",
      "requests"
    ],
    "preview": "import pprint\nimport random\nimport string\nfrom sys import exit\n\nfrom libs.user_agents import get_user_agent\nfrom libs.utils import ask_question, parse_proxy_file, print_error, print_status, print_success\nfrom requests import Session\n\npage_headers = {\n    \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\",\n    \"Accept-Encoding\": \"gzip, deflate\",\n    \"Accept-Language\": \"tr-TR,tr;q=0.8,en-US;q=0.5,en;q=0.3\",\n    \"Cache-Control\": \"no-cache\",\n    \"Connection\": \"keep-alive\",\n    \"DNT\": \"1\",\n}\n\nreport_headers = {\n    \"Accept\": \"*/*\",",
    "last_modified": "2025-09-13T05:53:40.882764"
  },
  {
    "id": "769",
    "name": "upscale-sub copy.py",
    "path": "02_media_processing/image_tools/upscale-sub copy.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 68,
    "size": 2591,
    "docstring": "",
    "keywords": [],
    "functions": [
      "convert_and_upscale_images",
      "main"
    ],
    "classes": [],
    "imports": [
      "os",
      "PIL"
    ],
    "preview": "import os\n\nfrom PIL import Image\n\n\ndef convert_and_upscale_images(source_directory, destination_directory):\n    # Create the destination directory if it doesn't exist\n    os.makedirs(destination_directory, exist_ok=True)\n\n    for root, dirs, files in os.walk(source_directory):\n        # Replicate the directory structure in the destination\n        relative_path = os.path.relpath(root, source_directory)\n        dest_dir = os.path.join(destination_directory, relative_path)\n        os.makedirs(dest_dir, exist_ok=True)\n\n        for filename in files:\n            if filename.endswith((\".tiff\", \".png\", \".jpg\", \".jpeg\")):\n                source_file = os.path.join(root, filename)\n                filename_no_ext, file_ext = os.path.splitext(filename)\n                file_ext = file_ext.lower()",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "770",
    "name": "main_20221230234008.py",
    "path": "02_media_processing/image_tools/main_20221230234008.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 429,
    "size": 15518,
    "docstring": "",
    "keywords": [
      "transcription",
      "video_processing",
      "analysis",
      "youtube",
      "web_tools",
      "organization"
    ],
    "functions": [
      "downloadVideo",
      "scrapeVideos",
      "usedQuoteToDifferentFile",
      "getQuoteFromApi",
      "getQuoteFromTxtFile",
      "videoIntro",
      "createVideo",
      "audioClip",
      "randomBgMusic",
      "deleteTempFiles"
    ],
    "classes": [
      "bcolors"
    ],
    "imports": [
      "glob",
      "json",
      "os",
      "random",
      "subprocess",
      "gtts",
      "requests",
      "moviepy.editor",
      "moviepy.video.io.VideoFileClip",
      "mutagen.mp3"
    ],
    "preview": "import glob\nimport json\nimport os\nimport random\nimport subprocess\n\nimport gtts\nimport requests\nfrom moviepy.editor import *\nfrom moviepy.video.io.VideoFileClip import VideoFileClip\nfrom mutagen.mp3 import MP3\nfrom tqdm.auto import tqdm\n\n\n# download background video from pexels - https://www.pexels.com/api/documentation/#videos-search__parameters\ndef downloadVideo(id) -> str:\n    \"\"\"Downloads video from Pexels with the according video ID\"\"\"\n    url = \"https://www.pexels.com/video/\" + str(id) + \"/download.mp4\"\n    # Streaming, so we can iterate over the response.\n    response = requests.get(url, stream=True)",
    "last_modified": "2025-09-13T05:53:57.329705"
  },
  {
    "id": "771",
    "name": "playlist_20221230180628.py",
    "path": "02_media_processing/image_tools/playlist_20221230180628.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 168,
    "size": 6451,
    "docstring": "",
    "keywords": [
      "youtube",
      "analysis",
      "web_tools"
    ],
    "functions": [
      "generate_token",
      "get_youtube_url",
      "download_mp3",
      "last_fm_artist_info",
      "set_metadata",
      "write_tracks",
      "write_playlist",
      "split_spotify_uri",
      "get_os",
      "get_folder"
    ],
    "classes": [],
    "imports": [
      "argparse",
      "json",
      "os",
      "re",
      "subprocess",
      "urllib.parse",
      "urllib.request",
      "secret",
      "spotipy",
      "spotipy.oauth2"
    ],
    "preview": "import argparse\nimport json\nimport os\nimport re\nimport subprocess\nimport urllib.parse\nimport urllib.request\n\nimport secret\nimport spotipy\nimport spotipy.oauth2 as oauth2\nfrom requests.utils import quote\n\n\ndef generate_token():\n    credentials = oauth2.SpotifyClientCredentials(\n        client_id=secret.28b20556906f4b75874c4ae98320c81d,\n        client_secret=secret.c7033fd14e1247cfb9eef73874dd2365 \n)\n    token = credentials.get_access_token()",
    "last_modified": "2025-05-04T23:28:21.394054"
  },
  {
    "id": "772",
    "name": "list_csv.py",
    "path": "02_media_processing/image_tools/list_csv.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 16,
    "size": 497,
    "docstring": "",
    "keywords": [],
    "functions": [
      "list_files_to_csv"
    ],
    "classes": [],
    "imports": [
      "csv",
      "os"
    ],
    "preview": "import csv\nimport os\n\n\ndef list_files_to_csv(startpath, output_file):\n    with open(output_file, \"w\", newline=\"\") as file:\n        writer = csv.writer(file)\n        writer.writerow([\"Folder Path\", \"File Name\"])  # Optional: headers for the CSV columns\n        for root, dirs, files in os.walk(startpath):\n            for f in files:\n                writer.writerow([root, f])\n\n\n# Replace the path with your actual directory path\nlist_files_to_csv(\"/Users/steven/Pictures/3dMothers\", \"output.csv\")\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "773",
    "name": "leodown_20250102105147.py",
    "path": "02_media_processing/image_tools/leodown_20250102105147.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 51,
    "size": 1326,
    "docstring": "",
    "keywords": [
      "web_tools"
    ],
    "functions": [],
    "classes": [],
    "imports": [
      "json",
      "requests"
    ],
    "preview": "import json\n\nimport requests\n\n# Base URL for API\nbase_url = (\n    \"https://cloud.leonardo.ai/api/rest/v1/generations/user/f7bb8476-e3f0-4f1f-9a06-4600866cc49c\"\n)\nheaders = {\n    \"accept\": \"application/json\",\n    \"authorization\": \"Bearer 93043291-957d-4ec1-8c79-ee734abcb6e3\",  # Replace with your actual token\n}\n\n# Output file\noutput_file = \"/Users/steven/Pictures/leonardo_library.json\"\n\n# Pagination parameters\noffset = 0\nlimit = 10  # Maximum number of items per request\nall_generations = []",
    "last_modified": "2025-09-13T05:53:49.560267"
  },
  {
    "id": "774",
    "name": "upload (1).py",
    "path": "02_media_processing/image_tools/upload (1).py",
    "category": "02_media_processing",
    "type": "youtube",
    "lines": 115,
    "size": 2815,
    "docstring": "",
    "keywords": [
      "image_processing",
      "youtube",
      "web_tools"
    ],
    "functions": [
      "upload_image",
      "upscale_image",
      "get_upscaled_image",
      "main"
    ],
    "classes": [],
    "imports": [
      "json",
      "time",
      "requests"
    ],
    "preview": "import json\nimport time\n\nimport requests\n\n\ndef upload_image(api_key, image_file_path):\n    headers = {\n        \"accept\": \"application/json\",\n        \"content-type\": \"application/json\",\n        \"authorization\": f\"Bearer {api_key}\"\n    }\n\n    # Get a presigned URL for uploading an image\n    url = \"https://cloud.leonardo.ai/api/rest/v1/init-image\"\n\n    payload = {\"extension\": \"jpg\"}\n    response = requests.post(url, json=payload, headers=headers)\n\n    if response.status_code != 200:",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "775",
    "name": "mp3-mp4-coverimg copy.py",
    "path": "02_media_processing/image_tools/mp3-mp4-coverimg copy.py",
    "category": "02_media_processing",
    "type": "image_processing",
    "lines": 53,
    "size": 1737,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "get_cover_image",
      "convert_mp3_to_mp4",
      "process_directory"
    ],
    "classes": [],
    "imports": [
      "glob",
      "os",
      "moviepy.editor",
      "PIL",
      "sys"
    ],
    "preview": "import glob\nimport os\n\nfrom moviepy.editor import AudioFileClip, ImageClip\nfrom PIL import Image\n\n\ndef get_cover_image(file_name, cover_image_directory):\n    # Check for both JPG and PNG extensions\n    jpg_path = os.path.join(cover_image_directory, f\"{file_name}.jpg\")\n    png_path = os.path.join(cover_image_directory, f\"{file_name}.png\")\n\n    if os.path.exists(jpg_path):\n        return jpg_path\n    elif os.path.exists(png_path):\n        return png_path\n    else:\n        print(f\"Cover image not found for {file_name}. Please ensure the cover image exists.\")\n        return None\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "776",
    "name": "youtube_dl_echo 3.py",
    "path": "02_media_processing/image_tools/youtube_dl_echo 3.py",
    "category": "02_media_processing",
    "type": "youtube",
    "lines": 319,
    "size": 13139,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "logging",
      "asyncio",
      "json",
      "math",
      "os",
      "time",
      "pyrogram",
      "translation",
      "helper_funcs.chat_base",
      "helper_funcs.display_progress"
    ],
    "preview": "import logging\n\nlogging.basicConfig(\n    level=logging.DEBUG, format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\n)\nlogger = logging.getLogger(__name__)\n\nimport asyncio\nimport json\nimport math\nimport os\nimport time\n\n# the secret configuration specific things\nif bool(os.environ.get(\"WEBHOOK\", False)):\n    from sample_config import Config\nelse:\n    from config import Config\n\n# the Strings used for this \"thing\"",
    "last_modified": "2025-09-13T05:54:10.389097"
  },
  {
    "id": "777",
    "name": "imgupscale.py",
    "path": "02_media_processing/image_tools/imgupscale.py",
    "category": "02_media_processing",
    "type": "image_processing",
    "lines": 69,
    "size": 2829,
    "docstring": "",
    "keywords": [],
    "functions": [
      "convert_and_upscale_images",
      "main"
    ],
    "classes": [],
    "imports": [
      "os",
      "PIL"
    ],
    "preview": "import os\n\nfrom PIL import Image, UnidentifiedImageError\n\n\n# Function to convert and upscale PNG and JPEG images by 200% with 300 DPI\ndef convert_and_upscale_images(source_directory, destination_directory, max_size_mb=8):\n    # Create the destination directory if it doesn't exist\n    os.makedirs(destination_directory, exist_ok=True)\n\n    for filename in os.listdir(source_directory):\n        if filename.lower().endswith((\".png\", \".jpeg\", \".jpg\")):\n            source_file = os.path.join(source_directory, filename)\n            filename_no_ext = os.path.splitext(filename)[0]\n            ext = filename.split(\".\")[-1].lower()\n            destination_file = os.path.join(destination_directory, f\"{filename_no_ext}.{ext}\")\n\n            try:\n                # Convert and upscale PNG or JPEG\n                with Image.open(source_file) as im:",
    "last_modified": "2025-09-13T05:53:55.286828"
  },
  {
    "id": "778",
    "name": "create_video.py",
    "path": "02_media_processing/image_tools/create_video.py",
    "category": "02_media_processing",
    "type": "youtube",
    "lines": 84,
    "size": 2657,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "select_images_based_on_analysis",
      "create_video"
    ],
    "classes": [],
    "imports": [
      "os",
      "random",
      "moviepy.editor",
      "PIL",
      "sys"
    ],
    "preview": "import os\nimport random\n\nfrom moviepy.editor import *\nfrom PIL import Image  # Ensure Pillow is installed\n\n# Check Pillow version to avoid ANTIALIAS issues\nif hasattr(Image, \"Resampling\"):\n    ANTIALIAS = Image.Resampling.LANCZOS  # For Pillow 10+\nelse:\n    ANTIALIAS = Image.ANTIALIAS  # Fallback for older versions\n\n\ndef select_images_based_on_analysis(image_dir, keywords):\n    images = [\n        os.path.join(image_dir, img)\n        for img in os.listdir(image_dir)\n        if img.endswith((\".png\", \".jpg\"))\n    ]\n    if len(images) == 0:",
    "last_modified": "2025-09-13T05:53:42.804912"
  },
  {
    "id": "779",
    "name": "parser.py",
    "path": "02_media_processing/image_tools/parser.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 80,
    "size": 2293,
    "docstring": "",
    "keywords": [],
    "functions": [
      "get_arg_parser"
    ],
    "classes": [],
    "imports": [
      "argparse"
    ],
    "preview": "import argparse\n\n\ndef get_arg_parser() -> argparse.ArgumentParser:\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        \"-g\",\n        \"--game\",\n        type=str,\n        default=\"fortnite\",\n        help=\"Declares for which game the compilation should be created. It uses fortnite as default\",\n        required=False,\n    )\n    parser.add_argument(\n        \"-ap\",\n        \"--asset_path\",\n        type=str,\n        default=\"assets\",\n        help=\"Path to the assets folder. If not declared it uses './assets' as default\",\n        required=False,",
    "last_modified": "2025-03-28T18:37:12.016423"
  },
  {
    "id": "780",
    "name": "organize_albums 1.py",
    "path": "02_media_processing/image_tools/organize_albums 1.py",
    "category": "02_media_processing",
    "type": "organization",
    "lines": 74,
    "size": 3111,
    "docstring": "",
    "keywords": [
      "organization"
    ],
    "functions": [
      "organize_files"
    ],
    "classes": [],
    "imports": [
      "os",
      "shutil"
    ],
    "preview": "import os\nimport shutil\n\n# Define the base directory\nbase_dir = \"/Users/steven/Music/nocTurneMeLoDieS/MP3\"\n\n\n# Helper function to create folders and move files\ndef organize_files():\n    # List all files in the base directory\n    files = os.listdir(base_dir)\n\n    # Process each file\n    for file in files:\n        # Skip directories\n        if os.path.isdir(os.path.join(base_dir, file)):\n            continue\n\n        # Extract the base name (album name) from the file\n        if file.endswith(\".mp3\"):",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "781",
    "name": "twitchClips.py",
    "path": "02_media_processing/image_tools/twitchClips.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 223,
    "size": 7292,
    "docstring": "",
    "keywords": [
      "youtube",
      "web_tools"
    ],
    "functions": [
      "is_channel",
      "is_category",
      "get_loaded_page_content",
      "fetch_clips_category",
      "fetch_clips_channel",
      "remove_all_clips",
      "download_clip",
      "__init__",
      "print_info"
    ],
    "classes": [
      "Clip"
    ],
    "imports": [
      "logging",
      "os",
      "time",
      "urllib.request",
      "requests",
      "bs4",
      "moviepy.editor",
      "selenium",
      "selenium.webdriver.support.ui"
    ],
    "preview": "#!/usr/bin/env python\nimport logging\nimport os\nimport time\nimport urllib.request\n\nimport requests as rs\nfrom bs4 import BeautifulSoup as bs\nfrom moviepy.editor import *\nfrom selenium import webdriver\nfrom selenium.webdriver.support.ui import WebDriverWait\n\n\nclass Clip:\n    url: str  # Url of the clip\n    title: str  # Title of the clip\n    channelName: str  # Name of the channel\n    duration: str  # Duration in seconds of the clip\n\n    def __init__(self, url: str, title: str, channelName: str, duration: str):",
    "last_modified": "2025-09-13T05:53:45.254860"
  },
  {
    "id": "782",
    "name": "loop-upscale.py",
    "path": "02_media_processing/image_tools/loop-upscale.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 58,
    "size": 2241,
    "docstring": "",
    "keywords": [
      "web_tools"
    ],
    "functions": [],
    "classes": [],
    "imports": [
      "json",
      "os",
      "time",
      "requests"
    ],
    "preview": "import json\nimport os\nimport time\n\nimport requests\n\napi_key = \"3ca7a396-64c0-4e22-8cad-b00bc972d4b3\"\nauthorization = \"Bearer %s\" % api_key\n\nheaders = {\n    \"accept\": \"application/json\",\n    \"content-type\": \"application/json\",\n    \"authorization\": authorization,\n}\n\n# Directory containing images\ndirectory_path = \"/Users/steven/Pictures/CookiMonster/1\"\n\n# Loop through each file in the directory\nfor filename in os.listdir(directory_path):",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "783",
    "name": "img-img-upscale (1).py",
    "path": "02_media_processing/image_tools/img-img-upscale (1).py",
    "category": "02_media_processing",
    "type": "image_processing",
    "lines": 36,
    "size": 1608,
    "docstring": "",
    "keywords": [],
    "functions": [
      "convert_and_upscale_images"
    ],
    "classes": [],
    "imports": [
      "os",
      "PIL"
    ],
    "preview": "import os\n\nfrom PIL import Image\n\n\ndef convert_and_upscale_images(directory):\n    # Walk through all directories and files within the specified directory\n    for root, _, files in os.walk(directory):\n        for filename in files:\n            if filename.lower().endswith(\".jpg\") or filename.lower().endswith(\".jpg\"):\n                # Construct full file path\n                file_path = os.path.join(root, filename)\n                # Open the .jpg image\n                with Image.open(file_path) as img:\n                    # Upscale the image by 2x\n                    img = img.resize((img.width * 2, img.height * 2), Image.LANCZOS)\n                    # Set DPI to 300\n                    img.info[\"dpi\"] = (300, 300)\n                    # Convert the image mode to RGB (if not already in that mode)\n                    if img.mode != \"RGB\":",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "784",
    "name": "leoup.py",
    "path": "02_media_processing/image_tools/leoup.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 76,
    "size": 1849,
    "docstring": "",
    "keywords": [
      "web_tools"
    ],
    "functions": [],
    "classes": [],
    "imports": [
      "json",
      "os",
      "time",
      "requests"
    ],
    "preview": "import json\nimport os\nimport time\n\nimport requests\n\napi_key = os.getenv(\"API_KEY\")\nif not api_key:\n    raise ValueError(\n        \"API key is not set. Please ensure the API_KEY environment variable is configured correctly.\"\n    )\n\nauthorization = f\"Bearer {api_key}\"\n\nheaders = {\n    \"accept\": \"application/json\",\n    \"content-type\": \"application/json\",\n    \"authorization\": authorization,\n}\n",
    "last_modified": "2025-05-04T22:47:12.936647"
  },
  {
    "id": "785",
    "name": "main_20221230223427.py",
    "path": "02_media_processing/image_tools/main_20221230223427.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 127,
    "size": 3768,
    "docstring": "",
    "keywords": [
      "opencv",
      "analysis",
      "web_tools"
    ],
    "functions": [
      "generate"
    ],
    "classes": [],
    "imports": [
      "os",
      "sys",
      "time",
      "urllib.request",
      "io",
      "chromedriver_autoinstaller",
      "cv2",
      "imutils",
      "requests",
      "bs4"
    ],
    "preview": "import os\nimport sys\nimport time\nimport urllib.request\nfrom io import BytesIO, StringIO\n\n# Optional\nimport chromedriver_autoinstaller\nimport cv2\nimport imutils\nimport requests\nfrom bs4 import BeautifulSoup\nfrom selenium import webdriver\nfrom wand.display import display\nfrom wand.image import Image\n\nchromedriver_autoinstaller.install()\n\nopts = webdriver.ChromeOptions()\nopts.headless = True",
    "last_modified": "2025-09-13T05:54:14.071402"
  },
  {
    "id": "786",
    "name": "csv-url-down.py",
    "path": "02_media_processing/image_tools/csv-url-down.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 36,
    "size": 1258,
    "docstring": "",
    "keywords": [
      "web_tools"
    ],
    "functions": [],
    "classes": [],
    "imports": [
      "csv",
      "os",
      "requests"
    ],
    "preview": "import csv\nimport os\n\nimport requests\n\n# Set the output directory\noutput_directory = \"downloaded_files\"\nos.makedirs(output_directory, exist_ok=True)\n\n# Path to the CSV file containing URLs\ncsv_file_path = \"/Users/steven/Pictures/etsy/printify/mydesigns-export.CSV\"\n\n# Column name in the CSV that contains the URLs\nurl_column_name = \"url\"  # Adjust to match your CSV file's structure\n\n# Read the CSV file and download each URL\nwith open(csv_file_path, newline=\"\", encoding=\"utf-8\") as csvfile:\n    reader = csv.DictReader(csvfile)\n    for row in reader:\n        url = row[url_column_name]",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "787",
    "name": "mp3_to_mp4.py",
    "path": "02_media_processing/image_tools/mp3_to_mp4.py",
    "category": "02_media_processing",
    "type": "video_processing",
    "lines": 18,
    "size": 622,
    "docstring": "",
    "keywords": [],
    "functions": [
      "convert_to_mp4"
    ],
    "classes": [],
    "imports": [
      "os"
    ],
    "preview": "import os\n\n\ndef convert_to_mp4(image, audio, output):\n    cmd = f\"ffmpeg -loop 1 -i {image} -i {audio} -c:v libx264 -c:a aac -strict experimental -b:a 192k -shortest {output}\"\n    os.system(cmd)\n\n\n# Prompt the user for file paths\nimage_file = input(\"Enter the path to the image file (e.g., cover_image.jpg): \")\naudio_file = input(\"Enter the path to the MP3 file (e.g., your_music.mp3): \")\noutput_file = input(\"Enter the desired output file name (e.g., your_video.mp4): \")\n\n# Convert MP3 to MP4\nconvert_to_mp4(image_file, audio_file, output_file)\n\nprint(\"Conversion Complete. The MP4 file is ready for upload to YouTube.\")\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "788",
    "name": "YouTubeBot.py",
    "path": "02_media_processing/image_tools/YouTubeBot.py",
    "category": "02_media_processing",
    "type": "youtube",
    "lines": 164,
    "size": 4928,
    "docstring": "",
    "keywords": [
      "web_tools"
    ],
    "functions": [
      "fetch",
      "filter",
      "duration_split",
      "start"
    ],
    "classes": [],
    "imports": [
      "time",
      "tkinter",
      "tkinter.ttk",
      "pyautogui",
      "PIL",
      "selenium",
      "requests"
    ],
    "preview": "import time\nimport tkinter as tk\nimport tkinter.ttk as ttk\n\nimport pyautogui\nfrom PIL import Image, ImageTk\nfrom selenium import webdriver\n\nheight = pyautogui.size()[1]\nwidth = pyautogui.size()[0]\nprint(\"resolution = \" + str(width) + \", \" + str(height))\nwindow = tk.Tk()\nwindow.title(\"YouTube Bot\")\n\nwindow.resizable(0, 0)\nwindow.configure(background=\"white\")\nwindow.rowconfigure([0], minsize=round(width / 96), weight=0)\nwindow.columnconfigure([0, 2], minsize=round(width / 24), weight=0)\nwindow.columnconfigure(1, minsize=round(width / 2.13), weight=0)\n",
    "last_modified": "2025-08-10T20:33:59.047658"
  },
  {
    "id": "789",
    "name": "lexica.py",
    "path": "02_media_processing/image_tools/lexica.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 36,
    "size": 1102,
    "docstring": "",
    "keywords": [
      "web_tools"
    ],
    "functions": [
      "get_image"
    ],
    "classes": [],
    "imports": [
      "json",
      "logging",
      "os",
      "time",
      "urllib",
      "pathlib",
      "urllib.request",
      "requests",
      "settings"
    ],
    "preview": "import json\nimport logging\nimport os\nimport time\nimport urllib\nfrom pathlib import Path\nfrom urllib.request import Request, urlopen\n\nimport requests\nimport settings\n\n\ndef get_image(file_path, sentence, number_of_images=1):\n\n    if settings.download_enabled:\n        if not os.path.exists(file_path):\n            safe_query = urllib.parse.quote(sentence.strip())\n            lexica_url = f\"https://lexica.art/api/v1/search?q={safe_query}\"\n            logging.info(f\"Downloading Image : {str(id)} - {sentence}\")\n            r = requests.get(lexica_url)",
    "last_modified": "2025-05-04T23:28:20.223267"
  },
  {
    "id": "790",
    "name": "gallery_logic.py",
    "path": "02_media_processing/image_tools/gallery_logic.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 55,
    "size": 2267,
    "docstring": "",
    "keywords": [],
    "functions": [
      "get_gallery_logic",
      "get_gallery_type"
    ],
    "classes": [],
    "imports": [
      "simplegallery.common",
      "simplegallery.logic.variants.files_gallery_logic",
      "simplegallery.logic.variants.google_gallery_logic",
      "simplegallery.logic.variants.onedrive_gallery_logic"
    ],
    "preview": "import simplegallery.common as spg_common\nfrom simplegallery.logic.variants.files_gallery_logic import FilesGalleryLogic\nfrom simplegallery.logic.variants.google_gallery_logic import GoogleGalleryLogic\nfrom simplegallery.logic.variants.onedrive_gallery_logic import OnedriveGalleryLogic\n\n\ndef get_gallery_logic(gallery_config):\n    \"\"\"\n    Factory function that returns an object of a class derived from BaseGalleryLogic based on the gallery config.\n    Supported gallery logics:\n    - FilesGalleryLogic - logic for local files gallery\n    - OneDriveGallerLogic - logic for shared album from OneDrive\n    - GoogleGallerLogic - logic for shared album from Google Photos\n\n    :param gallery_config: gallery config dictionary as read from the gallery.json\n    :return: gallery logic object\n    \"\"\"\n    if \"remote_gallery_type\" not in gallery_config:\n        return FilesGalleryLogic(gallery_config)\n    elif not gallery_config[\"remote_gallery_type\"]:",
    "last_modified": "2025-09-13T05:53:52.487946"
  },
  {
    "id": "791",
    "name": "YouTube Livestream Botter 2.py",
    "path": "02_media_processing/image_tools/YouTube Livestream Botter 2.py",
    "category": "02_media_processing",
    "type": "youtube",
    "lines": 171,
    "size": 7730,
    "docstring": "",
    "keywords": [
      "web_tools"
    ],
    "functions": [
      "bot",
      "__init__",
      "printservice",
      "update",
      "get_proxy",
      "FormatProxy",
      "__init__"
    ],
    "classes": [
      "main",
      "proxy"
    ],
    "imports": [
      "os",
      "platform",
      "random",
      "string",
      "threading",
      "time",
      "queue",
      "requests",
      "colorama"
    ],
    "preview": "import os\nimport platform\nimport random\nimport string\nimport threading\nimport time\nfrom queue import Queue\n\nimport requests\nfrom colorama import Fore, init\n\nintro = \"\"\"\n\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2588\u2557   \u2588\u2588\u2588\u2557      \u2588\u2588\u2588\u2588\u2588\u2588\u2557  \u2588\u2588\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2588\u2588\u2588\u2588\u2557 \n\u2588\u2588\u2554\u2550\u2550\u2550\u2550\u255d\u255a\u2550\u2550\u2588\u2588\u2554\u2550\u2550\u255d\u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2557\u2588\u2588\u2554\u2550\u2550\u2550\u2550\u255d\u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2557\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2588\u2588\u2551      \u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2557\u2588\u2588\u2554\u2550\u2550\u2550\u2588\u2588\u2557\u255a\u2550\u2550\u2588\u2588\u2554\u2550\u2550\u255d\u255a\u2550\u2550\u2588\u2588\u2554\u2550\u2550\u255d\u2588\u2588\u2554\u2550\u2550\u2550\u2550\u255d\u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2557\n\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557   \u2588\u2588\u2551   \u2588\u2588\u2588\u2588\u2588\u2588\u2554\u255d\u2588\u2588\u2588\u2588\u2588\u2557  \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2551\u2588\u2588\u2554\u2588\u2588\u2588\u2588\u2554\u2588\u2588\u2551\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2588\u2588\u2588\u2588\u2554\u255d\u2588\u2588\u2551   \u2588\u2588\u2551   \u2588\u2588\u2551      \u2588\u2588\u2551   \u2588\u2588\u2588\u2588\u2588\u2557  \u2588\u2588\u2588\u2588\u2588\u2588\u2554\u255d\n\u255a\u2550\u2550\u2550\u2550\u2588\u2588\u2551   \u2588\u2588\u2551   \u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2557\u2588\u2588\u2554\u2550\u2550\u255d  \u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2551\u2588\u2588\u2551\u255a\u2588\u2588\u2554\u255d\u2588\u2588\u2551\u255a\u2550\u2550\u2550\u2550\u255d\u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2557\u2588\u2588\u2551   \u2588\u2588\u2551   \u2588\u2588\u2551      \u2588\u2588\u2551   \u2588\u2588\u2554\u2550\u2550\u255d  \u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2557\n\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2551   \u2588\u2588\u2551   \u2588\u2588\u2551  \u2588\u2588\u2551\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2551  \u2588\u2588\u2551\u2588\u2588\u2551 \u255a\u2550\u255d \u2588\u2588\u2551      \u2588\u2588\u2588\u2588\u2588\u2588\u2554\u255d\u255a\u2588\u2588\u2588\u2588\u2588\u2588\u2554\u255d   \u2588\u2588\u2551      \u2588\u2588\u2551   \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2551  \u2588\u2588\u2551\n\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u255d   \u255a\u2550\u255d   \u255a\u2550\u255d  \u255a\u2550\u255d\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u255d\u255a\u2550\u255d  \u255a\u2550\u255d\u255a\u2550\u255d     \u255a\u2550\u255d      \u255a\u2550\u2550\u2550\u2550\u2550\u255d  \u255a\u2550\u2550\u2550\u2550\u2550\u255d    \u255a\u2550\u255d      \u255a\u2550\u255d   \u255a\u2550\u2550\u2550\u2550\u2550\u2550\u255d\u255a\u2550\u255d  \u255a\u2550\u255d\n\nhttps://github.com/KevinLage/YouTube-Livestream-Botter",
    "last_modified": "2025-08-06T14:23:44.339405"
  },
  {
    "id": "792",
    "name": "upload_photos.py",
    "path": "02_media_processing/image_tools/upload_photos.py",
    "category": "02_media_processing",
    "type": "youtube",
    "lines": 96,
    "size": 2909,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "__future__",
      "argparse",
      "os",
      "sys",
      "captions_for_medias",
      "instabot",
      "glob",
      "random"
    ],
    "preview": "#!/usr/bin/python\n# - * - coding: utf-8 - * -\nfrom __future__ import unicode_literals\n\nimport argparse\nimport os\nimport sys\n\nimport captions_for_medias\n\nsys.path.append(os.path.join(sys.path[0], \"../../\"))\nfrom instabot import Bot  # noqa: E402\n\nparser = argparse.ArgumentParser(add_help=True)\nparser.add_argument(\"-u\", type=str, help=\"username\")\nparser.add_argument(\"-p\", type=str, help=\"password\")\nparser.add_argument(\"-proxy\", type=str, help=\"proxy\")\nparser.add_argument(\"-photo\", type=str, help=\"photo name\")\nparser.add_argument(\"-caption\", type=str, help=\"caption for photo\")\nparser.add_argument(\"-tag\", action=\"append\", help=\"taged user id\")",
    "last_modified": "2025-09-13T05:54:55.699322"
  },
  {
    "id": "793",
    "name": "mp3-mp4-coverimg.py",
    "path": "02_media_processing/image_tools/mp3-mp4-coverimg.py",
    "category": "02_media_processing",
    "type": "image_processing",
    "lines": 53,
    "size": 1737,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "get_cover_image",
      "convert_mp3_to_mp4",
      "process_directory"
    ],
    "classes": [],
    "imports": [
      "glob",
      "os",
      "moviepy.editor",
      "PIL",
      "sys"
    ],
    "preview": "import glob\nimport os\n\nfrom moviepy.editor import AudioFileClip, ImageClip\nfrom PIL import Image\n\n\ndef get_cover_image(file_name, cover_image_directory):\n    # Check for both JPG and PNG extensions\n    jpg_path = os.path.join(cover_image_directory, f\"{file_name}.jpg\")\n    png_path = os.path.join(cover_image_directory, f\"{file_name}.png\")\n\n    if os.path.exists(jpg_path):\n        return jpg_path\n    elif os.path.exists(png_path):\n        return png_path\n    else:\n        print(f\"Cover image not found for {file_name}. Please ensure the cover image exists.\")\n        return None\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "794",
    "name": "codingstatemachine.py",
    "path": "02_media_processing/image_tools/codingstatemachine.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 91,
    "size": 3732,
    "docstring": "",
    "keywords": [],
    "functions": [
      "__init__",
      "reset",
      "next_state",
      "get_current_charlen",
      "get_coding_state_machine",
      "language"
    ],
    "classes": [
      "CodingStateMachine"
    ],
    "imports": [
      "logging",
      "codingstatemachinedict",
      "enums"
    ],
    "preview": "######################## BEGIN LICENSE BLOCK ########################\n# The Original Code is mozilla.org code.\n#\n# The Initial Developer of the Original Code is\n# Netscape Communications Corporation.\n# Portions created by the Initial Developer are Copyright (C) 1998\n# the Initial Developer. All Rights Reserved.\n#\n# Contributor(s):\n#   Mark Pilgrim - port to Python\n#\n# This library is free software; you can redistribute it and/or\n# modify it under the terms of the GNU Lesser General Public\n# License as published by the Free Software Foundation; either\n# version 2.1 of the License, or (at your option) any later version.\n#\n# This library is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU\n# Lesser General Public License for more details.",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "795",
    "name": "img2pdf.py",
    "path": "02_media_processing/image_tools/img2pdf.py",
    "category": "02_media_processing",
    "type": "image_processing",
    "lines": 39,
    "size": 1276,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "collect_image_files",
      "create_pdf_volumes",
      "main"
    ],
    "classes": [],
    "imports": [
      "os",
      "PIL"
    ],
    "preview": "import os\n\nfrom PIL import Image\n\n\ndef collect_image_files(source_directory):\n    image_files = []\n    for root, dirs, files in os.walk(source_directory):\n        for file in files:\n            if file.lower().endswith((\".png\", \".jpg\", \".jpeg\")):\n                file_path = os.path.join(root, file)\n                image_files.append(file_path)\n    return image_files\n\n\ndef create_pdf_volumes(image_files, target_directory, volume_size=100):\n    for i in range(0, len(image_files), volume_size):\n        volume_image_files = image_files[i : i + volume_size]\n        images = [Image.open(img).convert(\"RGB\") for img in volume_image_files]\n        volume_path = os.path.join(target_directory, f\"Image_Volume_{i//volume_size + 1}.pdf\")",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "796",
    "name": "scan_images--.py",
    "path": "02_media_processing/image_tools/scan_images--.py",
    "category": "02_media_processing",
    "type": "image_processing",
    "lines": 24,
    "size": 702,
    "docstring": "",
    "keywords": [],
    "functions": [
      "scan_directory"
    ],
    "classes": [],
    "imports": [
      "os"
    ],
    "preview": "import os\n\n# List of drives to scan\ndrives = [\"/Volumes/4t\", \"/Volumes/baKs\", \"/Volumes/iMac\", \"/Volumes/ogCho\"]\n# File types to look for\nfile_types = (\".jpg\", \".jpeg\", \".png\", \".gif\", \".bmp\")\n\n\n# Function to scan a directory for image files\ndef scan_directory(directory):\n    for root, dirs, files in os.walk(directory):\n        for file in files:\n            if file.lower().endswith(file_types):\n                yield os.path.join(root, file)\n\n\n# Open a file to write the paths\nwith open(\"image_paths.txt\", \"w\") as file:\n    for drive in drives:\n        for image_path in scan_directory(drive):",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "797",
    "name": "bot.py",
    "path": "02_media_processing/image_tools/bot.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 98,
    "size": 2704,
    "docstring": "",
    "keywords": [
      "web_tools"
    ],
    "functions": [
      "DLimage"
    ],
    "classes": [],
    "imports": [
      "math",
      "time",
      "urllib.request",
      "keyboard",
      "praw",
      "requests",
      "InstagramAPI",
      "PIL"
    ],
    "preview": "import math\nimport time\nimport urllib.request\n\nimport keyboard\nimport praw\nimport requests\nfrom InstagramAPI import InstagramAPI\nfrom PIL import Image\n\n# put it IG username/password\napi = InstagramAPI(\"username\", \"password\")\napi.login()\n\n\n# make a reddit acount and look up how to find this stuff. its called PRAW\nreddit = praw.Reddit(client_id=\"\", client_secret=\"\", username=\"\", password=\"\", user_agent=\"chrome\")\n\n\ndef DLimage(url, filePath, fileName):",
    "last_modified": "2025-09-13T05:55:13.854469"
  },
  {
    "id": "798",
    "name": "best-csv.py",
    "path": "02_media_processing/image_tools/best-csv.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 139,
    "size": 5042,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "categorize_image",
      "process_images",
      "main"
    ],
    "classes": [],
    "imports": [
      "csv",
      "os",
      "datetime",
      "PIL"
    ],
    "preview": "import csv\nimport os\nfrom datetime import datetime\n\nfrom PIL import Image, UnidentifiedImageError\n\n# \ud83c\udfaf Bestselling Product Categories for Etsy & TikTok\nPLATFORMS = {\n    \"tiktok\": {\n        \"hoodie\": [\"bold colors\", \"dark tones\", \"statement text\"],\n        \"t-shirt\": [\"minimalist\", \"memes\", \"high contrast\"],\n        \"tote bag\": [\"artistic\", \"neutral tones\", \"simple graphics\"],\n        \"phone case\": [\"vibrant\", \"pop culture\", \"sharp details\"],\n        \"sticker\": [\"high contrast\", \"small details\", \"text-heavy\"],\n        \"candle\": [\"aesthetic\", \"soft colors\", \"cozy themes\"],\n        \"plush blanket\": [\"soft tones\", \"cozy aesthetics\", \"neutral patterns\"],\n    },\n    \"etsy\": {\n        \"ceramic mug\": [\"custom text\", \"personalized gifts\", \"vintage aesthetics\"],\n        \"cotton tee\": [\"affordable\", \"durable\", \"versatile for daily wear\"],",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "799",
    "name": "cli.py",
    "path": "02_media_processing/image_tools/cli.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 181,
    "size": 7544,
    "docstring": "",
    "keywords": [
      "youtube"
    ],
    "functions": [
      "main"
    ],
    "classes": [],
    "imports": [
      "argparse",
      "logging",
      "os",
      "pkg_resources",
      "youtube_bulk_upload"
    ],
    "preview": "#!/usr/bin/env python\nimport argparse\nimport logging\nimport os\n\nimport pkg_resources\nfrom youtube_bulk_upload import YouTubeBulkUpload\n\n\ndef main():\n    logger = logging.getLogger(__name__)\n    log_handler = logging.StreamHandler()\n    log_formatter = logging.Formatter(\n        fmt=\"%(asctime)s.%(msecs)03d - %(levelname)s - %(module)s - %(message)s\",\n        datefmt=\"%Y-%m-%d %H:%M:%S\",\n    )\n    log_handler.setFormatter(log_formatter)\n    logger.addHandler(log_handler)\n\n    package_version = pkg_resources.get_distribution(\"youtube-bulk-upload\").version",
    "last_modified": "2025-09-13T05:53:46.490664"
  },
  {
    "id": "800",
    "name": "common.py",
    "path": "02_media_processing/image_tools/common.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 35,
    "size": 792,
    "docstring": "",
    "keywords": [],
    "functions": [
      "log",
      "read_gallery_config",
      "__init__"
    ],
    "classes": [
      "SPGException"
    ],
    "imports": [
      "json"
    ],
    "preview": "import json\n\n\nclass SPGException(Exception):\n    \"\"\"Exception raised for errors during the createion of the Simple Photo Gallery.\n\n    Attributes:\n        message -- explanation of the error that will be shown to the user\n    \"\"\"\n\n    def __init__(self, message):\n        super().__init__()\n        self.message = message\n\n\ndef log(message):\n    \"\"\"\n    Log a message to the console\n    :param message: message string\n    \"\"\"",
    "last_modified": "2025-05-04T22:47:13"
  },
  {
    "id": "801",
    "name": "image_test.py",
    "path": "02_media_processing/image_tools/image_test.py",
    "category": "02_media_processing",
    "type": "image_processing",
    "lines": 12,
    "size": 615,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "clips"
    ],
    "preview": "from clips import *\n\nimg = Image.fromarray(\n    gen_comment_image(\n        author=\"me\",\n        content=\"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Pellentesque vestibulum quis diam nec faucibus. Ut aliquet massa justo. Pellentesque habitant morbi tristique senectus et netus et malesuada fames ac turpis egestas. Pellentesque lacinia ipsum non suscipit ultrices. Cras finibus erat vel nulla ullamcorper tempor. Quisque laoreet imperdiet urna non ultrices. Nulla nibh diam, feugiat sit amet est eget, tincidunt hendrerit massa. Ut molestie urna eget ornare pulvinar.\",\n    ),\n    \"RGB\",\n)\n\nimg.show()\n",
    "last_modified": "2025-05-04T23:27:53.330588"
  },
  {
    "id": "802",
    "name": "organize_albums 5.py",
    "path": "02_media_processing/image_tools/organize_albums 5.py",
    "category": "02_media_processing",
    "type": "organization",
    "lines": 62,
    "size": 2470,
    "docstring": "",
    "keywords": [
      "organization"
    ],
    "functions": [
      "organize_files"
    ],
    "classes": [],
    "imports": [
      "os",
      "shutil"
    ],
    "preview": "import os\nimport shutil\n\n# Define the base directory\nbase_dir = \"/Users/steven/Music/NocTurnE-meLoDieS/albums/\"\n\n\n# Helper function to create folders and move files\ndef organize_files():\n    # List all files in the base directory\n    files = os.listdir(base_dir)\n\n    # Process each file\n    for file in files:\n        # Skip directories\n        if os.path.isdir(os.path.join(base_dir, file)):\n            continue\n\n        # Extract the base name (album name) from the file\n        if file.endswith(\".mp3\"):",
    "last_modified": "2025-05-04T22:47:11.429396"
  },
  {
    "id": "803",
    "name": "gemini_grabber.py",
    "path": "02_media_processing/image_tools/gemini_grabber.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 285,
    "size": 9033,
    "docstring": "",
    "keywords": [
      "youtube",
      "web_tools"
    ],
    "functions": [
      "sanitize",
      "extract_image_urls",
      "download_assets",
      "load_urls",
      "main"
    ],
    "classes": [],
    "imports": [
      "asyncio",
      "argparse",
      "re",
      "time",
      "pathlib",
      "urllib.parse",
      "requests",
      "bs4",
      "playwright.async_api",
      "json"
    ],
    "preview": "import asyncio\nimport argparse\nimport re\nimport time\nfrom pathlib import Path\nfrom urllib.parse import urlparse, urljoin\n\nimport requests\nfrom bs4 import BeautifulSoup\nfrom playwright.async_api import async_playwright, TimeoutError as PWTimeout\n\nWAIT_STRATEGIES = [\"networkidle\", \"load\", \"domcontentloaded\"]\n\nBUTTON_TEXTS = [\n    \"Expand\",\n    \"See more\",\n    \"Show more\",\n    \"Continue\",\n    \"Next\",\n    \"More\",",
    "last_modified": "2025-09-13T05:54:30.982854"
  },
  {
    "id": "804",
    "name": "BeautifulSoup.py",
    "path": "02_media_processing/image_tools/BeautifulSoup.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 1934,
    "size": 76366,
    "docstring": "Beautiful Soup",
    "keywords": [],
    "functions": [
      "setup",
      "replaceWith",
      "extract",
      "_lastRecursiveChild",
      "insert",
      "append",
      "findNext",
      "findAllNext",
      "findNextSibling",
      "findNextSiblings"
    ],
    "classes": [
      "PageElement",
      "NavigableString(unicode, PageElement)",
      "CData(NavigableString)",
      "ProcessingInstruction(NavigableString)",
      "Comment(NavigableString)",
      "Declaration(NavigableString)",
      "Tag(PageElement)",
      "SoupStrainer",
      "ResultSet(list)",
      "BeautifulStoneSoup(Tag, SGMLParser)",
      "has some tricks for dealing with some HTML that kills",
      "BeautifulSoup(BeautifulStoneSoup)",
      "StopParsing(Exception)",
      "ICantBelieveItsBeautifulSoup(BeautifulSoup)",
      "MinimalSoup(BeautifulSoup)",
      "BeautifulSOAP(BeautifulStoneSoup)",
      "RobustXMLParser(BeautifulStoneSoup)",
      "RobustHTMLParser(BeautifulSoup)",
      "RobustWackAssHTMLParser(ICantBelieveItsBeautifulSoup)",
      "RobustInsanelyWackAssHTMLParser(MinimalSoup)",
      "SimplifyingSOAPParser(BeautifulSOAP)",
      "UnicodeDammit"
    ],
    "imports": [
      "__future__",
      "codecs",
      "re",
      "types",
      "sgmllib",
      "sgmllib",
      "htmlentitydefs",
      "chardet",
      "cjkcodecs.aliases",
      "iconv_codec"
    ],
    "preview": "\"\"\"Beautiful Soup\nElixir and Tonic\n\"The Screen-Scraper's Friend\"\nhttp://www.crummy.com/software/BeautifulSoup/\n\nBeautiful Soup parses a (possibly invalid) XML or HTML document into a\ntree representation. It provides methods and Pythonic idioms that make\nit easy to navigate, search, and modify the tree.\n\nA well-formed XML/HTML document yields a well-formed data\nstructure. An ill-formed XML/HTML document yields a correspondingly\nill-formed data structure. If your document is only locally\nwell-formed, you can use this library to find and process the\nwell-formed part of it.\n\nBeautiful Soup works with Python 2.2 and up. It has no external\ndependencies, but you'll have more success at converting data to UTF-8\nif you also install these three packages:\n\n* chardet, for auto-detecting character encodings",
    "last_modified": "2025-05-04T23:28:20.748307"
  },
  {
    "id": "805",
    "name": "imgconvert copy.py",
    "path": "02_media_processing/image_tools/imgconvert copy.py",
    "category": "02_media_processing",
    "type": "image_processing",
    "lines": 53,
    "size": 1729,
    "docstring": "",
    "keywords": [],
    "functions": [
      "convert_and_upscale_images",
      "main"
    ],
    "classes": [],
    "imports": [
      "os",
      "PIL"
    ],
    "preview": "import os\n\nfrom PIL import Image\n\n# Function to convert WebP images to PNG and upscale by 200% with 300 DPI\n\n\ndef convert_and_upscale_images(source_directory, destination_directory):\n    # Create the destination directory if it doesn't exist\n    os.makedirs(destination_directory, exist_ok=True)\n\n    for filename in os.listdir(source_directory):\n        if filename.endswith(\".png\"):\n            source_file = os.path.join(source_directory, filename)\n            filename_no_ext = os.path.splitext(filename)[0]\n            destination_file = os.path.join(destination_directory, f\"{filename_no_ext}.png\")\n\n            # Convert WebP to PNG and upscale by 200% with 300 DPI\n            im = Image.open(source_file)\n            width, height = im.size",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "806",
    "name": "good_commenting_strategy_and_new_qs_system.py",
    "path": "02_media_processing/image_tools/good_commenting_strategy_and_new_qs_system.py",
    "category": "02_media_processing",
    "type": "organization",
    "lines": 159,
    "size": 5153,
    "docstring": "This template is written by @the-unknown\n\nWhat does this quickstart script aim to do?\n- This is my template which includes the new QS system.\n  It includes a randomizer for my hashtags... with every run, it selects 10\n  random hashtags from the list.\n\nNOTES:\n- I am using the bot headless on my vServer and proxy into a Raspberry PI I\nhave at home, to always use my home IP to connect to Instagram.\n  In my comments, I always ask for feedback, use more than 4 words and\n  always have emojis.\n  My comments work very well, as I get a lot of feedback to my posts and\n  profile visits since I use this tactic.\n\n  As I target mainly active accounts, I use two unfollow methods.\n  The first will unfollow everyone who did not follow back within 12h.\n  The second one will unfollow the followers within 24h.",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "random",
      "instapy"
    ],
    "preview": "\"\"\"\nThis template is written by @the-unknown\n\nWhat does this quickstart script aim to do?\n- This is my template which includes the new QS system.\n  It includes a randomizer for my hashtags... with every run, it selects 10\n  random hashtags from the list.\n\nNOTES:\n- I am using the bot headless on my vServer and proxy into a Raspberry PI I\nhave at home, to always use my home IP to connect to Instagram.\n  In my comments, I always ask for feedback, use more than 4 words and\n  always have emojis.\n  My comments work very well, as I get a lot of feedback to my posts and\n  profile visits since I use this tactic.\n\n  As I target mainly active accounts, I use two unfollow methods.\n  The first will unfollow everyone who did not follow back within 12h.\n  The second one will unfollow the followers within 24h.\n\"\"\"",
    "last_modified": "2025-09-13T05:53:49.408731"
  },
  {
    "id": "807",
    "name": "bulk_upload.py",
    "path": "02_media_processing/image_tools/bulk_upload.py",
    "category": "02_media_processing",
    "type": "youtube",
    "lines": 537,
    "size": 23244,
    "docstring": "",
    "keywords": [
      "analysis",
      "youtube",
      "web_tools"
    ],
    "functions": [
      "__init__",
      "find_input_files",
      "prompt_user_confirmation_or_raise_exception",
      "prompt_user_bool",
      "prompt_user_text",
      "validate_input_parameters",
      "authenticate_youtube",
      "get_channel_id",
      "check_if_video_title_exists_on_youtube_channel",
      "truncate_to_nearest_word"
    ],
    "classes": [
      "YouTubeBulkUpload"
    ],
    "imports": [
      "json",
      "logging",
      "os",
      "pickle",
      "re",
      "tempfile",
      "google.auth.transport.requests",
      "google_auth_oauthlib.flow",
      "googleapiclient.discovery",
      "googleapiclient.http"
    ],
    "preview": "import json\nimport logging\nimport os\nimport pickle\nimport re\nimport tempfile\n\nfrom google.auth.transport.requests import Request\nfrom google_auth_oauthlib.flow import InstalledAppFlow\nfrom googleapiclient.discovery import build\nfrom googleapiclient.http import MediaFileUpload\nfrom thefuzz import fuzz\n\nYOUTUBE_URL_PREFIX = \"https://www.youtube.com/watch?v=\"\n\n\nclass YouTubeBulkUpload:\n    def __init__(\n        self,\n        logger=None,",
    "last_modified": "2025-09-13T05:53:46.412163"
  },
  {
    "id": "808",
    "name": "universe4.py",
    "path": "02_media_processing/image_tools/universe4.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 79,
    "size": 3118,
    "docstring": "",
    "keywords": [
      "web_tools"
    ],
    "functions": [],
    "classes": [],
    "imports": [
      "json",
      "os",
      "time",
      "requests"
    ],
    "preview": "import json\nimport os\nimport time\n\nimport requests\n\napi_key = \"3ca7a396-64c0-4e22-8cad-b00bc972d4b3\"\nauthorization = \"Bearer %s\" % api_key\n\nheaders = {\n    \"accept\": \"application/json\",\n    \"content-type\": \"application/json\",\n    \"authorization\": authorization,\n}\n\n# Directory containing images\ndirectory_path = \"/Users/steven/Pictures/CookiMonster/cookie-midnight\"\n\n# Styles to apply\n# Update the styles list according to your needs",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "809",
    "name": "sort copy.py",
    "path": "02_media_processing/image_tools/sort copy.py",
    "category": "02_media_processing",
    "type": "organization",
    "lines": 61,
    "size": 2397,
    "docstring": "",
    "keywords": [
      "organization"
    ],
    "functions": [
      "sort_images"
    ],
    "classes": [],
    "imports": [
      "os",
      "shutil",
      "datetime",
      "PIL"
    ],
    "preview": "import os\nimport shutil\nfrom datetime import datetime\n\nfrom PIL import Image\n\n\ndef sort_images(source_dir, target_dir):\n    # Create the target directory if it doesn't exist\n    os.makedirs(target_dir, exist_ok=True)\n\n    # Supported image formats\n    extensions = (\".png\", \".jpg\", \".jpeg\", \".tiff\")\n\n    # Walk through the directory\n    for root, dirs, files in os.walk(source_dir):\n        for file in files:\n            if file.lower().endswith(extensions):\n                try:\n                    # Construct the full file path",
    "last_modified": "2025-09-13T05:53:42.963445"
  },
  {
    "id": "810",
    "name": "config_20241204083635.py",
    "path": "02_media_processing/image_tools/config_20241204083635.py",
    "category": "02_media_processing",
    "type": "setup",
    "lines": 3,
    "size": 56,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [],
    "preview": "# config.py\nSOURCE_DIRECTORY = \"/Users/steven/Pictures\"\n",
    "last_modified": "2024-12-13T00:56:52.178525"
  },
  {
    "id": "811",
    "name": "pdf2img.py",
    "path": "02_media_processing/image_tools/pdf2img.py",
    "category": "02_media_processing",
    "type": "image_processing",
    "lines": 20,
    "size": 531,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "os",
      "pdf2image"
    ],
    "preview": "import os\n\nfrom pdf2image import convert_from_path\n\n# Path to your PDF file\npdf_path = \"/Users/steven/Documents/tesla/1-ocr.pdf\"\n\n# Output directory for images\noutput_dir = \"/Users/steven/Documents/tesla/output_images\"\nos.makedirs(output_dir, exist_ok=True)\n\n# Convert PDF to images\nimages = convert_from_path(pdf_path, dpi=300)\n\n# Save images to the output directory\nfor i, image in enumerate(images):\n    image_path = os.path.join(output_dir, f\"page_{i+1}.png\")\n    image.save(image_path, \"PNG\")\n    print(f\"Saved {image_path}\")\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "812",
    "name": "main_20221230233748.py",
    "path": "02_media_processing/image_tools/main_20221230233748.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 429,
    "size": 15514,
    "docstring": "",
    "keywords": [
      "transcription",
      "video_processing",
      "analysis",
      "youtube",
      "web_tools",
      "organization"
    ],
    "functions": [
      "downloadVideo",
      "scrapeVideos",
      "usedQuoteToDifferentFile",
      "getQuoteFromApi",
      "getQuoteFromTxtFile",
      "videoIntro",
      "createVideo",
      "audioClip",
      "randomBgMusic",
      "deleteTempFiles"
    ],
    "classes": [
      "bcolors"
    ],
    "imports": [
      "glob",
      "json",
      "os",
      "random",
      "subprocess",
      "gtts",
      "requests",
      "moviepy.editor",
      "moviepy.video.io.VideoFileClip",
      "mutagen.mp3"
    ],
    "preview": "import glob\nimport json\nimport os\nimport random\nimport subprocess\n\nimport gtts\nimport requests\nfrom moviepy.editor import *\nfrom moviepy.video.io.VideoFileClip import VideoFileClip\nfrom mutagen.mp3 import MP3\nfrom tqdm.auto import tqdm\n\n\n# download background video from pexels - https://www.pexels.com/api/documentation/#videos-search__parameters\ndef downloadVideo(id) -> str:\n    \"\"\"Downloads video from Pexels with the according video ID\"\"\"\n    url = \"https://www.pexels.com/video/\" + str(id) + \"/download.mp4\"\n    # Streaming, so we can iterate over the response.\n    response = requests.get(url, stream=True)",
    "last_modified": "2025-09-13T05:53:57.141459"
  },
  {
    "id": "813",
    "name": "escprober.py",
    "path": "02_media_processing/image_tools/escprober.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 98,
    "size": 3985,
    "docstring": "",
    "keywords": [],
    "functions": [
      "__init__",
      "reset",
      "charset_name",
      "language",
      "get_confidence",
      "feed"
    ],
    "classes": [
      "EscCharSetProber"
    ],
    "imports": [
      "typing",
      "charsetprober",
      "codingstatemachine",
      "enums",
      "escsm"
    ],
    "preview": "######################## BEGIN LICENSE BLOCK ########################\n# The Original Code is mozilla.org code.\n#\n# The Initial Developer of the Original Code is\n# Netscape Communications Corporation.\n# Portions created by the Initial Developer are Copyright (C) 1998\n# the Initial Developer. All Rights Reserved.\n#\n# Contributor(s):\n#   Mark Pilgrim - port to Python\n#\n# This library is free software; you can redistribute it and/or\n# modify it under the terms of the GNU Lesser General Public\n# License as published by the Free Software Foundation; either\n# version 2.1 of the License, or (at your option) any later version.\n#\n# This library is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU\n# Lesser General Public License for more details.",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "814",
    "name": "test_gallery_init.py",
    "path": "02_media_processing/image_tools/test_gallery_init.py",
    "category": "02_media_processing",
    "type": "testing",
    "lines": 227,
    "size": 7778,
    "docstring": "",
    "keywords": [
      "testing"
    ],
    "functions": [
      "check_gallery_files",
      "test_nonexisting_gallery_path",
      "test_existing_gallery_no_force",
      "check_gallery_config",
      "test_new_gallery_created",
      "test_existing_gallery_override",
      "test_default_gallery_config",
      "test_new_onedrive_gallery_created",
      "test_new_google_gallery_created",
      "test_new_invalid_remote_gallery"
    ],
    "classes": [
      "SPGInitTestCase"
    ],
    "imports": [
      "json",
      "os",
      "sys",
      "unittest",
      "unittest",
      "simplegallery.gallery_init",
      "testfixtures"
    ],
    "preview": "import json\nimport os\nimport sys\nimport unittest\nfrom unittest import mock\n\nimport simplegallery.gallery_init as gallery_init\nfrom testfixtures import TempDirectory\n\n\ndef check_gallery_files(tempdir, files_photos, files_other):\n    tempdir.compare([\"templates\", \"public\", \"gallery.json\"] + files_other, recursive=False)\n    tempdir.compare(\n        [\"index_template.jinja\", \"gallery_macros.jinja\"],\n        path=\"templates\",\n        recursive=False,\n    )\n    tempdir.compare([\"css\", \"images\", \"js\"], path=\"public\", recursive=False)\n    tempdir.compare([\".empty\"] + files_photos, path=\"public/images/photos\")\n",
    "last_modified": "2025-09-13T05:53:53.046106"
  },
  {
    "id": "815",
    "name": "move-date.py",
    "path": "02_media_processing/image_tools/move-date.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 29,
    "size": 992,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "os",
      "shutil",
      "datetime"
    ],
    "preview": "import os\nimport shutil\nfrom datetime import datetime\n\n# Set the source directory where your images are stored\nsource_dir = \"/path/to/your/images\"\n\n# Loop through each file in the source directory\nfor filename in os.listdir(source_dir):\n    if filename.lower().endswith((\".png\", \".jpg\", \".jpeg\", \".gif\", \".bmp\")):\n        # Get the full path to the file\n        file_path = os.path.join(source_dir, filename)\n\n        # Get the creation time and convert it to a year\n        creation_time = os.path.getctime(file_path)\n        year = datetime.fromtimestamp(creation_time).strftime(\"%Y\")\n\n        # Define the destination directory based on the year\n        dest_dir = os.path.join(source_dir, year)\n",
    "last_modified": "2025-03-28T18:37:08"
  },
  {
    "id": "816",
    "name": "sort2.py",
    "path": "02_media_processing/image_tools/sort2.py",
    "category": "02_media_processing",
    "type": "organization",
    "lines": 87,
    "size": 3759,
    "docstring": "",
    "keywords": [
      "organization"
    ],
    "functions": [
      "sort_files"
    ],
    "classes": [],
    "imports": [
      "os",
      "shutil",
      "datetime",
      "mutagen.mp3",
      "mutagen.mp4",
      "PIL"
    ],
    "preview": "import os\nimport shutil\nfrom datetime import datetime\n\nfrom mutagen.mp3 import MP3\nfrom mutagen.mp4 import MP4\nfrom PIL import Image\n\n\ndef sort_files(source_dir, target_dir):\n    # Create the target directory if it doesn't exist\n    os.makedirs(target_dir, exist_ok=True)\n\n    # Supported image and media formats\n    image_extensions = (\".png\", \".jpg\", \".jpeg\", \".tiff\", \".webp\")\n    media_extensions = (\".mp3\", \".mp4\")\n\n    # Walk through the directory\n    for root, dirs, files in os.walk(source_dir):\n        for file in files:",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "817",
    "name": "YouTube Livestream Botter 3.py",
    "path": "02_media_processing/image_tools/YouTube Livestream Botter 3.py",
    "category": "02_media_processing",
    "type": "youtube",
    "lines": 171,
    "size": 7730,
    "docstring": "",
    "keywords": [
      "web_tools"
    ],
    "functions": [
      "bot",
      "__init__",
      "printservice",
      "update",
      "get_proxy",
      "FormatProxy",
      "__init__"
    ],
    "classes": [
      "main",
      "proxy"
    ],
    "imports": [
      "os",
      "platform",
      "random",
      "string",
      "threading",
      "time",
      "queue",
      "requests",
      "colorama"
    ],
    "preview": "import os\nimport platform\nimport random\nimport string\nimport threading\nimport time\nfrom queue import Queue\n\nimport requests\nfrom colorama import Fore, init\n\nintro = \"\"\"\n\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2588\u2557   \u2588\u2588\u2588\u2557      \u2588\u2588\u2588\u2588\u2588\u2588\u2557  \u2588\u2588\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2588\u2588\u2588\u2588\u2557 \n\u2588\u2588\u2554\u2550\u2550\u2550\u2550\u255d\u255a\u2550\u2550\u2588\u2588\u2554\u2550\u2550\u255d\u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2557\u2588\u2588\u2554\u2550\u2550\u2550\u2550\u255d\u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2557\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2588\u2588\u2551      \u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2557\u2588\u2588\u2554\u2550\u2550\u2550\u2588\u2588\u2557\u255a\u2550\u2550\u2588\u2588\u2554\u2550\u2550\u255d\u255a\u2550\u2550\u2588\u2588\u2554\u2550\u2550\u255d\u2588\u2588\u2554\u2550\u2550\u2550\u2550\u255d\u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2557\n\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557   \u2588\u2588\u2551   \u2588\u2588\u2588\u2588\u2588\u2588\u2554\u255d\u2588\u2588\u2588\u2588\u2588\u2557  \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2551\u2588\u2588\u2554\u2588\u2588\u2588\u2588\u2554\u2588\u2588\u2551\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2588\u2588\u2588\u2588\u2554\u255d\u2588\u2588\u2551   \u2588\u2588\u2551   \u2588\u2588\u2551      \u2588\u2588\u2551   \u2588\u2588\u2588\u2588\u2588\u2557  \u2588\u2588\u2588\u2588\u2588\u2588\u2554\u255d\n\u255a\u2550\u2550\u2550\u2550\u2588\u2588\u2551   \u2588\u2588\u2551   \u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2557\u2588\u2588\u2554\u2550\u2550\u255d  \u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2551\u2588\u2588\u2551\u255a\u2588\u2588\u2554\u255d\u2588\u2588\u2551\u255a\u2550\u2550\u2550\u2550\u255d\u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2557\u2588\u2588\u2551   \u2588\u2588\u2551   \u2588\u2588\u2551      \u2588\u2588\u2551   \u2588\u2588\u2554\u2550\u2550\u255d  \u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2557\n\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2551   \u2588\u2588\u2551   \u2588\u2588\u2551  \u2588\u2588\u2551\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2551  \u2588\u2588\u2551\u2588\u2588\u2551 \u255a\u2550\u255d \u2588\u2588\u2551      \u2588\u2588\u2588\u2588\u2588\u2588\u2554\u255d\u255a\u2588\u2588\u2588\u2588\u2588\u2588\u2554\u255d   \u2588\u2588\u2551      \u2588\u2588\u2551   \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2551  \u2588\u2588\u2551\n\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u255d   \u255a\u2550\u255d   \u255a\u2550\u255d  \u255a\u2550\u255d\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u255d\u255a\u2550\u255d  \u255a\u2550\u255d\u255a\u2550\u255d     \u255a\u2550\u255d      \u255a\u2550\u2550\u2550\u2550\u2550\u255d  \u255a\u2550\u2550\u2550\u2550\u2550\u255d    \u255a\u2550\u255d      \u255a\u2550\u255d   \u255a\u2550\u2550\u2550\u2550\u2550\u2550\u255d\u255a\u2550\u255d  \u255a\u2550\u255d\n\nhttps://github.com/KevinLage/YouTube-Livestream-Botter",
    "last_modified": "2025-08-06T14:19:39.936929"
  },
  {
    "id": "818",
    "name": "removebg2.py",
    "path": "02_media_processing/image_tools/removebg2.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 30,
    "size": 854,
    "docstring": "",
    "keywords": [],
    "functions": [
      "remove_background"
    ],
    "classes": [],
    "imports": [
      "os",
      "rembg"
    ],
    "preview": "import os\n\nfrom rembg import remove\n\n# Suppress OpenMP warnings\nos.environ[\"OMP_NUM_THREADS\"] = \"1\"\nos.environ[\"KMP_WARNINGS\"] = \"0\"\n\n# Paths\ninput_path = \"/Users/steven/Pictures/etsy/Cookie-all/Tzip/t cut/14.png\"\noutput_path = \"/Users/steven/Pictures/etsy/Cookie-all/Tzip/t cut/14_no_bg.png\"\n\n\n# Function to remove background\ndef remove_background(input_file, output_file):\n    try:\n        with open(input_file, \"rb\") as file:\n            input_data = file.read()\n        output_data = remove(input_data)  # Process image with rembg\n        with open(output_file, \"wb\") as file:",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "819",
    "name": "resizer.py",
    "path": "02_media_processing/image_tools/resizer.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 88,
    "size": 2567,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "parse_args",
      "process_image",
      "main"
    ],
    "classes": [],
    "imports": [
      "argparse",
      "math",
      "os",
      "pathlib",
      "PIL"
    ],
    "preview": "#!/usr/bin/env python3\nimport argparse\nimport math\nimport os\nfrom pathlib import Path\n\nfrom PIL import Image\n\n\ndef parse_args():\n    p = argparse.ArgumentParser(\n        description=\"Resize images \u2265 threshold to 300 DPI and scale them so they dip below the byte\u2010limit.\"\n    )\n    p.add_argument(\"input_dir\", type=Path, help=\"Root folder to scan (will recurse into subdirs)\")\n    p.add_argument(\n        \"--threshold\",\n        type=int,\n        default=9 * 1024 * 1024,\n        help=\"Only process files at or above this size in bytes (default 9 MB)\",\n    )",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "820",
    "name": "translation.py",
    "path": "02_media_processing/image_tools/translation.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 108,
    "size": 7729,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [
      "Translation"
    ],
    "imports": [],
    "preview": "class Translation(object):\n    START_TEXT = \"\"\"Hi Bro, This Is Telegrams Yet Another Sassiest Downloader com Renamer Bot Hit /help to know how to use me\n\u00a9 @RedbullFed\"\"\"\n    RENAME_403_ERR = \"\ud83e\udd23\ud83e\udd23 Bye Bye... My Dev Restricted You From here\"\n    ABS_TEXT = \"Go Away Stupid \ud83e\udd26\u200d\u2640\ufe0f.\"\n    # UPGRADE_TEXT = \"\ud83d\ude05 Ok Bie\"\n    UPGRADE_TEXT = \"\"\"MwK MegaBot Paid Plans\n-------\nPlan: FREE\nFilesize limit: 1500 MB\nDaily limit: UNLIMITED\nPrice \ud83c\udf0e: \u20b9 0 / 30 Days\nFEATURES:\n\ud83d\udc49 All Supported Video Formats of https, except HLS videos!\n\ud83d\udc49 Get a Telegram sticker as a Telegram downloadable media.\n\ud83d\udc49 Upload as file from any HTTP link, with custom thumbnail support.\n\ud83d\udc49 Get Low Speed Direct Download Link of any Telegram file.\n\n---@redbullfed----\"\"\"\n",
    "last_modified": "2025-09-13T05:53:44.822602"
  },
  {
    "id": "821",
    "name": "test_gallery_logic.py",
    "path": "02_media_processing/image_tools/test_gallery_logic.py",
    "category": "02_media_processing",
    "type": "testing",
    "lines": 61,
    "size": 2340,
    "docstring": "",
    "keywords": [
      "testing"
    ],
    "functions": [
      "test_get_gallery_logic",
      "test_get_gallery_type"
    ],
    "classes": [
      "GalleryLogicTestCase"
    ],
    "imports": [
      "unittest",
      "simplegallery.logic.gallery_logic",
      "simplegallery.logic.variants.files_gallery_logic",
      "simplegallery.logic.variants.google_gallery_logic",
      "simplegallery.logic.variants.onedrive_gallery_logic"
    ],
    "preview": "import unittest\n\nimport simplegallery.logic.gallery_logic as gallery_logic\nfrom simplegallery.logic.variants.files_gallery_logic import FilesGalleryLogic\nfrom simplegallery.logic.variants.google_gallery_logic import GoogleGalleryLogic\nfrom simplegallery.logic.variants.onedrive_gallery_logic import OnedriveGalleryLogic\n\n\nclass GalleryLogicTestCase(unittest.TestCase):\n    def test_get_gallery_logic(self):\n        config_logic_mapping = [\n            (dict(), FilesGalleryLogic),\n            (dict(remote_gallery_type=\"\"), FilesGalleryLogic),\n            (dict(remote_gallery_type=\"onedrive\"), OnedriveGalleryLogic),\n            (dict(remote_gallery_type=\"google\"), GoogleGalleryLogic),\n            (dict(remote_gallery_type=\"aaaaaaaa\"), FilesGalleryLogic),\n        ]\n\n        for config_logic in config_logic_mapping:\n            self.assertIs(",
    "last_modified": "2025-09-13T05:53:52.768974"
  },
  {
    "id": "822",
    "name": "resize_oct25.py",
    "path": "02_media_processing/image_tools/resize_oct25.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 67,
    "size": 2382,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "resize_image_to_target_size",
      "process_images"
    ],
    "classes": [],
    "imports": [
      "os",
      "sys",
      "logging",
      "PIL"
    ],
    "preview": "import os\nimport sys\nimport logging\nfrom PIL import Image, UnidentifiedImageError\n\n# \u2728 Constants\nTARGET_DPI = 300\nSIZE_THRESHOLD_MB = 9\n\ndef resize_image_to_target_size(image, output_path):\n    \"\"\"Resize an image to fit within target file size and maintain aspect ratio.\"\"\"\n    width, height = image.size\n    aspect_ratio = width / height\n\n    # Determine new dimensions based on aspect ratio\n    if width / height > aspect_ratio:\n        new_width = width\n        new_height = int(width / aspect_ratio)\n    else:\n        new_width = int(height * aspect_ratio)",
    "last_modified": "2025-10-09T02:50:08.561675"
  },
  {
    "id": "823",
    "name": "utf8prober.py",
    "path": "02_media_processing/image_tools/utf8prober.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 83,
    "size": 2812,
    "docstring": "",
    "keywords": [],
    "functions": [
      "__init__",
      "reset",
      "charset_name",
      "language",
      "feed",
      "get_confidence"
    ],
    "classes": [
      "UTF8Prober"
    ],
    "imports": [
      "typing",
      "charsetprober",
      "codingstatemachine",
      "enums",
      "mbcssm"
    ],
    "preview": "######################## BEGIN LICENSE BLOCK ########################\n# The Original Code is mozilla.org code.\n#\n# The Initial Developer of the Original Code is\n# Netscape Communications Corporation.\n# Portions created by the Initial Developer are Copyright (C) 1998\n# the Initial Developer. All Rights Reserved.\n#\n# Contributor(s):\n#   Mark Pilgrim - port to Python\n#\n# This library is free software; you can redistribute it and/or\n# modify it under the terms of the GNU Lesser General Public\n# License as published by the Free Software Foundation; either\n# version 2.1 of the License, or (at your option) any later version.\n#\n# This library is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU\n# Lesser General Public License for more details.",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "824",
    "name": "test_bot_unlike.py",
    "path": "02_media_processing/image_tools/test_bot_unlike.py",
    "category": "02_media_processing",
    "type": "testing",
    "lines": 184,
    "size": 6569,
    "docstring": "",
    "keywords": [
      "testing"
    ],
    "functions": [
      "test_unlike",
      "test_unlike_comment",
      "test_unlike_medias",
      "test_unlike_media_comments",
      "test_unlike_user"
    ],
    "classes": [
      "TestBotFilter"
    ],
    "imports": [
      "pytest",
      "responses",
      "instabot.api.config",
      "test_bot",
      "test_variables",
      "unittest.mock",
      "mock"
    ],
    "preview": "import pytest\nimport responses\nfrom instabot.api.config import API_URL\n\nfrom .test_bot import TestBot\nfrom .test_variables import (\n    TEST_CAPTION_ITEM,\n    TEST_COMMENT_ITEM,\n    TEST_PHOTO_ITEM,\n    TEST_USERNAME_INFO_ITEM,\n)\n\ntry:\n    from unittest.mock import patch\nexcept ImportError:\n    from mock import patch\n\n\nclass TestBotFilter(TestBot):\n    @responses.activate",
    "last_modified": "2025-09-13T05:55:00.117017"
  },
  {
    "id": "825",
    "name": "organize_albums 4.py",
    "path": "02_media_processing/image_tools/organize_albums 4.py",
    "category": "02_media_processing",
    "type": "organization",
    "lines": 62,
    "size": 2473,
    "docstring": "",
    "keywords": [
      "organization"
    ],
    "functions": [
      "organize_files"
    ],
    "classes": [],
    "imports": [
      "os",
      "shutil"
    ],
    "preview": "import os\nimport shutil\n\n# Define the base directory\nbase_dir = \"/Users/steven/Music/nocTurneMeLoDieS/assests/all\"\n\n\n# Helper function to create folders and move files\ndef organize_files():\n    # List all files in the base directory\n    files = os.listdir(base_dir)\n\n    # Process each file\n    for file in files:\n        # Skip directories\n        if os.path.isdir(os.path.join(base_dir, file)):\n            continue\n\n        # Extract the base name (album name) from the file\n        if file.endswith(\".mp3\"):",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "826",
    "name": "organize_albums1.py",
    "path": "02_media_processing/image_tools/organize_albums1.py",
    "category": "02_media_processing",
    "type": "organization",
    "lines": 74,
    "size": 3100,
    "docstring": "",
    "keywords": [
      "organization"
    ],
    "functions": [
      "organize_files"
    ],
    "classes": [],
    "imports": [
      "os",
      "shutil"
    ],
    "preview": "import os\nimport shutil\n\n# Define the base directory\nbase_dir = \"/Users/steven/Music/nocTurneMeLoDieS/MP3\"\n\n\n# Helper function to create folders and move files\ndef organize_files():\n    # List all files in the base directory\n    files = os.listdir(base_dir)\n\n    # Process each file\n    for file in files:\n        # Skip directories\n        if os.path.isdir(os.path.join(base_dir, file)):\n            continue\n\n        # Extract the base name (album name) from the file\n        if file.endswith(\".mp3\"):",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "827",
    "name": "imgconvert.py",
    "path": "02_media_processing/image_tools/imgconvert.py",
    "category": "02_media_processing",
    "type": "image_processing",
    "lines": 53,
    "size": 1739,
    "docstring": "",
    "keywords": [],
    "functions": [
      "convert_and_upscale_images",
      "main"
    ],
    "classes": [],
    "imports": [
      "os",
      "PIL"
    ],
    "preview": "import os\n\nfrom PIL import Image\n\n# Function to convert WebP images to PNG and upscale by 200% with 300 DPI\n\n\ndef convert_and_upscale_images(source_directory, destination_directory):\n    # Create the destination directory if it doesn't exist\n    os.makedirs(destination_directory, exist_ok=True)\n\n    for filename in os.listdir(source_directory):\n        if filename.endswith(\".webp\"):\n            source_file = os.path.join(source_directory, filename)\n            filename_no_ext = os.path.splitext(filename)[0]\n            destination_file = os.path.join(destination_directory, f\"{filename_no_ext}.png\")\n\n            # Convert WebP to PNG and upscale by 200% with 300 DPI\n            im = Image.open(source_file)\n            width, height = im.size",
    "last_modified": "2025-09-13T05:55:28.421680"
  },
  {
    "id": "828",
    "name": "move.py",
    "path": "02_media_processing/image_tools/move.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 34,
    "size": 916,
    "docstring": "",
    "keywords": [],
    "functions": [
      "move_files_with_rsync"
    ],
    "classes": [],
    "imports": [
      "subprocess"
    ],
    "preview": "import subprocess\n\n\ndef move_files_with_rsync(source_dir, destination_dir):\n    command = [\n        \"rsync\",\n        \"-avP\",\n        \"--remove-source-files\",\n        \"--include=*.jpg\",\n        \"--include=*.jpeg\",\n        \"--include=*.png\",\n        \"--include=*.bmp\",\n        \"--include=*.gif\",\n        \"--include=*.tiff\",\n        \"--include=*/\",\n        \"--exclude=*\",\n        source_dir,\n        destination_dir,\n    ]\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "829",
    "name": "imgsearch.py",
    "path": "02_media_processing/image_tools/imgsearch.py",
    "category": "02_media_processing",
    "type": "image_processing",
    "lines": 41,
    "size": 1349,
    "docstring": "",
    "keywords": [],
    "functions": [
      "is_excluded_path"
    ],
    "classes": [],
    "imports": [
      "datetime",
      "os"
    ],
    "preview": "import datetime\nimport os\n\n\ndef is_excluded_path(path):\n    if path.startswith('/Users/steven/') and '/.' in path:\n        return True\n    excluded_paths = ['/Users/steven/.' ,'/System', '/Library', '/usr', '/bin', '/sbin', '/var', '/private', '/etc', '/tmp']\n    return any(path.startswith(excluded_path) for excluded_path in excluded_paths)\n\ndirectory_to_search = input(\"Please enter the directory to search for .png and .jpg files: \")\n\nif is_excluded_path(directory_to_search):\n    print(\"Excluded directories are not allowed.\")\n    exit()\n\ncurrent_date = datetime.datetime.now().strftime('%Y%m%d')\ndirectory_name = os.path.basename(os.path.normpath(directory_to_search))\nfilename = f\"{directory_name}_Images_{current_date}.csv\"\noutput_file = os.path.join(directory_to_search, filename)",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "830",
    "name": "mbcsgroupprober.py",
    "path": "02_media_processing/image_tools/mbcsgroupprober.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 58,
    "size": 2131,
    "docstring": "",
    "keywords": [],
    "functions": [
      "__init__"
    ],
    "classes": [
      "MBCSGroupProber"
    ],
    "imports": [
      "big5prober",
      "charsetgroupprober",
      "cp949prober",
      "enums",
      "eucjpprober",
      "euckrprober",
      "euctwprober",
      "gb2312prober",
      "johabprober",
      "sjisprober"
    ],
    "preview": "######################## BEGIN LICENSE BLOCK ########################\n# The Original Code is Mozilla Universal charset detector code.\n#\n# The Initial Developer of the Original Code is\n# Netscape Communications Corporation.\n# Portions created by the Initial Developer are Copyright (C) 2001\n# the Initial Developer. All Rights Reserved.\n#\n# Contributor(s):\n#   Mark Pilgrim - port to Python\n#   Shy Shalom - original C code\n#   Proofpoint, Inc.\n#\n# This library is free software; you can redistribute it and/or\n# modify it under the terms of the GNU Lesser General Public\n# License as published by the Free Software Foundation; either\n# version 2.1 of the License, or (at your option) any later version.\n#\n# This library is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "831",
    "name": "imgg-test.py",
    "path": "02_media_processing/image_tools/imgg-test.py",
    "category": "02_media_processing",
    "type": "image_processing",
    "lines": 18,
    "size": 436,
    "docstring": "",
    "keywords": [
      "web_tools"
    ],
    "functions": [],
    "classes": [],
    "imports": [
      "os",
      "requests",
      "dotenv"
    ],
    "preview": "import os\n\nimport requests\nfrom dotenv import load_dotenv\n\n# Load .env file\nload_dotenv(\"~/.env\")\npat = os.getenv(\"CLARIFAI_PAT\")\n\nauth = (os.getenv(\"IMAGGA_API_KEY\"), os.getenv(\"IMAGGA_API_SECRET\"))\nurl = \"\"\n\nresponse = requests.get(f\"https://api.imagga.com/v2/tags?image_url={url}\", auth=auth)\n\nprint(\"\ud83d\udd16 Imagga Tags:\")\nfor tag in response.json()[\"result\"][\"tags\"][:10]:\n    print(f\"- {tag['tag']['en']} ({tag['confidence']:.2f})\")\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "832",
    "name": "settings.py",
    "path": "02_media_processing/image_tools/settings.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 51,
    "size": 1148,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "pathlib"
    ],
    "preview": "from pathlib import Path\n\nsubreddits = [\n    \"AmItheAsshole\",\n    \"antiwork\",\n    \"AskMen\",\n    \"ChoosingBeggars\",\n    \"hatemyjob\",\n    \"NoStupidQuestions\",\n    \"pettyrevenge\",\n    \"Showerthoughts\",\n    \"TooAfraidToAsk\",\n    \"TwoXChromosomes\",\n    \"unpopularopinion\",\n]\n\nmax_video_length = 600  # Seconds\ncomment_limit = 600\n\nassets_directory = \"assets\"",
    "last_modified": "2025-03-28T18:35:46.763402"
  },
  {
    "id": "833",
    "name": "youtube_dl_echo 2.py",
    "path": "02_media_processing/image_tools/youtube_dl_echo 2.py",
    "category": "02_media_processing",
    "type": "youtube",
    "lines": 319,
    "size": 13139,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "logging",
      "asyncio",
      "json",
      "math",
      "os",
      "time",
      "pyrogram",
      "translation",
      "helper_funcs.chat_base",
      "helper_funcs.display_progress"
    ],
    "preview": "import logging\n\nlogging.basicConfig(\n    level=logging.DEBUG, format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\n)\nlogger = logging.getLogger(__name__)\n\nimport asyncio\nimport json\nimport math\nimport os\nimport time\n\n# the secret configuration specific things\nif bool(os.environ.get(\"WEBHOOK\", False)):\n    from sample_config import Config\nelse:\n    from config import Config\n\n# the Strings used for this \"thing\"",
    "last_modified": "2025-09-13T05:54:10.235026"
  },
  {
    "id": "834",
    "name": "midj.py",
    "path": "02_media_processing/image_tools/midj.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 28,
    "size": 650,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "json",
      "re"
    ],
    "preview": "import json\nimport re\n\n# Load the JSONL file.\nwith open(\n    \"/Users/steven/Pictures/midjourneyDownload_2023-10-13_1697181545353/metadata.jsonl\",\n    \"r\",\n) as f:\n    jsonl_data = json.load(f)\n\n# Compile the regular expression.\nregex = re.compile(r\"https:\\/\\/([\\w-]+.){1,}\\.(png)\")\n\n# Find all URLs in the JSONL file that match the regular expression.\nmatching_urls = []\nfor jsonl_object in jsonl_data:\n    url = jsonl_object.get(\"url\")\n    if regex.match(url):\n        matching_urls.append(url)\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "835",
    "name": "cb_buttons.py",
    "path": "02_media_processing/image_tools/cb_buttons.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 138,
    "size": 4849,
    "docstring": "",
    "keywords": [
      "youtube"
    ],
    "functions": [],
    "classes": [],
    "imports": [
      "logging",
      "json",
      "math",
      "os",
      "shutil",
      "subprocess",
      "time",
      "pyrogram",
      "translation",
      "hachoir.metadata"
    ],
    "preview": "import logging\n\nlogging.basicConfig(\n    level=logging.DEBUG, format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\n)\nlogger = logging.getLogger(__name__)\n\nimport json\nimport math\nimport os\nimport shutil\nimport subprocess\nimport time\n\n# the secret configuration specific things\nif bool(os.environ.get(\"WEBHOOK\", False)):\n    from sample_config import Config\nelse:\n    from config import Config\n",
    "last_modified": "2025-09-13T05:53:43.645264"
  },
  {
    "id": "836",
    "name": "main_20221230225837.py",
    "path": "02_media_processing/image_tools/main_20221230225837.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 429,
    "size": 15518,
    "docstring": "",
    "keywords": [
      "transcription",
      "video_processing",
      "analysis",
      "youtube",
      "web_tools",
      "organization"
    ],
    "functions": [
      "downloadVideo",
      "scrapeVideos",
      "usedQuoteToDifferentFile",
      "getQuoteFromApi",
      "getQuoteFromTxtFile",
      "videoIntro",
      "createVideo",
      "audioClip",
      "randomBgMusic",
      "deleteTempFiles"
    ],
    "classes": [
      "bcolors"
    ],
    "imports": [
      "glob",
      "json",
      "os",
      "random",
      "subprocess",
      "gtts",
      "requests",
      "moviepy.editor",
      "moviepy.video.io.VideoFileClip",
      "mutagen.mp3"
    ],
    "preview": "import glob\nimport json\nimport os\nimport random\nimport subprocess\n\nimport gtts\nimport requests\nfrom moviepy.editor import *\nfrom moviepy.video.io.VideoFileClip import VideoFileClip\nfrom mutagen.mp3 import MP3\nfrom tqdm.auto import tqdm\n\n\n# download background video from pexels - https://www.pexels.com/api/documentation/#videos-search__parameters\ndef downloadVideo(id) -> str:\n    \"\"\"Downloads video from Pexels with the according video ID\"\"\"\n    url = \"https://www.pexels.com/video/\" + str(id) + \"/download.mp4\"\n    # Streaming, so we can iterate over the response.\n    response = requests.get(url, stream=True)",
    "last_modified": "2025-09-13T05:53:56.734743"
  },
  {
    "id": "837",
    "name": "botError.py",
    "path": "02_media_processing/image_tools/botError.py",
    "category": "02_media_processing",
    "type": "web_tools",
    "lines": 706,
    "size": 20655,
    "docstring": "Created in 10/2020\n@Author: Paulo https://github.com/alpdias",
    "keywords": [],
    "functions": [
      "artName",
      "functionLike",
      "functionComment",
      "functionStories",
      "functionDraw",
      "botlogin",
      "findhashtag",
      "like",
      "botlogin",
      "findhashtag"
    ],
    "classes": [],
    "imports": [
      "os",
      "platform",
      "pathlib",
      "time",
      "pyfiglet",
      "selenium",
      "selenium.webdriver.common.keys"
    ],
    "preview": "# -*- coding: utf-8 -*-\n\n\"\"\"\nCreated in 10/2020\n@Author: Paulo https://github.com/alpdias\n\"\"\"\n\n# imported libraries\nimport os\nimport platform\nfrom pathlib import Path\nfrom time import sleep\n\nfrom pyfiglet import Figlet\nfrom selenium import webdriver\nfrom selenium.webdriver.common.keys import Keys\n\n\ndef artName(timeSleep=0):\n    \"\"\"",
    "last_modified": "2025-09-13T05:54:06.932977"
  },
  {
    "id": "838",
    "name": "scan.py",
    "path": "02_media_processing/image_tools/scan.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 32,
    "size": 837,
    "docstring": "",
    "keywords": [],
    "functions": [
      "scan_directory"
    ],
    "classes": [],
    "imports": [
      "os"
    ],
    "preview": "import os\n\n# List of drives to scan\ndrives = [\n    \"/Users/steven/CoH-Story\",\n    \"/Users/steven/CoMic\",\n    \"/Users/steven/Downloads\",\n    \"/Users/steven/Documents\",\n    \"/Users/steven/\",\n    \"/Users/steven/Music\",\n    \"/Users/steven/Pictures\",\n]\n# File types to look for\nfile_types = (\".jpg\", \".jpeg\", \".png\", \".gif\", \"webp\")\n\n\n# Function to scan a directory for image files\ndef scan_directory(directory):\n    for root, dirs, files in os.walk(directory):\n        for file in files:",
    "last_modified": "2025-05-04T22:47:11.913272"
  },
  {
    "id": "839",
    "name": "converts.py",
    "path": "02_media_processing/image_tools/converts.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 62,
    "size": 2268,
    "docstring": "",
    "keywords": [],
    "functions": [
      "convert_and_upscale_images",
      "main"
    ],
    "classes": [],
    "imports": [
      "os",
      "PIL"
    ],
    "preview": "import os\n\nfrom PIL import Image\n\n\ndef convert_and_upscale_images(source_directory, destination_directory):\n    # Create the destination directory if it doesn't exist\n    os.makedirs(destination_directory, exist_ok=True)\n\n    for filename in os.listdir(source_directory):\n        if filename.endswith((\".tiff\", \".png\", \".jpg\", \".jpeg\")):\n            source_file = os.path.join(source_directory, filename)\n            filename_no_ext, file_ext = os.path.splitext(filename)\n            file_ext = file_ext.lower()\n\n            if file_ext == \".tiff\":\n                destination_file = os.path.join(destination_directory, f\"{filename_no_ext}.png\")\n            elif file_ext in [\".png\", \".jpg\", \".jpeg\"]:\n                destination_file = os.path.join(destination_directory, filename)\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "840",
    "name": "AskReddit.py",
    "path": "02_media_processing/image_tools/AskReddit.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 257,
    "size": 8138,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "random_title_msg",
      "write_to_log",
      "check_video_in_db",
      "prompt_create_submission_video",
      "create_submission_video",
      "gen_video_from_hot",
      "main"
    ],
    "classes": [],
    "imports": [
      "json",
      "praw",
      "clips",
      "tinydb",
      "yt_upload"
    ],
    "preview": "#!./venv/bin/python\nimport json\n\nimport praw\nfrom clips import *\nfrom tinydb import Query, TinyDB\nfrom yt_upload import upload_video\n\ndb = TinyDB(\"log/db.json\")\ncreated_vids_db = db.table(\"created_videos\")\nuploaded_vids_db = db.table(\"uploaded_vids\")\n\nFPS = 30\nBACKGROUND_TRACK_VOLUME = 0.12\nDURATION: int = 60 * 5\nDESCRIPTION = \"Yes I'm an actual robot. \\n\"\n\n\ndef random_title_msg():\n    return \"Subscribe or I'll end humanity.\"",
    "last_modified": "2025-09-13T05:53:51.391388"
  },
  {
    "id": "841",
    "name": "100sort.py",
    "path": "02_media_processing/image_tools/100sort.py",
    "category": "02_media_processing",
    "type": "organization",
    "lines": 38,
    "size": 1254,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "os",
      "shutil"
    ],
    "preview": "import os\nimport shutil\n\n# Prompt the user for the source directory\nsource_dir = input(\"Enter the source directory path: \")\n\n# Verify that the source directory exists\nif not os.path.exists(source_dir):\n    print(\"Source directory does not exist.\")\n    exit()\n\n# Prompt the user for the destination directory\ndestination_dir = input(\"Enter the destination directory path: \")\n\n# Create the destination directory if it doesn't exist\nos.makedirs(destination_dir, exist_ok=True)\n\n# Initialize variables\nimages_per_folder = 250\ncurrent_folder = None",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "842",
    "name": "instagram.py",
    "path": "02_media_processing/image_tools/instagram.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 2554,
    "size": 114277,
    "docstring": "",
    "keywords": [
      "web_tools"
    ],
    "functions": [
      "__init__",
      "script",
      "menu",
      "islemSec",
      "secilenIslemiGoster",
      "profilSec",
      "ilkGonderiTikla",
      "gonderileriIndir",
      "gonderileriBegen",
      "topluTakiptenCik"
    ],
    "classes": [
      "Instagram"
    ],
    "imports": [
      "getpass",
      "json",
      "os",
      "threading",
      "urllib.request",
      "builtins",
      "datetime",
      "random",
      "time",
      "requests"
    ],
    "preview": "import getpass\nimport json\nimport os\nimport threading\nimport urllib.request\nfrom builtins import print\nfrom datetime import datetime\nfrom random import randint\nfrom time import sleep\n\nimport requests\nfrom colorama import init\nfrom selenium import webdriver\nfrom selenium.webdriver.common.keys import Keys\nfrom selenium.webdriver.firefox.options import Options\nfrom termcolor import colored\n\n\nclass Instagram:\n    def __init__(self):",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "843",
    "name": "generate_album_html-pages_fixed 1.py",
    "path": "02_media_processing/image_tools/generate_album_html-pages_fixed 1.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 157,
    "size": 4911,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "os",
      "pathlib"
    ],
    "preview": "import os\nfrom pathlib import Path\n\nalbums_dir = Path(\"/Users/steven/Music/nocTurneMeLoDieS/mp3\")\noutput_file = albums_dir / \"disco25.html\"\n\nhtml_header = \"\"\"<!DOCTYPE html>\n<html lang=\"en\">\n<head>\n    <meta charset=\"UTF-8\">\n    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n    <title>Discography with MP3</title>\n    <style>\n        body { font-family: Arial, sans-serif; background-color: #f4f4f4; margin: 0; padding: 0; }\n        h1 { text-align: center; margin-top: 20px; font-size: 32px; color: #333; }\n        .grid-container {\n            display: grid;\n            grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));\n            gap: 20px;\n            padding: 20px;",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "844",
    "name": "9mbs.py",
    "path": "02_media_processing/image_tools/9mbs.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 90,
    "size": 3154,
    "docstring": "",
    "keywords": [],
    "functions": [
      "is_large_image",
      "resize_image_to_max_size",
      "resize_images_in_directory",
      "main"
    ],
    "classes": [],
    "imports": [
      "os",
      "PIL"
    ],
    "preview": "import os\n\nfrom PIL import Image\n\n# Set a limit for maximum pixels to avoid decompression bomb error\nImage.MAX_IMAGE_PIXELS = (\n    178956970  # Default limit, can be adjusted or removed entirely using 'None'\n)\n\n\ndef is_large_image(image_path):\n    \"\"\"Check if the image is too large based on pixel dimensions.\"\"\"\n    with Image.open(image_path) as img:\n        width, height = img.size\n        return (width * height) > Image.MAX_IMAGE_PIXELS\n\n\ndef resize_image_to_max_size(image_path, max_size_mb=9, upscale=True):\n    max_size_bytes = max_size_mb * 1024 * 1024  # Convert MB to bytes\n    target_dpi = (300, 300)  # Set target DPI",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "845",
    "name": "img-to-pdf-folder.py",
    "path": "02_media_processing/image_tools/img-to-pdf-folder.py",
    "category": "02_media_processing",
    "type": "image_processing",
    "lines": 33,
    "size": 1035,
    "docstring": "",
    "keywords": [
      "image_processing"
    ],
    "functions": [
      "image_to_pdf",
      "convert_images_to_pdf"
    ],
    "classes": [],
    "imports": [
      "os",
      "PIL",
      "reportlab.lib.pagesizes",
      "reportlab.pdfgen"
    ],
    "preview": "import os\n\nfrom PIL import Image\nfrom reportlab.lib.pagesizes import letter\nfrom reportlab.pdfgen import canvas\n\n\n# Function to convert an image to PDF\ndef image_to_pdf(image_path, pdf_path):\n    c = canvas.Canvas(pdf_path, pagesize=letter)\n    img = Image.open(image_path)\n    width, height = img.size\n    c.setPageSize((width, height))\n    c.drawImage(image_path, 0, 0, width, height)\n    c.showPage()\n    c.save()\n\n\n# Function to traverse the directory and convert images to PDFs\ndef convert_images_to_pdf(root_dir):",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "846",
    "name": "audiometadata.py",
    "path": "02_media_processing/image_tools/audiometadata.py",
    "category": "02_media_processing",
    "type": "transcription",
    "lines": 88,
    "size": 2862,
    "docstring": "Apply audio metadata",
    "keywords": [],
    "functions": [
      "__init__",
      "__load__",
      "apply_album_art",
      "__resize__",
      "apply_track_info"
    ],
    "classes": [
      "AudioMetadata"
    ],
    "imports": [
      "logging",
      "mutagen.easyid3",
      "mutagen.id3",
      "mutagen.mp3",
      "PIL",
      "ytdl.customerrors",
      "ytdl.oshelper"
    ],
    "preview": "\"Apply audio metadata\"\n\nimport logging\n\nfrom mutagen.easyid3 import EasyID3\nfrom mutagen.id3 import APIC, ID3, error\nfrom mutagen.mp3 import MP3\nfrom PIL import Image\nfrom ytdl.customerrors import FileNotFoundError\nfrom ytdl.oshelper import DEFAULT_FILE_NAME, dirname, filename_no_extension, join_paths\n\n\nclass AudioMetadata(object):\n    \"Responsible for applying metadata to file\"\n\n    def __init__(self, track_file):\n        self.track_file = track_file\n        self.logger = logging.getLogger(__name__)\n\n    def __load__(self):",
    "last_modified": "2025-09-13T05:54:14.912666"
  },
  {
    "id": "847",
    "name": "bookmarks.py",
    "path": "02_media_processing/image_tools/bookmarks.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 28,
    "size": 691,
    "docstring": "",
    "keywords": [],
    "functions": [
      "add_bookmarks"
    ],
    "classes": [],
    "imports": [
      "pypdf"
    ],
    "preview": "from pypdf import PdfReader, PdfWriter\n\n\ndef add_bookmarks(output_path, toc_items):\n    reader = PdfReader(output_path)\n    writer = PdfWriter()\n\n    for page in reader.pages:\n        writer.add_page(page)\n\n    for item in toc_items:\n        title = item[\"title\"]\n        page_num = item[\"page\"] - 1  # Page numbers are zero-indexed\n        writer.add_outline_item(title, page_num)\n\n    with open(output_path, \"wb\") as output_file:\n        writer.write(output_file)\n\n\ntoc_items = [",
    "last_modified": "2025-05-04T22:47:13.331578"
  },
  {
    "id": "848",
    "name": "organize_music_library.py",
    "path": "02_media_processing/image_tools/organize_music_library.py",
    "category": "02_media_processing",
    "type": "audio_processing",
    "lines": 55,
    "size": 1679,
    "docstring": "",
    "keywords": [
      "organization"
    ],
    "functions": [
      "normalize_name",
      "move_file",
      "organize_music_library"
    ],
    "classes": [],
    "imports": [
      "os",
      "re",
      "shutil"
    ],
    "preview": "import os\nimport re\nimport shutil\n\nBASE_DIR = \"/Users/steven/Movies/project2025/Media\"\n\n\ndef normalize_name(name):\n    return re.sub(r\"[^\\w\\s-]\", \"\", name).strip().replace(\" \", \"_\")\n\n\ndef move_file(src, dest):\n    os.makedirs(os.path.dirname(dest), exist_ok=True)\n    if not os.path.exists(dest):\n        shutil.move(src, dest)\n        print(f\"\u2705 Moved: {src} -> {dest}\")\n    else:\n        print(f\"\u26a0\ufe0f File exists, skipped: {dest}\")\n\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "849",
    "name": "youtube_dl_echo.py",
    "path": "02_media_processing/image_tools/youtube_dl_echo.py",
    "category": "02_media_processing",
    "type": "youtube",
    "lines": 319,
    "size": 13139,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "logging",
      "asyncio",
      "json",
      "math",
      "os",
      "time",
      "pyrogram",
      "translation",
      "helper_funcs.chat_base",
      "helper_funcs.display_progress"
    ],
    "preview": "import logging\n\nlogging.basicConfig(\n    level=logging.DEBUG, format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\n)\nlogger = logging.getLogger(__name__)\n\nimport asyncio\nimport json\nimport math\nimport os\nimport time\n\n# the secret configuration specific things\nif bool(os.environ.get(\"WEBHOOK\", False)):\n    from sample_config import Config\nelse:\n    from config import Config\n\n# the Strings used for this \"thing\"",
    "last_modified": "2025-09-13T05:53:44.747546"
  },
  {
    "id": "850",
    "name": "upscale 2 copy.py",
    "path": "02_media_processing/image_tools/upscale 2 copy.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 76,
    "size": 1850,
    "docstring": "",
    "keywords": [
      "web_tools"
    ],
    "functions": [],
    "classes": [],
    "imports": [
      "json",
      "os",
      "time",
      "requests"
    ],
    "preview": "import json\nimport os\nimport time\n\nimport requests\n\napi_key = os.getenv(\"API_KEY\")\nif not api_key:\n    raise ValueError(\n        \"API key is not set. Please ensure the API_KEY environment variable is configured correctly.\"\n    )\n\nauthorization = f\"Bearer {api_key}\"\n\nheaders = {\n    \"accept\": \"application/json\",\n    \"content-type\": \"application/json\",\n    \"authorization\": authorization,\n}\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "851",
    "name": "get_gifs.py",
    "path": "02_media_processing/image_tools/get_gifs.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 96,
    "size": 2071,
    "docstring": "",
    "keywords": [
      "youtube",
      "web_tools"
    ],
    "functions": [
      "download_gif",
      "get_giphy_results",
      "download_gifs_from_terms"
    ],
    "classes": [],
    "imports": [
      "json",
      "sys",
      "time",
      "pathlib",
      "requests"
    ],
    "preview": "# Overview\n# - download gifs from giphy\n\n# Dependencies\n\nimport json\nimport sys\nimport time\nfrom pathlib import Path\n\nimport requests\n\n# Funcs\n\n\ndef download_gif(giphy_id: str, output_path: str):\n    \"\"\"\n    download gif from giphy by giphy id\n\n    :param giphy_id: str, ID of gif from giphy",
    "last_modified": "2025-03-28T18:36:55.645232"
  },
  {
    "id": "852",
    "name": "YouTubeBot 3.py",
    "path": "02_media_processing/image_tools/YouTubeBot 3.py",
    "category": "02_media_processing",
    "type": "youtube",
    "lines": 158,
    "size": 4910,
    "docstring": "",
    "keywords": [
      "web_tools"
    ],
    "functions": [
      "fetch",
      "filter",
      "duration_split",
      "start"
    ],
    "classes": [],
    "imports": [
      "time",
      "tkinter",
      "tkinter.ttk",
      "pyautogui",
      "PIL",
      "selenium",
      "requests"
    ],
    "preview": "import time\nimport tkinter as tk\nimport tkinter.ttk as ttk\n\nimport pyautogui\nfrom PIL import Image, ImageTk\nfrom selenium import webdriver\n\nheight = pyautogui.size()[1]\nwidth = pyautogui.size()[0]\nprint(\"resolution = \" + str(width) + \", \" + str(height))\nwindow = tk.Tk()\nwindow.title(\"YouTube Bot\")\n\nwindow.resizable(0, 0)\nwindow.configure(background=\"white\")\nwindow.rowconfigure([0], minsize=round(width / 96), weight=0)\nwindow.columnconfigure([0, 2], minsize=round(width / 24), weight=0)\nwindow.columnconfigure(1, minsize=round(width / 2.13), weight=0)\n",
    "last_modified": "2025-09-13T05:54:08.258372"
  },
  {
    "id": "853",
    "name": "generate_album_html-pages_fixed.py",
    "path": "02_media_processing/image_tools/generate_album_html-pages_fixed.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 157,
    "size": 4911,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "os",
      "pathlib"
    ],
    "preview": "import os\nfrom pathlib import Path\n\nalbums_dir = Path(\"/Users/steven/Music/nocTurneMeLoDieS/Mp3\")\noutput_file = albums_dir / \"disco25.html\"\n\nhtml_header = \"\"\"<!DOCTYPE html>\n<html lang=\"en\">\n<head>\n    <meta charset=\"UTF-8\">\n    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n    <title>Discography with MP3</title>\n    <style>\n        body { font-family: Arial, sans-serif; background-color: #f4f4f4; margin: 0; padding: 0; }\n        h1 { text-align: center; margin-top: 20px; font-size: 32px; color: #333; }\n        .grid-container {\n            display: grid;\n            grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));\n            gap: 20px;\n            padding: 20px;",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "854",
    "name": "8mb (1).py",
    "path": "02_media_processing/image_tools/8mb (1).py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 71,
    "size": 2482,
    "docstring": "",
    "keywords": [],
    "functions": [
      "resize_image_to_max_size",
      "resize_images_in_directory"
    ],
    "classes": [],
    "imports": [
      "os",
      "PIL"
    ],
    "preview": "import os\n\nfrom PIL import Image\n\n\ndef resize_image_to_max_size(image_path, max_size_mb=8):\n    \"\"\"\n    Resize a PNG image to ensure it doesn't exceed the specified max size (in MB).\n\n    :param image_path: Path to the input PNG image.\n    :param max_size_mb: Maximum allowed size for the image in megabytes (default is 8MB).\n    \"\"\"\n    max_size_bytes = max_size_mb * 1024 * 1024  # Convert MB to bytes\n\n    img = Image.open(image_path)\n    current_size = os.path.getsize(image_path)\n\n    if current_size <= max_size_bytes:\n        print(f\"{os.path.basename(image_path)} is already under {max_size_mb}MB.\")\n        return",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "855",
    "name": "motion-upload copy.py",
    "path": "02_media_processing/image_tools/motion-upload copy.py",
    "category": "02_media_processing",
    "type": "youtube",
    "lines": 58,
    "size": 1678,
    "docstring": "",
    "keywords": [
      "web_tools"
    ],
    "functions": [],
    "classes": [],
    "imports": [
      "json",
      "time",
      "requests"
    ],
    "preview": "import json\nimport time\n\nimport requests\n\napi_key = \"<YOUR_API_KEY>\"\nauthorization = \"Bearer %s\" % api_key\n\nheaders = {\n    \"accept\": \"application/json\",\n    \"content-type\": \"application/json\",\n    \"authorization\": authorization,\n}\n\n# Get a presigned URL for uploading an image\nurl = \"https://cloud.leonardo.ai/api/rest/v1/init-image\"\n\npayload = {\"extension\": \"jpg\"}\n\nresponse = requests.post(url, json=payload, headers=headers)",
    "last_modified": "2025-05-04T22:47:12.937073"
  },
  {
    "id": "856",
    "name": "sort2 (1).py",
    "path": "02_media_processing/image_tools/sort2 (1).py",
    "category": "02_media_processing",
    "type": "organization",
    "lines": 61,
    "size": 2414,
    "docstring": "",
    "keywords": [
      "organization"
    ],
    "functions": [
      "sort_images"
    ],
    "classes": [],
    "imports": [
      "os",
      "shutil",
      "datetime",
      "PIL"
    ],
    "preview": "import os\nimport shutil\nfrom datetime import datetime\n\nfrom PIL import Image\n\n\ndef sort_images(source_dir, target_dir):\n    # Create the target directory if it doesn't exist\n    os.makedirs(target_dir, exist_ok=True)\n\n    # Supported image formats\n    extensions = (\".png\", \".jpg\", \".jpeg\", \".tiff\", \".mp4\", \".webp\")\n\n    # Walk through the directory\n    for root, dirs, files in os.walk(source_dir):\n        for file in files:\n            if file.lower().endswith(extensions):\n                try:\n                    # Construct the full file path",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "857",
    "name": "playlist_20221230180525.py",
    "path": "02_media_processing/image_tools/playlist_20221230180525.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 168,
    "size": 6429,
    "docstring": "",
    "keywords": [
      "youtube",
      "analysis",
      "web_tools"
    ],
    "functions": [
      "generate_token",
      "get_youtube_url",
      "download_mp3",
      "last_fm_artist_info",
      "set_metadata",
      "write_tracks",
      "write_playlist",
      "split_spotify_uri",
      "get_os",
      "get_folder"
    ],
    "classes": [],
    "imports": [
      "argparse",
      "json",
      "os",
      "re",
      "subprocess",
      "urllib.parse",
      "urllib.request",
      "secret",
      "spotipy",
      "spotipy.oauth2"
    ],
    "preview": "import argparse\nimport json\nimport os\nimport re\nimport subprocess\nimport urllib.parse\nimport urllib.request\n\nimport secret\nimport spotipy\nimport spotipy.oauth2 as oauth2\nfrom requests.utils import quote\n\n\ndef generate_token():\n    credentials = oauth2.SpotifyClientCredentials(\n        client_id=secret.28b20556906f4b75874c4ae98320c81d,\n        client_secret=secret.c7033fd14e1247cfb9eef73874dd2365 \n)\n    token = credentials.get_access_token()",
    "last_modified": "2025-05-04T23:28:21.397414"
  },
  {
    "id": "858",
    "name": "motion-upload 3.py",
    "path": "02_media_processing/image_tools/motion-upload 3.py",
    "category": "02_media_processing",
    "type": "youtube",
    "lines": 57,
    "size": 1679,
    "docstring": "",
    "keywords": [
      "web_tools"
    ],
    "functions": [],
    "classes": [],
    "imports": [
      "json",
      "time",
      "requests"
    ],
    "preview": "import json\nimport time\n\nimport requests\n\napi_key = \"<YOUR_API_KEY>\"\nauthorization = \"Bearer %s\" % api_key\n\nheaders = {\n    \"accept\": \"application/json\",\n    \"content-type\": \"application/json\",\n    \"authorization\": authorization,\n}\n\n# Get a presigned URL for uploading an image\nurl = \"https://cloud.leonardo.ai/api/rest/v1/init-image\"\n\npayload = {\"extension\": \"jpg\"}\n\nresponse = requests.post(url, json=payload, headers=headers)",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "859",
    "name": "vidGen.py",
    "path": "02_media_processing/image_tools/vidGen.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 241,
    "size": 8359,
    "docstring": "",
    "keywords": [
      "opencv"
    ],
    "functions": [
      "getFileNames",
      "deleteSkippedClips",
      "deleteAllFilesInPath",
      "renderThread",
      "renderVideo"
    ],
    "classes": [],
    "imports": [
      "datetime",
      "os",
      "pickle",
      "random",
      "re",
      "shutil",
      "subprocess",
      "time",
      "distutils.dir_util",
      "time"
    ],
    "preview": "import datetime\nimport os\nimport pickle\nimport random\nimport re\nimport shutil\nimport subprocess\nimport time\nfrom distutils.dir_util import copy_tree\nfrom time import sleep\n\nimport cv2\nimport settings\n\n# File Paths\n\n\n# Creating file paths that are needed\n\n",
    "last_modified": "2025-09-13T05:53:32.666070"
  },
  {
    "id": "860",
    "name": "web-png-upscale copy.py",
    "path": "02_media_processing/image_tools/web-png-upscale copy.py",
    "category": "02_media_processing",
    "type": "web_tools",
    "lines": 53,
    "size": 1739,
    "docstring": "",
    "keywords": [],
    "functions": [
      "convert_and_upscale_images",
      "main"
    ],
    "classes": [],
    "imports": [
      "os",
      "PIL"
    ],
    "preview": "import os\n\nfrom PIL import Image\n\n# Function to convert WebP images to PNG and upscale by 200% with 300 DPI\n\n\ndef convert_and_upscale_images(source_directory, destination_directory):\n    # Create the destination directory if it doesn't exist\n    os.makedirs(destination_directory, exist_ok=True)\n\n    for filename in os.listdir(source_directory):\n        if filename.endswith(\".webp\"):\n            source_file = os.path.join(source_directory, filename)\n            filename_no_ext = os.path.splitext(filename)[0]\n            destination_file = os.path.join(destination_directory, f\"{filename_no_ext}.png\")\n\n            # Convert WebP to PNG and upscale by 200% with 300 DPI\n            im = Image.open(source_file)\n            width, height = im.size",
    "last_modified": "2025-09-13T05:55:28.619379"
  },
  {
    "id": "861",
    "name": "organize_albums 10.py",
    "path": "02_media_processing/image_tools/organize_albums 10.py",
    "category": "02_media_processing",
    "type": "organization",
    "lines": 62,
    "size": 2466,
    "docstring": "",
    "keywords": [
      "organization"
    ],
    "functions": [
      "organize_files"
    ],
    "classes": [],
    "imports": [
      "os",
      "shutil"
    ],
    "preview": "import os\nimport shutil\n\n# Define the base directory\nbase_dir = \"/Users/steven/Music/NocTurnE-meLoDieS/mp3\"\n\n\n# Helper function to create folders and move files\ndef organize_files():\n    # List all files in the base directory\n    files = os.listdir(base_dir)\n\n    # Process each file\n    for file in files:\n        # Skip directories\n        if os.path.isdir(os.path.join(base_dir, file)):\n            continue\n\n        # Extract the base name (album name) from the file\n        if file.endswith(\".mp3\"):",
    "last_modified": "2025-05-04T22:47:13"
  },
  {
    "id": "862",
    "name": "api_photo.py",
    "path": "02_media_processing/image_tools/api_photo.py",
    "category": "02_media_processing",
    "type": "image_processing",
    "lines": 471,
    "size": 17197,
    "docstring": "",
    "keywords": [
      "youtube"
    ],
    "functions": [
      "download_photo",
      "compatible_aspect_ratio",
      "configure_photo",
      "upload_photo",
      "upload_album",
      "get_image_size",
      "resize_image",
      "stories_shaper"
    ],
    "classes": [],
    "imports": [
      "__future__",
      "imghdr",
      "json",
      "os",
      "random",
      "shutil",
      "struct",
      "time",
      "uuid",
      "math"
    ],
    "preview": "from __future__ import unicode_literals\n\nimport imghdr\nimport json\nimport os\nimport random\nimport shutil\nimport struct\nimport time\nfrom uuid import uuid4\n\nfrom . import config\n\n\ndef download_photo(self, media_id, filename, media=False, folder=\"photos\"):\n    if not media:\n        self.media_info(media_id)\n        if not self.last_json.get(\"items\"):\n            return True\n        media = self.last_json[\"items\"][0]",
    "last_modified": "2025-09-13T05:54:56.577285"
  },
  {
    "id": "863",
    "name": "upload_story_photo.py",
    "path": "02_media_processing/image_tools/upload_story_photo.py",
    "category": "02_media_processing",
    "type": "youtube",
    "lines": 20,
    "size": 596,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "argparse",
      "os",
      "sys",
      "instabot"
    ],
    "preview": "import argparse\nimport os\nimport sys\n\nsys.path.append(os.path.join(sys.path[0], \"../\"))\nfrom instabot import Bot  # noqa: E402\n\nparser = argparse.ArgumentParser(add_help=True)\nparser.add_argument(\"-u\", type=str, help=\"username\")\nparser.add_argument(\"-p\", type=str, help=\"password\")\nparser.add_argument(\"-proxy\", type=str, help=\"proxy\")\nparser.add_argument(\"-photo\", type=str, help=\"photo name like 'picture.jpg' \")\nargs = parser.parse_args()\n\nbot = Bot()\nbot.login(username=args.u, password=args.p, proxy=args.proxy)\n\n# Publish a new story with the given photo\nbot.upload_story_photo(args.photo)\n",
    "last_modified": "2025-03-28T18:35:47"
  },
  {
    "id": "864",
    "name": "gemini_storybook_downloader.py",
    "path": "02_media_processing/image_tools/gemini_storybook_downloader.py",
    "category": "02_media_processing",
    "type": "transcription",
    "lines": 410,
    "size": 12456,
    "docstring": "Gemini Storybook Downloader (macOS-friendly)\n--------------------------------------------\n\nDownloads assets (images, videos, audio) and text/metadata from public\nGemini \"storybook\" share links, e.g.:\n  https://gemini.google.com/gem/storybook/<id>\n  https://g.co/gemini/share/<id>\n\nIt renders pages with Playwright (Chromium) so JS-driven content is captured.\n\nUsage:\n  # one-off\n  python gemini_storybook_downloader.py <url1> <url2> ...\n\n  # or from a file (one URL per line)\n  python gemini_storybook_downloader.py --file urls.txt\n\nOutput:\n  ./downloads/<sanitized-title-or-id>/\n      page.html\n      metadata.json\n      text.md\n      assets/\n        img_001.jpg ...\n        video_001.mp4 ...\n        audio_001.mp3 ...\n      manifest.csv\n\nInstall:\n  pip install -r requirements.txt\n  playwright install chromium\n\nNotes:\n- Only public share links are supported. If a link requires login, this script\n  will still attempt to render but may save a \"Sign in\" page instead.\n- This script avoids duplicate downloads via a content-hash manifest.",
    "keywords": [],
    "functions": [
      "slugify",
      "ensure_dir",
      "sha256_bytes",
      "sanitize_url_to_id",
      "ext_for"
    ],
    "classes": [
      "DownloadRecord"
    ],
    "imports": [
      "__future__",
      "argparse",
      "asyncio",
      "csv",
      "hashlib",
      "json",
      "os",
      "re",
      "sys",
      "time"
    ],
    "preview": "#!/usr/bin/env python3\n\"\"\"\nGemini Storybook Downloader (macOS-friendly)\n--------------------------------------------\n\nDownloads assets (images, videos, audio) and text/metadata from public\nGemini \"storybook\" share links, e.g.:\n  https://gemini.google.com/gem/storybook/<id>\n  https://g.co/gemini/share/<id>\n\nIt renders pages with Playwright (Chromium) so JS-driven content is captured.\n\nUsage:\n  # one-off\n  python gemini_storybook_downloader.py <url1> <url2> ...\n\n  # or from a file (one URL per line)\n  python gemini_storybook_downloader.py --file urls.txt\n\nOutput:",
    "last_modified": "2025-09-13T05:54:31.192118"
  },
  {
    "id": "865",
    "name": "upscale2.py",
    "path": "02_media_processing/image_tools/upscale2.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 85,
    "size": 2862,
    "docstring": "",
    "keywords": [],
    "functions": [
      "upscale_and_save_image",
      "compress_image_to_size"
    ],
    "classes": [],
    "imports": [
      "os",
      "PIL"
    ],
    "preview": "import os\n\nfrom PIL import Image\n\n# Input directory (no output directory needed since we're replacing the original images)\ninput_dir = \"/Users/steven/Music/NocTurnE-meLoDieS\"\n\n# Max file size in bytes (9 MB = 9 * 1024 * 1024)\nmax_size = 9 * 1024 * 1024\n\n\ndef upscale_and_save_image(image_path, max_size=max_size, dpi=300):\n    \"\"\"\n    Upscale the image by 2x, set the DPI, and compress to ensure the file size is <= 9MB.\n    Args:\n        image_path (str): Path to the input image.\n        max_size (int): Maximum file size in bytes.\n        dpi (int): Target DPI (default is 300).\n    \"\"\"\n    # Open the image using PIL",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "866",
    "name": "simple_but_effective.py",
    "path": "02_media_processing/image_tools/simple_but_effective.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 74,
    "size": 1845,
    "docstring": "This template is written by @zackvega\n\nWhat does this quickstart script aim to do?\n- This is my simple but effective script.",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "instapy"
    ],
    "preview": "\"\"\"\nThis template is written by @zackvega\n\nWhat does this quickstart script aim to do?\n- This is my simple but effective script.\n\"\"\"\n\nfrom instapy import InstaPy, smart_run\n\ninsta_username = \"\"\ninsta_password = \"\"\n\n# get a session!\nsession = InstaPy(\n    username=insta_username,\n    password=insta_password,\n    headless_browser=True,\n    multi_logs=True,\n)\n",
    "last_modified": "2025-03-28T18:36:56"
  },
  {
    "id": "867",
    "name": "leonardo_script copy.py",
    "path": "02_media_processing/image_tools/leonardo_script copy.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 129,
    "size": 4439,
    "docstring": "",
    "keywords": [
      "analysis",
      "youtube",
      "web_tools"
    ],
    "functions": [
      "convert_image_to_jpeg",
      "get_presigned_url",
      "upload_image",
      "upscale_image",
      "get_upscaled_image",
      "process_images"
    ],
    "classes": [],
    "imports": [
      "json",
      "os",
      "sys",
      "time",
      "requests",
      "PIL"
    ],
    "preview": "import json\nimport os\nimport sys\nimport time\n\nimport requests\nfrom PIL import Image\n\napi_key = \"de7c9cb8-022f-42f8-8bf7-a8f9caadfaee\"\nauthorization = f\"Bearer {api_key}\"\n\nheaders = {\n    \"accept\": \"application/json\",\n    \"content-type\": \"application/json\",\n    \"authorization\": authorization,\n}\n\n# Styles to apply\nstyles = [\"GENERAL\", \"CINEMATIC\", \"2D_ART_ILLUSTRATION\", \"PHOTOREALISTIC\"]\n",
    "last_modified": "2025-09-13T05:53:50.450811"
  },
  {
    "id": "868",
    "name": "sort--.py",
    "path": "02_media_processing/image_tools/sort--.py",
    "category": "02_media_processing",
    "type": "organization",
    "lines": 33,
    "size": 1090,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "os",
      "shutil"
    ],
    "preview": "import os\nimport shutil\n\n# Source directory containing your images\nsource_dir = \"/Volumes/baKs/105-mids\"\n\n# Destination directory where sorted folders will be created\ndestination_dir = \"/Volumes/baKs/105-mids\"\n\n# Create the destination directory if it doesn't exist\nos.makedirs(destination_dir, exist_ok=True)\n\n# Initialize variables\nimages_per_folder = 100\ncurrent_folder = None\nfolder_count = 0\n\n# Iterate through the source directory\nfor root, _, files in os.walk(source_dir):\n    for filename in files:",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "869",
    "name": "8mb (2).py",
    "path": "02_media_processing/image_tools/8mb (2).py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 58,
    "size": 1929,
    "docstring": "",
    "keywords": [],
    "functions": [
      "resize_image_to_max_size",
      "main"
    ],
    "classes": [],
    "imports": [
      "os",
      "PIL"
    ],
    "preview": "import os\n\nfrom PIL import Image\n\n\ndef resize_image_to_max_size(image_path, max_size_mb=8):\n    \"\"\"\n    Resize a PNG image to ensure it doesn't exceed the specified max size (in MB).\n\n    :param image_path: Path to the input PNG image.\n    :param max_size_mb: Maximum allowed size for the image in megabytes (default is 8MB).\n    \"\"\"\n    max_size_bytes = max_size_mb * 1024 * 1024  # Convert MB to bytes\n    target_dpi = (300, 300)  # Set target DPI\n\n    img = Image.open(image_path)\n    current_size = os.path.getsize(image_path)\n\n    # Reduce image size by lowering the resolution\n    width, height = img.size",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "870",
    "name": "img2img copy.py",
    "path": "02_media_processing/image_tools/img2img copy.py",
    "category": "02_media_processing",
    "type": "image_processing",
    "lines": 47,
    "size": 1743,
    "docstring": "",
    "keywords": [],
    "functions": [
      "convert_and_upscale_images",
      "main"
    ],
    "classes": [],
    "imports": [
      "os",
      "PIL"
    ],
    "preview": "import os\n\nfrom PIL import Image\n\n\ndef convert_and_upscale_images(source_directory, destination_directory):\n    os.makedirs(destination_directory, exist_ok=True)\n\n    for filename in os.listdir(source_directory):\n        if filename.endswith(\".png\"):\n            source_file = os.path.join(source_directory, filename)\n            filename_no_ext = os.path.splitext(filename)[0]\n            destination_file = os.path.join(destination_directory, f\"{filename_no_ext}.png\")\n\n            im = Image.open(source_file)\n            width, height = im.size\n            upscale_width = width * 2\n            upscale_height = height * 2\n            im_resized = im.resize((upscale_width, upscale_height))\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "871",
    "name": "leodown_20250102105149.py",
    "path": "02_media_processing/image_tools/leodown_20250102105149.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 52,
    "size": 1328,
    "docstring": "",
    "keywords": [
      "web_tools"
    ],
    "functions": [],
    "classes": [],
    "imports": [
      "json",
      "requests"
    ],
    "preview": "import json\n\nimport requests\n\n# Base URL for API\nbase_url = (\n    \"https://cloud.leonardo.ai/api/rest/v1/generations/user/f7bb8476-e3f0-4f1f-9a06-4600866cc49c\"\n)\nheaders = {\n    \"accept\": \"application/json\",\n    \"authorization\": \"Bearer 93043291-957d-4ec1-8c79-ee734abcb6e3\",  # Replace with your actual token\n}\n\n# Output file\noutput_file = \"/Users/steven/Pictures/leonardo_library.json\"\n\n# Pagination parameters\noffset = 0\nlimit = 10  # Maximum number of items per request\nall_generations = []",
    "last_modified": "2025-09-13T05:53:49.579263"
  },
  {
    "id": "872",
    "name": "convert-loop2 2.py",
    "path": "02_media_processing/image_tools/convert-loop2 2.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 167,
    "size": 5642,
    "docstring": "",
    "keywords": [
      "youtube",
      "web_tools"
    ],
    "functions": [
      "convert_image_to_jpeg",
      "get_presigned_url",
      "upload_image",
      "upscale_image",
      "get_upscaled_image"
    ],
    "classes": [],
    "imports": [
      "csv",
      "json",
      "os",
      "time",
      "datetime",
      "requests",
      "PIL"
    ],
    "preview": "import csv\nimport json\nimport os\nimport time\nfrom datetime import datetime\n\nimport requests\nfrom PIL import Image\n\napi_key = \"de7c9cb8-022f-42f8-8bf7-a8f9caadfaee\"\nauthorization = f\"Bearer {api_key}\"\n\nheaders = {\n    \"accept\": \"application/json\",\n    \"content-type\": \"application/json\",\n    \"authorization\": authorization,\n}\n\n# Directory containing images\ndirectory_path = \"/Users/steven/Pictures/TrashCaT/trashy-heartbreak\"",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "873",
    "name": "codec.py",
    "path": "02_media_processing/image_tools/codec.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 117,
    "size": 3370,
    "docstring": "",
    "keywords": [],
    "functions": [
      "getregentry",
      "encode",
      "decode",
      "_buffer_encode",
      "_buffer_decode"
    ],
    "classes": [
      "Codec",
      "IncrementalEncoder",
      "IncrementalDecoder",
      "StreamWriter",
      "StreamReader"
    ],
    "imports": [
      "codecs",
      "re",
      "typing",
      "core"
    ],
    "preview": "import codecs\nimport re\nfrom typing import Optional, Tuple\n\nfrom .core import IDNAError, alabel, decode, encode, ulabel\n\n_unicode_dots_re = re.compile(\"[\\u002e\\u3002\\uff0e\\uff61]\")\n\n\nclass Codec(codecs.Codec):\n\n    def encode(self, data: str, errors: str = \"strict\") -> Tuple[bytes, int]:\n        if errors != \"strict\":\n            raise IDNAError('Unsupported error handling \"{}\"'.format(errors))\n\n        if not data:\n            return b\"\", 0\n\n        return encode(data), len(data)\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "874",
    "name": "sbcharsetprober.py",
    "path": "02_media_processing/image_tools/sbcharsetprober.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 163,
    "size": 6400,
    "docstring": "",
    "keywords": [],
    "functions": [
      "__init__",
      "reset",
      "charset_name",
      "language",
      "feed",
      "get_confidence"
    ],
    "classes": [
      "SingleByteCharSetModel",
      "SingleByteCharSetProber"
    ],
    "imports": [
      "typing",
      "charsetprober",
      "enums"
    ],
    "preview": "######################## BEGIN LICENSE BLOCK ########################\n# The Original Code is Mozilla Universal charset detector code.\n#\n# The Initial Developer of the Original Code is\n# Netscape Communications Corporation.\n# Portions created by the Initial Developer are Copyright (C) 2001\n# the Initial Developer. All Rights Reserved.\n#\n# Contributor(s):\n#   Mark Pilgrim - port to Python\n#   Shy Shalom - original C code\n#\n# This library is free software; you can redistribute it and/or\n# modify it under the terms of the GNU Lesser General Public\n# License as published by the Free Software Foundation; either\n# version 2.1 of the License, or (at your option) any later version.\n#\n# This library is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "875",
    "name": "index_repo.py",
    "path": "02_media_processing/image_tools/index_repo.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 190,
    "size": 5146,
    "docstring": "",
    "keywords": [],
    "functions": [
      "iter_files",
      "guess_project",
      "classify",
      "tag_for",
      "build_index",
      "write_csv",
      "write_json",
      "main"
    ],
    "classes": [
      "FileEntry"
    ],
    "imports": [
      "__future__",
      "csv",
      "json",
      "os",
      "dataclasses",
      "pathlib",
      "typing"
    ],
    "preview": "from __future__ import annotations\n\nimport csv\nimport json\nimport os\nfrom dataclasses import asdict, dataclass\nfrom pathlib import Path\nfrom typing import Iterable, List, Dict\n\nROOT = Path(__file__).resolve().parents[1]\n\nIGNORE_DIRS = {\n    \".git\",\n    \".venv\",\n    \"venv\",\n    \"__pycache__\",\n    \"node_modules\",\n    \"dist\",\n    \"build\",\n}",
    "last_modified": "2025-09-13T06:01:11.267457"
  },
  {
    "id": "876",
    "name": "dl_button.py",
    "path": "02_media_processing/image_tools/dl_button.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 321,
    "size": 13013,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "logging",
      "asyncio",
      "json",
      "math",
      "os",
      "shutil",
      "time",
      "datetime",
      "aiohttp",
      "pyrogram"
    ],
    "preview": "import logging\n\nlogging.basicConfig(\n    level=logging.DEBUG, format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\n)\nlogger = logging.getLogger(__name__)\n\nimport asyncio\nimport json\nimport math\nimport os\nimport shutil\nimport time\nfrom datetime import datetime\n\nimport aiohttp\n\n# the secret configuration specific things\nif bool(os.environ.get(\"WEBHOOK\", False)):\n    from sample_config import Config",
    "last_modified": "2025-09-13T05:53:44.016329"
  },
  {
    "id": "877",
    "name": "organize_albums 14.py",
    "path": "02_media_processing/image_tools/organize_albums 14.py",
    "category": "02_media_processing",
    "type": "organization",
    "lines": 62,
    "size": 2469,
    "docstring": "",
    "keywords": [
      "organization"
    ],
    "functions": [
      "organize_files"
    ],
    "classes": [],
    "imports": [
      "os",
      "shutil"
    ],
    "preview": "import os\nimport shutil\n\n# Define the base directory\nbase_dir = \"/Users/steven/Music/NocTurnE-meLoDieS/v4/mp3\"\n\n\n# Helper function to create folders and move files\ndef organize_files():\n    # List all files in the base directory\n    files = os.listdir(base_dir)\n\n    # Process each file\n    for file in files:\n        # Skip directories\n        if os.path.isdir(os.path.join(base_dir, file)):\n            continue\n\n        # Extract the base name (album name) from the file\n        if file.endswith(\".mp3\"):",
    "last_modified": "2025-08-02T18:25:58.480553"
  },
  {
    "id": "878",
    "name": "Mp3toMp4ximg.py",
    "path": "02_media_processing/image_tools/Mp3toMp4ximg.py",
    "category": "02_media_processing",
    "type": "image_processing",
    "lines": 53,
    "size": 1737,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "get_cover_image",
      "convert_mp3_to_mp4",
      "process_directory"
    ],
    "classes": [],
    "imports": [
      "glob",
      "os",
      "moviepy.editor",
      "PIL",
      "sys"
    ],
    "preview": "import glob\nimport os\n\nfrom moviepy.editor import AudioFileClip, ImageClip\nfrom PIL import Image\n\n\ndef get_cover_image(file_name, cover_image_directory):\n    # Check for both JPG and PNG extensions\n    jpg_path = os.path.join(cover_image_directory, f\"{file_name}.jpg\")\n    png_path = os.path.join(cover_image_directory, f\"{file_name}.png\")\n\n    if os.path.exists(jpg_path):\n        return jpg_path\n    elif os.path.exists(png_path):\n        return png_path\n    else:\n        print(f\"Cover image not found for {file_name}. Please ensure the cover image exists.\")\n        return None\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "879",
    "name": "gui_utils.py",
    "path": "02_media_processing/image_tools/gui_utils.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 213,
    "size": 6455,
    "docstring": "",
    "keywords": [],
    "functions": [
      "get_checks",
      "get_config",
      "check",
      "modify_settings",
      "delete_background",
      "add_background",
      "unpack_checks",
      "modify_config"
    ],
    "classes": [],
    "imports": [
      "json",
      "re",
      "pathlib",
      "toml",
      "tomlkit",
      "flask"
    ],
    "preview": "import json\nimport re\nfrom pathlib import Path\n\nimport toml\nimport tomlkit\nfrom flask import flash\n\n\n# Get validation checks from template\ndef get_checks():\n    template = toml.load(\"utils/.config.template.toml\")\n    checks = {}\n\n    def unpack_checks(obj: dict):\n        for key in obj.keys():\n            if \"optional\" in obj[key].keys():\n                checks[key] = obj[key]\n            else:\n                unpack_checks(obj[key])",
    "last_modified": "2025-09-13T05:54:00.273216"
  },
  {
    "id": "880",
    "name": "sagemaker_huggingface.py",
    "path": "02_media_processing/image_tools/sagemaker_huggingface.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 191,
    "size": 7653,
    "docstring": "",
    "keywords": [],
    "functions": [
      "__init__",
      "deploy",
      "__init__",
      "deploy",
      "prepare_and_deploy_model"
    ],
    "classes": [
      "SagemakerHuggingfaceStrategy",
      "DeploymentService"
    ],
    "imports": [
      "enum",
      "typing",
      "loguru",
      "llm_engineering.domain.inference",
      "llm_engineering.settings",
      "boto3",
      "sagemaker.enums",
      "sagemaker.huggingface"
    ],
    "preview": "import enum\nfrom typing import Optional\n\nfrom loguru import logger\n\ntry:\n    import boto3\n    from sagemaker.enums import EndpointType\n    from sagemaker.huggingface import HuggingFaceModel\nexcept ModuleNotFoundError:\n    logger.warning(\n        \"Couldn't load AWS or SageMaker imports. Run 'poetry install --with aws' to support AWS.\"\n    )\n\nfrom llm_engineering.domain.inference import DeploymentStrategy\nfrom llm_engineering.settings import settings\n\n\nclass SagemakerHuggingfaceStrategy(DeploymentStrategy):\n    def __init__(self, deployment_service) -> None:",
    "last_modified": "2025-09-13T05:53:42.048901"
  },
  {
    "id": "881",
    "name": "info.py",
    "path": "02_media_processing/image_tools/info.py",
    "category": "02_media_processing",
    "type": "organization",
    "lines": 45,
    "size": 1807,
    "docstring": "PrNdOwN module that holds package information.\nNote:\n    All those info could be stored in the __init__ file\n    but we keep them here to keep the code clean.",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "__future__"
    ],
    "preview": "# -*- coding: utf-8 -*-\n\n\"\"\"PrNdOwN module that holds package information.\nNote:\n    All those info could be stored in the __init__ file\n    but we keep them here to keep the code clean.\n\"\"\"\n\nfrom __future__ import unicode_literals\n\n__author__ = \"Younes Ben-El\"\n__contact__ = \"ybenel@pm.me\"\n__projecturl__ = \"https://github.com/m1ndo/PrNdOwN/\"\n\n__appname__ = \"PrNdOwN\"\n__license__ = \"UNLICENSE\"\n\n__description__ = \"Multi Video Sharing Platform Downloader.\"\n\n__descriptionfull__ = (",
    "last_modified": "2025-09-13T05:53:58.946213"
  },
  {
    "id": "882",
    "name": "best.py",
    "path": "02_media_processing/image_tools/best.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 76,
    "size": 2296,
    "docstring": "",
    "keywords": [
      "web_tools"
    ],
    "functions": [
      "get_bestseller_blueprints",
      "main"
    ],
    "classes": [],
    "imports": [
      "json",
      "os",
      "requests",
      "dotenv"
    ],
    "preview": "import json\nimport os\n\nimport requests\nfrom dotenv import load_dotenv\n\n# Load .env variables\nload_dotenv(dotenv_path=os.path.expanduser(\"~/.env\"))\n\n# Retrieve API Key & Shop Data\nAPI_TOKEN = os.getenv(\"PRINTIFY_API_KEY\")\nSHOP_DATA_RAW = os.getenv(\"PRINTIFY_SHOPS\")\n\n# Ensure environment variable is loaded\nif SHOP_DATA_RAW is None:\n    print(\"\u274c ERROR: PRINTIFY_SHOPS not found in environment!\")\n    exit(1)\n\n# Convert JSON string to a dictionary\nSHOP_DATA = json.loads(SHOP_DATA_RAW)",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "883",
    "name": "img-img-upscale.py",
    "path": "02_media_processing/image_tools/img-img-upscale.py",
    "category": "02_media_processing",
    "type": "image_processing",
    "lines": 36,
    "size": 1606,
    "docstring": "",
    "keywords": [],
    "functions": [
      "convert_and_upscale_images"
    ],
    "classes": [],
    "imports": [
      "os",
      "PIL"
    ],
    "preview": "import os\n\nfrom PIL import Image\n\n\ndef convert_and_upscale_images(directory):\n    # Walk through all directories and files within the specified directory\n    for root, _, files in os.walk(directory):\n        for filename in files:\n            if filename.lower().endswith(\".jpg\") or filename.lower().endswith(\".jpg\"):\n                # Construct full file path\n                file_path = os.path.join(root, filename)\n                # Open the .jpg image\n                with Image.open(file_path) as img:\n                    # Upscale the image by 2x\n                    img = img.resize((img.width * 2, img.height * 2), Image.LANCZOS)\n                    # Set DPI to 300\n                    img.info[\"dpi\"] = (300, 300)\n                    # Convert the image mode to RGB (if not already in that mode)\n                    if img.mode != \"RGB\":",
    "last_modified": "2025-05-04T22:47:13.378971"
  },
  {
    "id": "884",
    "name": "bot_photo.py",
    "path": "02_media_processing/image_tools/bot_photo.py",
    "category": "02_media_processing",
    "type": "image_processing",
    "lines": 125,
    "size": 4025,
    "docstring": "",
    "keywords": [
      "youtube"
    ],
    "functions": [
      "upload_photo",
      "upload_album",
      "download_photo",
      "download_photos"
    ],
    "classes": [],
    "imports": [
      "os",
      "io",
      "tqdm"
    ],
    "preview": "import os\nfrom io import open\n\nfrom tqdm import tqdm\n\n\ndef upload_photo(\n    self,\n    photo,\n    caption=None,\n    upload_id=None,\n    from_video=False,\n    options={},\n    user_tags=None,\n    is_sidecar=False,\n):\n    \"\"\"Upload photo to Instagram\n\n    @param photo       Path to photo file (String)\n    @param caption     Media description (String)",
    "last_modified": "2025-09-13T05:54:57.950281"
  },
  {
    "id": "885",
    "name": "9mb.py",
    "path": "02_media_processing/image_tools/9mb.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 101,
    "size": 3925,
    "docstring": "",
    "keywords": [],
    "functions": [
      "resize_image_to_max_size",
      "resize_images_in_directory",
      "main"
    ],
    "classes": [],
    "imports": [
      "os",
      "PIL"
    ],
    "preview": "import os\n\nfrom PIL import Image\n\n\ndef resize_image_to_max_size(image_path, max_size_mb=9, upscale=True):\n    \"\"\"\n    Resize a PNG image to ensure it doesn't exceed the specified max size (in MB) and optionally upscale.\n\n    :param image_path: Path to the input PNG image.\n    :param max_size_mb: Maximum allowed size for the image in megabytes (default is 9MB).\n    :param upscale: Whether to upscale the image if it's smaller than the max size (default is False).\n    \"\"\"\n    max_size_bytes = max_size_mb * 1024 * 1024  # Convert MB to bytes\n    target_dpi = (300, 300)  # Set target DPI\n\n    img = Image.open(image_path)\n    current_size = os.path.getsize(image_path)\n\n    # If the image is smaller than the limit and upscaling is allowed, upscale it",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "886",
    "name": "auto-image-gallery.py",
    "path": "02_media_processing/image_tools/auto-image-gallery.py",
    "category": "02_media_processing",
    "type": "image_processing",
    "lines": 81,
    "size": 2251,
    "docstring": "",
    "keywords": [],
    "functions": [
      "csv_to_html"
    ],
    "classes": [],
    "imports": [
      "csv"
    ],
    "preview": "import csv\n\n\ndef csv_to_html(csv_file, output_html):\n    # Start the HTML structure\n    html_content = \"\"\"\n    <!DOCTYPE html>\n    <html lang=\"en\">\n    <head>\n        <meta charset=\"UTF-8\">\n        <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n        <title>Image Gallery</title>\n        <style>\n            body {\n                font-family: Arial, sans-serif;\n                background-color: #f0f0f0;\n                padding: 20px;\n            }\n            h1 {\n                text-align: center;",
    "last_modified": "2025-05-04T22:47:13.344023"
  },
  {
    "id": "887",
    "name": "test_gallery_upload.py",
    "path": "02_media_processing/image_tools/test_gallery_upload.py",
    "category": "02_media_processing",
    "type": "youtube",
    "lines": 171,
    "size": 5632,
    "docstring": "",
    "keywords": [
      "testing",
      "analysis"
    ],
    "functions": [
      "create_mock_image",
      "setup_gallery",
      "add_remote_location",
      "test_aws_without_location",
      "test_gallery_not_initialized",
      "test_gallery_not_built",
      "test_upload_aws",
      "test_upload_netlify"
    ],
    "classes": [
      "SPGUploadTestCase"
    ],
    "imports": [
      "json",
      "os",
      "subprocess",
      "sys",
      "unittest",
      "unittest",
      "simplegallery.gallery_build",
      "simplegallery.gallery_init",
      "simplegallery.gallery_upload",
      "PIL"
    ],
    "preview": "import json\nimport os\nimport subprocess\nimport sys\nimport unittest\nfrom unittest import mock\n\nimport simplegallery.gallery_build as gallery_build\nimport simplegallery.gallery_init as gallery_init\nimport simplegallery.gallery_upload as gallery_upload\nfrom PIL import Image\nfrom testfixtures import TempDirectory\n\n\ndef create_mock_image(path, width, height):\n    img = Image.new(\"RGB\", (width, height), color=\"red\")\n    img.save(path)\n    img.close()\n\n",
    "last_modified": "2025-09-13T05:53:53.163234"
  },
  {
    "id": "888",
    "name": "download_photos_by_hashtag.py",
    "path": "02_media_processing/image_tools/download_photos_by_hashtag.py",
    "category": "02_media_processing",
    "type": "youtube",
    "lines": 28,
    "size": 693,
    "docstring": "instabot example\n\nWorkflow:\n    Download media photos with hashtag.",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "argparse",
      "os",
      "sys",
      "instabot"
    ],
    "preview": "\"\"\"\ninstabot example\n\nWorkflow:\n    Download media photos with hashtag.\n\"\"\"\n\nimport argparse\nimport os\nimport sys\n\nsys.path.append(os.path.join(sys.path[0], \"../\"))\nfrom instabot import Bot  # noqa: E402\n\nparser = argparse.ArgumentParser(add_help=True)\nparser.add_argument(\"-u\", type=str, help=\"username\")\nparser.add_argument(\"-p\", type=str, help=\"password\")\nparser.add_argument(\"-proxy\", type=str, help=\"proxy\")\nparser.add_argument(\"hashtags\", type=str, nargs=\"+\", help=\"hashtags\")\nargs = parser.parse_args()",
    "last_modified": "2025-03-28T18:35:47"
  },
  {
    "id": "889",
    "name": "botDraw.py",
    "path": "02_media_processing/image_tools/botDraw.py",
    "category": "02_media_processing",
    "type": "web_tools",
    "lines": 220,
    "size": 6471,
    "docstring": "Created in 11/2020\n@Author: Paulo https://github.com/alpdias",
    "keywords": [],
    "functions": [
      "functionDraw",
      "botlogin",
      "findImg",
      "typephrase",
      "commentDraw"
    ],
    "classes": [],
    "imports": [
      "os",
      "random",
      "pathlib",
      "time",
      "art",
      "selenium",
      "selenium.webdriver.common.keys"
    ],
    "preview": "# -*- coding: utf-8 -*-\n\n\"\"\"\nCreated in 11/2020\n@Author: Paulo https://github.com/alpdias\n\"\"\"\n\n# imported libraries\nimport os\nimport random\nfrom pathlib import Path\nfrom time import sleep\n\nimport art\nfrom selenium import webdriver\nfrom selenium.webdriver.common.keys import Keys\n\n\ndef functionDraw(mySystem):\n    \"\"\"",
    "last_modified": "2025-05-04T23:28:20.980430"
  },
  {
    "id": "890",
    "name": "8mb.py",
    "path": "02_media_processing/image_tools/8mb.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 58,
    "size": 1929,
    "docstring": "",
    "keywords": [],
    "functions": [
      "resize_image_to_max_size",
      "main"
    ],
    "classes": [],
    "imports": [
      "os",
      "PIL"
    ],
    "preview": "import os\n\nfrom PIL import Image\n\n\ndef resize_image_to_max_size(image_path, max_size_mb=8):\n    \"\"\"\n    Resize a PNG image to ensure it doesn't exceed the specified max size (in MB).\n\n    :param image_path: Path to the input PNG image.\n    :param max_size_mb: Maximum allowed size for the image in megabytes (default is 8MB).\n    \"\"\"\n    max_size_bytes = max_size_mb * 1024 * 1024  # Convert MB to bytes\n    target_dpi = (300, 300)  # Set target DPI\n\n    img = Image.open(image_path)\n    current_size = os.path.getsize(image_path)\n\n    # Reduce image size by lowering the resolution\n    width, height = img.size",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "891",
    "name": "macromanprober.py",
    "path": "02_media_processing/image_tools/macromanprober.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 161,
    "size": 6053,
    "docstring": "",
    "keywords": [],
    "functions": [
      "__init__",
      "reset",
      "charset_name",
      "language",
      "feed",
      "get_confidence"
    ],
    "classes": [
      "MacRomanProber"
    ],
    "imports": [
      "typing",
      "charsetprober",
      "enums"
    ],
    "preview": "######################## BEGIN LICENSE BLOCK ########################\n# This code was modified from latin1prober.py by Rob Speer <rob@lumino.so>.\n# The Original Code is Mozilla Universal charset detector code.\n#\n# The Initial Developer of the Original Code is\n# Netscape Communications Corporation.\n# Portions created by the Initial Developer are Copyright (C) 2001\n# the Initial Developer. All Rights Reserved.\n#\n# Contributor(s):\n#   Rob Speer - adapt to MacRoman encoding\n#   Mark Pilgrim - port to Python\n#   Shy Shalom - original C code\n#\n# This library is free software; you can redistribute it and/or\n# modify it under the terms of the GNU Lesser General Public\n# License as published by the Free Software Foundation; either\n# version 2.1 of the License, or (at your option) any later version.\n#\n# This library is distributed in the hope that it will be useful,",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "892",
    "name": "organize_media.py",
    "path": "02_media_processing/image_tools/organize_media.py",
    "category": "02_media_processing",
    "type": "analysis",
    "lines": 130,
    "size": 4232,
    "docstring": "Organize media and related text files by album base name.\n\n- Scans a single-level directory (no recursion) for files.\n- Derives an album_name from the filename by removing the extension and\n  stripping known suffixes like \"_analysis\" and \"_transcript\".\n- Creates a folder named after that album_name.\n- Moves any of these files into that folder, renaming them to a canonical pattern:\n    * {album_name}.mp3\n    * {album_name}.m4a\n    * {album_name}.mp4\n    * {album_name}_analysis.txt\n    * {album_name}_transcript.txt\n    * {album_name}.png    (cover image if present in the base directory)\n- Skips moving if the destination already exists (to avoid overwriting).\n\nUsage:\n    python organize_media.py [BASE_DIR]\n\nIf BASE_DIR is not provided, defaults to:\n    /Users/steven/Music/nocTurneMeLoDieS/MP3\n\nNotes:\n- This script is macOS-friendly and uses only the standard library.\n- File extensions are matched case-insensitively (.MP3 == .mp3).",
    "keywords": [
      "organization"
    ],
    "functions": [
      "normalized_album_name",
      "canonical_dest_name",
      "organize_files"
    ],
    "classes": [],
    "imports": [
      "os",
      "shutil",
      "sys",
      "pathlib"
    ],
    "preview": "#!/usr/bin/env python3\n\"\"\"\nOrganize media and related text files by album base name.\n\n- Scans a single-level directory (no recursion) for files.\n- Derives an album_name from the filename by removing the extension and\n  stripping known suffixes like \"_analysis\" and \"_transcript\".\n- Creates a folder named after that album_name.\n- Moves any of these files into that folder, renaming them to a canonical pattern:\n    * {album_name}.mp3\n    * {album_name}.m4a\n    * {album_name}.mp4\n    * {album_name}_analysis.txt\n    * {album_name}_transcript.txt\n    * {album_name}.png    (cover image if present in the base directory)\n- Skips moving if the destination already exists (to avoid overwriting).\n\nUsage:\n    python organize_media.py [BASE_DIR]\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "893",
    "name": "re_custom.py",
    "path": "02_media_processing/image_tools/re_custom.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 18,
    "size": 74552,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "re"
    ],
    "preview": "import re\n\nurls = ['https: // i.etsystatic.com / 39809135 / r / il / 54e062 / 5612464624 / il_fullxfull.5612464624_pe3m.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 952b6e / 5612464494 / il_fullxfull.5612464494_p1f2.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 3a4749 / 5660551873 / il_fullxfull.5660551873_c6gk.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 0c6bd6 / 5660551875 / il_fullxfull.5660551875_b6bs.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 7558b7 / 5612464116 / il_fullxfull.5612464116_6xq7.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / fd2792 / 5612463620 / il_fullxfull.5612463620_pjsl.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 762abb / 5660551663 / il_fullxfull.5660551663_lzmu.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 0ab9a7 / 5660551265 / il_fullxfull.5660551265_e2pp.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 62dacf / 5612464406 / il_fullxfull.5612464406_6of2.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 73a610 / 5612464432 / il_fullxfull.5612464432_pznr.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / e5cd54 / 5660551795 / il_fullxfull.5660551795_1hw6.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / b58018 / 5660551369 / il_fullxfull.5660551369_k7xo.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 5ced92 / 5660551203 / il_fullxfull.5660551203_cgmz.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 67556d / 5612464582 / il_fullxfull.5612464582_kxew.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 358db0 / 5660551839 / il_fullxfull.5660551839_l9iq.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 1532d6 / 5612464208 / il_fullxfull.5612464208_ibxx.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 3a6217 / 5612464286 / il_fullxfull.5612464286_hgvh.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 76f34c / 5612464152 / il_fullxfull.5612464152_pb3l.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / baf696 / 5612463472 / il_fullxfull.5612463472_emz8.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 532529 / 5660551691 / il_fullxfull.5660551691_n72r.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / c28b90 / 5660550567 / il_fullxfull.5660550567_fxcv.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 9f45bd / 5612463372 / il_fullxfull.5612463372_na2j.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / f060b3 / 5612463222 / il_fullxfull.5612463222_sm48.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / b20eab / 5612338004 / il_fullxfull.5612338004_3apk.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 644ff0 / 5612464586 / il_fullxfull.5612464586_cvey.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 1b3f8b / 5612464330 / il_fullxfull.5612464330_rhim.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 4511f0 / 5612464386 / il_fullxfull.5612464386_etp5.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / c64cbe / 5660551653 / il_fullxfull.5660551653_1ht3.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / ea20ee / 5612464540 / il_fullxfull.5612464540_6lbq.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / c71f9f / 5660551725 / il_fullxfull.5660551725_710g.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / e986c9 / 5660551415 / il_fullxfull.5660551415_dqlq.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / eaf4ec / 5612464388 / il_fullxfull.5612464388_frc6.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 61ed07 / 5612464234 / il_fullxfull.5612464234_ffmg.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 62a492 / 5660550945 / il_fullxfull.5660550945_sg9d.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 41aadb / 5660551267 / il_fullxfull.5660551267_570y.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 5f5ac8 / 5660550455 / il_fullxfull.5660550455_adji.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 768acc / 5660552075 / il_fullxfull.5660552075_cavd.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 78965b / 5612464410 / il_fullxfull.5612464410_h5yb.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 18399c / 5660551669 / il_fullxfull.5660551669_jeuq.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / ea38d7 / 5612464538 / il_fullxfull.5612464538_kkyy.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 998599 / 5612464414 / il_fullxfull.5612464414_ptp9.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 91354b / 5660551555 / il_fullxfull.5660551555_gh73.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 771197 / 5660426721 / il_fullxfull.5660426721_s34s.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / e5a551 / 5612338616 / il_fullxfull.5612338616_o9tq.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 7b67a9 / 5612338942 / il_fullxfull.5612338942_elej.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / aefa74 / 5612338768 / il_fullxfull.5612338768_ofb1.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / c2e59b / 5612338834 / il_fullxfull.5612338834_b7mh.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 2f7a79 / 5612338724 / il_fullxfull.5612338724_3b06.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / f00993 / 5660426273 / il_fullxfull.5660426273_duvq.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 00ebd7 / 5660426753 / il_fullxfull.5660426753_bvil.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / aba6d6 / 5612339172 / il_fullxfull.5612339172_hp5q.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 1a8e4f / 5660425925 / il_fullxfull.5660425925_4mao.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 690156 / 5660426679 / il_fullxfull.5660426679_8t2p.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 7b56d2 / 5660426179 / il_fullxfull.5660426179_f0fe.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / fcb522 / 5660426519 / il_fullxfull.5660426519_4etg.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 13fd98 / 5612338308 / il_fullxfull.5612338308_9053.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / e8af75 / 5660425973 / il_fullxfull.5660425973_8i2s.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 2904f8 / 5660426325 / il_fullxfull.5660426325_is5i.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 0b5b80 / 5660426871 / il_fullxfull.5660426871_gyk8.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 8414a8 / 5612338648 / il_fullxfull.5612338648_t3gf.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 624d10 / 5612339344 / il_fullxfull.5612339344_d7p8.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 52a291 / 5660426387 / il_fullxfull.5660426387_9tyw.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 86050b / 5660426639 / il_fullxfull.5660426639_et9f.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 7380fb / 5660426635 / il_fullxfull.5660426635_1tmu.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / f152ae / 5660426369 / il_fullxfull.5660426369_ocwc.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / a29f60 / 5612338800 / il_fullxfull.5612338800_4bts.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 0255c8 / 5612338968 / il_fullxfull.5612338968_b65a.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 747a25 / 5660425683 / il_fullxfull.5660425683_r1e6.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 6a1aa3 / 5660425987 / il_fullxfull.5660425987_43i5.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 16be65 / 5660426725 / il_fullxfull.5660426725_cfrf.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / c16fb0 / 5660426593 / il_fullxfull.5660426593_4015.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 8ee097 / 5612338722 / il_fullxfull.5612338722_ngnf.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 34463e / 5660425947 / il_fullxfull.5660425947_cfhn.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 5e08ff / 5660426239 / il_fullxfull.5660426239_hczx.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / d2b5e4 / 5612338904 / il_fullxfull.5612338904_e6md.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / f7484b / 5612338882 / il_fullxfull.5612338882_ipw5.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / da0dc5 / 5660426629 / il_fullxfull.5660426629_6pcq.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / e28058 / 5612338310 / il_fullxfull.5612338310_oep4.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 5c8d0e / 5660426139 / il_fullxfull.5660426139_extf.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 87709c / 5612339126 / il_fullxfull.5612339126_hnyd.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 39f6e8 / 5612338480 / il_fullxfull.5612338480_8tmh.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 010556 / 5660425917 / il_fullxfull.5660425917_tft9.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / b1aafc / 5348014299 / il_fullxfull.5348014299_acqz.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 54873b / 5548272527 / il_fullxfull.5548272527_qeo4.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 6822ca / 5587789252 / il_fullxfull.5587789252_42ax.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 231f48 / 5635868653 / il_fullxfull.5635868653_i1gx.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 230e06 / 5635891967 / il_fullxfull.5635891967_tsc6.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 0775af / 5348014153 / il_fullxfull.5348014153_9vgo.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 9d1074 / 5635869097 / il_fullxfull.5635869097_j9x8.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 633b45 / 5635867527 / il_fullxfull.5635867527_j20z.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / e9f196 / 5635851651 / il_fullxfull.5635851651_1a1t.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / abd359 / 5635891671 / il_fullxfull.5635891671_m33w.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / ee3f20 / 5635892385 / il_fullxfull.5635892385_cs6z.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 465f10 / 5587766032 / il_fullxfull.5587766032_l933.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 761cdd / 5587765822 / il_fullxfull.5587765822_b69l.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 3d4ba1 / 5587764566 / il_fullxfull.5587764566_a7tm.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / e2f5bd / 5635851051 / il_fullxfull.5635851051_mxhg.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 9c0afa / 5635890893 / il_fullxfull.5635890893_4gy5.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 3657cb / 5635868889 / il_fullxfull.5635868889_i4o8.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / af6bed / 5587765920 / il_fullxfull.5587765920_8y0i.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / d2ec6c / 5587748896 / il_fullxfull.5587748896_mjg0.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 6207e6 / 5587789102 / il_fullxfull.5587789102_bg36.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / e218b6 / 5587748288 / il_fullxfull.5587748288_j911.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / a1d5f7 / 5587748292 / il_fullxfull.5587748292_jv6c.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / bd008a / 5635890579 / il_fullxfull.5635890579_mbee.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 4f90d0 / 5587749046 / il_fullxfull.5587749046_bcz7.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 6dbf2c / 5635851873 / il_fullxfull.5635851873_1jj0.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 53aa1c / 5635851767 / il_fullxfull.5635851767_c46u.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 1c22c0 / 5635891869 / il_fullxfull.5635891869_onlb.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / fb627a / 5635891677 / il_fullxfull.5635891677_egof.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 7f6eae / 5587765814 / il_fullxfull.5587765814_fzou.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 461978 / 5635851189 / il_fullxfull.5635851189_9ooc.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 0e1653 / 5635851129 / il_fullxfull.5635851129_a713.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / bf765e / 5587788202 / il_fullxfull.5587788202_mq7g.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 9bc5ab / 5635868931 / il_fullxfull.5635868931_4dz3.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 5ff018 / 5587765866 / il_fullxfull.5587765866_4zmx.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 25c6c4 / 5635892323 / il_fullxfull.5635892323_504i.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 671fb3 / 5587787234 / il_fullxfull.5587787234_qlr3.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 6869d9 / 5587765752 / il_fullxfull.5587765752_762g.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 3919e0 / 5635892317 / il_fullxfull.5635892317_l6h6.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / d32ec5 / 5587788652 / il_fullxfull.5587788652_fsiz.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 99d91f / 5635868995 / il_fullxfull.5635868995_2j4k.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 38ce5c / 5587764886 / il_fullxfull.5587764886_c79a.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / efb2f7 / 5635851525 / il_fullxfull.5635851525_c60z.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 2b092e / 5587789586 / il_fullxfull.5587789586_4wsd.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 069b58 / 5635891201 / il_fullxfull.5635891201_7pcw.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 15ff01 / 5587765580 / il_fullxfull.5587765580_tqb3.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / c3a4ab / 5587787412 / il_fullxfull.5587787412_5dx4.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / ca6b9d / 5587766194 / il_fullxfull.5587766194_gnae.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / a22735 / 5587765878 / il_fullxfull.5587765878_grla.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 88ec7a / 5587765100 / il_fullxfull.5587765100_pi7b.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / eee24e / 5587748230 / il_fullxfull.5587748230_d5lt.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / d2bb30 / 5587748176 / il_fullxfull.5587748176_p3k6.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / e933c7 / 5635891565 / il_fullxfull.5635891565_sgny.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 7a8f3c / 5587787284 / il_fullxfull.5587787284_6mxy.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 2bb685 / 5587749130 / il_fullxfull.5587749130_3mne.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 7207cb / 5635852117 / il_fullxfull.5635852117_gdfn.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / aa96a9 / 5587788582 / il_fullxfull.5587788582_chva.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 8b0aa1 / 5587765998 / il_fullxfull.5587765998_stzf.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 7dcde3 / 5635868759 / il_fullxfull.5635868759_ifol.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / c0936e / 5635868403 / il_fullxfull.5635868403_jwkq.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / b74464 / 5587788708 / il_fullxfull.5587788708_7l1n.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 8f386a / 5635891667 / il_fullxfull.5635891667_4io5.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 02a42c / 5635890031 / il_fullxfull.5635890031_t1vb.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 9a02d4 / 5635868709 / il_fullxfull.5635868709_f948.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 22f43d / 5635851143 / il_fullxfull.5635851143_1a9n.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 561bd4 / 5635890777 / il_fullxfull.5635890777_6wil.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / fd4c5b / 5635868825 / il_fullxfull.5635868825_jbzr.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 2d5173 / 5635867273 / il_fullxfull.5635867273_20x1.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 0da93a / 5635852119 / il_fullxfull.5635852119_4496.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / bf709c / 5635869071 / il_fullxfull.5635869071_gimj.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 877739 / 5635867761 / il_fullxfull.5635867761_8z8t.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 815f14 / 5635867277 / il_fullxfull.5635867277_1crg.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / dbb7c1 / 5635867449 / il_fullxfull.5635867449_rnve.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / f71716 / 5635851123 / il_fullxfull.5635851123_h40k.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 33c828 / 5587788026 / il_fullxfull.5587788026_2y3s.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 0ef547 / 5587765036 / il_fullxfull.5587765036_f6m5.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 872209 / 5587748954 / il_fullxfull.5587748954_53li.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / dcef45 / 5587788392 / il_fullxfull.5587788392_8bnr.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / b55a04 / 5587765756 / il_fullxfull.5587765756_kj2w.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 174c25 / 5635867209 / il_fullxfull.5635867209_p2h8.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / d866ec / 5635851045 / il_fullxfull.5635851045_igcs.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / cb5291 / 5635892457 / il_fullxfull.5635892457_jlij.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / f41303 / 5635890945 / il_fullxfull.5635890945_74yu.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / c9c48f / 5635868251 / il_fullxfull.5635868251_gut4.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / ba8743 / 5635851867 / il_fullxfull.5635851867_tljl.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 3fcfa0 / 5635851933 / il_fullxfull.5635851933_m9h7.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / d32963 / 5635868489 / il_fullxfull.5635868489_iucc.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / ec8cd0 / 5587749202 / il_fullxfull.5587749202_nf67.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / ff142e / 5635850943 / il_fullxfull.5635850943_nf1y.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / c7a5a6 / 5635891795 / il_fullxfull.5635891795_7jsl.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / a7d067 / 5587765324 / il_fullxfull.5587765324_hjqu.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / d7e6d8 / 5587765046 / il_fullxfull.5587765046_ar5f.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 801d04 / 5635851773 / il_fullxfull.5635851773_scyb.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 812a83 / 5587748756 / il_fullxfull.5587748756_csou.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / afc543 / 5587748644 / il_fullxfull.5587748644_cdi9.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / e1d183 / 5587748586 / il_fullxfull.5587748586_nqz1.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / c9e4d9 / 5635892401 / il_fullxfull.5635892401_4pam.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / c83ba5 / 5635868545 / il_fullxfull.5635868545_62dk.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / a8c068 / 5587766106 / il_fullxfull.5587766106_tmox.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 531c1b / 5587747884 / il_fullxfull.5587747884_774s.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 024c0a / 5635892015 / il_fullxfull.5635892015_dnjk.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 52903d / 5587765386 / il_fullxfull.5587765386_7aea.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 2f5201 / 5587764696 / il_fullxfull.5587764696_4z1s.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 814356 / 5587748824 / il_fullxfull.5587748824_7pml.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / d49c62 / 5587789620 / il_fullxfull.5587789620_trhh.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / ed83a2 / 5587765636 / il_fullxfull.5587765636_3vme.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 23be88 / 5635851713 / il_fullxfull.5635851713_2sow.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 9c5d7f / 5587747818 / il_fullxfull.5587747818_dfn7.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / b57202 / 5635892409 / il_fullxfull.5635892409_nsg8.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / c8e72f / 5635851469 / il_fullxfull.5635851469_lj5d.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 39a8bb / 5587748828 / il_fullxfull.5587748828_1efs.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 89a341 / 5635851413 / il_fullxfull.5635851413_gxoc.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 08ebc4 / 5587789250 / il_fullxfull.5587789250_rwpr.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / e1f54d / 5587789186 / il_fullxfull.5587789186_f3zr.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 6d6300 / 5587765682 / il_fullxfull.5587765682_1ujw.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 35d1d1 / 5635850899 / il_fullxfull.5635850899_dhe4.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 876ae6 / 5635850847 / il_fullxfull.5635850847_hmu4.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 5e402f / 5635892523 / il_fullxfull.5635892523_ijzi.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 37d167 / 5587789314 / il_fullxfull.5587789314_tbtt.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 794461 / 5587788710 / il_fullxfull.5587788710_cgq6.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / e2b2b6 / 5635869013 / il_fullxfull.5635869013_2gwg.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / b861d4 / 5635851719 / il_fullxfull.5635851719_7ylx.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / ee412b / 5635851295 / il_fullxfull.5635851295_58pu.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 8df5ce / 5587788884 / il_fullxfull.5587788884_1n64.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / bea484 / 5635891725 / il_fullxfull.5635891725_3ux8.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 83ded5 / 5587749048 / il_fullxfull.5587749048_dc4k.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 5dccd2 / 5635852121 / il_fullxfull.5635852121_gvqm.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 9ab5f2 / 5587747722 / il_fullxfull.5587747722_749o.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / d3b5ba / 5587789310 / il_fullxfull.5587789310_mhhd.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / bb70a8 / 5587766078 / il_fullxfull.5587766078_rrc8.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / fac023 / 5635867653 / il_fullxfull.5635867653_imni.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 6f889b / 5635867763 / il_fullxfull.5635867763_avdu.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / b4f5e0 / 5635851673 / il_fullxfull.5635851673_rcgc.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 8ac530 / 5587789188 / il_fullxfull.5587789188_23v5.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / f730bb / 5635891411 / il_fullxfull.5635891411_6va6.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / c86f1b / 5635852115 / il_fullxfull.5635852115_9wvu.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / bf6ea3 / 5635850513 / il_fullxfull.5635850513_puel.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 3cf201 / 5584460266 / il_fullxfull.5584460266_of6u.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 1b1c5e / 5632572291 / il_fullxfull.5632572291_1054.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / a1dc06 / 5584460590 / il_fullxfull.5584460590_p9ll.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 234873 / 5632572727 / il_fullxfull.5632572727_8uo1.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 5ad017 / 5584460732 / il_fullxfull.5584460732_188a.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 3e5d24 / 5584460960 / il_fullxfull.5584460960_6xww.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / fb34d5 / 5584461104 / il_fullxfull.5584461104_hy5h.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 6a0cad / 5632573123 / il_fullxfull.5632573123_ei41.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 92dc41 / 5632573243 / il_fullxfull.5632573243_18em.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / e3b0f8 / 5584461322 / il_fullxfull.5584461322_bejn.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 941630 / 5632573351 / il_fullxfull.5632573351_7vlj.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 89ee52 / 5632573387 / il_fullxfull.5632573387_pak1.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / cc5652 / 5584461376 / il_fullxfull.5584461376_aygg.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / b64084 / 5584461488 / il_fullxfull.5584461488_9fbq.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / b9cc41 / 5632573485 / il_fullxfull.5632573485_hkiu.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 74f206 / 5584461470 / il_fullxfull.5584461470_6t7k.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 72315c / 5632573443 / il_fullxfull.5632573443_kboa.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 5762aa / 5632573607 / il_fullxfull.5632573607_25qz.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / bab30e / 5632573671 / il_fullxfull.5632573671_ayk5.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 1721e9 / 5632573663 / il_fullxfull.5632573663_h8wr.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / e750e4 / 5584461744 / il_fullxfull.5584461744_tuzl.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 1e081b / 5584461718 / il_fullxfull.5584461718_rblf.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 2a7e33 / 5584461570 / il_fullxfull.5584461570_7aoz.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 08fd92 / 5583100720 / il_fullxfull.5583100720_ec8t.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / b578fe / 5631207829 / il_fullxfull.5631207829_3jxp.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 92f782 / 5583085208 / il_fullxfull.5583085208_scrh.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 56b0cb / 5583085194 / il_fullxfull.5583085194_1pvv.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / aa4c70 / 5631192157 / il_fullxfull.5631192157_qo5g.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / d4f7e6 / 5583085170 / il_fullxfull.5583085170_pqqe.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / bafba6 / 5618898593 / il_fullxfull.5618898593_dpc5.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 86b5f4 / 5570793404 / il_fullxfull.5570793404_1usg.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 422e36 / 5618894133 / il_fullxfull.5618894133_pti4.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / b9722f / 5570793314 / il_fullxfull.5570793314_1e6z.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / c4d010 / 5570793418 / il_fullxfull.5570793418_cu0g.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 66a664 / 5618894259 / il_fullxfull.5618894259_mk5n.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 96d8de / 5618898595 / il_fullxfull.5618898595_dc02.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 001c5e / 5570797636 / il_fullxfull.5570797636_qe85.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 9699d9 / 5618898543 / il_fullxfull.5618898543_1ohg.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 56e1b0 / 5618894557 / il_fullxfull.5618894557_133i.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / cc414b / 5618894289 / il_fullxfull.5618894289_ihwc.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 3a5ce1 / 5618894185 / il_fullxfull.5618894185_lgmj.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / ecdd4c / 5570793462 / il_fullxfull.5570793462_fpnb.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 030069 / 5618894327 / il_fullxfull.5618894327_k05l.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / be2e88 / 5570797728 / il_fullxfull.5570797728_jyoi.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 0ae18b / 5618898601 / il_fullxfull.5618898601_mx0u.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 91f168 / 5570797628 / il_fullxfull.5570797628_ibgh.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / acb34b / 5618894535 / il_fullxfull.5618894535_6gjf.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 781d98 / 5618894085 / il_fullxfull.5618894085_5z3f.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 790a42 / 5570793180 / il_fullxfull.5570793180_f3z8.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 05363f / 5618894581 / il_fullxfull.5618894581_i1sq.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 6c894a / 5618894593 / il_fullxfull.5618894593_n4a7.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / ce39d2 / 5618894361 / il_fullxfull.5618894361_7zap.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / c04749 / 5618894087 / il_fullxfull.5618894087_gsmq.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 5f95e0 / 5618894421 / il_fullxfull.5618894421_j0c1.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 6b7f4d / 5618894413 / il_fullxfull.5618894413_f82l.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 079ddc / 5618893973 / il_fullxfull.5618893973_m9ht.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / f1a6e2 / 5570797638 / il_fullxfull.5570797638_4g9y.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 3d20de / 5570797724 / il_fullxfull.5570797724_cmdf.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 8e3aa2 / 5570797646 / il_fullxfull.5570797646_8o22.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 6a3c4d / 5570793734 / il_fullxfull.5570793734_cu8i.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 6b88ec / 5570793370 / il_fullxfull.5570793370_lvq4.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 72fd41 / 5618894187 / il_fullxfull.5618894187_9zaz.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 5a0d15 / 5570793176 / il_fullxfull.5570793176_qous.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 714f50 / 5570643774 / il_fullxfull.5570643774_s78j.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / a00904 / 5299837054 / il_fullxfull.5299837054_3g2f.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 691815 / 5563057752 / il_fullxfull.5563057752_spmb.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / d43ee4 / 5611157649 / il_fullxfull.5611157649_7f13.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / ba24d3 / 5563057676 / il_fullxfull.5563057676_apk1.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 62b1ad / 5611157647 / il_fullxfull.5611157647_pfbo.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / b33220 / 5611157469 / il_fullxfull.5611157469_h2md.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 042ee9 / 5611157823 / il_fullxfull.5611157823_nr3q.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / bcc600 / 5563057794 / il_fullxfull.5563057794_6kq9.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 600d32 / 5563057662 / il_fullxfull.5563057662_msdx.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 38240a / 5563057668 / il_fullxfull.5563057668_a5uq.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 5d5d53 / 5611157701 / il_fullxfull.5611157701_jg8n.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 9fdbe1 / 5611157757 / il_fullxfull.5611157757_ausj.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / d2a2ba / 5611157587 / il_fullxfull.5611157587_hv19.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 92ab6d / 5611157619 / il_fullxfull.5611157619_qevz.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 57eadb / 5611157667 / il_fullxfull.5611157667_nk1r.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 32cc4e / 5611157761 / il_fullxfull.5611157761_8x0v.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 5f197f / 5563057574 / il_fullxfull.5563057574_5c1q.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 04b47f / 5563057374 / il_fullxfull.5563057374_60yr.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / e6cb2d / 5611157733 / il_fullxfull.5611157733_5cje.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 21767b / 5611157871 / il_fullxfull.5611157871_j0b2.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 0082f6 / 5611157783 / il_fullxfull.5611157783_pm1a.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 871437 / 5608671961 / il_fullxfull.5608671961_cni4.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 6f2ebd / 5560570518 / il_fullxfull.5560570518_q0os.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / caffd4 / 5560570366 / il_fullxfull.5560570366_o8dc.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / ac7bb7 / 5560570832 / il_fullxfull.5560570832_il0d.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / e3cde7 / 5608672087 / il_fullxfull.5608672087_p5oe.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 5b9676 / 5608672505 / il_fullxfull.5608672505_dzkf.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 6f9645 / 5608672313 / il_fullxfull.5608672313_mywb.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 81bbb8 / 5608672449 / il_fullxfull.5608672449_luje.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / e27536 / 5560570244 / il_fullxfull.5560570244_jai6.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 112e00 / 5560570410 / il_fullxfull.5560570410_nx7q.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 811e21 / 5608671939 / il_fullxfull.5608671939_isrc.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 4f8107 / 5560570332 / il_fullxfull.5560570332_3pek.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 9f126c / 5608672139 / il_fullxfull.5608672139_ie8j.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 9fe3da / 5560571160 / il_fullxfull.5560571160_lsz8.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 8fa972 / 5608672057 / il_fullxfull.5608672057_1be5.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 6a7fc4 / 5608671899 / il_fullxfull.5608671899_js1i.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / c4a9ec / 5608672055 / il_fullxfull.5608672055_cpph.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 2b2cb8 / 5608671769 / il_fullxfull.5608671769_t4ea.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / b86d0d / 5608671859 / il_fullxfull.5608671859_pims.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 713f2e / 5560570068 / il_fullxfull.5560570068_nbvr.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / a10799 / 5608671345 / il_fullxfull.5608671345_qtf6.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 97a33f / 5560571104 / il_fullxfull.5560571104_e8iq.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / f72563 / 5608672315 / il_fullxfull.5608672315_68xa.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 6f1c1b / 5608672371 / il_fullxfull.5608672371_b88j.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / acf221 / 5608671833 / il_fullxfull.5608671833_lnrb.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / f99dfc / 5560571018 / il_fullxfull.5560571018_gsqo.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 007ce0 / 5560570386 / il_fullxfull.5560570386_o9tb.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 95d594 / 5560570184 / il_fullxfull.5560570184_mbf5.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 08437e / 5560570268 / il_fullxfull.5560570268_d4ym.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 8536b1 / 5560570502 / il_fullxfull.5560570502_mfb6.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / c0fbb3 / 5560569954 / il_fullxfull.5560569954_3n4y.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / b1b4a9 / 5608672375 / il_fullxfull.5608672375_a7lr.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 980bd3 / 5560571172 / il_fullxfull.5560571172_bb17.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 506577 / 5608672081 / il_fullxfull.5608672081_61qh.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / a51a96 / 5560570630 / il_fullxfull.5560570630_7x5q.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / f6ed31 / 5560570460 / il_fullxfull.5560570460_j3ns.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 6fcb9f / 5560570414 / il_fullxfull.5560570414_6cnb.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 3fd28c / 5608671787 / il_fullxfull.5608671787_tely.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 122fa5 / 5560570474 / il_fullxfull.5560570474_6fno.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / d61857 / 5608671455 / il_fullxfull.5608671455_qw8s.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 5ef07f / 5560570362 / il_fullxfull.5560570362_11sl.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / b730e6 / 5608671973 / il_fullxfull.5608671973_qggc.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / a78007 / 5560570096 / il_fullxfull.5560570096_rh5r.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / b641a2 / 5608672351 / il_fullxfull.5608672351_a7u8.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 748a36 / 5608671981 / il_fullxfull.5608671981_ptt3.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 5e3866 / 5608672229 / il_fullxfull.5608672229_t1ha.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 848531 / 5560570162 / il_fullxfull.5560570162_b5yg.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / b077fb / 5608671883 / il_fullxfull.5608671883_113c.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 3a28ab / 5560570420 / il_fullxfull.5560570420_ldxg.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 4c1e8f / 5560570590 / il_fullxfull.5560570590_drxy.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 552463 / 5608671139 / il_fullxfull.5608671139_rjvy.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 66be5b / 5607123301 / il_fullxfull.5607123301_g4hd.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / a32fb7 / 5546976064 / il_fullxfull.5546976064_3jm7.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / d792e2 / 5595091411 / il_fullxfull.5595091411_3hsw.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 86840f / 5548210248 / il_fullxfull.5548210248_8unk.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / b87b80 / 5548209960 / il_fullxfull.5548209960_sj00.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 0e1431 / 5596330047 / il_fullxfull.5596330047_qe0a.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 5ede9f / 5596329927 / il_fullxfull.5596329927_aruc.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 18257a / 5596329823 / il_fullxfull.5596329823_fpvj.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 5c2b84 / 5548209998 / il_fullxfull.5548209998_oqtk.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 4f89ab / 5596329745 / il_fullxfull.5596329745_hpp5.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 288db7 / 5596330157 / il_fullxfull.5596330157_l61h.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 41a2b3 / 5596330053 / il_fullxfull.5596330053_afzu.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 361464 / 5548209968 / il_fullxfull.5548209968_98wu.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 3c2ad8 / 5596330139 / il_fullxfull.5596330139_jdze.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 9ac5e2 / 5596329819 / il_fullxfull.5596329819_bhk4.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / c8bf02 / 5548210254 / il_fullxfull.5548210254_q205.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 0376d2 / 5596329991 / il_fullxfull.5596329991_cknj.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 9cb2d5 / 5548209902 / il_fullxfull.5548209902_ojoj.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / c691a3 / 5548209846 / il_fullxfull.5548209846_rdbi.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 890c93 / 5596329989 / il_fullxfull.5596329989_mzup.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 69a72c / 5548147576 / il_fullxfull.5548147576_smrc.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 0bc12c / 5596268149 / il_fullxfull.5596268149_1ta7.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 8c268c / 5548147428 / il_fullxfull.5548147428_inoz.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 3852d9 / 5596267993 / il_fullxfull.5596267993_6xxn.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / ad9062 / 5547063976 / il_fullxfull.5547063976_e1q7.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 65d411 / 5547063898 / il_fullxfull.5547063898_6c8x.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / b0da0a / 5547063554 / il_fullxfull.5547063554_lyc9.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / d874be / 5595179663 / il_fullxfull.5595179663_cgjn.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 50c79f / 5547064020 / il_fullxfull.5547064020_b28f.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 074e99 / 5595179919 / il_fullxfull.5595179919_rpl8.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 41d61c / 5547063932 / il_fullxfull.5547063932_a5tz.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 2a94cc / 5595180117 / il_fullxfull.5595180117_31w8.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 6ac9d2 / 5547063892 / il_fullxfull.5547063892_oxnv.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / ce0305 / 5547064080 / il_fullxfull.5547064080_gjl5.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 725627 / 5547064014 / il_fullxfull.5547064014_gb4n.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / cffd1b / 5547064024 / il_fullxfull.5547064024_1okl.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 60bf42 / 5547063700 / il_fullxfull.5547063700_e06e.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / c1cce8 / 5595180043 / il_fullxfull.5595180043_dso4.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 0100e1 / 5547063608 / il_fullxfull.5547063608_gqf7.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / ad0eaa / 5547063946 / il_fullxfull.5547063946_mq6r.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 79e0a7 / 5595179767 / il_fullxfull.5595179767_9e7g.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 60b038 / 5547063810 / il_fullxfull.5547063810_frio.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / ef8aa8 / 5595180081 / il_fullxfull.5595180081_rej6.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 665b4b / 5547064060 / il_fullxfull.5547064060_tw1r.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / f6f3a1 / 5547064094 / il_fullxfull.5547064094_ffo4.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 457559 / 5547063502 / il_fullxfull.5547063502_dnex.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 662b21 / 5595179507 / il_fullxfull.5595179507_2rc7.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 0b1772 / 5546997006 / il_fullxfull.5546997006_rd8z.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 7db867 / 5547013878 / il_fullxfull.5547013878_70vt.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 6b94ce / 5546996408 / il_fullxfull.5546996408_tict.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 3170dc / 5546997446 / il_fullxfull.5546997446_hrqt.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 8872f8 / 5546997404 / il_fullxfull.5546997404_ny8p.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 2752b8 / 5547013876 / il_fullxfull.5547013876_aay5.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 747129 / 5546996214 / il_fullxfull.5546996214_nwpx.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 6bf33f / 5595111187 / il_fullxfull.5595111187_577c.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / da48f5 / 5595111193 / il_fullxfull.5595111193_6b9t.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 3789bd / 5547013962 / il_fullxfull.5547013962_9zd2.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 01838b / 5547014012 / il_fullxfull.5547014012_rm62.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / b47477 / 5547013800 / il_fullxfull.5547013800_o4op.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 1f4b62 / 5546996334 / il_fullxfull.5546996334_gt0y.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 34d69e / 5546996884 / il_fullxfull.5546996884_6ejb.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / fd1203 / 5595111271 / il_fullxfull.5595111271_ofd5.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 9da5df / 5547013576 / il_fullxfull.5547013576_93le.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / f3c5ad / 5595129401 / il_fullxfull.5595129401_8xhj.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 05685f / 5547013794 / il_fullxfull.5547013794_swxn.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / d3fef0 / 5546997226 / il_fullxfull.5546997226_bwft.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 27503d / 5546997198 / il_fullxfull.5546997198_2de8.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / c8255e / 5595111429 / il_fullxfull.5595111429_o13y.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 6d8460 / 5546995848 / il_fullxfull.5546995848_qyga.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 4b9663 / 5595111227 / il_fullxfull.5595111227_pbd3.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 8d14aa / 5595129783 / il_fullxfull.5595129783_jz3v.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / b41986 / 5595112643 / il_fullxfull.5595112643_50y8.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 91dc59 / 5546995948 / il_fullxfull.5546995948_2jqa.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / eac666 / 5595129403 / il_fullxfull.5595129403_p013.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 884482 / 5546997396 / il_fullxfull.5546997396_zm36.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 96b91a / 5546997144 / il_fullxfull.5546997144_8poq.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 0a38bb / 5595111183 / il_fullxfull.5595111183_m0zk.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / e65cfd / 5547013928 / il_fullxfull.5547013928_dlmv.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / ad599f / 5595129649 / il_fullxfull.5595129649_ggvg.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / cf659f / 5546997322 / il_fullxfull.5546997322_mamh.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 0416bb / 5595111151 / il_fullxfull.5595111151_3dy9.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 2a92bd / 5547013964 / il_fullxfull.5547013964_l122.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 98b9aa / 5546997398 / il_fullxfull.5546997398_26p0.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / ef6c88 / 5546996846 / il_fullxfull.5546996846_7khd.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / a68d2b / 5546995794 / il_fullxfull.5546995794_t2zo.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 26d642 / 5595111297 / il_fullxfull.5595111297_ljfv.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 6d7319 / 5595129687 / il_fullxfull.5595129687_bv5r.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 568bca / 5547013826 / il_fullxfull.5547013826_p3w6.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / e6ccfb / 5595129525 / il_fullxfull.5595129525_5vv2.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 5c4664 / 5595111605 / il_fullxfull.5595111605_ksfq.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 9308fd / 5546997042 / il_fullxfull.5546997042_31ro.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 797652 / 5595112753 / il_fullxfull.5595112753_7egl.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / fe56c4 / 5595129641 / il_fullxfull.5595129641_mr3f.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / d9bd05 / 5547013616 / il_fullxfull.5547013616_7tef.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 6ca03e / 5595129533 / il_fullxfull.5595129533_pbpb.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / e8ad43 / 5546997282 / il_fullxfull.5546997282_6jk5.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 42e41c / 5595112821 / il_fullxfull.5595112821_gad0.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 282e3e / 5595111487 / il_fullxfull.5595111487_lcd8.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / e49ec7 / 5595111391 / il_fullxfull.5595111391_1z5s.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / ebbcdf / 5595112013 / il_fullxfull.5595112013_5ump.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / cd2bc4 / 5595129723 / il_fullxfull.5595129723_r1k1.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / b261d6 / 5547013832 / il_fullxfull.5547013832_oroa.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / a44bd6 / 5595111813 / il_fullxfull.5595111813_8qwk.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / d87152 / 5546997224 / il_fullxfull.5546997224_abxg.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / bf6d4b / 5546996274 / il_fullxfull.5546996274_rylv.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 66af71 / 5546997188 / il_fullxfull.5546997188_h4dx.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / bcd51d / 5595129521 / il_fullxfull.5595129521_hflm.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / db4a1f / 5547013802 / il_fullxfull.5547013802_98u0.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / ef4b93 / 5595112997 / il_fullxfull.5595112997_l669.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 623b66 / 5595112635 / il_fullxfull.5595112635_t3q5.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / fd2dfd / 5595112723 / il_fullxfull.5595112723_qmjo.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 08c5b9 / 5595112955 / il_fullxfull.5595112955_5hqy.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 81afb1 / 5595112171 / il_fullxfull.5595112171_50mz.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / b13c03 / 5546996956 / il_fullxfull.5546996956_bomv.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / fa5377 / 5546996184 / il_fullxfull.5546996184_rnto.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 91a80f / 5595091583 / il_fullxfull.5595091583_nnp1.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 58a277 / 5546976036 / il_fullxfull.5546976036_ahr8.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 5f402e / 5546862250 / il_fullxfull.5546862250_ktq2.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 669fb6 / 5546862212 / il_fullxfull.5546862212_q2k7.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 9ee009 / 5594977679 / il_fullxfull.5594977679_fq94.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / d5743a / 5594977691 / il_fullxfull.5594977691_eb27.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / fb9aa6 / 5546862210 / il_fullxfull.5546862210_4hz1.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 7edb28 / 5546862136 / il_fullxfull.5546862136_3lm5.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / f4ffa7 / 5546862150 / il_fullxfull.5546862150_36r9.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 1cee2e / 5546862170 / il_fullxfull.5546862170_fvxi.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 5ab591 / 5594952441 / il_fullxfull.5594952441_ho2b.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 74abbd / 5546836878 / il_fullxfull.5546836878_jjml.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 7b877e / 5546836976 / il_fullxfull.5546836976_i6r1.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 26f57e / 5546836550 / il_fullxfull.5546836550_4hfi.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 8381b4 / 5594952363 / il_fullxfull.5594952363_b5ss.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / f6dcec / 5546837654 / il_fullxfull.5546837654_gudk.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 2eff8c / 5594952899 / il_fullxfull.5594952899_bguk.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 23a8d9 / 5546837270 / il_fullxfull.5546837270_6lg8.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 9fea39 / 5594952493 / il_fullxfull.5594952493_4ibk.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / c1480e / 5594951151 / il_fullxfull.5594951151_k7jm.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / dddc9b / 5594953051 / il_fullxfull.5594953051_g3jp.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 338710 / 5546837774 / il_fullxfull.5546837774_fh83.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 5a5704 / 5546837452 / il_fullxfull.5546837452_r62j.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 15d742 / 5594952959 / il_fullxfull.5594952959_crsw.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 7d10fb / 5594952955 / il_fullxfull.5594952955_77k2.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / ee79a7 / 5546835916 / il_fullxfull.5546835916_lxzh.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 463aec / 5594952657 / il_fullxfull.5594952657_ojqo.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 292919 / 5594953255 / il_fullxfull.5594953255_ae91.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 9bb9ba / 5594953225 / il_fullxfull.5594953225_1vhd.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / d482aa / 5546837600 / il_fullxfull.5546837600_h9vr.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / b1802f / 5594951353 / il_fullxfull.5594951353_rifd.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / e64099 / 5546837846 / il_fullxfull.5546837846_s7jh.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / a3bd51 / 5594953163 / il_fullxfull.5594953163_9fu1.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / e70b3f / 5546837416 / il_fullxfull.5546837416_qfhc.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / f84306 / 5546837686 / il_fullxfull.5546837686_nm1e.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 095e2e / 5546837364 / il_fullxfull.5546837364_ia3d.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 15495a / 5546837148 / il_fullxfull.5546837148_mveq.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 364bcb / 5594953219 / il_fullxfull.5594953219_ryo2.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 2146a1 / 5594953103 / il_fullxfull.5594953103_c0mb.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 57ef36 / 5546838050 / il_fullxfull.5546838050_7wgr.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 9472a6 / 5546837844 / il_fullxfull.5546837844_hqdq.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 0b86d3 / 5594952533 / il_fullxfull.5594952533_lkhv.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 3503dc / 5594952821 / il_fullxfull.5594952821_afz9.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 59a964 / 5546837186 / il_fullxfull.5546837186_6wet.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / d86e5d / 5594952695 / il_fullxfull.5594952695_i0vg.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 956482 / 5594953193 / il_fullxfull.5594953193_qu9e.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 4fb6bb / 5546837680 / il_fullxfull.5546837680_la2v.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 2f1f27 / 5546837322 / il_fullxfull.5546837322_5bhu.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 476363 / 5546837874 / il_fullxfull.5546837874_fkjg.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / dcadf8 / 5594952853 / il_fullxfull.5594952853_4wjh.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 4511c4 / 5546837560 / il_fullxfull.5546837560_h5gc.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / e3d5c0 / 5546836234 / il_fullxfull.5546836234_mory.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / ae4bde / 5546837318 / il_fullxfull.5546837318_na2x.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 3727cf / 5546837646 / il_fullxfull.5546837646_kg6b.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 6479fc / 5594953131 / il_fullxfull.5594953131_qjq3.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / f7b33c / 5546730500 / il_fullxfull.5546730500_bkyf.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 3636e2 / 5594846235 / il_fullxfull.5594846235_9cc8.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 5b27a9 / 5546729780 / il_fullxfull.5546729780_3ms2.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 5b7116 / 5594846463 / il_fullxfull.5594846463_hboc.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 19aced / 5594846241 / il_fullxfull.5594846241_remk.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / b1e4ce / 5546729852 / il_fullxfull.5546729852_7evj.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 77b432 / 5546731094 / il_fullxfull.5546731094_d9oh.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / cb8168 / 5546731176 / il_fullxfull.5546731176_t9dd.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / bb0d8c / 5526167088 / il_fullxfull.5526167088_ccmf.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / b5fbdf / 5526163178 / il_fullxfull.5526163178_96ti.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / cd2ead / 5574276177 / il_fullxfull.5574276177_a116.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 93423e / 5526157570 / il_fullxfull.5526157570_jrb1.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / f0a3ff / 5526158626 / il_fullxfull.5526158626_1nkk.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 9b9db7 / 5574265251 / il_fullxfull.5574265251_toit.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 5e90cb / 5526146532 / il_fullxfull.5526146532_6qu9.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / b8dff9 / 5500166038 / il_fullxfull.5500166038_5r68.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 186ac5 / 5548272517 / il_fullxfull.5548272517_o3sx.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / b8c6b9 / 5500165760 / il_fullxfull.5500165760_lwzo.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 6a237c / 5548272471 / il_fullxfull.5548272471_thyq.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / ab6afe / 5548272747 / il_fullxfull.5548272747_d122.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / e45eb1 / 5500165810 / il_fullxfull.5500165810_by0c.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 20aa38 / 5548272707 / il_fullxfull.5548272707_igmr.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 37a835 / 5500165744 / il_fullxfull.5500165744_708o.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 576e2d / 5500166126 / il_fullxfull.5500166126_fbpa.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 97ecde / 5548272507 / il_fullxfull.5548272507_8cvg.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 8c3a50 / 5548272433 / il_fullxfull.5548272433_b6ak.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / cb9789 / 5546891081 / il_fullxfull.5546891081_58o1.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / de9b15 / 5546400945 / il_fullxfull.5546400945_5fs7.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / c4064b / 5530638307 / il_fullxfull.5530638307_q9sp.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / bf8229 / 5530597947 / il_fullxfull.5530597947_bolg.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / ce2085 / 5482430876 / il_fullxfull.5482430876_idud.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 70c174 / 5530539203 / il_fullxfull.5530539203_6l6i.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / e63379 / 5530323023 / il_fullxfull.5530323023_ocpl.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 08e1a1 / 5530315219 / il_fullxfull.5530315219_n970.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 0c2d59 / 5530247937 / il_fullxfull.5530247937_fhll.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 6957f9 / 5481866192 / il_fullxfull.5481866192_f9yu.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 711533 / 5529791743 / il_fullxfull.5529791743_s9cw.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 8f861f / 5414215299 / il_fullxfull.5414215299_2vmm.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 78fa9a / 5366049074 / il_fullxfull.5366049074_pm4b.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / a15b8f / 5414214957 / il_fullxfull.5414214957_5v18.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / dfbf1a / 5414214817 / il_fullxfull.5414214817_jxb1.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 2f9f49 / 5414162631 / il_fullxfull.5414162631_l8qi.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / df5809 / 5414162515 / il_fullxfull.5414162515_1nq8.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 0907c3 / 5414162493 / il_fullxfull.5414162493_8ro4.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 872097 / 5365996244 / il_fullxfull.5365996244_elme.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 6a2465 / 5414162777 / il_fullxfull.5414162777_o0pc.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 3cb083 / 5414162819 / il_fullxfull.5414162819_l34z.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / e1d256 / 5414162727 / il_fullxfull.5414162727_429e.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / e26549 / 5366048664 / il_fullxfull.5366048664_giwl.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 904d02 / 5366048692 / il_fullxfull.5366048692_kgad.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 8b4021 / 5365996266 / il_fullxfull.5365996266_anuk.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / fb0080 / 5365996052 / il_fullxfull.5365996052_stp8.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 62c4df / 5414215057 / il_fullxfull.5414215057_2jt6.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 3c53d0 / 5366049080 / il_fullxfull.5366049080_3zu4.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 8cdefb / 5414162721 / il_fullxfull.5414162721_m5cj.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 916a15 / 5414162577 / il_fullxfull.5414162577_jg24.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / f44eee / 5411820397 / il_fullxfull.5411820397_po0w.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 502467 / 5329912762 / il_fullxfull.5329912762_nwqe.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / cc90cd / 5377993355 / il_fullxfull.5377993355_bd5n.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / d2e8fb / 5377782179 / il_fullxfull.5377782179_sxdu.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 25a255 / 5329471914 / il_fullxfull.5329471914_o3jh.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / f963c2 / 5325332548 / il_fullxfull.5325332548_mqcr.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 252b1c / 5348013979 / il_fullxfull.5348013979_2bt8.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 5a7b1f / 5299836412 / il_fullxfull.5299836412_9r2r.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 44500f / 5348014053 / il_fullxfull.5348014053_r58i.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 36807f / 5348013947 / il_fullxfull.5348013947_2wf8.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / eed0dc / 5299836636 / il_fullxfull.5299836636_6e5g.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 101644 / 5299836622 / il_fullxfull.5299836622_4qsb.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 88c711 / 5299835936 / il_fullxfull.5299835936_hd85.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / b66632 / 5299836964 / il_fullxfull.5299836964_9z1s.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / def0df / 5348014321 / il_fullxfull.5348014321_mw08.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 14b18a / 5299836838 / il_fullxfull.5299836838_5sgu.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 639528 / 5299836914 / il_fullxfull.5299836914_gzjp.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 7f968d / 5348014377 / il_fullxfull.5348014377_rece.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 792b4a / 5299836726 / il_fullxfull.5299836726_fuqs.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / cf4d6e / 5348014285 / il_fullxfull.5348014285_iwk7.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 4bab85 / 5348014289 / il_fullxfull.5348014289_fh86.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 2c190e / 5348014081 / il_fullxfull.5348014081_88vg.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 217a53 / 5299836836 / il_fullxfull.5299836836_nddh.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 3b17dc / 5348014401 / il_fullxfull.5348014401_lvfs.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 3a4fb7 / 5348014375 / il_fullxfull.5348014375_3s4c.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 66b650 / 5299836866 / il_fullxfull.5299836866_6sz3.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 567875 / 5299836834 / il_fullxfull.5299836834_5y0e.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / a2ff8f / 5299836770 / il_fullxfull.5299836770_9z16.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / abec50 / 5299837254 / il_fullxfull.5299837254_nkjw.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 69a93e / 5299836860 / il_fullxfull.5299836860_6hbt.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / eb0722 / 5299836968 / il_fullxfull.5299836968_t82y.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / fae3a9 / 5299837048 / il_fullxfull.5299837048_88ot.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 8c9ae8 / 5348014183 / il_fullxfull.5348014183_4pzg.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 736e91 / 5348014311 / il_fullxfull.5348014311_opd9.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / b21d1b / 5348014403 / il_fullxfull.5348014403_yd9x.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 1cf383 / 5348014179 / il_fullxfull.5348014179_lh6t.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 13b439 / 5299836862 / il_fullxfull.5299836862_f5tb.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / e3958d / 5299837044 / il_fullxfull.5299837044_iwsk.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 8271fd / 5348014611 / il_fullxfull.5348014611_nowu.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / d492fc / 5348014161 / il_fullxfull.5348014161_4eap.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / ce97a7 / 5348014397 / il_fullxfull.5348014397_d0ld.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 4cce27 / 5348014287 / il_fullxfull.5348014287_1nx2.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / e36891 / 5299836940 / il_fullxfull.5299836940_40pq.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 405ddc / 5348014227 / il_fullxfull.5348014227_jps9.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 7a514c / 5299836912 / il_fullxfull.5299836912_60zx.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / c4cfd6 / 5299836832 / il_fullxfull.5299836832_69tu.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 6d488a / 5270374412 / il_fullxfull.5270374412_1xck.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / a1c91a / 5318563419 / il_fullxfull.5318563419_34ta.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / b8e736 / 5318563609 / il_fullxfull.5318563609_sg43.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / bbdd69 / 5318563707 / il_fullxfull.5318563707_4y0u.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 0147df / 5270374540 / il_fullxfull.5270374540_iavc.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 52a36d / 5318563691 / il_fullxfull.5318563691_e0ez.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 3dbdcc / 5270374486 / il_fullxfull.5270374486_ebgt.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 1591a9 / 5318563457 / il_fullxfull.5318563457_a8jz.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / c0cebe / 5318563661 / il_fullxfull.5318563661_174s.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 574d48 / 5318563721 / il_fullxfull.5318563721_1ctl.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / a8e27f / 5270374480 / il_fullxfull.5270374480_1h0z.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 4d9cbe / 5312745099 / il_fullxfull.5312745099_pd96.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 6c0f8a / 5294836353 / il_fullxfull.5294836353_jdv4.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / a9e4dc / 5294836291 / il_fullxfull.5294836291_4m0b.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / fdf286 / 5246648932 / il_fullxfull.5246648932_emis.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 81ac57 / 5246648846 / il_fullxfull.5246648846_9fho.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / cdf566 / 5294836255 / il_fullxfull.5294836255_30uz.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / c1e98f / 5294836249 / il_fullxfull.5294836249_nzf7.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / ec6638 / 5246648886 / il_fullxfull.5246648886_2obh.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 9f894f / 5294836361 / il_fullxfull.5294836361_nvmw.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / e6353b / 5294836367 / il_fullxfull.5294836367_mpof.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 013fc2 / 5246648854 / il_fullxfull.5246648854_p6rt.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 9dca20 / 5228617450 / il_fullxfull.5228617450_7042.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 4121a9 / 5276750835 / il_fullxfull.5276750835_fyko.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / ccf5c4 / 5276767997 / il_fullxfull.5276767997_p2qk.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / fd8eff / 5215043473 / il_fullxfull.5215043473_lstk.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 471b3b / 5214965187 / il_fullxfull.5214965187_nt4h.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 73f3e4 / 5214448171 / il_fullxfull.5214448171_o8w0.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / f16328 / 5090876676 / il_fullxfull.5090876676_edmc.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 7b8e6d / 5090866272 / il_fullxfull.5090866272_fory.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 47f6ea / 5090860042 / il_fullxfull.5090860042_3gh2.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 08bfaa / 5090855734 / il_fullxfull.5090855734_3atf.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / f7846b / 5139076229 / il_fullxfull.5139076229_l1d9.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / f41438 / 5082443774 / il_fullxfull.5082443774_czyo.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / e78c66 / 5166800768 / il_fullxfull.5166800768_ib50.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 0148d9 / 5082406136 / il_fullxfull.5082406136_a9jz.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / ce4d6b / 5082387368 / il_fullxfull.5082387368_9qsj.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 73d0e5 / 5130608965 / il_fullxfull.5130608965_fixu.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / c248dc / 5130589291 / il_fullxfull.5130589291_guhn.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / bc7bd0 / 5130573223 / il_fullxfull.5130573223_afv0.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 348f6d / 5082329378 / il_fullxfull.5082329378_6oj1.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / f01389 / 5082301910 / il_fullxfull.5082301910_l8jh.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 77a54b / 5130696949 / il_fullxfull.5130696949_s218.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 241189 / 4737567890 / il_fullxfull.4737567890_saw5.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / e80f29 / 4785805099 / il_fullxfull.4785805099_23to.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / ffdc9e / 4779429393 / il_fullxfull.4779429393_fjmu.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 397892 / 4729964776 / il_fullxfull.4729964776_esj4.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / aacc2b / 4727859616 / il_fullxfull.4727859616_l33j.jpg,\n        ]\n\n# Regex pattern to capture the variable part up to 'il_fullxfull.'\npattern = r''https // i\\.etsystatic\\.com /\\d + /r / il / [0 - 9a - f] + /\\d + /il_fullxfull\\.'\n\n# Replace the matched pattern with an empty string\nupdated_urls = [\n    re.sub(\n        pattern,\n        '/Users/steven/Pictures/etsy-bulk',\n        url) for url in urls]\n\n# Display the updated URLs\nprint(updated_urls)\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "894",
    "name": "leodown_20250102111110.py",
    "path": "02_media_processing/image_tools/leodown_20250102111110.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 51,
    "size": 1326,
    "docstring": "",
    "keywords": [
      "web_tools"
    ],
    "functions": [],
    "classes": [],
    "imports": [
      "json",
      "requests"
    ],
    "preview": "import json\n\nimport requests\n\n# Base URL for API\nbase_url = (\n    \"https://cloud.leonardo.ai/api/rest/v1/generations/user/f7bb8476-e3f0-4f1f-9a06-4600866cc49c\"\n)\nheaders = {\n    \"accept\": \"application/json\",\n    \"authorization\": \"Bearer 93043291-957d-4ec1-8c79-ee734abcb6e3\",  # Replace with your actual token\n}\n\n# Output file\noutput_file = \"/Users/steven/Pictures/leonardo_library.json\"\n\n# Pagination parameters\noffset = 0\nlimit = 10  # Maximum number of items per request\nall_generations = []",
    "last_modified": "2025-09-13T05:53:49.638943"
  },
  {
    "id": "895",
    "name": "organize_albums 15.py",
    "path": "02_media_processing/image_tools/organize_albums 15.py",
    "category": "02_media_processing",
    "type": "organization",
    "lines": 62,
    "size": 2470,
    "docstring": "",
    "keywords": [
      "organization"
    ],
    "functions": [
      "organize_files"
    ],
    "classes": [],
    "imports": [
      "os",
      "shutil"
    ],
    "preview": "import os\nimport shutil\n\n# Define the base directory\nbase_dir = \"/Users/steven/Music/NocTurnE-meLoDieS/albums/\"\n\n\n# Helper function to create folders and move files\ndef organize_files():\n    # List all files in the base directory\n    files = os.listdir(base_dir)\n\n    # Process each file\n    for file in files:\n        # Skip directories\n        if os.path.isdir(os.path.join(base_dir, file)):\n            continue\n\n        # Extract the base name (album name) from the file\n        if file.endswith(\".mp3\"):",
    "last_modified": "2025-08-02T18:25:58.493471"
  },
  {
    "id": "896",
    "name": "gui.py",
    "path": "02_media_processing/image_tools/gui.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 1041,
    "size": 45274,
    "docstring": "",
    "keywords": [
      "analysis",
      "youtube"
    ],
    "functions": [
      "main",
      "__init__",
      "show_welcome_popup",
      "open_link",
      "load_gui_config_options",
      "save_gui_config_options",
      "on_log_level_change",
      "create_gui_frames_widgets",
      "add_general_options_widgets",
      "add_youtube_title_widgets"
    ],
    "classes": [
      "YouTubeBulkUploaderGUI",
      "ReusableWidgetFrame",
      "Tooltip",
      "TextHandler",
      "DualLogger"
    ],
    "imports": [
      "json",
      "logging",
      "os",
      "sys",
      "threading",
      "tkinter",
      "pathlib",
      "tkinter",
      "pkg_resources",
      "youtube_bulk_upload"
    ],
    "preview": "import json\nimport logging\nimport os\nimport sys\nimport threading\nimport tkinter as tk\nfrom pathlib import Path\nfrom tkinter import filedialog, messagebox, scrolledtext, simpledialog\n\nimport pkg_resources\nfrom youtube_bulk_upload import YouTubeBulkUpload\n\n\nclass YouTubeBulkUploaderGUI:\n    def __init__(\n        self,\n        gui_root: tk.Tk,\n        logger: logging.Logger,\n        bundle_dir: Path,\n        running_in_pyinstaller: bool,",
    "last_modified": "2025-09-13T05:53:47.026555"
  },
  {
    "id": "897",
    "name": "2leomotion 1.py",
    "path": "02_media_processing/image_tools/2leomotion 1.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 90,
    "size": 3097,
    "docstring": "",
    "keywords": [
      "web_tools"
    ],
    "functions": [],
    "classes": [],
    "imports": [
      "time",
      "requests"
    ],
    "preview": "import time\n\nimport requests\n\napi_key = \"b5b99021-8e7a-42ef-8df9-4eca2c6efd3c\"\nauthorization = \"Bearer %s\" % api_key\n\nheaders = {\n    \"accept\": \"application/json\",\n    \"content-type\": \"application/json\",\n    \"authorization\": authorization,\n}\n\n# Generate an image\nurl = \"https://cloud.leonardo.ai/api/rest/v1/generations\"\n\npayload = {\n    \"height\": 960,\n    \"modelId\": \"ac614f96-1082-45bf-be9d-757f2d31c174\",\n    \"prompt\": \"A detailed photograph of a serious cyberpunk Hacker Cyborg transhumanist the past looking directly at the camera, standing straight, hands relaxed, square jaws, masculine face, dark scruff and no wrinkles, slightly buff looking, wearing a dark graphic t-shirt, detailed clothing texture realistic skin texture, black background, sharp focus, front view, waist up shot, high contrast, strong backlighting, action film dark color lut, cinematic luts\",",
    "last_modified": "2025-05-04T22:47:13.035804"
  },
  {
    "id": "898",
    "name": "charsetgroupprober.py",
    "path": "02_media_processing/image_tools/charsetgroupprober.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 105,
    "size": 3885,
    "docstring": "",
    "keywords": [],
    "functions": [
      "__init__",
      "reset",
      "charset_name",
      "language",
      "feed",
      "get_confidence"
    ],
    "classes": [
      "CharSetGroupProber"
    ],
    "imports": [
      "typing",
      "charsetprober",
      "enums"
    ],
    "preview": "######################## BEGIN LICENSE BLOCK ########################\n# The Original Code is Mozilla Communicator client code.\n#\n# The Initial Developer of the Original Code is\n# Netscape Communications Corporation.\n# Portions created by the Initial Developer are Copyright (C) 1998\n# the Initial Developer. All Rights Reserved.\n#\n# Contributor(s):\n#   Mark Pilgrim - port to Python\n#\n# This library is free software; you can redistribute it and/or\n# modify it under the terms of the GNU Lesser General Public\n# License as published by the Free Software Foundation; either\n# version 2.1 of the License, or (at your option) any later version.\n#\n# This library is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU\n# Lesser General Public License for more details.",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "899",
    "name": "sbcsgroupprober.py",
    "path": "02_media_processing/image_tools/sbcsgroupprober.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 89,
    "size": 4137,
    "docstring": "",
    "keywords": [],
    "functions": [
      "__init__"
    ],
    "classes": [
      "SBCSGroupProber"
    ],
    "imports": [
      "charsetgroupprober",
      "hebrewprober",
      "langbulgarianmodel",
      "langgreekmodel",
      "langhebrewmodel",
      "langrussianmodel",
      "langthaimodel",
      "langturkishmodel",
      "sbcharsetprober"
    ],
    "preview": "######################## BEGIN LICENSE BLOCK ########################\n# The Original Code is Mozilla Universal charset detector code.\n#\n# The Initial Developer of the Original Code is\n# Netscape Communications Corporation.\n# Portions created by the Initial Developer are Copyright (C) 2001\n# the Initial Developer. All Rights Reserved.\n#\n# Contributor(s):\n#   Mark Pilgrim - port to Python\n#   Shy Shalom - original C code\n#\n# This library is free software; you can redistribute it and/or\n# modify it under the terms of the GNU Lesser General Public\n# License as published by the Free Software Foundation; either\n# version 2.1 of the License, or (at your option) any later version.\n#\n# This library is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "900",
    "name": "good_usage_of_blacklist.py",
    "path": "02_media_processing/image_tools/good_usage_of_blacklist.py",
    "category": "02_media_processing",
    "type": "organization",
    "lines": 151,
    "size": 5054,
    "docstring": "This template is written by @jeremycjang\n\nWhat does this quickstart script aim to do?\n- Here's the configuration I use the most.\n\nNOTES:\n- Read the incredibly amazing advices & ideas from my experience at the end\nof this file :>",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "instapy"
    ],
    "preview": "\"\"\"\nThis template is written by @jeremycjang\n\nWhat does this quickstart script aim to do?\n- Here's the configuration I use the most.\n\nNOTES:\n- Read the incredibly amazing advices & ideas from my experience at the end\nof this file :>\n\"\"\"\n\nfrom instapy import InstaPy, smart_run\n\ninsta_username = \"username\"\ninsta_password = \"password\"\n\n# get a session!\nsession = InstaPy(\n    username=insta_username,\n    password=insta_password,",
    "last_modified": "2025-03-28T18:36:56"
  },
  {
    "id": "901",
    "name": "leonardo2.py",
    "path": "02_media_processing/image_tools/leonardo2.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 129,
    "size": 4447,
    "docstring": "",
    "keywords": [
      "analysis",
      "youtube",
      "web_tools"
    ],
    "functions": [
      "convert_image_to_jpeg",
      "get_presigned_url",
      "upload_image",
      "upscale_image",
      "get_upscaled_image",
      "process_images"
    ],
    "classes": [],
    "imports": [
      "json",
      "os",
      "sys",
      "time",
      "requests",
      "PIL"
    ],
    "preview": "import json\nimport os\nimport sys\nimport time\n\nimport requests\nfrom PIL import Image\n\napi_key = \"de7c9cb8-022f-42f8-8bf7-a8f9caadfaee\"\nauthorization = f\"Bearer {api_key}\"\n\nheaders = {\n    \"accept\": \"application/json\",\n    \"content-type\": \"application/json\",\n    \"authorization\": authorization,\n}\n\n# Styles to apply\nstyles = [\"GENERAL\", \"CINEMATIC\", \"2D ART & ILLUSTRATION\", \"CG ART & GAME ASSETS\"]\n",
    "last_modified": "2025-09-13T05:53:50.276394"
  },
  {
    "id": "902",
    "name": "virtualenv.py",
    "path": "02_media_processing/image_tools/virtualenv.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 105,
    "size": 3456,
    "docstring": "",
    "keywords": [],
    "functions": [
      "_running_under_venv",
      "_running_under_legacy_virtualenv",
      "running_under_virtualenv",
      "_get_pyvenv_cfg_lines",
      "_no_global_under_venv",
      "_no_global_under_legacy_virtualenv",
      "virtualenv_no_global"
    ],
    "classes": [],
    "imports": [
      "logging",
      "os",
      "re",
      "site",
      "sys",
      "typing"
    ],
    "preview": "import logging\nimport os\nimport re\nimport site\nimport sys\nfrom typing import List, Optional\n\nlogger = logging.getLogger(__name__)\n_INCLUDE_SYSTEM_SITE_PACKAGES_REGEX = re.compile(\n    r\"include-system-site-packages\\s*=\\s*(?P<value>true|false)\"\n)\n\n\ndef _running_under_venv() -> bool:\n    \"\"\"Checks if sys.base_prefix and sys.prefix match.\n\n    This handles PEP 405 compliant virtual environments.\n    \"\"\"\n    return sys.prefix != getattr(sys, \"base_prefix\", sys.prefix)\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "903",
    "name": "test_variables.py",
    "path": "02_media_processing/image_tools/test_variables.py",
    "category": "02_media_processing",
    "type": "testing",
    "lines": 708,
    "size": 26281,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [],
    "preview": "DEFAULT_RESPONSE = {\"status\": \"ok\"}\n\nTEST_CAPTION_ITEM = {\n    \"bit_flags\": 0,\n    \"content_type\": \"comment\",\n    \"created_at\": 1494733796,\n    \"created_at_utc\": 1494733796,\n    \"did_report_as_spam\": False,\n    \"pk\": 17856098620165444,\n    \"status\": \"Active\",\n    \"text\": \"Old Harry Rocks, Dorset, UK\\n\\n#oldharryrocks #dorset #uk \"\n    + \"#rocks #clouds #water #photoshoot #nature #amazing #beautifulsky #sky \"\n    + \"#landscape #nice #beautiful #awesome #landscapes #l4l #f4f\",\n    \"type\": 1,\n    \"user\": {\n        \"full_name\": \"The best earth places\",\n        \"is_private\": False,\n        \"is_verified\": False,\n        \"pk\": 182696006,\n        \"profile_pic_id\": \"1477989239094674784_182696006\",",
    "last_modified": "2025-03-28T18:35:47"
  },
  {
    "id": "904",
    "name": "YouTubeBot 2.py",
    "path": "02_media_processing/image_tools/YouTubeBot 2.py",
    "category": "02_media_processing",
    "type": "youtube",
    "lines": 160,
    "size": 5082,
    "docstring": "",
    "keywords": [
      "web_tools"
    ],
    "functions": [
      "fetch",
      "filter",
      "duration_split",
      "start"
    ],
    "classes": [],
    "imports": [
      "time",
      "tkinter",
      "tkinter.ttk",
      "pyautogui",
      "PIL",
      "selenium",
      "requests"
    ],
    "preview": "import time\nimport tkinter as tk\nimport tkinter.ttk as ttk\n\nimport pyautogui\nfrom PIL import Image, ImageTk\nfrom selenium import webdriver\n\nheight = pyautogui.size()[1]\nwidth = pyautogui.size()[0]\nprint(\"resolution = \" + str(width) + \", \" + str(height))\nwindow = tk.Tk()\nwindow.title(\"YouTube Bot\")\nwindow.resizable(0, 0)\nwindow.configure(background=\"white\")\nwindow.rowconfigure([0], minsize=round(width / 96), weight=0)\nwindow.columnconfigure([0, 2], minsize=round(width / 24), weight=0)\nwindow.columnconfigure(1, minsize=round(width / 2.13), weight=0)\n\n",
    "last_modified": "2025-09-13T05:54:08.142856"
  },
  {
    "id": "905",
    "name": "loop-upscale copy 2.py",
    "path": "02_media_processing/image_tools/loop-upscale copy 2.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 58,
    "size": 2241,
    "docstring": "",
    "keywords": [
      "web_tools"
    ],
    "functions": [],
    "classes": [],
    "imports": [
      "json",
      "os",
      "time",
      "requests"
    ],
    "preview": "import json\nimport os\nimport time\n\nimport requests\n\napi_key = \"3ca7a396-64c0-4e22-8cad-b00bc972d4b3\"\nauthorization = \"Bearer %s\" % api_key\n\nheaders = {\n    \"accept\": \"application/json\",\n    \"content-type\": \"application/json\",\n    \"authorization\": authorization,\n}\n\n# Directory containing images\ndirectory_path = \"/Users/steven/Pictures/CookiMonster/1\"\n\n# Loop through each file in the directory\nfor filename in os.listdir(directory_path):",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "906",
    "name": "motion-upload 2.py",
    "path": "02_media_processing/image_tools/motion-upload 2.py",
    "category": "02_media_processing",
    "type": "youtube",
    "lines": 58,
    "size": 1678,
    "docstring": "",
    "keywords": [
      "web_tools"
    ],
    "functions": [],
    "classes": [],
    "imports": [
      "json",
      "time",
      "requests"
    ],
    "preview": "import json\nimport time\n\nimport requests\n\napi_key = \"<YOUR_API_KEY>\"\nauthorization = \"Bearer %s\" % api_key\n\nheaders = {\n    \"accept\": \"application/json\",\n    \"content-type\": \"application/json\",\n    \"authorization\": authorization,\n}\n\n# Get a presigned URL for uploading an image\nurl = \"https://cloud.leonardo.ai/api/rest/v1/init-image\"\n\npayload = {\"extension\": \"jpg\"}\n\nresponse = requests.post(url, json=payload, headers=headers)",
    "last_modified": "2025-05-04T22:47:12.936867"
  },
  {
    "id": "907",
    "name": "imgconvert_colab copy.py",
    "path": "02_media_processing/image_tools/imgconvert_colab copy.py",
    "category": "02_media_processing",
    "type": "image_processing",
    "lines": 53,
    "size": 1729,
    "docstring": "",
    "keywords": [],
    "functions": [
      "convert_and_upscale_images",
      "main"
    ],
    "classes": [],
    "imports": [
      "os",
      "PIL"
    ],
    "preview": "import os\n\nfrom PIL import Image\n\n# Function to convert WebP images to PNG and upscale by 200% with 300 DPI\n\n\ndef convert_and_upscale_images(source_directory, destination_directory):\n    # Create the destination directory if it doesn't exist\n    os.makedirs(destination_directory, exist_ok=True)\n\n    for filename in os.listdir(source_directory):\n        if filename.endswith(\".png\"):\n            source_file = os.path.join(source_directory, filename)\n            filename_no_ext = os.path.splitext(filename)[0]\n            destination_file = os.path.join(destination_directory, f\"{filename_no_ext}.png\")\n\n            # Convert WebP to PNG and upscale by 200% with 300 DPI\n            im = Image.open(source_file)\n            width, height = im.size",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "908",
    "name": "upscaled.py",
    "path": "02_media_processing/image_tools/upscaled.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 174,
    "size": 5876,
    "docstring": "",
    "keywords": [
      "youtube",
      "web_tools"
    ],
    "functions": [
      "convert_image_to_jpeg",
      "get_presigned_url",
      "upload_image",
      "upscale_image",
      "get_upscaled_image",
      "initialize_csv",
      "log_to_csv"
    ],
    "classes": [],
    "imports": [
      "csv",
      "json",
      "os",
      "time",
      "datetime",
      "requests",
      "PIL"
    ],
    "preview": "import csv\nimport json\nimport os\nimport time\nfrom datetime import datetime\n\nimport requests\nfrom PIL import Image\n\napi_key = \"de7c9cb8-022f-42f8-8bf7-a8f9caadfaee\"\nauthorization = f\"Bearer {api_key}\"\n\nheaders = {\n    \"accept\": \"application/json\",\n    \"content-type\": \"application/json\",\n    \"authorization\": authorization,\n}\n\n# Directory containing images\ndirectory_path = \"/Users/steven/Pictures/TrashCaT/trashy-heartbreak\"",
    "last_modified": "2025-09-13T05:53:56.051707"
  },
  {
    "id": "909",
    "name": "csv_from_json.py",
    "path": "02_media_processing/image_tools/csv_from_json.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 29,
    "size": 913,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "csv",
      "json"
    ],
    "preview": "import csv\nimport json\n\n# Paths for the JSON file and CSV output\njson_file_path = \"/Users/steven/Pictures/etsy/bubble-html/Tshirt/ChatGPT-TrashCat_Junkyard_Singer_Inspires_Change.json\"\ncsv_file_path = \"/Users/steven/Documents/output.csv\"\n\n# Load JSON data\nwith open(json_file_path, \"r\") as json_file:\n    data = json.load(json_file)\n\n# Open CSV file for writing\nwith open(csv_file_path, \"w\", newline=\"\") as csv_file:\n    csv_writer = csv.writer(csv_file)\n\n    # Write header (keys from JSON)\n    csv_writer.writerow([\"Title\", \"Author\", \"Theme\", \"Story\", \"Date\"])\n\n    # Write data\n    title = data.get(\"title\", \"N/A\")",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "910",
    "name": "leo-download-csv.py",
    "path": "02_media_processing/image_tools/leo-download-csv.py",
    "category": "02_media_processing",
    "type": "youtube",
    "lines": 83,
    "size": 2454,
    "docstring": "",
    "keywords": [
      "web_tools"
    ],
    "functions": [
      "save_urls_to_csv",
      "fetch_and_save_all_urls"
    ],
    "classes": [],
    "imports": [
      "csv",
      "os",
      "requests",
      "tqdm"
    ],
    "preview": "import csv\nimport os\n\nimport requests\nfrom tqdm import tqdm\n\n# Configuration\nBASE_URL = (\n    \"https://cloud.leonardo.ai/api/rest/v1/generations/user/f7bb8476-e3f0-4f1f-9a06-4600866cc49c\"\n)\nAUTH_TOKEN = \"Bearer 93043291-957d-4ec1-8c79-ee734abcb6e3\"\nOUTPUT_DIR = \"/Users/steven/Pictures/leodowns\"\nCSV_FILE = os.path.join(OUTPUT_DIR, \"leonardo_urls.csv\")\nMAX_RECORDS_PER_BATCH = 50  # Adjust based on API constraints\n\nHEADERS = {\n    \"accept\": \"application/json\",\n    \"authorization\": AUTH_TOKEN,\n}\n",
    "last_modified": "2025-09-13T05:53:50.017720"
  },
  {
    "id": "911",
    "name": "imgupscale copy.py",
    "path": "02_media_processing/image_tools/imgupscale copy.py",
    "category": "02_media_processing",
    "type": "image_processing",
    "lines": 69,
    "size": 2829,
    "docstring": "",
    "keywords": [],
    "functions": [
      "convert_and_upscale_images",
      "main"
    ],
    "classes": [],
    "imports": [
      "os",
      "PIL"
    ],
    "preview": "import os\n\nfrom PIL import Image, UnidentifiedImageError\n\n\n# Function to convert and upscale PNG and JPEG images by 200% with 300 DPI\ndef convert_and_upscale_images(source_directory, destination_directory, max_size_mb=8):\n    # Create the destination directory if it doesn't exist\n    os.makedirs(destination_directory, exist_ok=True)\n\n    for filename in os.listdir(source_directory):\n        if filename.lower().endswith((\".png\", \".jpeg\", \".jpg\")):\n            source_file = os.path.join(source_directory, filename)\n            filename_no_ext = os.path.splitext(filename)[0]\n            ext = filename.split(\".\")[-1].lower()\n            destination_file = os.path.join(destination_directory, f\"{filename_no_ext}.{ext}\")\n\n            try:\n                # Convert and upscale PNG or JPEG\n                with Image.open(source_file) as im:",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "912",
    "name": "auto_post.py",
    "path": "02_media_processing/image_tools/auto_post.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 60,
    "size": 1545,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "glob",
      "os",
      "sys",
      "time",
      "io",
      "instabot"
    ],
    "preview": "import glob\nimport os\nimport sys\nimport time\nfrom io import open\n\nsys.path.append(os.path.join(sys.path[0], \"../../\"))\nfrom instabot import Bot  # noqa: E402\n\nposted_pic_list = []\ntry:\n    with open(\"pics.txt\", \"r\", encoding=\"utf8\") as f:\n        posted_pic_list = f.read().splitlines()\nexcept Exception:\n    posted_pic_list = []\n\ntimeout = 24 * 60 * 60  # pics will be posted every 24 hours\n\nbot = Bot()\nbot.login()",
    "last_modified": "2025-03-28T18:35:47"
  },
  {
    "id": "913",
    "name": "leonardo_script.py",
    "path": "02_media_processing/image_tools/leonardo_script.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 129,
    "size": 4439,
    "docstring": "",
    "keywords": [
      "analysis",
      "youtube",
      "web_tools"
    ],
    "functions": [
      "convert_image_to_jpeg",
      "get_presigned_url",
      "upload_image",
      "upscale_image",
      "get_upscaled_image",
      "process_images"
    ],
    "classes": [],
    "imports": [
      "json",
      "os",
      "sys",
      "time",
      "requests",
      "PIL"
    ],
    "preview": "import json\nimport os\nimport sys\nimport time\n\nimport requests\nfrom PIL import Image\n\napi_key = \"de7c9cb8-022f-42f8-8bf7-a8f9caadfaee\"\nauthorization = f\"Bearer {api_key}\"\n\nheaders = {\n    \"accept\": \"application/json\",\n    \"content-type\": \"application/json\",\n    \"authorization\": authorization,\n}\n\n# Styles to apply\nstyles = [\"GENERAL\", \"CINEMATIC\", \"2D_ART_ILLUSTRATION\", \"PHOTOREALISTIC\"]\n",
    "last_modified": "2025-09-13T05:53:50.510331"
  },
  {
    "id": "914",
    "name": "dalle-html.py",
    "path": "02_media_processing/image_tools/dalle-html.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 110,
    "size": 3240,
    "docstring": "",
    "keywords": [],
    "functions": [
      "csv_to_html"
    ],
    "classes": [],
    "imports": [
      "csv"
    ],
    "preview": "import csv\n\n\ndef csv_to_html(csv_file, output_html):\n    # Start the HTML structure\n    html_content = \"\"\"\n    <!DOCTYPE html>\n    <html lang=\"en\">\n    <head>\n        <meta charset=\"UTF-8\">\n        <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n        <title>Image Gallery</title>\n        <style>\n            body {\n                font-family: Arial, sans-serif;\n                background-color: #1a1a1a;\n                color: #fff;\n                margin: 0;\n                padding: 0;\n            }",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "915",
    "name": "organize_albums 11.py",
    "path": "02_media_processing/image_tools/organize_albums 11.py",
    "category": "02_media_processing",
    "type": "organization",
    "lines": 62,
    "size": 2466,
    "docstring": "",
    "keywords": [
      "organization"
    ],
    "functions": [
      "organize_files"
    ],
    "classes": [],
    "imports": [
      "os",
      "shutil"
    ],
    "preview": "import os\nimport shutil\n\n# Define the base directory\nbase_dir = \"/Users/steven/Music/NocTurnE-meLoDieS/mp3\"\n\n\n# Helper function to create folders and move files\ndef organize_files():\n    # List all files in the base directory\n    files = os.listdir(base_dir)\n\n    # Process each file\n    for file in files:\n        # Skip directories\n        if os.path.isdir(os.path.join(base_dir, file)):\n            continue\n\n        # Extract the base name (album name) from the file\n        if file.endswith(\".mp3\"):",
    "last_modified": "2025-05-05T01:51:28"
  },
  {
    "id": "916",
    "name": "downsize.py",
    "path": "02_media_processing/image_tools/downsize.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 56,
    "size": 2159,
    "docstring": "",
    "keywords": [],
    "functions": [
      "convert_and_downscale_images_in_subfolders",
      "main"
    ],
    "classes": [],
    "imports": [
      "os",
      "PIL"
    ],
    "preview": "import os\n\nfrom PIL import Image\n\n\ndef convert_and_downscale_images_in_subfolders(source_directory):\n    for root, dirs, files in os.walk(source_directory):\n        for filename in files:\n            if filename.endswith(\".png\"):\n                source_file = os.path.join(root, filename)\n                filename_no_ext = os.path.splitext(filename)[0]\n                temp_file = os.path.join(root, f\"{filename_no_ext}_temp.png\")\n\n                # Open the image and retrieve the original dimensions\n                im = Image.open(source_file)\n                width, height = im.size\n                print(f\"\ud83d\uddbc\ufe0f Processing {filename}: Original size: {width}x{height}\")\n\n                # Downscale the image by 50%\n                downscale_width = width // 2",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "917",
    "name": "save_urls_to_csv.py",
    "path": "02_media_processing/image_tools/save_urls_to_csv.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 75,
    "size": 2240,
    "docstring": "",
    "keywords": [
      "web_tools"
    ],
    "functions": [
      "save_urls_to_csv",
      "fetch_and_save_all_urls"
    ],
    "classes": [],
    "imports": [
      "csv",
      "os",
      "requests",
      "tqdm"
    ],
    "preview": "import csv\nimport os\n\nimport requests\nfrom tqdm import tqdm\n\n# Configuration\nBASE_URL = (\n    \"https://cloud.leonardo.ai/api/rest/v1/generations/user/f7bb8476-e3f0-4f1f-9a06-4600866cc49c\"\n)\nAUTH_TOKEN = \"Bearer 93043291-957d-4ec1-8c79-ee734abcb6e3\"\nOUTPUT_DIR = \"/Users/steven/Pictures/leodowns\"\nCSV_FILE = os.path.join(OUTPUT_DIR, \"leonardo_urls.csv\")\nMAX_RECORDS_PER_BATCH = 50  # Limit records per API request\n\nHEADERS = {\n    \"accept\": \"application/json\",\n    \"authorization\": AUTH_TOKEN,\n}\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "918",
    "name": "massive_follow_then_unfollow_works-non-stop.py",
    "path": "02_media_processing/image_tools/massive_follow_then_unfollow_works-non-stop.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 58,
    "size": 2007,
    "docstring": "This template is written by @loopypanda\n\nWhat does this quickstart script aim to do?\n- My settings is for running InstaPY 24/7 with approximately 1400\nfollows/day - 1400 unfollows/day running follow until reaches 7500 and than\nswitch to unfollow until reaches 0.",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "instapy"
    ],
    "preview": "\"\"\"\nThis template is written by @loopypanda\n\nWhat does this quickstart script aim to do?\n- My settings is for running InstaPY 24/7 with approximately 1400\nfollows/day - 1400 unfollows/day running follow until reaches 7500 and than\nswitch to unfollow until reaches 0.\n\"\"\"\n\nfrom instapy import InstaPy, smart_run\n\n# get a session!\nsession = InstaPy(username=\"\", password=\"\")\n\n# let's go! :>\nwith smart_run(session):\n    # general settings\n\n    # session.set_relationship_bounds(enabled=True,\n    # delimit_by_numbers=False, max_followers=12000, max_following=4500,",
    "last_modified": "2025-03-28T18:36:56"
  },
  {
    "id": "919",
    "name": "upscale3.py",
    "path": "02_media_processing/image_tools/upscale3.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 34,
    "size": 1127,
    "docstring": "",
    "keywords": [],
    "functions": [
      "upscale_image"
    ],
    "classes": [],
    "imports": [
      "os",
      "PIL"
    ],
    "preview": "import os\n\nfrom PIL import Image\n\n# Function to upscale and set DPI of an image\n\n\ndef upscale_image(input_path, output_path, scale_factor=2, dpi=(300, 300)):\n    with Image.open(input_path) as img:\n        # Upscale the image\n        new_size = (img.width * scale_factor, img.height * scale_factor)\n        img_resized = img.resize(new_size, Image.ANTIALIAS)\n        # Set the DPI\n        img_resized.save(output_path, dpi=dpi)\n\n\n# Directory containing images to upscale\ninput_dir = input(\"Enter the path to the directory containing the images: \")\n# Directory to save the upscaled images\noutput_dir = input(\"Enter the path to the directory to save the upscaled images: \")",
    "last_modified": "2025-05-04T22:47:13.383044"
  },
  {
    "id": "920",
    "name": "captions_for_medias.py.py",
    "path": "02_media_processing/captions_for_medias.py_consolidated/captions_for_medias.py.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 9,
    "size": 289,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [],
    "preview": "# flake8: noqa\n\nCAPTIONS = {\n    \"01.jpg\": \"I'm the caption of #horizontal #pic 01 that will be resized \"\n    + \"and cropped, with a mention to @maxdevblock\",\n    \"02.jpg\": \"I'm the caption of #vertical #pic 02 that will be resized and\"\n    + \" cropped, with a mention to @maxdevblock\",\n}\n",
    "last_modified": "2025-03-28T18:35:47"
  },
  {
    "id": "921",
    "name": "captions_for_medias.py_02.py",
    "path": "02_media_processing/captions_for_medias.py_consolidated/captions_for_medias.py_02.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 10,
    "size": 345,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [],
    "preview": "# flake8: noqa\n\nCAPTIONS = {\n    \"60s.mp4\": \"I'm the caption of #vertical #mp4 #video that will be \"\n    + \"cropped only and cut at 30 seconds from start, with a mention \"\n    + \"to @maxdevblock\",\n    \"02.mov\": \"I'm the caption of #horizontal #mov #video that will be\"\n    + \" resized and cropped but not cut, with a mention to @maxdevblock\",\n}\n",
    "last_modified": "2025-03-28T18:35:47"
  },
  {
    "id": "922",
    "name": "main.py_03.py",
    "path": "02_media_processing/main.py_consolidated/main.py_03.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 91,
    "size": 2778,
    "docstring": "",
    "keywords": [],
    "functions": [
      "setup",
      "start"
    ],
    "classes": [],
    "imports": [
      "json",
      "pathlib",
      "pprint",
      "googleapiclient.errors",
      "InquirerPy",
      "InquirerPy.utils",
      "auth",
      "constants",
      "presets",
      "upload"
    ],
    "preview": "import json\nfrom pathlib import Path\nfrom pprint import pprint\n\nfrom googleapiclient.errors import HttpError\nfrom InquirerPy import inquirer\nfrom InquirerPy.utils import color_print\n\nfrom .auth import get_authenticated_service\nfrom .constants import SETTINGS_FILE\nfrom .presets import PRESETS, Preset\nfrom .upload import initialize_upload\nfrom .utils import load_local_file, save_local_file\n\n\ndef setup() -> Preset:\n    settings = json.loads(load_local_file(SETTINGS_FILE, \"{}\"))\n    folder = settings.get(\"folder\", None)\n    video_filepath = None\n    while not video_filepath:",
    "last_modified": "2025-09-13T05:53:47.224353"
  },
  {
    "id": "923",
    "name": "main.py.py",
    "path": "02_media_processing/main.py_consolidated/main.py.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 218,
    "size": 6775,
    "docstring": "",
    "keywords": [
      "web_tools"
    ],
    "functions": [
      "listener",
      "speak",
      "q"
    ],
    "classes": [],
    "imports": [
      "re",
      "time",
      "mpv",
      "mpvListener",
      "pafy",
      "pyttsx3",
      "requests",
      "speech_recognition",
      "bs4",
      "selenium"
    ],
    "preview": "# This is the main file that will hold the majority\n# of the voice assistants functionality\n\n# TODO Eventually create a GUI interface akin to Suri\n\nimport re  # Regex library for manipulating strings\nimport time  # Library that allows us to manipulate time\n\n# PROTOTYPE\nimport mpv\nimport mpvListener\nimport pafy\nimport pyttsx3  # Library that allows for text to speech\nimport requests  # Library that allows us to send HTTP requests\nimport speech_recognition as sr  # Library that allows us to find\nfrom bs4 import BeautifulSoup  # Library that allows us to scrape elements from an HTML file\nfrom selenium import webdriver\nfrom selenium.webdriver.firefox.options import Options\n\n# TODO Find a way to change the driver to espeak",
    "last_modified": "2025-09-13T05:53:29.959028"
  },
  {
    "id": "924",
    "name": "main.py_02.py",
    "path": "02_media_processing/main.py_consolidated/main.py_02.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 96,
    "size": 2379,
    "docstring": "",
    "keywords": [],
    "functions": [
      "remove_old_files",
      "main"
    ],
    "classes": [],
    "imports": [
      "logging",
      "os",
      "threading",
      "modules.clipEditor",
      "modules.cmd_logs",
      "modules.configHandler",
      "modules.input_handler",
      "modules.twitchClips",
      "tqdm"
    ],
    "preview": "#!/usr/bin/env python\nimport logging\nimport os\nimport threading\n\nfrom modules.clipEditor import *\nfrom modules.cmd_logs import *\nfrom modules.configHandler import *\nfrom modules.input_handler import *\nfrom modules.twitchClips import *\nfrom tqdm import tqdm\n\n\ndef remove_old_files() -> None:\n    # Delete temporary file that may still exist if the program was\n    # interrupted during the editing of the clips\n    try:\n        if os.path.isfile(get_output_title() + \"TEMP_MPY_wvf_snd.mp3\"):\n            os.remove(get_output_title() + \"TEMP_MPY_wvf_snd.mp3\")\n        remove_all_clips()",
    "last_modified": "2025-09-13T05:53:44.863541"
  },
  {
    "id": "925",
    "name": "config.py.py",
    "path": "02_media_processing/config.py_consolidated/config.py.py",
    "category": "02_media_processing",
    "type": "setup",
    "lines": 73,
    "size": 2926,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "pathlib"
    ],
    "preview": "import pathlib\n\n# Note:\n# Changing FRAMES and or RESOLUTION will heavily impact load on CPU.\n# If you have a powerful enough computer you may set it to 1080p60\n\n# other\nPATH = str(pathlib.Path().absolute()).replace(\"\\\\\", \"/\")\nCLIP_PATH = PATH + \"/clips/{}/{}\"\nCHECK_VERSION = True  # see if you're running the latest versions\nDEBUG = True  # If additional/debug information should be printed (True/False)\n\nDATA = [\"c xQcOW\", \"c Trainwreckstv\", \"g Just Chatting\"]\nBLACKLIST = [\n    \"c ludwig\",\n    \"g Pools, Hot Tubs, and Beaches\",\n]  # channels/games you dont want to be included in the video\n\n# twitch\nCLIENT_ID = \"\"  # Twitch Client ID",
    "last_modified": "2025-09-13T05:53:56.213378"
  },
  {
    "id": "926",
    "name": "config.py_02.py",
    "path": "02_media_processing/config.py_consolidated/config.py_02.py",
    "category": "02_media_processing",
    "type": "setup",
    "lines": 33,
    "size": 780,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [],
    "preview": "subreddit = \"funny\"  # exclude 'r/'\n\nreddit_login = {  # more info to set up rovided by reddit api documentation\n    \"client_id\": \"\",\n    \"client_secret\": \"\",\n    \"password\": \"\",\n    \"user_agent\": \"\",\n    \"username\": \"\",\n}\n\n\nyoutube = {\n    \"title\": \"\",\n    \"description\": \"\",\n    \"tags\": \"\",\n    \"category\": 23,  # has to be an int, more about category below\n    \"status\": \"\",  # {public, private, unlisted}\n}\n\nvideo = {",
    "last_modified": "2025-05-06T04:35:15"
  },
  {
    "id": "927",
    "name": "y--.py",
    "path": "02_media_processing/audio_tools/y--.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 28,
    "size": 783,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "pytube"
    ],
    "preview": "from pytube import Playlist\n\n# Replace the playlist URL below with your desired playlist URL\nplaylist_url = (\n    \"https://www.youtube.com/playlist?list=PLfudK7D_bQIj3NLnflIYxIWHwpmz_Muyc\"\n)\n\n# Define the directory where you want to save the downloads\ndownload_directory = \"/Volumes/baKs/shorts-need-mp3\"\n\n# Create a Playlist object\nplaylist = Playlist(playlist_url)\n\n# Loop through the videos in the playlist\nfor video in playlist.videos:\n    # Download the video\n    video.streams.get_highest_resolution().download()\n\n    # Get video metadata\n    title = video.title",
    "last_modified": "2025-08-10T20:33:59.515623"
  },
  {
    "id": "928",
    "name": "file-sort.py",
    "path": "02_media_processing/audio_tools/file-sort.py",
    "category": "02_media_processing",
    "type": "organization",
    "lines": 28,
    "size": 1070,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "csv",
      "os",
      "collections"
    ],
    "preview": "import csv\nimport os\nfrom collections import defaultdict\n\n# Directory containing the files\ndirectory = \"/Users/steven/Music/NocTurnE-meLoDieS/v4/mp3\"\noutput_csv = \"/Users/steven/Music/NocTurnE-meLoDieS/v4/song-info.csv\"\n\n# Step 1: Group files by song title (ignoring version indicators)\nfile_groups = defaultdict(list)\n\nfor filename in os.listdir(directory):\n    if filename.endswith(\".txt\"):\n        # Group by the base title, ignoring version-specific suffixes\n        base_title = filename.split(\"_analysis\")[0].split(\"-analysis\")[0].split(\"(\")[0]\n        file_groups[base_title.strip()].append(os.path.join(directory, filename))\n\n# Step 2: Write grouped files to CSV\nwith open(output_csv, mode=\"w\", newline=\"\", encoding=\"utf-8\") as csv_file:\n    writer = csv.writer(csv_file)",
    "last_modified": "2025-05-04T22:47:13.346025"
  },
  {
    "id": "929",
    "name": "analyze_migration.py",
    "path": "02_media_processing/audio_tools/analyze_migration.py",
    "category": "02_media_processing",
    "type": "analysis",
    "lines": 211,
    "size": 7798,
    "docstring": "Analyze current structure and show what will be migrated where",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "analyze_current_structure",
      "show_migration_plan",
      "show_benefits",
      "show_risks_and_mitigation",
      "main"
    ],
    "classes": [],
    "imports": [
      "os",
      "pathlib",
      "collections"
    ],
    "preview": "#!/usr/bin/env python3\n\"\"\"\nAnalyze current structure and show what will be migrated where\n\"\"\"\n\nimport os\nfrom pathlib import Path\nfrom collections import defaultdict\n\ndef analyze_current_structure():\n    \"\"\"Analyze the current directory structure.\"\"\"\n    base_path = Path(\"/Users/steven/Documents/python\")\n    \n    # Categories for analysis\n    categories = {\n        \"analysis_scripts\": [],\n        \"youtube_projects\": [],\n        \"ai_creative\": [],\n        \"web_scraping\": [],\n        \"audio_video\": [],",
    "last_modified": "2025-10-09T05:26:14.631739"
  },
  {
    "id": "930",
    "name": "split-2.py",
    "path": "02_media_processing/audio_tools/split-2.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 44,
    "size": 1491,
    "docstring": "",
    "keywords": [],
    "functions": [
      "split_html_file"
    ],
    "classes": [],
    "imports": [
      "os"
    ],
    "preview": "import os\n\n\ndef split_html_file(file_path, lines_per_chunk=1000):\n    # Ensure the file exists\n    if not os.path.isfile(file_path):\n        print(f\"File '{file_path}' not found.\")\n        return\n\n    # Create output directory\n    output_dir = os.path.join(os.path.dirname(file_path), \"chunks\")\n    os.makedirs(output_dir, exist_ok=True)\n\n    # Initialize variables\n    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n        lines = file.readlines()\n        total_lines = len(lines)\n        chunk_count = 0\n\n        # Iterate over lines in chunks",
    "last_modified": "2025-05-04T22:47:13.351739"
  },
  {
    "id": "931",
    "name": "tts_main.py",
    "path": "02_media_processing/audio_tools/tts_main.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 27,
    "size": 1239,
    "docstring": "",
    "keywords": [],
    "functions": [
      "tts_start"
    ],
    "classes": [],
    "imports": [
      "time",
      "polly_main",
      "s3bucket"
    ],
    "preview": "import time\n\nfrom polly_main import polly, tts_task_resp\nfrom s3bucket import audio_down\n\n\ndef tts_start():\n    audio_book_topic = input(\"enter audiobook name: \")\n    audio_content = input(\" enter audiobook content: \\n\")\n    file_link = polly(audio_book_topic, audio_content)\n    filename_to_save_local = audio_book_topic + file_link[\"SynthesisTask\"][\"TaskId\"]\n    print(\"audio book generated: \" + audio_book_topic)\n    print(audio_book_topic, file_link[\"SynthesisTask\"][\"TaskId\"], filename_to_save_local)\n    file_to_download = file_link[\"SynthesisTask\"][\"TaskId\"] + \".mp3\"\n    filename_to_save_local_with_ext = filename_to_save_local + \".mp3\"\n    s3_name = \"qa-ai-bucket\"\n    print(file_to_download, filename_to_save_local_with_ext)\n    tts_task_status = tts_task_resp(file_link[\"SynthesisTask\"][\"TaskId\"])\n\n    while tts_task_status[\"SynthesisTask\"][\"TaskStatus\"] == \"scheduled\":",
    "last_modified": "2025-09-13T05:53:28.699715"
  },
  {
    "id": "932",
    "name": "mp3_processor.py",
    "path": "02_media_processing/audio_tools/mp3_processor.py",
    "category": "02_media_processing",
    "type": "audio_processing",
    "lines": 97,
    "size": 3371,
    "docstring": "",
    "keywords": [
      "organization"
    ],
    "functions": [
      "organize_files"
    ],
    "classes": [],
    "imports": [
      "os",
      "shutil"
    ],
    "preview": "import os\nimport shutil\n\n# Define the base directory\nbase_dir = \"/Users/steven/Music/nocTurneMeLoDieS/mp4\"\n\n\ndef organize_files():\n    # Check if the base directory exists\n    if not os.path.exists(base_dir):\n        print(f\"\u274c Error: The directory '{base_dir}' does not exist.\")\n        return\n\n    # List all files in the base directory\n    try:\n        files = os.listdir(base_dir)\n    except Exception as e:\n        print(f\"\u274c Error accessing directory '{base_dir}': {e}\")\n        return\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "933",
    "name": "gmupload.py",
    "path": "02_media_processing/audio_tools/gmupload.py",
    "category": "02_media_processing",
    "type": "youtube",
    "lines": 103,
    "size": 3127,
    "docstring": "This is used for uploading downloaded track to google play music",
    "keywords": [
      "youtube"
    ],
    "functions": [
      "__init__",
      "login",
      "logout",
      "upload",
      "__upload_file__"
    ],
    "classes": [
      "GoolgeMusicUploader"
    ],
    "imports": [
      "logging",
      "gmusicapi",
      "ytdl.audiometadata",
      "ytdl.customerrors",
      "ytdl.models",
      "ytdl.oshelper"
    ],
    "preview": "\"This is used for uploading downloaded track to google play music\"\n\nimport logging\n\nfrom gmusicapi import Musicmanager, clients\nfrom ytdl.audiometadata import AudioMetadata\nfrom ytdl.customerrors import AuthError, DirectoryNotFoundError\nfrom ytdl.models import TrackInfo, UploadResult\nfrom ytdl.oshelper import (\n    DEFAULT_FILE_NAME,\n    absolute_files,\n    get_album_art_file,\n    get_track_file,\n    get_track_info_file,\n    isdir,\n    lock_file_exists,\n)\n\n\nclass GoolgeMusicUploader(object):",
    "last_modified": "2025-09-13T05:54:15.015688"
  },
  {
    "id": "934",
    "name": "server.py",
    "path": "02_media_processing/audio_tools/server.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 168,
    "size": 5399,
    "docstring": "",
    "keywords": [
      "testing",
      "youtube"
    ],
    "functions": [
      "testFTPConnection",
      "getFileNames",
      "uploadCompleteVideo",
      "sendThread",
      "startFTPServer",
      "startHTTPServer",
      "init",
      "_set_headers",
      "do_HEAD",
      "do_GET"
    ],
    "classes": [
      "HTTPHandler"
    ],
    "imports": [
      "cgi",
      "ftplib",
      "http.server",
      "json",
      "os",
      "pickle",
      "random",
      "socketserver",
      "sys",
      "traceback"
    ],
    "preview": "import cgi\nimport ftplib\nimport http.server\nimport json\nimport os\nimport pickle\nimport random\nimport socketserver\nimport sys\nimport traceback\nfrom threading import Thread\nfrom time import sleep\n\nimport scriptwrapper\nimport settings\nimport vidGen\nfrom pyftpdlib.authorizers import DummyAuthorizer\nfrom pyftpdlib.handlers import FTPHandler\nfrom pyftpdlib.servers import FTPServer\n",
    "last_modified": "2025-09-13T05:53:32.505733"
  },
  {
    "id": "935",
    "name": "webm-to-mp3.py",
    "path": "02_media_processing/audio_tools/webm-to-mp3.py",
    "category": "02_media_processing",
    "type": "audio_processing",
    "lines": 44,
    "size": 1450,
    "docstring": "",
    "keywords": [],
    "functions": [
      "convert_webm_to_mp3"
    ],
    "classes": [],
    "imports": [
      "os",
      "subprocess"
    ],
    "preview": "import os\nimport subprocess\n\n\ndef convert_webm_to_mp3(directory_path):\n    \"\"\"\n    Converts all .webm files in the specified directory to .mp3 format.\n\n    Args:\n    directory_path (str): The path to the directory containing .webm files.\n    \"\"\"\n    for filename in os.listdir(directory_path):\n        if filename.endswith(\".webm\"):\n            # Construct the full path to the source .webm file\n            webm_path = os.path.join(directory_path, filename)\n            # Construct the full path for the output .mp3 file\n            mp3_path = os.path.join(directory_path, os.path.splitext(filename)[0] + \".mp3\")\n\n            # Construct the ffmpeg command for converting .webm to .mp3\n            command = [",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "936",
    "name": "style_engine.py",
    "path": "02_media_processing/audio_tools/style_engine.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 82,
    "size": 2247,
    "docstring": "",
    "keywords": [],
    "functions": [
      "guess_domain",
      "sentiment",
      "decide_style"
    ],
    "classes": [],
    "imports": [
      "__future__",
      "typing",
      "textblob"
    ],
    "preview": "from __future__ import annotations\n\nfrom typing import Any, Dict, Optional\n\ntry:\n    from textblob import TextBlob\nexcept Exception:\n    TextBlob = None\n\nDOMAIN_HINTS = {\n    \"tech\": {\n        \"style\": \"neonpunk\",\n        \"font\": \"JetBrains Mono\",\n        \"palette\": [\"#00FF9C\", \"#00E1FF\", \"#111111\"],\n    },\n    \"finance\": {\n        \"style\": \"minimal\",\n        \"font\": \"Helvetica Neue\",\n        \"palette\": [\"#0E1E3A\", \"#1F497D\", \"#FFFFFF\"],\n    },",
    "last_modified": "2025-09-13T05:55:10.297687"
  },
  {
    "id": "937",
    "name": "config_20241213005737.py",
    "path": "02_media_processing/audio_tools/config_20241213005737.py",
    "category": "02_media_processing",
    "type": "setup",
    "lines": 3,
    "size": 71,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [],
    "preview": "# config.py\nSOURCE_DIRECTORY = \"/Users/steven/Music/NocTurnE-meLoDieS\"\n",
    "last_modified": "2025-09-13T05:53:47.430511"
  },
  {
    "id": "938",
    "name": "mp3.py",
    "path": "02_media_processing/audio_tools/mp3.py",
    "category": "02_media_processing",
    "type": "audio_processing",
    "lines": 45,
    "size": 1453,
    "docstring": "",
    "keywords": [],
    "functions": [
      "convert_webm_to_mp3"
    ],
    "classes": [],
    "imports": [
      "os",
      "subprocess"
    ],
    "preview": "import os\nimport subprocess\n\n\ndef convert_webm_to_mp3(directory_path):\n    \"\"\"\n    Converts all .webm files in the specified directory to .mp3 format.\n\n    Args:\n    directory_path (str): The path to the directory containing .webm files.\n    \"\"\"\n    for filename in os.listdir(directory_path):\n        if filename.endswith(\".webm\"):\n            # Construct the full path to the source .webm file\n            webm_path = os.path.join(directory_path, filename)\n            # Construct the full path for the output .mp3 file\n            mp3_path = os.path.join(directory_path, os.path.splitext(filename)[0] + \".mp3\")\n\n            # Construct the ffmpeg command for converting .webm to .mp3\n            command = [",
    "last_modified": "2025-09-13T05:55:28.499128"
  },
  {
    "id": "939",
    "name": "ffmpegdl.py",
    "path": "02_media_processing/audio_tools/ffmpegdl.py",
    "category": "02_media_processing",
    "type": "video_processing",
    "lines": 87,
    "size": 2781,
    "docstring": "",
    "keywords": [
      "youtube"
    ],
    "functions": [
      "__init__",
      "check_if_file",
      "download",
      "_download",
      "_download_win",
      "_download_mac",
      "_download_linux",
      "_unzip",
      "_untar",
      "_cleanup"
    ],
    "classes": [
      "FFmpegDL"
    ],
    "imports": [
      "os",
      "pathlib",
      "shutil",
      "sys",
      "uuid",
      "urllib.request",
      "zipfile",
      "tarfile"
    ],
    "preview": "import os\nfrom pathlib import Path\nfrom shutil import move, rmtree\nfrom sys import platform\nfrom uuid import uuid1\n\nFFMPEG_STATIC_LINUX = \"https://johnvansickle.com/ffmpeg/builds/ffmpeg-git-amd64-static.tar.xz\"\nFFMPEG_STATIC_WIN = \"https://www.gyan.dev/ffmpeg/builds/ffmpeg-release-essentials.zip\"\nFFMPEG_STATIC_MAC = \"https://evermeet.cx/ffmpeg/getrelease/zip\"\n\n\nclass FFmpegDL:\n    def __init__(self, data: str) -> None:\n        self.data_path = Path(data) / \"ffmpeg\"\n\n        if platform == \"win32\":\n            self.temp = self.data_path / str(uuid1())\n            self.download_link = FFMPEG_STATIC_WIN\n            self.final_location = self.data_path / \"bin\" / \"ffmpeg.exe\"\n            self.platform_task = self._download_win",
    "last_modified": "2025-09-13T05:55:15.548316"
  },
  {
    "id": "940",
    "name": "yt-playlist2.py",
    "path": "02_media_processing/audio_tools/yt-playlist2.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 28,
    "size": 783,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "pytube"
    ],
    "preview": "from pytube import Playlist\n\n# Replace the playlist URL below with your desired playlist URL\nplaylist_url = (\n    \"https://www.youtube.com/playlist?list=PLfudK7D_bQIjRgAqVU_jkbnb1N5V_pNiG\"\n)\n\n# Define the directory where you want to save the downloads\ndownload_directory = \"/Volumes/baKs/shorts-need-mp3\"\n\n# Create a Playlist object\nplaylist = Playlist(playlist_url)\n\n# Loop through the videos in the playlist\nfor video in playlist.videos:\n    # Download the video\n    video.streams.get_highest_resolution().download()\n\n    # Get video metadata\n    title = video.title",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "941",
    "name": "downloader.py",
    "path": "02_media_processing/audio_tools/downloader.py",
    "category": "02_media_processing",
    "type": "youtube",
    "lines": 1537,
    "size": 51724,
    "docstring": "Important Notes:\nExternal Downloader And its args do only work in specific video formats In platforms like (youtube)\nUnfortunately It doesn't work on PH And Other Sites.\nEdit: It Works Now :)",
    "keywords": [
      "youtube",
      "web_tools"
    ],
    "functions": [
      "find_config",
      "read_config",
      "__init__",
      "add_values",
      "clear",
      "convert_size",
      "check_url",
      "check_connection",
      "animation",
      "get_current_dir"
    ],
    "classes": [
      "config_reader",
      "parser_args",
      "download"
    ],
    "imports": [
      "__future__",
      "configparser",
      "itertools",
      "json",
      "optparse",
      "os",
      "shutil",
      "sys",
      "threading",
      "distutils"
    ],
    "preview": "#!/usr/bin/python3\n\n# Updated On 01/01/2022\n# Created By ybenel\n\"\"\"\nImportant Notes:\nExternal Downloader And its args do only work in specific video formats In platforms like (youtube)\nUnfortunately It doesn't work on PH And Other Sites.\nEdit: It Works Now :)\n\"\"\"\nfrom __future__ import unicode_literals\n\nimport configparser\nimport itertools\nimport json\nimport optparse\nimport os\nimport shutil\nimport sys\nimport threading",
    "last_modified": "2025-09-13T05:53:58.934360"
  },
  {
    "id": "942",
    "name": "sound_test.py",
    "path": "02_media_processing/audio_tools/sound_test.py",
    "category": "02_media_processing",
    "type": "audio_processing",
    "lines": 4,
    "size": 49,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "clips"
    ],
    "preview": "from clips import *\n\ngen_audio_clip(\"wooo hooo\")\n",
    "last_modified": "2025-03-28T18:35:48"
  },
  {
    "id": "943",
    "name": "downloadupload.py",
    "path": "02_media_processing/audio_tools/downloadupload.py",
    "category": "02_media_processing",
    "type": "youtube",
    "lines": 120,
    "size": 4162,
    "docstring": "Ytdl",
    "keywords": [
      "youtube"
    ],
    "functions": [
      "__init__",
      "__download_tracks__",
      "__upload_tracks__",
      "__successful_upload_tasks__",
      "__failed_upload_tasks__",
      "download_and_upload"
    ],
    "classes": [
      "Downloadupload"
    ],
    "imports": [
      "__future__",
      "json",
      "logging",
      "ytdl.audiodownload",
      "ytdl.awsqueue",
      "ytdl.customerrors",
      "ytdl.gmupload",
      "ytdl.models",
      "ytdl.notify",
      "ytdl.oshelper"
    ],
    "preview": "\"Ytdl\"\n\nfrom __future__ import unicode_literals\n\nimport json\nimport logging\n\nfrom ytdl.audiodownload import AudioDownload\nfrom ytdl.awsqueue import Awsqueue\nfrom ytdl.customerrors import AuthError, DirectoryNotFoundError\nfrom ytdl.gmupload import GoolgeMusicUploader\nfrom ytdl.models import Payload\nfrom ytdl.notify import Iftttnotify\nfrom ytdl.oshelper import absolute_dirs, copy, isdir, remove\n\n\nclass Downloadupload(object):\n    \"Youtube downloader\"\n\n    def __init__(self, ytdl_config):",
    "last_modified": "2025-09-13T05:54:14.975384"
  },
  {
    "id": "944",
    "name": "y.py",
    "path": "02_media_processing/audio_tools/y.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 28,
    "size": 783,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "pytube"
    ],
    "preview": "from pytube import Playlist\n\n# Replace the playlist URL below with your desired playlist URL\nplaylist_url = (\n    \"https://www.youtube.com/playlist?list=PLfudK7D_bQIjRgAqVU_jkbnb1N5V_pNiG\"\n)\n\n# Define the directory where you want to save the downloads\ndownload_directory = \"/Volumes/baKs/shorts-need-mp3\"\n\n# Create a Playlist object\nplaylist = Playlist(playlist_url)\n\n# Loop through the videos in the playlist\nfor video in playlist.videos:\n    # Download the video\n    video.streams.get_highest_resolution().download()\n\n    # Get video metadata\n    title = video.title",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "945",
    "name": "y copy.py",
    "path": "02_media_processing/audio_tools/y copy.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 28,
    "size": 783,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "pytube"
    ],
    "preview": "from pytube import Playlist\n\n# Replace the playlist URL below with your desired playlist URL\nplaylist_url = (\n    \"https://www.youtube.com/playlist?list=PLfudK7D_bQIjRgAqVU_jkbnb1N5V_pNiG\"\n)\n\n# Define the directory where you want to save the downloads\ndownload_directory = \"/Volumes/baKs/shorts-need-mp3\"\n\n# Create a Playlist object\nplaylist = Playlist(playlist_url)\n\n# Loop through the videos in the playlist\nfor video in playlist.videos:\n    # Download the video\n    video.streams.get_highest_resolution().download()\n\n    # Get video metadata\n    title = video.title",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "946",
    "name": "mp3_colab.py",
    "path": "02_media_processing/audio_tools/mp3_colab.py",
    "category": "02_media_processing",
    "type": "audio_processing",
    "lines": 45,
    "size": 1453,
    "docstring": "",
    "keywords": [],
    "functions": [
      "convert_webm_to_mp3"
    ],
    "classes": [],
    "imports": [
      "os",
      "subprocess"
    ],
    "preview": "import os\nimport subprocess\n\n\ndef convert_webm_to_mp3(directory_path):\n    \"\"\"\n    Converts all .webm files in the specified directory to .mp3 format.\n\n    Args:\n    directory_path (str): The path to the directory containing .webm files.\n    \"\"\"\n    for filename in os.listdir(directory_path):\n        if filename.endswith(\".webm\"):\n            # Construct the full path to the source .webm file\n            webm_path = os.path.join(directory_path, filename)\n            # Construct the full path for the output .mp3 file\n            mp3_path = os.path.join(directory_path, os.path.splitext(filename)[0] + \".mp3\")\n\n            # Construct the ffmpeg command for converting .webm to .mp3\n            command = [",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "947",
    "name": "speech--.py",
    "path": "02_media_processing/audio_tools/speech--.py",
    "category": "02_media_processing",
    "type": "transcription",
    "lines": 52,
    "size": 1562,
    "docstring": "",
    "keywords": [],
    "functions": [
      "extract_text_from_audio"
    ],
    "classes": [],
    "imports": [
      "glob",
      "os",
      "moviepy.editor",
      "speech_recognition"
    ],
    "preview": "import glob\nimport os\n\nimport moviepy.editor as mp\nimport speech_recognition as sr\n\n# Prompt for the movie directory path\nmovie_directory = input(\"Enter the path to the directory containing your movies: \")\n\n# Prompt for the output .txt file name\noutput_file_name = input(\"Enter the name for the output .txt file (without extension): \")\n\n# Add the .txt file extension\noutput_txt_file = output_file_name + \".txt\"\n\n# Initialize the recognizer\nrecognizer = sr.Recognizer()\n\n\n# Function to extract text from audio",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "948",
    "name": "s3bucket.py",
    "path": "02_media_processing/audio_tools/s3bucket.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 46,
    "size": 1584,
    "docstring": "",
    "keywords": [
      "transcription"
    ],
    "functions": [
      "audio_down",
      "get_audio_link",
      "set_public_access"
    ],
    "classes": [],
    "imports": [
      "boto3",
      "botocore.config",
      "utilities.const"
    ],
    "preview": "import boto3\nfrom botocore.config import Config\nfrom utilities.const import AWS_ACCESS_KEY, AWS_SEC_KEY, get_current_date\n\nmy_config = Config(\n    region_name=\"ap-south-1\",\n    signature_version=\"v4\",\n    retries={\"max_attempts\": 3, \"mode\": \"standard\"},\n)\ns3 = boto3.client(\n    \"s3\",\n    config=my_config,\n    aws_access_key_id=AWS_ACCESS_KEY,\n    aws_secret_access_key=AWS_SEC_KEY,\n)\n\n\ndef audio_down(s3_name, audio_book_topic, aws_filename, filename_to_save_local):\n    str_get_current_date = get_current_date()\n    s3.download_file(",
    "last_modified": "2025-09-13T05:53:28.676404"
  },
  {
    "id": "949",
    "name": "background.py",
    "path": "02_media_processing/audio_tools/background.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 49,
    "size": 1676,
    "docstring": "",
    "keywords": [
      "youtube"
    ],
    "functions": [
      "get_start_and_end_times",
      "download_background",
      "chop_background_video"
    ],
    "classes": [],
    "imports": [
      "pathlib",
      "random",
      "moviepy.editor",
      "moviepy.video.io.ffmpeg_tools",
      "pytube",
      "utils.console"
    ],
    "preview": "from pathlib import Path\nfrom random import randrange\n\nfrom moviepy.editor import VideoFileClip\nfrom moviepy.video.io.ffmpeg_tools import ffmpeg_extract_subclip\nfrom pytube import YouTube\n\nfrom utils.console import print_step, print_substep\n\n\ndef get_start_and_end_times(video_length, length_of_clip):\n\n    random_time = randrange(180, int(length_of_clip) - int(video_length))\n    return random_time, random_time + video_length\n\n\ndef download_background():\n    \"\"\"Downloads the background video from youtube.\n\n    Shoutout to: bbswitzer (https://www.youtube.com/watch?v=n_Dv4JMiwK8)",
    "last_modified": "2025-09-13T05:53:59.387864"
  },
  {
    "id": "950",
    "name": "organize_albums 2.py",
    "path": "02_media_processing/audio_tools/organize_albums 2.py",
    "category": "02_media_processing",
    "type": "organization",
    "lines": 106,
    "size": 4065,
    "docstring": "",
    "keywords": [
      "organization"
    ],
    "functions": [
      "organize_files"
    ],
    "classes": [],
    "imports": [
      "os",
      "shutil"
    ],
    "preview": "import os\nimport shutil\n\n# Define the base directory\nbase_dir = \"/Users/steven/Music/nocTurneMeLoDieS/mp3\"\n\n\ndef organize_files():\n    # Check if the base directory exists\n    if not os.path.exists(base_dir):\n        print(f\"\u274c Error: The directory '{base_dir}' does not exist.\")\n        return\n\n    # List all files in the base directory\n    try:\n        files = os.listdir(base_dir)\n    except Exception as e:\n        print(f\"\u274c Error accessing directory '{base_dir}': {e}\")\n        return\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "951",
    "name": "splt-1.py",
    "path": "02_media_processing/audio_tools/splt-1.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 39,
    "size": 1315,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "os",
      "bs4"
    ],
    "preview": "import os\n\nfrom bs4 import BeautifulSoup\n\n# Path to the large HTML file\ninput_path = (\n    \"/Users/steven/Music/NocTurnE-meLoDieS/Song-origins-html/Raccoon Alley Album Art(83% copy).html\"\n)\noutput_dir = \"/Users/steven/Music/NocTurnE-meLoDieS/Song-origins-html/chunks\"\n\n# Create output directory if it doesn't exist\nos.makedirs(output_dir, exist_ok=True)\n\n# Load the HTML content\nwith open(input_path, \"r\", encoding=\"utf-8\") as file:\n    html_content = file.read()\n\n# Parse HTML using BeautifulSoup\nsoup = BeautifulSoup(html_content, \"html.parser\")\n",
    "last_modified": "2025-09-13T05:53:55.451673"
  },
  {
    "id": "952",
    "name": "types.py",
    "path": "02_media_processing/audio_tools/types.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 37,
    "size": 552,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [
      "Type",
      "Platform",
      "Format",
      "Quality"
    ],
    "imports": [],
    "preview": "__all__ = [\"Type\", \"Platform\", \"Format\", \"Quality\"]\n\n\nclass Type:\n    TRACK = \"track\"\n    ALBUM = \"album\"\n    PLAYLIST = \"playlist\"\n    EPISODE = \"episode\"\n    SHOW = \"show\"\n    ARTIST = \"artist\"\n\n\nclass Platform:\n    SPOTIFY = \"spotify\"\n    YOUTUBE = \"youtube\"\n\n\nclass Format:\n    MP3 = \"mp3\"\n    AAC = \"aac\"",
    "last_modified": "2025-05-04T23:27:53.630386"
  },
  {
    "id": "953",
    "name": "aws_polly.py",
    "path": "02_media_processing/audio_tools/aws_polly.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 78,
    "size": 2422,
    "docstring": "",
    "keywords": [],
    "functions": [
      "__init__",
      "run",
      "randomvoice"
    ],
    "classes": [
      "AWSPolly"
    ],
    "imports": [
      "random",
      "sys",
      "boto3",
      "botocore.exceptions",
      "utils"
    ],
    "preview": "import random\nimport sys\n\nfrom boto3 import Session\nfrom botocore.exceptions import BotoCoreError, ClientError, ProfileNotFound\n\nfrom utils import settings\n\nvoices = [\n    \"Brian\",\n    \"Emma\",\n    \"Russell\",\n    \"Joey\",\n    \"Matthew\",\n    \"Joanna\",\n    \"Kimberly\",\n    \"Amy\",\n    \"Geraint\",\n    \"Nicole\",\n    \"Justin\",",
    "last_modified": "2025-09-13T05:53:59.605179"
  },
  {
    "id": "954",
    "name": "ffmpeg_install.py",
    "path": "02_media_processing/audio_tools/ffmpeg_install.py",
    "category": "02_media_processing",
    "type": "video_processing",
    "lines": 143,
    "size": 5243,
    "docstring": "",
    "keywords": [
      "web_tools"
    ],
    "functions": [
      "ffmpeg_install_windows",
      "ffmpeg_install_linux",
      "ffmpeg_install_mac",
      "ffmpeg_install"
    ],
    "classes": [],
    "imports": [
      "os",
      "subprocess",
      "zipfile",
      "requests"
    ],
    "preview": "import os\nimport subprocess\nimport zipfile\n\nimport requests\n\n\ndef ffmpeg_install_windows():\n    try:\n        ffmpeg_url = (\n            \"https://github.com/GyanD/codexffmpeg/releases/download/6.0/ffmpeg-6.0-full_build.zip\"\n        )\n        ffmpeg_zip_filename = \"ffmpeg.zip\"\n        ffmpeg_extracted_folder = \"ffmpeg\"\n\n        # Check if ffmpeg.zip already exists\n        if os.path.exists(ffmpeg_zip_filename):\n            os.remove(ffmpeg_zip_filename)\n\n        # Download FFmpeg",
    "last_modified": "2025-09-13T05:54:00.180946"
  },
  {
    "id": "955",
    "name": "play.py",
    "path": "02_media_processing/audio_tools/play.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 2,
    "size": 44,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [],
    "preview": "# This file handles playing music or videos\n",
    "last_modified": "2025-03-28T18:35:47.781740"
  },
  {
    "id": "956",
    "name": "text_to_speech.py",
    "path": "02_media_processing/audio_tools/text_to_speech.py",
    "category": "02_media_processing",
    "type": "transcription",
    "lines": 27,
    "size": 983,
    "docstring": "",
    "keywords": [],
    "functions": [
      "main"
    ],
    "classes": [],
    "imports": [
      "gtts"
    ],
    "preview": "# This program will convert a text string into a text to speech audio file\n\nfrom gtts import gTTS\n\n\ndef main() -> int:\n    # A title, simulating scraping a title from a subreddit\n    title = \"What\u2019s a \u201cboring\u201d hobby that\u2019s not boring at all?\"\n\n    # A comment, simulating scraping a comment to the post\n    comment = \"I do counted cross stitch. I'm not creative at all, but give me a coded pattern that creates a map (I always do maps - but you can make way more intricate things than you think if you invest the time) and I'm all over that shit. It's how I quit smoking.\"\n\n    # Creating text to speech creation on those strings making mp3 data\n    tts_entry1 = gTTS(title, lang=\"en\", tld=\"ie\")\n    tts_entry2 = gTTS(comment, lang=\"en\", tld=\"ie\")\n\n    # Write the mp3 data to a file to create audio object\n    with open(\"reddit.mp3\", \"wb\") as f:\n        tts_entry1.write_to_fp(f)\n        tts_entry2.write_to_fp(f)",
    "last_modified": "2025-05-04T23:27:53.407788"
  },
  {
    "id": "957",
    "name": "scriptwrapper.py",
    "path": "02_media_processing/audio_tools/scriptwrapper.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 168,
    "size": 5207,
    "docstring": "",
    "keywords": [],
    "functions": [
      "__init__",
      "__init__",
      "__init__",
      "addClipAtStart",
      "addScriptWrapper",
      "moveDown",
      "moveUp",
      "setupScriptMap",
      "keep",
      "skip"
    ],
    "classes": [
      "TwitchVideo",
      "DownloadedTwitchClipWrapper",
      "ScriptWrapper"
    ],
    "imports": [
      "datetime",
      "math",
      "os",
      "subprocess"
    ],
    "preview": "import datetime\nimport math\nimport os\nimport subprocess\n\ncurrent_path = os.path.dirname(os.path.realpath(__file__))\n\n\nclass TwitchVideo:\n    def __init__(self, scriptwrapper):\n        self.scriptWrapper = scriptwrapper\n        self.final_clips = None\n\n\nclass DownloadedTwitchClipWrapper:\n    def __init__(\n        self,\n        id,\n        author_name,\n        clip_title,",
    "last_modified": "2025-09-13T05:53:31.550137"
  },
  {
    "id": "958",
    "name": "voices.py",
    "path": "02_media_processing/audio_tools/voices.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 37,
    "size": 1296,
    "docstring": "",
    "keywords": [],
    "functions": [
      "save_text_to_mp3"
    ],
    "classes": [],
    "imports": [
      "pathlib",
      "gtts",
      "mutagen.mp3",
      "rich.progress",
      "utils.console"
    ],
    "preview": "from pathlib import Path\n\nfrom gtts import gTTS\nfrom mutagen.mp3 import MP3\nfrom rich.progress import track\n\nfrom utils.console import print_step, print_substep\n\n\ndef save_text_to_mp3(reddit_obj):\n    \"\"\"Saves Text to MP3 files.\n\n    Args:\n        reddit_obj : The reddit object you received from the reddit API in the askreddit.py file.\n    \"\"\"\n    print_step(\"Saving Text to MP3 files \ud83c\udfb6\")\n    length = 0\n\n    # Create a folder for the mp3 files.\n    Path(\"assets/mp3\").mkdir(parents=True, exist_ok=True)",
    "last_modified": "2025-09-11T13:27:05.489415"
  },
  {
    "id": "959",
    "name": "audio_test.py",
    "path": "02_media_processing/audio_tools/audio_test.py",
    "category": "02_media_processing",
    "type": "transcription",
    "lines": 6,
    "size": 191,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "clips"
    ],
    "preview": "from clips import *\n\nvideoclip = VideoFileClip(\"media/askreddit_submission_test0.mp4\")\nvideoclip.audio = gen_background_audio_clip(videoclip.duration)\nvideoclip.to_videofile(\"temp/loop.mp4\")\n",
    "last_modified": "2025-05-04T23:27:53.305626"
  },
  {
    "id": "960",
    "name": "visual_fx.py",
    "path": "02_media_processing/audio_tools/visual_fx.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 15,
    "size": 539,
    "docstring": "",
    "keywords": [],
    "functions": [
      "lut_filter"
    ],
    "classes": [],
    "imports": [
      "__future__"
    ],
    "preview": "from __future__ import annotations\n\n\ndef lut_filter(style: str) -> str:\n    s = (style or \"\").lower()\n    if s == \"neonpunk\":\n        return \"eq=contrast=1.2:saturation=1.45:brightness=0.02,curves=blue='0/0 0.5/0.45 1/1'\"\n    if s == \"retrovhs\":\n        return \"curves=m='0/0 0.4/0.35 1/1',noise=alls=10:allf=t,format=yuv420p\"\n    if s == \"comicbook\":\n        return \"edgedetect=low=0.1:high=0.2,unsharp=7:7:1.0,eq=saturation=1.6\"\n    if s == \"dreamwave\":\n        return \"eq=contrast=1.05:saturation=1.3:brightness=0.03\"\n    return \"null\"\n",
    "last_modified": "2025-09-11T13:24:00.303408"
  },
  {
    "id": "961",
    "name": "basics.py",
    "path": "02_media_processing/audio_tools/basics.py",
    "category": "02_media_processing",
    "type": "organization",
    "lines": 275,
    "size": 8866,
    "docstring": "MIT License\n\nCopyright (c) 2021-2022 MShawon\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "create_proxy_folder",
      "get_driver",
      "play_video",
      "play_music",
      "type_keyword",
      "scroll_search",
      "search_video"
    ],
    "classes": [],
    "imports": [
      "os",
      "glob",
      "features"
    ],
    "preview": "\"\"\"\nMIT License\n\nCopyright (c) 2021-2022 MShawon\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,",
    "last_modified": "2025-09-13T05:54:11.568125"
  },
  {
    "id": "962",
    "name": "tiktok.py",
    "path": "02_media_processing/audio_tools/tiktok.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 168,
    "size": 5653,
    "docstring": "",
    "keywords": [
      "web_tools"
    ],
    "functions": [
      "__init__",
      "run",
      "get_voices",
      "random_voice",
      "__init__",
      "__str__"
    ],
    "classes": [
      "TikTok",
      "TikTokTTSException"
    ],
    "imports": [
      "base64",
      "random",
      "time",
      "typing",
      "requests",
      "utils"
    ],
    "preview": "# documentation for tiktok api: https://github.com/oscie57/tiktok-voice/wiki\nimport base64\nimport random\nimport time\nfrom typing import Final, Optional\n\nimport requests\n\nfrom utils import settings\n\n__all__ = [\"TikTok\", \"TikTokTTSException\"]\n\ndisney_voices: Final[tuple] = (\n    \"en_us_ghostface\",  # Ghost Face\n    \"en_us_chewbacca\",  # Chewbacca\n    \"en_us_c3po\",  # C3PO\n    \"en_us_stitch\",  # Stitch\n    \"en_us_stormtrooper\",  # Stormtrooper\n    \"en_us_rocket\",  # Rocket\n    \"en_female_madam_leota\",  # Madame Leota",
    "last_modified": "2025-09-13T05:53:59.570995"
  },
  {
    "id": "963",
    "name": "audiodownload.py",
    "path": "02_media_processing/audio_tools/audiodownload.py",
    "category": "02_media_processing",
    "type": "transcription",
    "lines": 64,
    "size": 2183,
    "docstring": "This is used for downloading a youtube video as mp3",
    "keywords": [
      "youtube"
    ],
    "functions": [
      "__init__",
      "__my_hook__",
      "download"
    ],
    "classes": [
      "AudioDownload"
    ],
    "imports": [
      "__future__",
      "logging",
      "youtube_dl",
      "ytdl.models",
      "ytdl.oshelper"
    ],
    "preview": "\"This is used for downloading a youtube video as mp3\"\n\nfrom __future__ import unicode_literals\n\nimport logging\n\nfrom youtube_dl import DownloadError, YoutubeDL\nfrom ytdl.models import DownloadResult\nfrom ytdl.oshelper import dirname, join_paths, try_create_lock_file, try_delete_lock_file\n\n\nclass AudioDownload(object):\n    \"This is used for downloading a youtube video as mp3\"\n\n    def __init__(self, config):\n        self.download_folder = config.download_folder\n        self.downloaded_to_folder = \"\"\n        self.logger = logging.getLogger(__name__)\n\n    def __my_hook__(self, hook):",
    "last_modified": "2025-09-13T05:54:14.869695"
  },
  {
    "id": "964",
    "name": "webm-to-mp4.py",
    "path": "02_media_processing/audio_tools/webm-to-mp4.py",
    "category": "02_media_processing",
    "type": "video_processing",
    "lines": 44,
    "size": 1443,
    "docstring": "",
    "keywords": [],
    "functions": [
      "convert_webm_to_mp4"
    ],
    "classes": [],
    "imports": [
      "os",
      "subprocess"
    ],
    "preview": "import os\nimport subprocess\n\n\ndef convert_webm_to_mp4(directory_path):\n    \"\"\"\n    Converts all .webm files in the specified directory to .mp4 format.\n\n    Args:\n    directory_path (str): The path to the directory containing .webm files.\n    \"\"\"\n    for filename in os.listdir(directory_path):\n        if filename.endswith(\".webm\"):\n            # Construct the full path to the source .webm file\n            webm_path = os.path.join(directory_path, filename)\n            # Construct the full path for the output .mp4 file\n            mp4_path = os.path.join(directory_path, filename[:-5] + \".mp4\")\n\n            # Construct the ffmpeg command for converting .webm to .mp4\n            command = [",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "965",
    "name": "speech.py",
    "path": "02_media_processing/audio_tools/speech.py",
    "category": "02_media_processing",
    "type": "transcription",
    "lines": 52,
    "size": 1562,
    "docstring": "",
    "keywords": [],
    "functions": [
      "extract_text_from_audio"
    ],
    "classes": [],
    "imports": [
      "glob",
      "os",
      "moviepy.editor",
      "speech_recognition"
    ],
    "preview": "import glob\nimport os\n\nimport moviepy.editor as mp\nimport speech_recognition as sr\n\n# Prompt for the movie directory path\nmovie_directory = input(\"Enter the path to the directory containing your movies: \")\n\n# Prompt for the output .txt file name\noutput_file_name = input(\"Enter the name for the output .txt file (without extension): \")\n\n# Add the .txt file extension\noutput_txt_file = output_file_name + \".txt\"\n\n# Initialize the recognizer\nrecognizer = sr.Recognizer()\n\n\n# Function to extract text from audio",
    "last_modified": "2025-09-13T05:54:14.333067"
  },
  {
    "id": "966",
    "name": "config_20241213005714.py",
    "path": "02_media_processing/audio_tools/config_20241213005714.py",
    "category": "02_media_processing",
    "type": "setup",
    "lines": 3,
    "size": 71,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [],
    "preview": "# config.py\nSOURCE_DIRECTORY = \"/Users/steven/Music/NocTurnE-meLoDieS\"\n",
    "last_modified": "2024-12-13T00:57:14.700472"
  },
  {
    "id": "967",
    "name": "yt_auto_main.py",
    "path": "02_media_processing/audio_tools/yt_auto_main.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 188,
    "size": 7985,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "_news",
      "generate_video",
      "check_and_add_topic",
      "__init__",
      "get_logger"
    ],
    "classes": [
      "MultiLogger"
    ],
    "imports": [
      "json",
      "logging",
      "os",
      "re",
      "processing.extract_topic",
      "utilities.const",
      "utilities.create_directories",
      "video.create_vd",
      "video.subtitle"
    ],
    "preview": "import json\nimport logging\nimport os\nimport re\n\nfrom processing.extract_topic import ExtractNews\nfrom utilities.const import (\n    EXISTING_TOPICS,\n    LOG_PATH,\n    NEWS_API_KEY,\n    OUTPUT_FINAL_INFO,\n    OUTPUT_FINAL_VIDEO,\n    OUTPUT_TMP,\n    STOCK_VIDEO_FOLDER,\n    get_current_date,\n)\nfrom utilities.create_directories import create_directories\nfrom video.create_vd import VideoProcessor\nfrom video.subtitle import AddAudio, VideoTextOverlay\n",
    "last_modified": "2025-09-13T05:53:28.956420"
  },
  {
    "id": "968",
    "name": "song-process2.py",
    "path": "02_media_processing/audio_tools/song-process2.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 106,
    "size": 4054,
    "docstring": "",
    "keywords": [
      "organization"
    ],
    "functions": [
      "organize_files"
    ],
    "classes": [],
    "imports": [
      "os",
      "shutil"
    ],
    "preview": "import os\nimport shutil\n\n# Define the base directory\nbase_dir = \"/Users/steven/Movies/2025/mp4\"\n\n\ndef organize_files():\n    # Check if the base directory exists\n    if not os.path.exists(base_dir):\n        print(f\"\u274c Error: The directory '{base_dir}' does not exist.\")\n        return\n\n    # List all files in the base directory\n    try:\n        files = os.listdir(base_dir)\n    except Exception as e:\n        print(f\"\u274c Error accessing directory '{base_dir}': {e}\")\n        return\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "969",
    "name": "video.py",
    "path": "02_media_processing/audio_tools/video.py",
    "category": "02_media_processing",
    "type": "youtube",
    "lines": 304,
    "size": 8225,
    "docstring": "",
    "keywords": [],
    "functions": [
      "make_video",
      "get_clip_paths",
      "add_clip",
      "render"
    ],
    "classes": [],
    "imports": [
      "os",
      "glob",
      "json",
      "pathlib",
      "moviepy.editor",
      "opplast",
      "opplast",
      "twitchtube",
      "clips",
      "config"
    ],
    "preview": "import os\nfrom glob import glob\nfrom json import dump\nfrom pathlib import Path\n\nfrom moviepy.editor import VideoFileClip, concatenate_videoclips\nfrom opplast import Upload\nfrom opplast import __version__ as opplast_version\n\nfrom twitchtube import __version__ as twitchtube_version\n\nfrom .clips import download_clips, get_clips\nfrom .config import *\nfrom .exceptions import *\nfrom .logging import Log as log\nfrom .utils import *\n\n\n# add language as param\ndef make_video(",
    "last_modified": "2025-09-13T05:53:56.433409"
  },
  {
    "id": "970",
    "name": "streamlabs_polly.py",
    "path": "02_media_processing/audio_tools/streamlabs_polly.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 65,
    "size": 1893,
    "docstring": "",
    "keywords": [
      "web_tools"
    ],
    "functions": [
      "__init__",
      "run",
      "randomvoice"
    ],
    "classes": [
      "StreamlabsPolly"
    ],
    "imports": [
      "random",
      "requests",
      "requests.exceptions",
      "utils",
      "utils.voice"
    ],
    "preview": "import random\n\nimport requests\nfrom requests.exceptions import JSONDecodeError\n\nfrom utils import settings\nfrom utils.voice import check_ratelimit\n\nvoices = [\n    \"Brian\",\n    \"Emma\",\n    \"Russell\",\n    \"Joey\",\n    \"Matthew\",\n    \"Joanna\",\n    \"Kimberly\",\n    \"Amy\",\n    \"Geraint\",\n    \"Nicole\",\n    \"Justin\",",
    "last_modified": "2025-09-13T05:53:59.777684"
  },
  {
    "id": "971",
    "name": "up-down-mp4.py",
    "path": "02_media_processing/audio_tools/up-down-mp4.py",
    "category": "02_media_processing",
    "type": "video_processing",
    "lines": 41,
    "size": 1275,
    "docstring": "",
    "keywords": [],
    "functions": [
      "downscale_video",
      "main"
    ],
    "classes": [],
    "imports": [
      "os",
      "moviepy.editor"
    ],
    "preview": "import os\n\nfrom moviepy.editor import VideoFileClip\n\n# Basic target size reduction (in MB)\nTARGET_SIZE_MB = 500  # Feel free to adjust based on how much you want to reduce\n\n\ndef downscale_video(input_path, output_path):\n    clip = VideoFileClip(input_path)\n\n    # Reduce resolution to 720p (if it's HD)\n    target_resolution = (1280, 720)\n    print(f\"Resizing to {target_resolution}...\")\n\n    try:\n        # Resize the video and set a moderate bitrate\n        clip_resized = clip.resize(height=target_resolution[1])\n        clip_resized.write_videofile(\n            output_path, codec=\"libx264\", bitrate=\"1500k\"",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "972",
    "name": "RedditScrape.py",
    "path": "02_media_processing/audio_tools/RedditScrape.py",
    "category": "02_media_processing",
    "type": "web_tools",
    "lines": 86,
    "size": 3142,
    "docstring": "",
    "keywords": [
      "web_tools"
    ],
    "functions": [
      "__init__",
      "scrape_post"
    ],
    "classes": [
      "RedditScrape"
    ],
    "imports": [
      "sys",
      "gtts.tokenizer.symbols",
      "praw",
      "gtts.tokenizer",
      "config"
    ],
    "preview": "# RedditScrape.py\n# Last edited: June 28th 2021\n#\n# Called from run.py\n# Given a URL and an argument for number of posts to scrape, Class will connect to reddit API\n# and scrape the content from the post returning a title, authors, and replies\n#\n\n\nimport sys\n\nimport gtts.tokenizer.symbols\n\n# including the reddit api wrapper\nimport praw\n\n# Pre process can let us exchange words. ie: exchanging curse words for others\nfrom gtts.tokenizer import pre_processors\n\n# importing the config.py file to connect to PRAW",
    "last_modified": "2025-09-13T05:53:51.680878"
  },
  {
    "id": "973",
    "name": "config_20241213005701.py",
    "path": "02_media_processing/audio_tools/config_20241213005701.py",
    "category": "02_media_processing",
    "type": "setup",
    "lines": 3,
    "size": 72,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [],
    "preview": "# config.py\nSOURCE_DIRECTORY = \"/Users/steven/Music/NocTurnE-meLoDieSs\"\n",
    "last_modified": "2024-12-13T00:57:01.721926"
  },
  {
    "id": "974",
    "name": "clean-album.py",
    "path": "02_media_processing/audio_tools/clean-album.py",
    "category": "02_media_processing",
    "type": "organization",
    "lines": 104,
    "size": 3421,
    "docstring": "",
    "keywords": [
      "organization"
    ],
    "functions": [
      "hash_file",
      "flatten_directory",
      "remove_duplicates",
      "organize_by_type",
      "clean_directory"
    ],
    "classes": [],
    "imports": [
      "hashlib",
      "os",
      "shutil"
    ],
    "preview": "import hashlib\nimport os\nimport shutil\n\n# Define the base directory\nbase_dir = \"/Users/steven/Music/nocTurneMeLoDieS/mp4\"\n\n\ndef hash_file(file_path):\n    \"\"\"Generate a hash for a file to identify duplicates.\"\"\"\n    hasher = hashlib.md5()\n    with open(file_path, \"rb\") as f:\n        while chunk := f.read(8192):\n            hasher.update(chunk)\n    return hasher.hexdigest()\n\n\ndef flatten_directory(directory):\n    \"\"\"Move all files from nested folders to the base directory.\"\"\"\n    for root, _, files in os.walk(directory, topdown=False):",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "975",
    "name": "organize_albums.py_02.py",
    "path": "02_media_processing/organize_albums.py_consolidated/organize_albums.py_02.py",
    "category": "02_media_processing",
    "type": "organization",
    "lines": 62,
    "size": 2355,
    "docstring": "",
    "keywords": [
      "organization"
    ],
    "functions": [
      "organize_files"
    ],
    "classes": [],
    "imports": [
      "os",
      "shutil"
    ],
    "preview": "import os\nimport shutil\n\n# Define the base directory\nbase_dir = \"/Users/steven/Movies/2025/mp4\"\n\n# Helper function to create folders and move files\ndef organize_files():\n    # List all files in the base directory\n    files = os.listdir(base_dir)\n\n    # Process each file\n    for file in files:\n        # Skip directories\n        if os.path.isdir(os.path.join(base_dir, file)):\n            continue\n\n        # Extract the base name (album name) from the file\n        if file.endswith(\".mp4\"):\n            album_name = file.replace(\".mp4\", \"\")",
    "last_modified": "2025-02-02T12:46:54.971548"
  },
  {
    "id": "976",
    "name": "organize_albums.py.py",
    "path": "02_media_processing/organize_albums.py_consolidated/organize_albums.py.py",
    "category": "02_media_processing",
    "type": "organization",
    "lines": 62,
    "size": 2470,
    "docstring": "",
    "keywords": [
      "organization"
    ],
    "functions": [
      "organize_files"
    ],
    "classes": [],
    "imports": [
      "os",
      "shutil"
    ],
    "preview": "import os\nimport shutil\n\n# Define the base directory\nbase_dir = \"/Users/steven/Music/NocTurnE-meLoDieS/albums/\"\n\n\n# Helper function to create folders and move files\ndef organize_files():\n    # List all files in the base directory\n    files = os.listdir(base_dir)\n\n    # Process each file\n    for file in files:\n        # Skip directories\n        if os.path.isdir(os.path.join(base_dir, file)):\n            continue\n\n        # Extract the base name (album name) from the file\n        if file.endswith(\".mp3\"):",
    "last_modified": "2025-05-04T22:47:11.429396"
  },
  {
    "id": "977",
    "name": "cleanups.py",
    "path": "02_media_processing/video_tools/cleanups.py",
    "category": "02_media_processing",
    "type": "organization",
    "lines": 56,
    "size": 1958,
    "docstring": "",
    "keywords": [],
    "functions": [
      "list_venv_directories",
      "main"
    ],
    "classes": [],
    "imports": [
      "os",
      "subprocess"
    ],
    "preview": "import os\nimport subprocess\n\n# Define the versions you want to keep\nrequired_versions = [\"3.10\", \"3.12.4\"]\n\n\ndef list_venv_directories(base_path):\n    \"\"\"List all virtual environment directories.\"\"\"\n    venv_dirs = []\n    for root, dirs, files in os.walk(base_path):\n        for dir in dirs:\n            if os.path.exists(os.path.join(root, dir, \"bin\", \"python\")):\n                venv_dirs.append(os.path.join(root, dir))\n    return venv_dirs\n\n\ndef main():\n    # Base path where your virtual environments are stored\n    venv_base_path = os.path.expanduser(\"~/venvs\")",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "978",
    "name": "sponsoredlinks.py",
    "path": "02_media_processing/video_tools/sponsoredlinks.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 258,
    "size": 8181,
    "docstring": "",
    "keywords": [],
    "functions": [
      "__init__",
      "__str__",
      "html",
      "__init__",
      "__init__",
      "num_results",
      "_get_results_per_page",
      "_set_results_par_page",
      "get_results",
      "_get_all_results_sleep_fn"
    ],
    "classes": [
      "SLError(Exception)",
      "SLParseError(Exception)",
      "SponsoredLink(object)",
      "SponsoredLinks(object)"
    ],
    "imports": [
      "random",
      "re",
      "urllib",
      "BeautifulSoup",
      "browser",
      "htmlentitydefs"
    ],
    "preview": "#!/usr/bin/python\n#\n# Peteris Krumins (peter@catonmat.net)\n# http://www.catonmat.net  --  good coders code, great reuse\n#\n# http://www.catonmat.net/blog/python-library-for-google-sponsored-links-search/\n#\n# Code is licensed under MIT license.\n#\n\nimport random\nimport re\nimport urllib\n\nfrom BeautifulSoup import BeautifulSoup\nfrom browser import Browser, BrowserError\nfrom htmlentitydefs import name2codepoint\n\n#\n# TODO: join GoogleSearch and SponsoredLinks classes under a single base class",
    "last_modified": "2025-05-04T23:28:20.721929"
  },
  {
    "id": "979",
    "name": "serialize.py",
    "path": "02_media_processing/video_tools/serialize.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 203,
    "size": 7129,
    "docstring": "",
    "keywords": [
      "web_tools"
    ],
    "functions": [
      "dumps",
      "serialize",
      "loads",
      "prepare_response",
      "_loads_v0",
      "_loads_v1",
      "_loads_v2",
      "_loads_v3",
      "_loads_v4"
    ],
    "classes": [
      "Serializer"
    ],
    "imports": [
      "__future__",
      "io",
      "typing",
      "pip._vendor",
      "pip._vendor.requests.structures",
      "pip._vendor.urllib3",
      "pip._vendor.requests"
    ],
    "preview": "# SPDX-FileCopyrightText: 2015 Eric Larson\n#\n# SPDX-License-Identifier: Apache-2.0\nfrom __future__ import annotations\n\nimport io\nfrom typing import IO, TYPE_CHECKING, Any, Mapping, cast\n\nfrom pip._vendor import msgpack\nfrom pip._vendor.requests.structures import CaseInsensitiveDict\nfrom pip._vendor.urllib3 import HTTPResponse\n\nif TYPE_CHECKING:\n    from pip._vendor.requests import PreparedRequest\n\n\nclass Serializer:\n    serde_version = \"4\"\n\n    def dumps(",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "980",
    "name": "gback.py",
    "path": "02_media_processing/video_tools/gback.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 32,
    "size": 985,
    "docstring": "",
    "keywords": [],
    "functions": [
      "move_files_back"
    ],
    "classes": [],
    "imports": [
      "json",
      "os",
      "shutil"
    ],
    "preview": "import json\nimport os\nimport shutil\n\n# Paths configuration\nbackup_file_path = \"/Volumes/iMac/15days/file_order_backup.json\"\n\n\ndef move_files_back(backup_path):\n    \"\"\"Move files back to their original locations.\"\"\"\n    # Load the backup file to get the file metadata\n    with open(backup_path, \"r\") as backup_file:\n        files_metadata = json.load(backup_file)\n\n    for file in files_metadata:\n        original_path = file[\"original_path\"]\n        destination_path = file[\"destination_path\"]\n\n        # Ensure the original directory exists\n        if not os.path.exists(os.path.dirname(original_path)):",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "981",
    "name": "resultdict.py",
    "path": "02_media_processing/video_tools/resultdict.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 17,
    "size": 402,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [
      "ResultDict"
    ],
    "imports": [
      "typing",
      "typing"
    ],
    "preview": "from typing import TYPE_CHECKING, Optional\n\nif TYPE_CHECKING:\n    # TypedDict was introduced in Python 3.8.\n    #\n    # TODO: Remove the else block and TYPE_CHECKING check when dropping support\n    # for Python 3.7.\n    from typing import TypedDict\n\n    class ResultDict(TypedDict):\n        encoding: Optional[str]\n        confidence: float\n        language: Optional[str]\n\nelse:\n    ResultDict = dict\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "982",
    "name": "my_copy_utils.py",
    "path": "02_media_processing/video_tools/my_copy_utils.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 39,
    "size": 1386,
    "docstring": "",
    "keywords": [],
    "functions": [
      "ensure_dir"
    ],
    "classes": [],
    "imports": [
      "csv",
      "os",
      "shutil"
    ],
    "preview": "import csv\nimport os\nimport shutil\n\n# Paths configuration\ncsv_file_path = \"/Users/steven/15days.csv\"\ndestination_root = \"/Volumes/iMac/15days\"\nbackup_root = os.path.join(destination_root, \"backup\")\n\n\ndef ensure_dir(directory):\n    \"\"\"Ensure the directory exists. If not, create it.\"\"\"\n    if not os.path.exists(directory):\n        os.makedirs(directory)\n\n\nwith open(csv_file_path, newline=\"\") as csvfile:\n    filereader = csv.reader(csvfile)\n    for row in filereader:\n        # Assuming each row has one column with the file path",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "983",
    "name": "static_test.py",
    "path": "02_media_processing/video_tools/static_test.py",
    "category": "02_media_processing",
    "type": "testing",
    "lines": 9,
    "size": 158,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "clips"
    ],
    "preview": "from clips import *\n\nclips = []\n\nfor _ in range(0, 3):\n    clips.append(gen_transition_clip())\n\nconcatenate_videoclips(clips).to_videofile(\"temp/static.mp4\")\n",
    "last_modified": "2025-05-04T23:27:53.352828"
  },
  {
    "id": "984",
    "name": "undo-move-csv.py",
    "path": "02_media_processing/video_tools/undo-move-csv.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 32,
    "size": 985,
    "docstring": "",
    "keywords": [],
    "functions": [
      "move_files_back"
    ],
    "classes": [],
    "imports": [
      "json",
      "os",
      "shutil"
    ],
    "preview": "import json\nimport os\nimport shutil\n\n# Paths configuration\nbackup_file_path = \"/Volumes/iMac/15days/file_order_backup.json\"\n\n\ndef move_files_back(backup_path):\n    \"\"\"Move files back to their original locations.\"\"\"\n    # Load the backup file to get the file metadata\n    with open(backup_path, \"r\") as backup_file:\n        files_metadata = json.load(backup_file)\n\n    for file in files_metadata:\n        original_path = file[\"original_path\"]\n        destination_path = file[\"destination_path\"]\n\n        # Ensure the original directory exists\n        if not os.path.exists(os.path.dirname(original_path)):",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "985",
    "name": "config_20241213005652.py",
    "path": "02_media_processing/video_tools/config_20241213005652.py",
    "category": "02_media_processing",
    "type": "setup",
    "lines": 4,
    "size": 99,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [],
    "preview": "# config.py\nSOURCE_DIRECTORY = \"        r\".*\\/Movies\\/CapCut\\/.*\",  # Specific capcut directory\ns\"\n",
    "last_modified": "2024-12-13T00:56:52.227762"
  },
  {
    "id": "986",
    "name": "get_external_link.py",
    "path": "02_media_processing/video_tools/get_external_link.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 123,
    "size": 4133,
    "docstring": "",
    "keywords": [
      "web_tools"
    ],
    "functions": [],
    "classes": [],
    "imports": [
      "logging",
      "os",
      "subprocess",
      "time",
      "datetime",
      "requests",
      "pyrogram",
      "translation",
      "helper_funcs.chat_base",
      "helper_funcs.display_progress"
    ],
    "preview": "import logging\n\nlogging.basicConfig(\n    level=logging.DEBUG, format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\n)\nlogger = logging.getLogger(__name__)\n\nimport os\nimport subprocess\nimport time\nfrom datetime import datetime\n\nimport requests\n\n# the secret configuration specific things\nif bool(os.environ.get(\"WEBHOOK\", False)):\n    from sample_config import Config\nelse:\n    from config import Config\n",
    "last_modified": "2025-09-13T05:53:44.214082"
  },
  {
    "id": "987",
    "name": "NewUpload_20250607131239.py",
    "path": "02_media_processing/video_tools/NewUpload_20250607131239.py",
    "category": "02_media_processing",
    "type": "youtube",
    "lines": 78,
    "size": 3077,
    "docstring": "",
    "keywords": [
      "youtube",
      "web_tools"
    ],
    "functions": [
      "get_authenticated_service",
      "upload_video"
    ],
    "classes": [],
    "imports": [
      "os",
      "google.auth.transport.requests",
      "google_auth_oauthlib.flow",
      "googleapiclient.discovery",
      "googleapiclient.errors"
    ],
    "preview": "import os\n\nimport google.auth.transport.requests\nimport google_auth_oauthlib.flow\nimport googleapiclient.discovery\nimport googleapiclient.errors\n\nSCOPES = [\"https://www.googleapis.com/auth/youtube.upload\"]\nCLIENT_SECRETS_FILE = \"/Users/steven/Documents/python/Youtube/client_secrets.json\"  # Replace with your client secrets file\nAPI_SERVICE_NAME = \"youtube\"\nAPI_VERSION = \"v3\"\n\n\ndef get_authenticated_service():\n    \"\"\"Authenticates and returns the YouTube Data API service.\"\"\"\n    flow = google_auth_oauthlib.flow.InstalledAppFlow.from_client_secrets_file(\n        CLIENT_SECRETS_FILE, SCOPES\n    )\n    credentials = flow.run_local_server(port=0)\n    return googleapiclient.discovery.build(API_SERVICE_NAME, API_VERSION, credentials=credentials)",
    "last_modified": "2025-09-13T05:54:12.178653"
  },
  {
    "id": "988",
    "name": "help_uploadbot.py",
    "path": "02_media_processing/video_tools/help_uploadbot.py",
    "category": "02_media_processing",
    "type": "youtube",
    "lines": 47,
    "size": 1533,
    "docstring": "",
    "keywords": [
      "web_tools"
    ],
    "functions": [
      "DetectFileSize",
      "DownLoadFile"
    ],
    "classes": [],
    "imports": [
      "logging",
      "os",
      "requests"
    ],
    "preview": "import logging\n\nlogging.basicConfig(\n    level=logging.DEBUG, format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\n)\nlogger = logging.getLogger(__name__)\n\nimport os\n\nimport requests\n\n\ndef DetectFileSize(url):\n    r = requests.get(url, allow_redirects=True, stream=True)\n    total_size = int(r.headers.get(\"content-length\", 0))\n    return total_size\n\n\ndef DownLoadFile(url, file_name, chunk_size, client, ud_type, message_id, chat_id):\n    if os.path.exists(file_name):",
    "last_modified": "2025-05-06T04:35:15.017768"
  },
  {
    "id": "989",
    "name": "NewUpload_20250607125012.py",
    "path": "02_media_processing/video_tools/NewUpload_20250607125012.py",
    "category": "02_media_processing",
    "type": "youtube",
    "lines": 78,
    "size": 2618,
    "docstring": "",
    "keywords": [
      "youtube",
      "web_tools"
    ],
    "functions": [
      "get_authenticated_service",
      "upload_video"
    ],
    "classes": [],
    "imports": [
      "os",
      "google.auth.transport.requests",
      "google_auth_oauthlib.flow",
      "googleapiclient.discovery",
      "googleapiclient.errors"
    ],
    "preview": "import os\n\nimport google.auth.transport.requests\nimport google_auth_oauthlib.flow\nimport googleapiclient.discovery\nimport googleapiclient.errors\n\nSCOPES = [\"https://www.googleapis.com/auth/youtube.upload\"]\nCLIENT_SECRETS_FILE = \"/Users/steven/Documents/python/Youtube/client_secrets.json\"  # Replace with your client secrets file\nAPI_SERVICE_NAME = \"youtube\"\nAPI_VERSION = \"v3\"\n\n\ndef get_authenticated_service():\n    \"\"\"Authenticates and returns the YouTube Data API service.\"\"\"\n    flow = google_auth_oauthlib.flow.InstalledAppFlow.from_client_secrets_file(\n        CLIENT_SECRETS_FILE, SCOPES\n    )\n    credentials = flow.run_local_server(port=0)\n    return googleapiclient.discovery.build(API_SERVICE_NAME, API_VERSION, credentials=credentials)",
    "last_modified": "2025-09-13T05:54:11.820695"
  },
  {
    "id": "990",
    "name": "sample_config.py",
    "path": "02_media_processing/video_tools/sample_config.py",
    "category": "02_media_processing",
    "type": "setup",
    "lines": 50,
    "size": 1954,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [
      "Config"
    ],
    "imports": [
      "os"
    ],
    "preview": "import os\n\n\nclass Config(object):\n    # get a token from https://chatbase.com\n    CHAT_BASE_TOKEN = os.environ.get(\"CHAT_BASE_TOKEN\", \"\")\n    # get a token from @BotFather\n    TG_BOT_TOKEN = os.environ.get(\"TG_BOT_TOKEN\", \"\")\n    # The Telegram API things\n    APP_ID = int(os.environ.get(\"APP_ID\", 12345))\n    API_HASH = os.environ.get(\"API_HASH\")\n    # Get these values from my.telegram.org\n    # Array to store users who are authorized to use the bot\n    AUTH_USERS = set(str(x) for x in os.environ.get(\"AUTH_USERS\", \"\").split())\n    # reg: Procedures\n    UTUBE_BOT_USERS = AUTH_USERS\n    SUPER_DLBOT_USERS = AUTH_USERS\n    SUPER3X_DLBOT_USERS = AUTH_USERS\n    SUPER7X_DLBOT_USERS = AUTH_USERS\n    BANNED_USERS = []",
    "last_modified": "2025-09-13T05:53:44.774704"
  },
  {
    "id": "991",
    "name": "version.py",
    "path": "02_media_processing/video_tools/version.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 22,
    "size": 827,
    "docstring": "",
    "keywords": [
      "web_tools"
    ],
    "functions": [
      "checkversion"
    ],
    "classes": [],
    "imports": [
      "requests",
      "utils.console"
    ],
    "preview": "import requests\n\nfrom utils.console import print_step\n\n\ndef checkversion(__VERSION__: str):\n    response = requests.get(\n        \"https://api.github.com/repos/elebumm/RedditVideoMakerBot/releases/latest\"\n    )\n    latestversion = response.json()[\"tag_name\"]\n    if __VERSION__ == latestversion:\n        print_step(f\"You are using the newest version ({__VERSION__}) of the bot\")\n        return True\n    elif __VERSION__ < latestversion:\n        print_step(\n            f\"You are using an older version ({__VERSION__}) of the bot. Download the newest version ({latestversion}) from https://github.com/elebumm/RedditVideoMakerBot/releases/latest\"\n        )\n    else:\n        print_step(\n            f\"Welcome to the test version ({__VERSION__}) of the bot. Thanks for testing and feel free to report any bugs you find.\"",
    "last_modified": "2025-09-11T13:27:01.773005"
  },
  {
    "id": "992",
    "name": "youtube.py",
    "path": "02_media_processing/video_tools/youtube.py",
    "category": "02_media_processing",
    "type": "youtube",
    "lines": 37,
    "size": 1066,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "googleapiclient.discovery",
      "csv"
    ],
    "preview": "from googleapiclient.discovery import build\n\n# Set up YouTube Data API\nyoutube = build(\"youtube\", \"v3\", developerKey=\"AIzaSyC08MXHwy-tkAwAhvW0TumdKJmSfOJYFqw\")\n\n# Retrieve channel's videos\nvideos = []\nnext_page_token = None\nwhile True:\n    request = youtube.search().list(\n        part=\"snippet\",\n        channelId=\"UCDl7VmS3gD2BQBVZUlL21-A\",\n        maxResults=50,  # Max allowed value\n        pageToken=next_page_token,\n    )\n    response = request.execute()\n    videos.extend(response[\"items\"])\n    next_page_token = response.get(\"nextPageToken\")\n    if not next_page_token:\n        break",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "993",
    "name": "NewUpload_20250607130948.py",
    "path": "02_media_processing/video_tools/NewUpload_20250607130948.py",
    "category": "02_media_processing",
    "type": "youtube",
    "lines": 95,
    "size": 4012,
    "docstring": "",
    "keywords": [
      "video_processing",
      "youtube",
      "web_tools"
    ],
    "functions": [
      "get_authenticated_service",
      "upload_video"
    ],
    "classes": [],
    "imports": [
      "os",
      "google.auth.transport.requests",
      "google_auth_oauthlib.flow",
      "googleapiclient.discovery",
      "googleapiclient.errors"
    ],
    "preview": "import os\n\nimport google.auth.transport.requests\nimport google_auth_oauthlib.flow\nimport googleapiclient.discovery\nimport googleapiclient.errors\n\nSCOPES = [\"https://www.googleapis.com/auth/youtube.upload\"]\nCLIENT_SECRETS_FILE = \"/Users/steven/Documents/python/Youtube/client_secrets.json\"  # Replace with your client secrets file\nAPI_SERVICE_NAME = \"youtube\"\nAPI_VERSION = \"v3\"\n\ndef get_authenticated_service():\n    \"\"\"Authenticates and returns the YouTube Data API service.\"\"\"\n    flow = google_auth_oauthlib.flow.InstalledAppFlow.from_client_secrets_file(\n        CLIENT_SECRETS_FILE, SCOPES)\n    credentials = flow.run_local_server(port=0)\n    return googleapiclient.discovery.build(API_SERVICE_NAME, API_VERSION,\n                                            credentials=credentials)\n",
    "last_modified": "2025-09-06T12:24:11.130048"
  },
  {
    "id": "994",
    "name": "autoscaling_sagemaker_endpoint.py",
    "path": "02_media_processing/video_tools/autoscaling_sagemaker_endpoint.py",
    "category": "02_media_processing",
    "type": "setup",
    "lines": 176,
    "size": 7897,
    "docstring": "In Amazon SageMaker and other AWS services, Application Auto Scaling allows you to automatically scale resources in and out based on configurable policies. Within this context, registering a scalable target and creating a scalable policy are two critical steps that work together to enable this functionality. Here's a breakdown of each and how they differ:\nRegister Scalable Target\n\nWhen you register a scalable target with Application Auto Scaling, you are essentially telling AWS which resource you want to scale and defining the minimum and maximum capacity limits for that resource. This step does not define how the scaling should occur; rather, it sets up the parameters within which scaling can happen. In your example with SageMaker:\n\n    Resource ID: This is a unique identifier for the scalable target. For SageMaker inference components, it typically includes the inference component name.\n    Service Namespace: This indicates the AWS service where the resource resides, which is \"sagemaker\" in this case.\n    Scalable Dimension: This specifies the aspect of the resource you want to scale. For SageMaker inference components, this is often the desired number of copies (instances) of an inference component.\n    MinCapacity and MaxCapacity: These values define the minimum and maximum number of copies that the auto scaling can adjust to.\n\nBy registering a scalable target, you prepare your SageMaker inference component for scaling but do not specify when or how the scaling should occur.\nScalable Policy\n\nCreating a scalable policy is where you define the specific criteria and rules for scaling. This policy uses metrics and thresholds to automatically adjust the resource's capacity within the limits set by the registered scalable target. In your SageMaker example:\n\n    Policy Type: You've chosen \"TargetTrackingScaling,\" which adjusts the scalable target's capacity as required to maintain a target value for a specific metric.\n    Target Tracking Configuration: This includes the metric to track (e.g., SageMakerInferenceComponentInvocationsPerCopy), the target value for that metric, and cooldown periods for scaling in and out. The policy uses these parameters to decide when to scale the resources up or down.\n\nThe scalable policy is what actively manages the scaling process. It monitors the specified metric and, based on its value relative to the target value, triggers scaling actions to increase or decrease the number of copies of the inference component within the bounds set by the registered scalable target.",
    "keywords": [
      "organization"
    ],
    "functions": [
      "register_scalable_target",
      "put_scaling_policy",
      "describe_scalable_targets",
      "describe_scaling_policies",
      "delete_scaling_policy",
      "deregister_scalable_target",
      "apply_policy",
      "__init__",
      "apply_policy",
      "__init__"
    ],
    "classes": [
      "IAutoScalingClient",
      "ScalingPolicyStrategy",
      "TargetTrackingScalingPolicy",
      "ScalableTarget",
      "AutoscalingSagemakerEndpoint"
    ],
    "imports": [],
    "preview": "\"\"\"\nIn Amazon SageMaker and other AWS services, Application Auto Scaling allows you to automatically scale resources in and out based on configurable policies. Within this context, registering a scalable target and creating a scalable policy are two critical steps that work together to enable this functionality. Here's a breakdown of each and how they differ:\nRegister Scalable Target\n\nWhen you register a scalable target with Application Auto Scaling, you are essentially telling AWS which resource you want to scale and defining the minimum and maximum capacity limits for that resource. This step does not define how the scaling should occur; rather, it sets up the parameters within which scaling can happen. In your example with SageMaker:\n\n    Resource ID: This is a unique identifier for the scalable target. For SageMaker inference components, it typically includes the inference component name.\n    Service Namespace: This indicates the AWS service where the resource resides, which is \"sagemaker\" in this case.\n    Scalable Dimension: This specifies the aspect of the resource you want to scale. For SageMaker inference components, this is often the desired number of copies (instances) of an inference component.\n    MinCapacity and MaxCapacity: These values define the minimum and maximum number of copies that the auto scaling can adjust to.\n\nBy registering a scalable target, you prepare your SageMaker inference component for scaling but do not specify when or how the scaling should occur.\nScalable Policy\n\nCreating a scalable policy is where you define the specific criteria and rules for scaling. This policy uses metrics and thresholds to automatically adjust the resource's capacity within the limits set by the registered scalable target. In your SageMaker example:\n\n    Policy Type: You've chosen \"TargetTrackingScaling,\" which adjusts the scalable target's capacity as required to maintain a target value for a specific metric.\n    Target Tracking Configuration: This includes the metric to track (e.g., SageMakerInferenceComponentInvocationsPerCopy), the target value for that metric, and cooldown periods for scaling in and out. The policy uses these parameters to decide when to scale the resources up or down.\n\nThe scalable policy is what actively manages the scaling process. It monitors the specified metric and, based on its value relative to the target value, triggers scaling actions to increase or decrease the number of copies of the inference component within the bounds set by the registered scalable target.",
    "last_modified": "2025-05-06T04:35:14.984234"
  },
  {
    "id": "995",
    "name": "copy_images.py",
    "path": "02_media_processing/video_tools/copy_images.py",
    "category": "02_media_processing",
    "type": "image_processing",
    "lines": 39,
    "size": 1386,
    "docstring": "",
    "keywords": [],
    "functions": [
      "ensure_dir"
    ],
    "classes": [],
    "imports": [
      "csv",
      "os",
      "shutil"
    ],
    "preview": "import csv\nimport os\nimport shutil\n\n# Paths configuration\ncsv_file_path = \"/Users/steven/15days.csv\"\ndestination_root = \"/Volumes/iMac/15days\"\nbackup_root = os.path.join(destination_root, \"backup\")\n\n\ndef ensure_dir(directory):\n    \"\"\"Ensure the directory exists. If not, create it.\"\"\"\n    if not os.path.exists(directory):\n        os.makedirs(directory)\n\n\nwith open(csv_file_path, newline=\"\") as csvfile:\n    filereader = csv.reader(csvfile)\n    for row in filereader:\n        # Assuming each row has one column with the file path",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "996",
    "name": "index.py",
    "path": "02_media_processing/video_tools/index.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 135,
    "size": 4698,
    "docstring": "",
    "keywords": [],
    "functions": [
      "add_options",
      "run",
      "_build_package_finder",
      "get_available_package_versions"
    ],
    "classes": [
      "IndexCommand"
    ],
    "imports": [
      "logging",
      "optparse",
      "typing",
      "pip._internal.cli",
      "pip._internal.cli.req_command",
      "pip._internal.cli.status_codes",
      "pip._internal.commands.search",
      "pip._internal.exceptions",
      "pip._internal.index.collector",
      "pip._internal.index.package_finder"
    ],
    "preview": "import logging\nfrom optparse import Values\nfrom typing import Any, Iterable, List, Optional, Union\n\nfrom pip._internal.cli import cmdoptions\nfrom pip._internal.cli.req_command import IndexGroupCommand\nfrom pip._internal.cli.status_codes import ERROR, SUCCESS\nfrom pip._internal.commands.search import print_dist_installation_info\nfrom pip._internal.exceptions import CommandError, DistributionNotFound, PipError\nfrom pip._internal.index.collector import LinkCollector\nfrom pip._internal.index.package_finder import PackageFinder\nfrom pip._internal.models.selection_prefs import SelectionPreferences\nfrom pip._internal.models.target_python import TargetPython\nfrom pip._internal.network.session import PipSession\nfrom pip._internal.utils.misc import write_output\nfrom pip._vendor.packaging.version import LegacyVersion, Version\n\nlogger = logging.getLogger(__name__)\n\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "997",
    "name": "ytcsv.py",
    "path": "02_media_processing/video_tools/ytcsv.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 26,
    "size": 778,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "pytube"
    ],
    "preview": "from pytube import Playlist\n\n# Replace the playlist URL below with your desired playlist URL\nplaylist_url = \"https://www.youtube.com/playlist?list=PLfudK7D_bQIjRgAqVU_jkbnb1N5V_pNiG\"\n\n# Define the directory where you want to save the downloads\ndownload_directory = \"/Users/steven/Movies/ESO/qshorts\"\n\n# Create a Playlist object\nplaylist = Playlist(playlist_url)\n\n# Loop through the videos in the playlist\nfor video in playlist.videos:\n    # Download the video\n    video.streams.get_highest_resolution().download()\n\n    # Get video metadata\n    title = video.title\n    description = video.description\n    tags = video.keywords",
    "last_modified": "2025-09-13T05:53:55.665101"
  },
  {
    "id": "998",
    "name": "completion.py",
    "path": "02_media_processing/video_tools/completion.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 129,
    "size": 4257,
    "docstring": "",
    "keywords": [],
    "functions": [
      "add_options",
      "run"
    ],
    "classes": [
      "CompletionCommand"
    ],
    "imports": [
      "sys",
      "textwrap",
      "optparse",
      "typing",
      "pip._internal.cli.base_command",
      "pip._internal.cli.status_codes",
      "pip._internal.utils.misc"
    ],
    "preview": "import sys\nimport textwrap\nfrom optparse import Values\nfrom typing import List\n\nfrom pip._internal.cli.base_command import Command\nfrom pip._internal.cli.status_codes import SUCCESS\nfrom pip._internal.utils.misc import get_prog\n\nBASE_COMPLETION = \"\"\"\n# pip {shell} completion start{script}# pip {shell} completion end\n\"\"\"\n\nCOMPLETION_SCRIPTS = {\n    \"bash\": \"\"\"\n        _pip_completion()\n        {{\n            COMPREPLY=( $( COMP_WORDS=\"${{COMP_WORDS[*]}}\" \\\\\n                           COMP_CWORD=$COMP_CWORD \\\\\n                           PIP_AUTO_COMPLETE=1 $1 2>/dev/null ) )",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "999",
    "name": "youtube_analytics.py",
    "path": "02_media_processing/video_tools/youtube_analytics.py",
    "category": "02_media_processing",
    "type": "youtube",
    "lines": 150,
    "size": 4751,
    "docstring": "",
    "keywords": [
      "youtube",
      "web_tools"
    ],
    "functions": [
      "authenticate",
      "build_youtube_service",
      "get_video_details",
      "get_channel_videos",
      "save_to_csv",
      "download_channel_videos_to_csv"
    ],
    "classes": [],
    "imports": [
      "csv",
      "os",
      "pickle",
      "google.auth.transport.requests",
      "google_auth_oauthlib.flow",
      "googleapiclient.discovery"
    ],
    "preview": "import csv\nimport os\nimport pickle\n\nfrom google.auth.transport.requests import Request\nfrom google_auth_oauthlib.flow import InstalledAppFlow\nfrom googleapiclient.discovery import build\n\n# Path to your client_secret.json file\nCLIENT_SECRETS_FILE = (\n    \"/Users/steven/Documents/client_secret.json\"  # Update with your file path\n)\n\n# Scopes for the YouTube Data API v3\nSCOPES = [\"https://www.googleapis.com/auth/youtube.readonly\"]\n\n\n# Authenticate the user using OAuth 2.0\ndef authenticate():\n    credentials = None",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1000",
    "name": "botStories.py",
    "path": "02_media_processing/video_tools/botStories.py",
    "category": "02_media_processing",
    "type": "web_tools",
    "lines": 186,
    "size": 5037,
    "docstring": "Created in 12/2019\n@Author: Paulo https://github.com/alpdias",
    "keywords": [],
    "functions": [
      "functionStories",
      "botlogin",
      "stories"
    ],
    "classes": [],
    "imports": [
      "os",
      "random",
      "pathlib",
      "time",
      "art",
      "selenium",
      "selenium.webdriver.common.keys"
    ],
    "preview": "# -*- coding: utf-8 -*-\n\n\"\"\"\nCreated in 12/2019\n@Author: Paulo https://github.com/alpdias\n\"\"\"\n\n# imported libraries\nimport os\nimport random\nfrom pathlib import Path\nfrom time import sleep\n\nimport art\nfrom selenium import webdriver\nfrom selenium.webdriver.common.keys import Keys\n\n\ndef functionStories(mySystem):\n    \"\"\"",
    "last_modified": "2025-05-04T23:28:20.961006"
  },
  {
    "id": "1001",
    "name": "deprecation.py",
    "path": "02_media_processing/video_tools/deprecation.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 124,
    "size": 3706,
    "docstring": "A module that implements tooling to enable easy warnings about deprecations.",
    "keywords": [],
    "functions": [
      "_showwarning",
      "install_warning_logger",
      "deprecated"
    ],
    "classes": [
      "PipDeprecationWarning"
    ],
    "imports": [
      "logging",
      "warnings",
      "typing",
      "pip",
      "pip._vendor.packaging.version"
    ],
    "preview": "\"\"\"\nA module that implements tooling to enable easy warnings about deprecations.\n\"\"\"\n\nimport logging\nimport warnings\nfrom typing import Any, Optional, TextIO, Type, Union\n\nfrom pip import __version__ as current_version  # NOTE: tests patch this name.\nfrom pip._vendor.packaging.version import parse\n\nDEPRECATION_MSG_PREFIX = \"DEPRECATION: \"\n\n\nclass PipDeprecationWarning(Warning):\n    pass\n\n\n_original_showwarning: Any = None\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1002",
    "name": "youtube_download 2.py",
    "path": "02_media_processing/video_tools/youtube_download 2.py",
    "category": "02_media_processing",
    "type": "youtube",
    "lines": 53,
    "size": 1213,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "json",
      "os",
      "sys",
      "urllib",
      "datetime",
      "subprocess"
    ],
    "preview": "#!/usr/bin/python\n\"\"\"\nYouTube Channel Downloader\n\"\"\"\n \nimport json\nimport os\nimport sys\nimport urllib\nfrom datetime import date\nfrom subprocess import call\n\nprint \"--------------------------\"\nprint \"YouTube Channel Downloader\"\nprint \"--------------------------\"\nprint \"\"\nwhile True:\n    try:\n        author = raw_input(\"Enter username of YouTube channel: \")\n        break",
    "last_modified": "2025-08-06T14:24:26.086338"
  },
  {
    "id": "1003",
    "name": "yt-meta 2.py",
    "path": "02_media_processing/video_tools/yt-meta 2.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 44,
    "size": 1223,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "csv",
      "os",
      "googleapiclient.discovery"
    ],
    "preview": "import csv\nimport os\n\nfrom googleapiclient.discovery import build\n\n# Securely load the API key\napi_key = os.getenv(\"YOUTUBE_API_KEY\")\n\n# Set up YouTube Data API\nyoutube = build(\"youtube\", \"v3\", developerKey=api_key)\n\n# Retrieve channel's videos\nvideos = []\nnext_page_token = None\ntry:\n    while True:\n        request = youtube.search().list(\n            part=\"snippet\",\n            channelId=\"UCDl7VmS3gD2BQBVZUlL21-A\",\n            maxResults=50,  # Max allowed value",
    "last_modified": "2025-08-06T13:51:17.086091"
  },
  {
    "id": "1004",
    "name": "NewUpload_20250607131031.py",
    "path": "02_media_processing/video_tools/NewUpload_20250607131031.py",
    "category": "02_media_processing",
    "type": "youtube",
    "lines": 120,
    "size": 4753,
    "docstring": "",
    "keywords": [
      "video_processing",
      "youtube",
      "web_tools"
    ],
    "functions": [
      "get_authenticated_service",
      "upload_video"
    ],
    "classes": [],
    "imports": [
      "os",
      "google.auth.transport.requests",
      "google_auth_oauthlib.flow",
      "googleapiclient.discovery",
      "googleapiclient.errors"
    ],
    "preview": "import os\n\nimport google.auth.transport.requests\nimport google_auth_oauthlib.flow\nimport googleapiclient.discovery\nimport googleapiclient.errors\n\n# --- Analysis of the code structure and logic ---\n\n# 1. Imports:\n#    - The script imports necessary modules for Google API client, authentication, and OS operations.\n#    - It assumes the googleapiclient and google_auth_oauthlib libraries are installed.\n\n# 2. Constants:\n#    - SCOPES: Defines the OAuth scope for uploading to YouTube.\n#    - CLIENT_SECRETS_FILE: Path to the OAuth client secrets JSON file.\n#    - API_SERVICE_NAME and API_VERSION: Used to build the YouTube API client.\n\n# 3. Authentication Function:\ndef get_authenticated_service():",
    "last_modified": "2025-09-06T12:24:11.133340"
  },
  {
    "id": "1005",
    "name": "goapi.py",
    "path": "02_media_processing/video_tools/goapi.py",
    "category": "02_media_processing",
    "type": "web_tools",
    "lines": 21,
    "size": 943,
    "docstring": "",
    "keywords": [
      "web_tools"
    ],
    "functions": [],
    "classes": [],
    "imports": [
      "requests"
    ],
    "preview": "import requests\n\nX - API - KEY = \"k-r4PvyLSTQ6122zbwdky3T3BlbkFJCCdmdHniFBJTDOi8cKjV\"\n\nendpoint = \"https://api.goapi.ai/mj/v2/imagine\"\n\nheaders = {\"X-API-KEY\": X - API - KEY}\n\ndata = {\n    \"prompt\": \"Wraith, a master of stealth and assassination, moves unseen through the Rogue Isles, his blades \ufb01nding marks unseen until it's too late. His tale is one of vengeance and shadow, as he cuts a silent path through his enemies, from the treacherous jungles of Mercy Island to the dark alleys of St. Martial. Wraith's journey explores the depths of the Stalker's path, where invisibility and the element of surprise are wielded with deadly precision, illustrating that the most formidable threats are those unseen.\",\n    \"aspect_ratio\": \"9:16\",\n    \"process_mode\": \"mixed\",\n    \"webhook_endpoint\": \"\",\n    \"webhook_secret\": \"\",\n}\n\nresponse = requests.post(endpoint, headers=headers, json=data)\n\nprint(response.status_code)\nprint(response.json())",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1006",
    "name": "NewUpload_20250607131227.py",
    "path": "02_media_processing/video_tools/NewUpload_20250607131227.py",
    "category": "02_media_processing",
    "type": "youtube",
    "lines": 92,
    "size": 3847,
    "docstring": "",
    "keywords": [
      "video_processing",
      "youtube",
      "web_tools"
    ],
    "functions": [
      "get_authenticated_service",
      "upload_video"
    ],
    "classes": [],
    "imports": [
      "os",
      "google.auth.transport.requests",
      "google_auth_oauthlib.flow",
      "googleapiclient.discovery",
      "googleapiclient.errors"
    ],
    "preview": "import os\n\nimport google.auth.transport.requests\nimport google_auth_oauthlib.flow\nimport googleapiclient.discovery\nimport googleapiclient.errors\n\nSCOPES = [\"https://www.googleapis.com/auth/youtube.upload\"]\nCLIENT_SECRETS_FILE = \"/Users/steven/Documents/python/Youtube/client_secrets.json\"  # Replace with your client secrets file\nAPI_SERVICE_NAME = \"youtube\"\nAPI_VERSION = \"v3\"\n\ndef get_authenticated_service():\n    \"\"\"Authenticates and returns the YouTube Data API service.\"\"\"\n    flow = google_auth_oauthlib.flow.InstalledAppFlow.from_client_secrets_file(\n        CLIENT_SECRETS_FILE, SCOPES)\n    credentials = flow.run_local_server(port=0)\n    return googleapiclient.discovery.build(API_SERVICE_NAME, API_VERSION,\n                                            credentials=credentials)\n",
    "last_modified": "2025-09-06T12:24:11.136449"
  },
  {
    "id": "1007",
    "name": "file_cache.py",
    "path": "02_media_processing/video_tools/file_cache.py",
    "category": "02_media_processing",
    "type": "organization",
    "lines": 180,
    "size": 5338,
    "docstring": "",
    "keywords": [],
    "functions": [
      "_secure_open_write",
      "url_to_file_path",
      "__init__",
      "encode",
      "_fn",
      "get",
      "set",
      "_write",
      "_delete",
      "delete"
    ],
    "classes": [
      "_FileCacheMixin",
      "FileCache",
      "SeparateBodyFileCache"
    ],
    "imports": [
      "__future__",
      "hashlib",
      "os",
      "textwrap",
      "typing",
      "pip._vendor.cachecontrol.cache",
      "pip._vendor.cachecontrol.controller",
      "datetime",
      "filelock",
      "filelock"
    ],
    "preview": "# SPDX-FileCopyrightText: 2015 Eric Larson\n#\n# SPDX-License-Identifier: Apache-2.0\nfrom __future__ import annotations\n\nimport hashlib\nimport os\nfrom textwrap import dedent\nfrom typing import IO, TYPE_CHECKING\n\nfrom pip._vendor.cachecontrol.cache import BaseCache, SeparateBodyBaseCache\nfrom pip._vendor.cachecontrol.controller import CacheController\n\nif TYPE_CHECKING:\n    from datetime import datetime\n\n    from filelock import BaseFileLock\n\n\ndef _secure_open_write(filename: str, fmode: int) -> IO[bytes]:",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1008",
    "name": "util.py",
    "path": "02_media_processing/video_tools/util.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 279,
    "size": 8596,
    "docstring": "",
    "keywords": [],
    "functions": [
      "col",
      "lineno",
      "line",
      "_escape_regex_range_chars",
      "_collapse_string_to_ranges",
      "_flatten",
      "_make_synonym_function",
      "replaced_by_pep8",
      "_set",
      "__init__"
    ],
    "classes": [
      "__config_flags",
      "_UnboundedCache",
      "_FifoCache",
      "LRUMemo",
      "UnboundedMemo"
    ],
    "imports": [
      "collections",
      "inspect",
      "itertools",
      "types",
      "warnings",
      "functools",
      "typing"
    ],
    "preview": "# util.py\nimport collections\nimport inspect\nimport itertools\nimport types\nimport warnings\nfrom functools import lru_cache, wraps\nfrom typing import Callable, Iterable, List, TypeVar, Union, cast\n\n_bslash = chr(92)\nC = TypeVar(\"C\", bound=Callable)\n\n\nclass __config_flags:\n    \"\"\"Internal class for defining compatibility and debugging flags\"\"\"\n\n    _all_names: List[str] = []\n    _fixed_names: List[str] = []\n    _type_desc = \"configuration\"\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1009",
    "name": "unzip.py",
    "path": "02_media_processing/video_tools/unzip.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 149,
    "size": 5530,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "logging",
      "os",
      "shutil",
      "subprocess",
      "time",
      "pyrogram",
      "translation",
      "helper_funcs.chat_base",
      "helper_funcs.display_progress",
      "sample_config"
    ],
    "preview": "import logging\n\nlogging.basicConfig(\n    level=logging.DEBUG, format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\n)\nlogger = logging.getLogger(__name__)\n\nimport os\nimport shutil\nimport subprocess\nimport time\n\n# the secret configuration specific things\nif bool(os.environ.get(\"WEBHOOK\", False)):\n    from sample_config import Config\nelse:\n    from config import Config\n\n# the Strings used for this \"thing\"\nimport pyrogram",
    "last_modified": "2025-09-13T05:53:44.359071"
  },
  {
    "id": "1010",
    "name": "generate-category.py",
    "path": "02_media_processing/video_tools/generate-category.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 188,
    "size": 6306,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "read_html_file",
      "extract_title",
      "categorize_file",
      "scan_and_categorize",
      "generate_html",
      "save_html"
    ],
    "classes": [],
    "imports": [
      "os",
      "re"
    ],
    "preview": "import os\nimport re\n\n# Directory where your HTML files are located\nHTML_DIRECTORY = \"/Users/steven/Documents/HTML\"\n\n# Categories and keywords to search for in HTML files\nCATEGORIES = {\n    \"Art & Design\": [\n        \"art\",\n        \"design\",\n        \"creative\",\n        \"raccoon\",\n        \"fantasy\",\n        \"cosmic\",\n        \"whimsical\",\n        \"coverart\",\n        \"mystical\",\n    ],\n    \"Technology\": [",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1011",
    "name": "reset_following.py",
    "path": "02_media_processing/video_tools/reset_following.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 91,
    "size": 2307,
    "docstring": "This is a account reset tool.\nUse this before you start boting.\nYou can then reset the users you follow to what you had before botting.",
    "keywords": [],
    "functions": [
      "start",
      "one",
      "two"
    ],
    "classes": [
      "Task"
    ],
    "imports": [
      "os",
      "sys",
      "instabot"
    ],
    "preview": "\"\"\"\n\nThis is a account reset tool.\nUse this before you start boting.\nYou can then reset the users you follow to what you had before botting.\n\n\"\"\"\n\nimport os\nimport sys\n\nsys.path.append(os.path.join(sys.path[0], \"../\"))\nfrom instabot import Bot  # noqa: E402\n\n\n# class of all the tasks\nclass Task(object):\n    # getting the user to pick what to do\n    @staticmethod\n    def start(bot):",
    "last_modified": "2025-09-13T05:54:55.789013"
  },
  {
    "id": "1012",
    "name": "my_copy.py",
    "path": "02_media_processing/video_tools/my_copy.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 39,
    "size": 1386,
    "docstring": "",
    "keywords": [],
    "functions": [
      "ensure_dir"
    ],
    "classes": [],
    "imports": [
      "csv",
      "os",
      "shutil"
    ],
    "preview": "import csv\nimport os\nimport shutil\n\n# Paths configuration\ncsv_file_path = \"/Users/steven/15days.csv\"\ndestination_root = \"/Volumes/iMac/15days\"\nbackup_root = os.path.join(destination_root, \"backup\")\n\n\ndef ensure_dir(directory):\n    \"\"\"Ensure the directory exists. If not, create it.\"\"\"\n    if not os.path.exists(directory):\n        os.makedirs(directory)\n\n\nwith open(csv_file_path, newline=\"\") as csvfile:\n    filereader = csv.reader(csvfile)\n    for row in filereader:\n        # Assuming each row has one column with the file path",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1013",
    "name": "NewUpload.py",
    "path": "02_media_processing/video_tools/NewUpload.py",
    "category": "02_media_processing",
    "type": "youtube",
    "lines": 78,
    "size": 3077,
    "docstring": "",
    "keywords": [
      "youtube",
      "web_tools"
    ],
    "functions": [
      "get_authenticated_service",
      "upload_video"
    ],
    "classes": [],
    "imports": [
      "os",
      "google.auth.transport.requests",
      "google_auth_oauthlib.flow",
      "googleapiclient.discovery",
      "googleapiclient.errors"
    ],
    "preview": "import os\n\nimport google.auth.transport.requests\nimport google_auth_oauthlib.flow\nimport googleapiclient.discovery\nimport googleapiclient.errors\n\nSCOPES = [\"https://www.googleapis.com/auth/youtube.upload\"]\nCLIENT_SECRETS_FILE = \"/Users/steven/Documents/python/Youtube/client_secrets.json\"  # Replace with your client secrets file\nAPI_SERVICE_NAME = \"youtube\"\nAPI_VERSION = \"v3\"\n\n\ndef get_authenticated_service():\n    \"\"\"Authenticates and returns the YouTube Data API service.\"\"\"\n    flow = google_auth_oauthlib.flow.InstalledAppFlow.from_client_secrets_file(\n        CLIENT_SECRETS_FILE, SCOPES\n    )\n    credentials = flow.run_local_server(port=0)\n    return googleapiclient.discovery.build(API_SERVICE_NAME, API_VERSION, credentials=credentials)",
    "last_modified": "2025-09-13T05:54:12.214304"
  },
  {
    "id": "1014",
    "name": "yt-meta.py",
    "path": "02_media_processing/video_tools/yt-meta.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 44,
    "size": 1223,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "csv",
      "os",
      "googleapiclient.discovery"
    ],
    "preview": "import csv\nimport os\n\nfrom googleapiclient.discovery import build\n\n# Securely load the API key\napi_key = os.getenv(\"YOUTUBE_API_KEY\")\n\n# Set up YouTube Data API\nyoutube = build(\"youtube\", \"v3\", developerKey=api_key)\n\n# Retrieve channel's videos\nvideos = []\nnext_page_token = None\ntry:\n    while True:\n        request = youtube.search().list(\n            part=\"snippet\",\n            channelId=\"UCDl7VmS3gD2BQBVZUlL21-A\",\n            maxResults=50,  # Max allowed value",
    "last_modified": "2025-05-04T22:47:13.354999"
  },
  {
    "id": "1015",
    "name": "test_bot_support.py",
    "path": "02_media_processing/video_tools/test_bot_support.py",
    "category": "02_media_processing",
    "type": "testing",
    "lines": 64,
    "size": 1920,
    "docstring": "",
    "keywords": [
      "testing"
    ],
    "functions": [
      "test_extract_urls",
      "test_check_if_file_exist",
      "test_check_if_file_exist_fail",
      "test_console_print"
    ],
    "classes": [
      "TestBotSupport"
    ],
    "imports": [
      "__future__",
      "os",
      "sys",
      "pytest",
      "test_bot",
      "io",
      "StringIO"
    ],
    "preview": "# -*- coding: utf-8 -*-\nfrom __future__ import unicode_literals\n\nimport os\nimport sys\n\nimport pytest\n\nfrom .test_bot import TestBot\n\n\nclass TestBotSupport(TestBot):\n    @pytest.mark.parametrize(\n        \"url,result\",\n        [\n            (\"https://google.com\", [\"https://google.com\"]),\n            (\"google.com\", [\"google.com\"]),\n            (\"google.com/search?q=instabot\", [\"google.com/search?q=instabot\"]),\n            (\n                \"https://google.com/search?q=instabot\",",
    "last_modified": "2025-09-13T05:55:00.005854"
  },
  {
    "id": "1016",
    "name": "youtube 2.py",
    "path": "02_media_processing/video_tools/youtube 2.py",
    "category": "02_media_processing",
    "type": "youtube",
    "lines": 64,
    "size": 2227,
    "docstring": "",
    "keywords": [],
    "functions": [
      "add_video_to_playlist"
    ],
    "classes": [],
    "imports": [
      "csv",
      "google_auth_oauthlib.flow",
      "googleapiclient.discovery",
      "googleapiclient.http"
    ],
    "preview": "import csv\n\nfrom google_auth_oauthlib.flow import InstalledAppFlow\nfrom googleapiclient.discovery import build\nfrom googleapiclient.http import MediaFileUpload\n\n# Define the scopes\nSCOPES = [\n    \"https://www.googleapis.com/auth/youtube.upload\",\n    \"https://www.googleapis.com/auth/youtube\",\n]\n\n## Authenticate and build the YouTube API service\nflow = InstalledAppFlow.from_client_secrets_file(\n    \"/Users/steven/Movies/youtube-upload/client_secret.json\", SCOPES\n)\n\n# Use run_local_server instead of run_console\ncredentials = flow.run_local_server(port=0)\nyoutube = build(\"youtube\", \"v3\", credentials=credentials)",
    "last_modified": "2025-09-13T05:54:08.725834"
  },
  {
    "id": "1017",
    "name": "req_set.py",
    "path": "02_media_processing/video_tools/req_set.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 116,
    "size": 4667,
    "docstring": "",
    "keywords": [],
    "functions": [
      "__init__",
      "__str__",
      "__repr__",
      "add_unnamed_requirement",
      "add_named_requirement",
      "has_requirement",
      "get_requirement",
      "all_requirements",
      "requirements_to_install",
      "warn_legacy_versions_and_specifiers"
    ],
    "classes": [
      "RequirementSet"
    ],
    "imports": [
      "logging",
      "collections",
      "typing",
      "pip._internal.req.req_install",
      "pip._internal.utils.deprecation",
      "pip._vendor.packaging.specifiers",
      "pip._vendor.packaging.utils",
      "pip._vendor.packaging.version"
    ],
    "preview": "import logging\nfrom collections import OrderedDict\nfrom typing import Dict, List\n\nfrom pip._internal.req.req_install import InstallRequirement\nfrom pip._internal.utils.deprecation import deprecated\nfrom pip._vendor.packaging.specifiers import LegacySpecifier\nfrom pip._vendor.packaging.utils import canonicalize_name\nfrom pip._vendor.packaging.version import LegacyVersion\n\nlogger = logging.getLogger(__name__)\n\n\nclass RequirementSet:\n    def __init__(self, check_supported_wheels: bool = True) -> None:\n        \"\"\"Create a RequirementSet.\"\"\"\n\n        self.requirements: Dict[str, InstallRequirement] = OrderedDict()\n        self.check_supported_wheels = check_supported_wheels\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1018",
    "name": "found_candidates.py",
    "path": "02_media_processing/video_tools/found_candidates.py",
    "category": "02_media_processing",
    "type": "analysis",
    "lines": 156,
    "size": 5705,
    "docstring": "Utilities to lazily create and visit candidates found.\n\nCreating and visiting a candidate is a *very* costly operation. It involves\nfetching, extracting, potentially building modules from source, and verifying\ndistribution metadata. It is therefore crucial for performance to keep\neverything here lazy all the way down, so we only touch candidates that we\nabsolutely need, and not \"download the world\" when we only need one version of\nsomething.",
    "keywords": [],
    "functions": [
      "_iter_built",
      "_iter_built_with_prepended",
      "_iter_built_with_inserted",
      "__init__",
      "__getitem__",
      "__iter__",
      "__len__",
      "__bool__"
    ],
    "classes": [
      "FoundCandidates"
    ],
    "imports": [
      "functools",
      "collections.abc",
      "typing",
      "pip._vendor.packaging.version",
      "base"
    ],
    "preview": "\"\"\"Utilities to lazily create and visit candidates found.\n\nCreating and visiting a candidate is a *very* costly operation. It involves\nfetching, extracting, potentially building modules from source, and verifying\ndistribution metadata. It is therefore crucial for performance to keep\neverything here lazy all the way down, so we only touch candidates that we\nabsolutely need, and not \"download the world\" when we only need one version of\nsomething.\n\"\"\"\n\nimport functools\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING, Any, Callable, Iterator, Optional, Set, Tuple\n\nfrom pip._vendor.packaging.version import _BaseVersion\n\nfrom .base import Candidate\n\nIndexCandidateInfo = Tuple[_BaseVersion, Callable[[], Optional[Candidate]]]\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1019",
    "name": "YouTube_VIEWBOT.py",
    "path": "02_media_processing/video_tools/YouTube_VIEWBOT.py",
    "category": "02_media_processing",
    "type": "youtube",
    "lines": 279,
    "size": 15983,
    "docstring": "",
    "keywords": [],
    "functions": [
      "open_autoplaying_window"
    ],
    "classes": [
      "Color"
    ],
    "imports": [
      "random",
      "threading",
      "time",
      "webbrowser"
    ],
    "preview": "##############################################################################################\n#                                                                                           #\n# INSTRUCTIONS TO RUN THE SCRIPT:                                                           #\n# STEP ZERO                                                                                 #\n# MUTE YOUR SPEAKERS, SAVE ALL OF YOUR WORK                                                 #\n# YOU CAN USE THIS ON A POOR LAPTOP IN THE CORNER                                           #\n# IT JUST USE MANY RAMS SO START WITH LONGER DURATION AND IT WILL BE BETTER                 #\n#       FULL COMMENTS BELOW                                                                 #\n#      BUT YOU CAN RUN THE SCRIPT RIGHT NOW IN CMD OR TERMINAL OR VSCODE (I RECOMMEND VSCODE)#\n#                                 VSCODE IF YOU WANT  https://code.visualstudio.com/download #\n# 1. Prerequisites:                                                                         #\n#    - Ensure you have Python installed on your system. You can download it from            #\n#      https://www.python.org/downloads/                                                    #\n#                                                                                           #\n# 2. Download the Script:                                                                   #\n#    - Download the script file and save it with a '.py' extension, e.g.,                   #\n#      'youtube_viewbot.py'.                                                                #\n#                                                                                           #\n# 3. Open a Terminal or Command Prompt:                                                     #\n#    - On Windows, open the Command Prompt. On macOS or Linux, open the Terminal.           #",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1020",
    "name": "youtube 3.py",
    "path": "02_media_processing/video_tools/youtube 3.py",
    "category": "02_media_processing",
    "type": "youtube",
    "lines": 64,
    "size": 2227,
    "docstring": "",
    "keywords": [],
    "functions": [
      "add_video_to_playlist"
    ],
    "classes": [],
    "imports": [
      "csv",
      "google_auth_oauthlib.flow",
      "googleapiclient.discovery",
      "googleapiclient.http"
    ],
    "preview": "import csv\n\nfrom google_auth_oauthlib.flow import InstalledAppFlow\nfrom googleapiclient.discovery import build\nfrom googleapiclient.http import MediaFileUpload\n\n# Define the scopes\nSCOPES = [\n    \"https://www.googleapis.com/auth/youtube.upload\",\n    \"https://www.googleapis.com/auth/youtube\",\n]\n\n## Authenticate and build the YouTube API service\nflow = InstalledAppFlow.from_client_secrets_file(\n    \"/Users/steven/Movies/youtube-upload/client_secret.json\", SCOPES\n)\n\n# Use run_local_server instead of run_console\ncredentials = flow.run_local_server(port=0)\nyoutube = build(\"youtube\", \"v3\", credentials=credentials)",
    "last_modified": "2025-09-13T05:54:08.761972"
  },
  {
    "id": "1021",
    "name": "NewUpload_20250607130507.py",
    "path": "02_media_processing/video_tools/NewUpload_20250607130507.py",
    "category": "02_media_processing",
    "type": "youtube",
    "lines": 78,
    "size": 2714,
    "docstring": "",
    "keywords": [
      "youtube",
      "web_tools"
    ],
    "functions": [
      "get_authenticated_service",
      "upload_video"
    ],
    "classes": [],
    "imports": [
      "os",
      "google.auth.transport.requests",
      "google_auth_oauthlib.flow",
      "googleapiclient.discovery",
      "googleapiclient.errors"
    ],
    "preview": "import os\n\nimport google.auth.transport.requests\nimport google_auth_oauthlib.flow\nimport googleapiclient.discovery\nimport googleapiclient.errors\n\nSCOPES = [\"https://www.googleapis.com/auth/youtube.upload\"]\nCLIENT_SECRETS_FILE = \"/Users/steven/Documents/python/Youtube/client_secrets.json\"  # Replace with your client secrets file\nAPI_SERVICE_NAME = \"youtube\"\nAPI_VERSION = \"v3\"\n\n\ndef get_authenticated_service():\n    \"\"\"Authenticates and returns the YouTube Data API service.\"\"\"\n    flow = google_auth_oauthlib.flow.InstalledAppFlow.from_client_secrets_file(\n        CLIENT_SECRETS_FILE, SCOPES\n    )\n    credentials = flow.run_local_server(port=0)\n    return googleapiclient.discovery.build(API_SERVICE_NAME, API_VERSION, credentials=credentials)",
    "last_modified": "2025-09-13T05:54:11.938776"
  },
  {
    "id": "1022",
    "name": "cache.py",
    "path": "02_media_processing/video_tools/cache.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 224,
    "size": 7922,
    "docstring": "",
    "keywords": [],
    "functions": [
      "add_options",
      "run",
      "get_cache_dir",
      "get_cache_info",
      "list_cache_items",
      "format_for_human",
      "format_for_abspath",
      "remove_cache_items",
      "purge_cache",
      "_cache_dir"
    ],
    "classes": [
      "CacheCommand"
    ],
    "imports": [
      "os",
      "textwrap",
      "optparse",
      "typing",
      "pip._internal.cli.base_command",
      "pip._internal.cli.status_codes",
      "pip._internal.exceptions",
      "pip._internal.utils",
      "pip._internal.utils.logging"
    ],
    "preview": "import os\nimport textwrap\nfrom optparse import Values\nfrom typing import Any, List\n\nfrom pip._internal.cli.base_command import Command\nfrom pip._internal.cli.status_codes import ERROR, SUCCESS\nfrom pip._internal.exceptions import CommandError, PipError\nfrom pip._internal.utils import filesystem\nfrom pip._internal.utils.logging import getLogger\n\nlogger = getLogger(__name__)\n\n\nclass CacheCommand(Command):\n    \"\"\"\n    Inspect and manage pip's wheel cache.\n\n    Subcommands:\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1023",
    "name": "actions.py",
    "path": "02_media_processing/video_tools/actions.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 218,
    "size": 6567,
    "docstring": "",
    "keywords": [],
    "functions": [
      "match_only_at_col",
      "replace_with",
      "remove_quotes",
      "with_attribute",
      "with_class",
      "replaceWith",
      "removeQuotes",
      "withAttribute",
      "withClass",
      "matchOnlyAtCol"
    ],
    "classes": [
      "OnlyOnce"
    ],
    "imports": [
      "exceptions",
      "util",
      "core"
    ],
    "preview": "# actions.py\n\nfrom .exceptions import ParseException\nfrom .util import col, replaced_by_pep8\n\n\nclass OnlyOnce:\n    \"\"\"\n    Wrapper for parse actions, to ensure they are only called once.\n    \"\"\"\n\n    def __init__(self, method_call):\n        from .core import _trim_arity\n\n        self.callable = _trim_arity(method_call)\n        self.called = False\n\n    def __call__(self, s, l, t):\n        if not self.called:\n            results = self.callable(s, l, t)",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1024",
    "name": "cleanupd.py",
    "path": "02_media_processing/video_tools/cleanupd.py",
    "category": "02_media_processing",
    "type": "organization",
    "lines": 62,
    "size": 2217,
    "docstring": "",
    "keywords": [],
    "functions": [
      "list_venv_directories",
      "main"
    ],
    "classes": [],
    "imports": [
      "os",
      "subprocess"
    ],
    "preview": "import os\nimport subprocess\n\n# Define the versions you want to keep\nrequired_versions = [\"3.10\", \"3.12.4\"]\n\n\ndef list_venv_directories(base_path):\n    \"\"\"List all virtual environment directories.\"\"\"\n    venv_dirs = []\n    for root, dirs, files in os.walk(base_path):\n        for dir in dirs:\n            if os.path.exists(os.path.join(root, dir, \"bin\", \"python\")):\n                venv_dirs.append(os.path.join(root, dir))\n    return venv_dirs\n\n\ndef main():\n    # Base path where your virtual environments are stored\n    venv_base_path = os.path.expanduser(\"~/venvs\")",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1025",
    "name": "NewUpload_20250607131212.py",
    "path": "02_media_processing/video_tools/NewUpload_20250607131212.py",
    "category": "02_media_processing",
    "type": "youtube",
    "lines": 79,
    "size": 3078,
    "docstring": "",
    "keywords": [
      "youtube",
      "web_tools"
    ],
    "functions": [
      "get_authenticated_service",
      "upload_video"
    ],
    "classes": [],
    "imports": [
      "os",
      "google.auth.transport.requests",
      "google_auth_oauthlib.flow",
      "googleapiclient.discovery",
      "googleapiclient.errors"
    ],
    "preview": "import os\n\nimport google.auth.transport.requests\nimport google_auth_oauthlib.flow\nimport googleapiclient.discovery\nimport googleapiclient.errors\n\nSCOPES = [\"https://www.googleapis.com/auth/youtube.upload\"]\nCLIENT_SECRETS_FILE = \"/Users/steven/Documents/python/Youtube/client_secrets.json\"  # Replace with your client secrets file\nAPI_SERVICE_NAME = \"youtube\"\nAPI_VERSION = \"v3\"\n\n\ndef get_authenticated_service():\n    \"\"\"Authenticates and returns the YouTube Data API service.\"\"\"\n    flow = google_auth_oauthlib.flow.InstalledAppFlow.from_client_secrets_file(\n        CLIENT_SECRETS_FILE, SCOPES\n    )\n    credentials = flow.run_local_server(port=0)\n    return googleapiclient.discovery.build(API_SERVICE_NAME, API_VERSION, credentials=credentials)",
    "last_modified": "2025-09-13T05:54:12.083841"
  },
  {
    "id": "1026",
    "name": "youtube_download 3.py",
    "path": "02_media_processing/video_tools/youtube_download 3.py",
    "category": "02_media_processing",
    "type": "youtube",
    "lines": 53,
    "size": 1213,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "json",
      "os",
      "sys",
      "urllib",
      "datetime",
      "subprocess"
    ],
    "preview": "#!/usr/bin/python\n\"\"\"\nYouTube Channel Downloader\n\"\"\"\n \nimport json\nimport os\nimport sys\nimport urllib\nfrom datetime import date\nfrom subprocess import call\n\nprint \"--------------------------\"\nprint \"YouTube Channel Downloader\"\nprint \"--------------------------\"\nprint \"\"\nwhile True:\n    try:\n        author = raw_input(\"Enter username of YouTube channel: \")\n        break",
    "last_modified": "2025-08-06T13:43:33.302820"
  },
  {
    "id": "1027",
    "name": "playlistlistener.py",
    "path": "02_media_processing/video_tools/playlistlistener.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 103,
    "size": 3495,
    "docstring": "Listens to playlist for new tracks",
    "keywords": [],
    "functions": [
      "__init__",
      "__init__",
      "listen_and_add_to_queue",
      "__get_last_upload_time__",
      "__save_last_upload_time__"
    ],
    "classes": [
      "Youtubeentity",
      "Playlistlistener"
    ],
    "imports": [
      "logging",
      "datetime",
      "dateutil.parser",
      "pytz",
      "googleapiclient.discovery",
      "ytdl.awsqueue",
      "ytdl.models",
      "ytdl.notify",
      "ytdl.oshelper"
    ],
    "preview": "\"Listens to playlist for new tracks\"\n\nimport logging\nfrom datetime import datetime\n\nimport dateutil.parser\nimport pytz\nfrom googleapiclient.discovery import build\nfrom ytdl.awsqueue import Awsqueue\nfrom ytdl.models import Payload\nfrom ytdl.notify import Iftttnotify\nfrom ytdl.oshelper import file_exists\n\n\nclass Youtubeentity(object):\n    \"Youtube entity object\"\n\n    def __init__(self, title, link, upload_time):\n        self.title = title\n        self.link = link",
    "last_modified": "2025-09-13T05:54:15.165599"
  },
  {
    "id": "1028",
    "name": "preset.py",
    "path": "02_media_processing/video_tools/preset.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 82,
    "size": 2474,
    "docstring": "",
    "keywords": [],
    "functions": [
      "to_dict",
      "__init__",
      "construct",
      "confirm",
      "confirm_publish_at"
    ],
    "classes": [
      "PresetOptions",
      "Preset"
    ],
    "imports": [
      "abc",
      "dataclasses",
      "datetime",
      "pathlib",
      "typing",
      "InquirerPy",
      "constants"
    ],
    "preview": "from abc import ABC, abstractmethod\nfrom dataclasses import asdict, dataclass\nfrom datetime import datetime, timedelta\nfrom pathlib import Path\nfrom typing import List, Optional\n\nfrom InquirerPy import inquirer\n\nfrom ..constants import DESCRIPTION, PUBLISH_AT, TAGS, TITLE\n\n\n@dataclass\nclass PresetOptions:\n    file: str = \"\"\n    title: str = \"\"\n    description: str = \"\"\n    tags: Optional[List[str]] = None\n    category_id: Optional[int] = None\n    publish_at: Optional[datetime] = None\n    playlist_id: Optional[str] = None",
    "last_modified": "2025-09-13T05:53:47.295524"
  },
  {
    "id": "1029",
    "name": "logger.py",
    "path": "02_media_processing/video_tools/logger.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 46,
    "size": 1515,
    "docstring": "",
    "keywords": [],
    "functions": [
      "__init__",
      "log_traceback",
      "debug",
      "warning",
      "error",
      "info"
    ],
    "classes": [
      "Logger"
    ],
    "imports": [
      "logging",
      "traceback",
      "datetime",
      "pathlib",
      "utils"
    ],
    "preview": "import logging\nimport traceback\nfrom datetime import datetime\nfrom pathlib import Path\n\nfrom .utils import create_dir\n\n\nclass Logger:\n    def __init__(self, log_location: str = \"\", log_level=logging.INFO) -> None:\n        self.logger = logging.getLogger(\"savify\")\n        self.logger.setLevel(logging.DEBUG)\n\n        time = (str(datetime.now()).replace(\" \", \"_\")).replace(\":\", \"_\")\n        log_location = f\"{log_location}/logs/{time}_savify.log\"\n        formatter = logging.Formatter(\"[%(levelname)s]\\t%(message)s\")\n\n        create_dir(Path(log_location).parent)\n\n        if log_level is not None:",
    "last_modified": "2025-05-04T23:28:25.606847"
  },
  {
    "id": "1030",
    "name": "createdb.py",
    "path": "02_media_processing/video_tools/createdb.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 6,
    "size": 149,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "tinydb"
    ],
    "preview": "from tinydb import Query, TinyDB\n\ndb = TinyDB(\"log/db.json\")\ntable = db.table(\"created_videos\")\ntable.insert({\"url\": \"www.example.com\", \"id\": \"00\"})\n",
    "last_modified": "2025-05-04T23:28:22.816317"
  },
  {
    "id": "1031",
    "name": "uploadYT.py",
    "path": "02_media_processing/video_tools/uploadYT.py",
    "category": "02_media_processing",
    "type": "youtube",
    "lines": 201,
    "size": 6868,
    "docstring": "",
    "keywords": [
      "youtube"
    ],
    "functions": [
      "get_authenticated_service",
      "initialize_upload",
      "resumable_upload",
      "upload2YT"
    ],
    "classes": [],
    "imports": [
      "os",
      "random",
      "sys",
      "time",
      "httplib2",
      "apiclient.discovery",
      "apiclient.errors",
      "apiclient.http",
      "oauth2client.client",
      "oauth2client.file"
    ],
    "preview": "#!/usr/bin/python\n\nimport os\nimport random\nimport sys\nimport time\n\nimport httplib2\nfrom apiclient.discovery import build\nfrom apiclient.errors import HttpError\nfrom apiclient.http import MediaFileUpload\nfrom oauth2client.client import flow_from_clientsecrets\nfrom oauth2client.file import Storage\nfrom oauth2client.tools import argparser, run_flow\n\n# Explicitly tell the underlying HTTP transport library not to retry, since\n# we are handling retry logic ourselves.\nhttplib2.RETRIES = 1\n\n# Maximum number of times to retry before giving up.",
    "last_modified": "2025-05-06T04:35:15"
  },
  {
    "id": "1032",
    "name": "NewUpload_20250607131143.py",
    "path": "02_media_processing/video_tools/NewUpload_20250607131143.py",
    "category": "02_media_processing",
    "type": "youtube",
    "lines": 95,
    "size": 4012,
    "docstring": "",
    "keywords": [
      "video_processing",
      "youtube",
      "web_tools"
    ],
    "functions": [
      "get_authenticated_service",
      "upload_video"
    ],
    "classes": [],
    "imports": [
      "os",
      "google.auth.transport.requests",
      "google_auth_oauthlib.flow",
      "googleapiclient.discovery",
      "googleapiclient.errors"
    ],
    "preview": "import os\n\nimport google.auth.transport.requests\nimport google_auth_oauthlib.flow\nimport googleapiclient.discovery\nimport googleapiclient.errors\n\nSCOPES = [\"https://www.googleapis.com/auth/youtube.upload\"]\nCLIENT_SECRETS_FILE = \"/Users/steven/Documents/python/Youtube/client_secrets.json\"  # Replace with your client secrets file\nAPI_SERVICE_NAME = \"youtube\"\nAPI_VERSION = \"v3\"\n\ndef get_authenticated_service():\n    \"\"\"Authenticates and returns the YouTube Data API service.\"\"\"\n    flow = google_auth_oauthlib.flow.InstalledAppFlow.from_client_secrets_file(\n        CLIENT_SECRETS_FILE, SCOPES)\n    credentials = flow.run_local_server(port=0)\n    return googleapiclient.discovery.build(API_SERVICE_NAME, API_VERSION,\n                                            credentials=credentials)\n",
    "last_modified": "2025-09-06T12:24:11.145729"
  },
  {
    "id": "1033",
    "name": "DownloadVideos.py",
    "path": "02_media_processing/video_tools/DownloadVideos.py",
    "category": "02_media_processing",
    "type": "youtube",
    "lines": 30,
    "size": 823,
    "docstring": "1. Download daily trending TikTok clips",
    "keywords": [
      "youtube"
    ],
    "functions": [
      "download_tiktoks"
    ],
    "classes": [],
    "imports": [
      "os",
      "random",
      "string",
      "TikTokApi"
    ],
    "preview": "\"\"\"\n1. Download daily trending TikTok clips\n\"\"\"\n\nimport os\nimport random\nimport string\n\nfrom TikTokApi import TikTokApi\n\nDAILY_TRENDING_DIR = r\"directory location for downloaded tiktok videos\"\nverifyFp = \"use s_v_web_id cookie from tiktok.com\"\ndid = \"\".join(random.choice(string.digits) for num in range(19))\n\napi = TikTokApi.get_instance(custom_verifyFp=verifyFp, use_test_endpoints=True)\n\ntiktoks = api.trending()\nvideo_bytes = api.get_Video_By_TikTok(tiktoks[0])\n\n",
    "last_modified": "2025-05-04T23:28:21"
  },
  {
    "id": "1034",
    "name": "NewUpload_20250607131221.py",
    "path": "02_media_processing/video_tools/NewUpload_20250607131221.py",
    "category": "02_media_processing",
    "type": "youtube",
    "lines": 94,
    "size": 3858,
    "docstring": "",
    "keywords": [
      "video_processing",
      "youtube",
      "web_tools"
    ],
    "functions": [
      "get_authenticated_service",
      "upload_video"
    ],
    "classes": [],
    "imports": [
      "os",
      "google.auth.transport.requests",
      "google_auth_oauthlib.flow",
      "googleapiclient.discovery",
      "googleapiclient.errors"
    ],
    "preview": "import os\n\nimport google.auth.transport.requests\nimport google_auth_oauthlib.flow\nimport googleapiclient.discovery\nimport googleapiclient.errors\n\nSCOPES = [\"https://www.googleapis.com/auth/youtube.upload\"]\nCLIENT_SECRETS_FILE = \"/Users/steven/Documents/python/Youtube/client_secrets.json\"  # Replace with your client secrets file\nAPI_SERVICE_NAME = \"youtube\"\nAPI_VERSION = \"v3\"\n\ndef get_authenticated_service():\n    \"\"\"Authenticates and returns the YouTube Data API service.\"\"\"\n    flow = google_auth_oauthlib.flow.InstalledAppFlow.from_client_secrets_file(\n        CLIENT_SECRETS_FILE, SCOPES)\n    credentials = flow.run_local_server(port=0)\n    return googleapiclient.discovery.build(API_SERVICE_NAME, API_VERSION,\n                                            credentials=credentials)\n",
    "last_modified": "2025-09-06T12:24:11.148448"
  },
  {
    "id": "1035",
    "name": "NewUpload_20250607131215.py",
    "path": "02_media_processing/video_tools/NewUpload_20250607131215.py",
    "category": "02_media_processing",
    "type": "youtube",
    "lines": 95,
    "size": 4012,
    "docstring": "",
    "keywords": [
      "video_processing",
      "youtube",
      "web_tools"
    ],
    "functions": [
      "get_authenticated_service",
      "upload_video"
    ],
    "classes": [],
    "imports": [
      "os",
      "google.auth.transport.requests",
      "google_auth_oauthlib.flow",
      "googleapiclient.discovery",
      "googleapiclient.errors"
    ],
    "preview": "import os\n\nimport google.auth.transport.requests\nimport google_auth_oauthlib.flow\nimport googleapiclient.discovery\nimport googleapiclient.errors\n\nSCOPES = [\"https://www.googleapis.com/auth/youtube.upload\"]\nCLIENT_SECRETS_FILE = \"/Users/steven/Documents/python/Youtube/client_secrets.json\"  # Replace with your client secrets file\nAPI_SERVICE_NAME = \"youtube\"\nAPI_VERSION = \"v3\"\n\ndef get_authenticated_service():\n    \"\"\"Authenticates and returns the YouTube Data API service.\"\"\"\n    flow = google_auth_oauthlib.flow.InstalledAppFlow.from_client_secrets_file(\n        CLIENT_SECRETS_FILE, SCOPES)\n    credentials = flow.run_local_server(port=0)\n    return googleapiclient.discovery.build(API_SERVICE_NAME, API_VERSION,\n                                            credentials=credentials)\n",
    "last_modified": "2025-09-06T12:24:11.151474"
  },
  {
    "id": "1036",
    "name": "bazaar.py",
    "path": "02_media_processing/video_tools/bazaar.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 109,
    "size": 3475,
    "docstring": "",
    "keywords": [],
    "functions": [
      "get_base_rev_args",
      "fetch_new",
      "switch",
      "update",
      "get_url_rev_and_auth",
      "get_remote_url",
      "get_revision",
      "is_commit_id_equal"
    ],
    "classes": [
      "Bazaar"
    ],
    "imports": [
      "logging",
      "typing",
      "pip._internal.utils.misc",
      "pip._internal.utils.subprocess",
      "pip._internal.utils.urls",
      "pip._internal.vcs.versioncontrol"
    ],
    "preview": "import logging\nfrom typing import List, Optional, Tuple\n\nfrom pip._internal.utils.misc import HiddenText, display_path\nfrom pip._internal.utils.subprocess import make_command\nfrom pip._internal.utils.urls import path_to_url\nfrom pip._internal.vcs.versioncontrol import (\n    AuthInfo,\n    RemoteNotFoundError,\n    RevOptions,\n    VersionControl,\n    vcs,\n)\n\nlogger = logging.getLogger(__name__)\n\n\nclass Bazaar(VersionControl):\n    name = \"bzr\"\n    dirname = \".bzr\"",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1037",
    "name": "yt_upload.py",
    "path": "02_media_processing/video_tools/yt_upload.py",
    "category": "02_media_processing",
    "type": "youtube",
    "lines": 209,
    "size": 7218,
    "docstring": "",
    "keywords": [
      "youtube"
    ],
    "functions": [
      "get_authenticated_service",
      "initialize_upload",
      "resumable_upload",
      "upload_video"
    ],
    "classes": [],
    "imports": [
      "http.client",
      "os",
      "random",
      "sys",
      "time",
      "types",
      "httplib2",
      "apiclient.discovery",
      "apiclient.errors",
      "apiclient.http"
    ],
    "preview": "import http.client as httplib\nimport os\nimport random\nimport sys\nimport time\nfrom types import SimpleNamespace\n\nimport httplib2\nfrom apiclient.discovery import build\nfrom apiclient.errors import HttpError\nfrom apiclient.http import MediaFileUpload\nfrom oauth2client.client import flow_from_clientsecrets\nfrom oauth2client.file import Storage\nfrom oauth2client.tools import argparser, run_flow\n\n# Explicitly tell the underlying HTTP transport library not to retry, since\n# we are handling retry logic ourselves.\nhttplib2.RETRIES = 1\n\n# Maximum number of times to retry before giving up.",
    "last_modified": "2025-09-13T05:53:51.618361"
  },
  {
    "id": "1038",
    "name": "voice.py",
    "path": "02_media_processing/video_tools/voice.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 97,
    "size": 2904,
    "docstring": "",
    "keywords": [
      "web_tools"
    ],
    "functions": [
      "check_ratelimit",
      "sleep_until",
      "sanitize_text"
    ],
    "classes": [],
    "imports": [
      "re",
      "sys",
      "time",
      "datetime",
      "time",
      "cleantext",
      "requests",
      "utils",
      "datetime"
    ],
    "preview": "import re\nimport sys\nimport time as pytime\nfrom datetime import datetime\nfrom time import sleep\n\nfrom cleantext import clean\nfrom requests import Response\n\nfrom utils import settings\n\nif sys.version_info[0] >= 3:\n    from datetime import timezone\n\n\ndef check_ratelimit(response: Response) -> bool:\n    \"\"\"\n    Checks if the response is a ratelimit response.\n    If it is, it sleeps for the time specified in the response.\n    \"\"\"",
    "last_modified": "2025-09-13T05:54:00.571706"
  },
  {
    "id": "1039",
    "name": "api.py",
    "path": "02_media_processing/video_tools/api.py",
    "category": "02_media_processing",
    "type": "web_tools",
    "lines": 39,
    "size": 1166,
    "docstring": "",
    "keywords": [
      "web_tools"
    ],
    "functions": [
      "request",
      "data",
      "helix",
      "top_clips",
      "get"
    ],
    "classes": [],
    "imports": [
      "requests"
    ],
    "preview": "import requests\n\nlocal = locals()\n\n\ndef request(endpoint: str, headers: dict, params: dict) -> requests.Response:\n    return requests.get(\"https://api.twitch.tv/\" + endpoint, headers=headers, params=params)\n\n\ndef data(slug: str, oauth_token: str, client_id: str) -> requests.Response:\n    return request(\n        \"helix/clips\",\n        {\"Authorization\": \"Bearer \" + oauth_token, \"Client-Id\": client_id},\n        {\"id\": slug},\n    )\n\n\ndef helix(category: str, data: list, oauth_token: str, client_id: str) -> requests.Response:\n    return request(\n        \"helix/\" + category,",
    "last_modified": "2025-09-13T05:53:56.104544"
  },
  {
    "id": "1040",
    "name": "subreddit.py",
    "path": "02_media_processing/video_tools/subreddit.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 161,
    "size": 6898,
    "docstring": "",
    "keywords": [],
    "functions": [
      "get_subreddit_threads"
    ],
    "classes": [],
    "imports": [
      "re",
      "praw",
      "praw.models",
      "prawcore.exceptions",
      "utils",
      "utils.ai_methods",
      "utils.console",
      "utils.posttextparser",
      "utils.subreddit",
      "utils.videos"
    ],
    "preview": "import re\n\nimport praw\nfrom praw.models import MoreComments\nfrom prawcore.exceptions import ResponseException\n\nfrom utils import settings\nfrom utils.ai_methods import sort_by_similarity\nfrom utils.console import print_step, print_substep\nfrom utils.posttextparser import posttextparser\nfrom utils.subreddit import get_subreddit_undone\nfrom utils.videos import check_done\nfrom utils.voice import sanitize_text\n\n\ndef get_subreddit_threads(POST_ID: str):\n    \"\"\"\n    Returns a list of threads from the AskReddit subreddit.\n    \"\"\"\n",
    "last_modified": "2025-09-13T05:54:00.009385"
  },
  {
    "id": "1041",
    "name": "NewUpload_20250607131205.py",
    "path": "02_media_processing/video_tools/NewUpload_20250607131205.py",
    "category": "02_media_processing",
    "type": "youtube",
    "lines": 95,
    "size": 4013,
    "docstring": "",
    "keywords": [
      "video_processing",
      "youtube",
      "web_tools"
    ],
    "functions": [
      "get_authenticated_service",
      "upload_video"
    ],
    "classes": [],
    "imports": [
      "os",
      "google.auth.transport.requests",
      "google_auth_oauthlib.flow",
      "googleapiclient.discovery",
      "googleapiclient.errors"
    ],
    "preview": "import os\n\nimport google.auth.transport.requests\nimport google_auth_oauthlib.flow\nimport googleapiclient.discovery\nimport googleapiclient.errors\n\nSCOPES = [\"https://www.googleapis.com/auth/youtube.upload\"]\nCLIENT_SECRETS_FILE = \"/Users/steven/Documents/python/Youtube/client_secrets.json\"  # Replace with your client secrets file\nAPI_SERVICE_NAME = \"youtube\"\nAPI_VERSION = \"v3\"\n\ndef get_authenticated_service():\n    \"\"\"Authenticates and returns the YouTube Data API service.\"\"\"\n    flow = google_auth_oauthlib.flow.InstalledAppFlow.from_client_secrets_file(\n        CLIENT_SECRETS_FILE, SCOPES)\n    credentials = flow.run_local_server(port=0)\n    return googleapiclient.discovery.build(API_SERVICE_NAME, API_VERSION,\n                                            credentials=credentials)\n",
    "last_modified": "2025-09-06T12:24:11.154325"
  },
  {
    "id": "1042",
    "name": "bot_video.py",
    "path": "02_media_processing/video_tools/bot_video.py",
    "category": "02_media_processing",
    "type": "youtube",
    "lines": 43,
    "size": 1779,
    "docstring": "",
    "keywords": [
      "youtube"
    ],
    "functions": [
      "upload_video",
      "download_video"
    ],
    "classes": [],
    "imports": [
      "os"
    ],
    "preview": "import os\n\n\ndef upload_video(self, video, caption=\"\", thumbnail=None, options={}):\n    \"\"\"Upload video to Instagram\n\n    @param video      Path to video file (String)\n    @param caption    Media description (String)\n    @param thumbnail  Path to thumbnail for video (String). When None, then\n                      thumbnail is generate automatically\n    @param options    Object with difference options, e.g. configure_timeout,\n                      rename_thumbnail, rename (Dict)\n                      Designed to reduce the number of function arguments!\n\n    @return           Object with state of uploading to Instagram (or False)\n    \"\"\"\n    self.small_delay()\n    self.logger.info(\"Started uploading '{video}'\".format(video=video))\n    result = self.api.upload_video(video, caption=caption, thumbnail=thumbnail, options=options)\n    if not result:",
    "last_modified": "2025-09-13T05:54:58.124236"
  },
  {
    "id": "1043",
    "name": "dynamic.py",
    "path": "02_media_processing/video_tools/dynamic.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 118,
    "size": 3475,
    "docstring": "",
    "keywords": [],
    "functions": [
      "run_command",
      "dynamic_progress_bar",
      "log",
      "update_pip3",
      "update_brew",
      "update_all"
    ],
    "classes": [],
    "imports": [
      "os",
      "shutil",
      "subprocess",
      "time",
      "datetime"
    ],
    "preview": "import os\nimport shutil\nimport subprocess\nimport time\nfrom datetime import datetime\n\n\n# Function to execute a system command and print its output\ndef run_command(command):\n    process = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=True)\n    stdout, stderr = process.communicate()\n\n    if stdout:\n        print(stdout.decode())\n    if stderr:\n        print(stderr.decode())\n\n\n# Define the log directory and create it if it does not exist\nlog_dir = \"/Users/Steven/Documents/updateLog\"",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1044",
    "name": "youtube 4.py",
    "path": "02_media_processing/video_tools/youtube 4.py",
    "category": "02_media_processing",
    "type": "youtube",
    "lines": 70,
    "size": 2248,
    "docstring": "",
    "keywords": [],
    "functions": [
      "publish"
    ],
    "classes": [],
    "imports": [
      "argparse",
      "logging",
      "argparse",
      "datetime",
      "login",
      "selenium",
      "selenium.webdriver.common.desired_capabilities",
      "selenium.webdriver.firefox.options",
      "selenium.webdriver.remote.file_detector",
      "upload"
    ],
    "preview": "import argparse\nimport logging\nfrom argparse import ArgumentError\nfrom datetime import datetime\n\nfrom login import confirm_logged_in, login_using_cookie_file\nfrom selenium import webdriver\nfrom selenium.webdriver.common.desired_capabilities import DesiredCapabilities\nfrom selenium.webdriver.firefox.options import Options\nfrom selenium.webdriver.remote.file_detector import LocalFileDetector\n\nfrom upload import upload_file\n\nlogin_cookies = \"cookies.json\"\n\nlogging.basicConfig(\n    format=\"%(asctime)s %(levelname)-8s %(message)s\",\n    level=logging.INFO,\n    datefmt=\"%Y-%m-%d %H:%M:%S\",\n    handlers=[logging.FileHandler(\"debug.log\"), logging.StreamHandler()],",
    "last_modified": "2025-09-11T13:26:57.296323"
  },
  {
    "id": "1045",
    "name": "ytube.py",
    "path": "02_media_processing/video_tools/ytube.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 63,
    "size": 2260,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "googleapiclient.discovery",
      "csv"
    ],
    "preview": "from googleapiclient.discovery import build\n\n# Set up YouTube Data API\nyoutube = build(\"youtube\", \"v3\", developerKey=\"AIzaSyC08MXHwy-tkAwAhvW0TumdKJmSfOJYFqw\")\n\n# Retrieve channel's videos\nvideos = []\nnext_page_token = None\nwhile True:\n    request = youtube.search().list(\n        part=\"snippet\",\n        channelId=\"UCDl7VmS3gD2BQBVZUlL21-A\",\n        maxResults=50,  # Max allowed value\n        pageToken=next_page_token,\n    )\n    response = request.execute()\n    videos.extend(response[\"items\"])\n    next_page_token = response.get(\"nextPageToken\")\n    if not next_page_token:\n        break",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1046",
    "name": "youtube_download.py",
    "path": "02_media_processing/video_tools/youtube_download.py",
    "category": "02_media_processing",
    "type": "youtube",
    "lines": 53,
    "size": 1213,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "json",
      "os",
      "sys",
      "urllib",
      "datetime",
      "subprocess"
    ],
    "preview": "#!/usr/bin/python\n\"\"\"\nYouTube Channel Downloader\n\"\"\"\n \nimport json\nimport os\nimport sys\nimport urllib\nfrom datetime import date\nfrom subprocess import call\n\nprint \"--------------------------\"\nprint \"YouTube Channel Downloader\"\nprint \"--------------------------\"\nprint \"\"\nwhile True:\n    try:\n        author = raw_input(\"Enter username of YouTube channel: \")\n        break",
    "last_modified": "2025-05-04T23:28:20"
  },
  {
    "id": "1047",
    "name": "bot_follow.py",
    "path": "02_media_processing/video_tools/bot_follow.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 182,
    "size": 7285,
    "docstring": "",
    "keywords": [],
    "functions": [
      "follow",
      "follow_users",
      "follow_followers",
      "follow_following",
      "approve_pending_follow_requests",
      "reject_pending_follow_requests"
    ],
    "classes": [],
    "imports": [
      "time",
      "tqdm"
    ],
    "preview": "import time\n\nfrom tqdm import tqdm\n\n\ndef follow(self, user_id, check_user):\n    user_id = self.convert_to_user_id(user_id)\n    if self.log_follow_unfollow:\n        msg = \"Going to follow `user_id` {}.\".format(user_id)\n        self.logger.info(msg)\n    else:\n        msg = \" ===> Going to follow `user_id`: {}.\".format(user_id)\n        self.console_print(msg)\n    if check_user and not self.check_user(user_id):\n        return False\n    if not self.reached_limit(\"follows\"):\n        if self.blocked_actions[\"follows\"]:\n            self.logger.warning(\"YOUR `FOLLOW` ACTION IS BLOCKED\")\n            if self.blocked_actions_protection:\n                self.logger.warning(",
    "last_modified": "2025-09-13T05:54:57.658879"
  },
  {
    "id": "1048",
    "name": "proxy_check.py",
    "path": "02_media_processing/video_tools/proxy_check.py",
    "category": "02_media_processing",
    "type": "organization",
    "lines": 246,
    "size": 6734,
    "docstring": "MIT License\n\nCopyright (c) 2021-2022 MShawon\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.",
    "keywords": [
      "organization",
      "web_tools"
    ],
    "functions": [
      "backup",
      "clean_exe_temp",
      "load_proxy",
      "main_checker",
      "proxy_check",
      "main"
    ],
    "classes": [
      "bcolors"
    ],
    "imports": [
      "os",
      "shutil",
      "sys",
      "concurrent.futures",
      "glob",
      "time",
      "requests",
      "fake_headers"
    ],
    "preview": "\"\"\"\nMIT License\n\nCopyright (c) 2021-2022 MShawon\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,",
    "last_modified": "2025-09-13T05:54:12.640931"
  },
  {
    "id": "1049",
    "name": "NewUpload_20250607130440.py",
    "path": "02_media_processing/video_tools/NewUpload_20250607130440.py",
    "category": "02_media_processing",
    "type": "youtube",
    "lines": 78,
    "size": 2697,
    "docstring": "",
    "keywords": [
      "youtube",
      "web_tools"
    ],
    "functions": [
      "get_authenticated_service",
      "upload_video"
    ],
    "classes": [],
    "imports": [
      "os",
      "google.auth.transport.requests",
      "google_auth_oauthlib.flow",
      "googleapiclient.discovery",
      "googleapiclient.errors"
    ],
    "preview": "import os\n\nimport google.auth.transport.requests\nimport google_auth_oauthlib.flow\nimport googleapiclient.discovery\nimport googleapiclient.errors\n\nSCOPES = [\"https://www.googleapis.com/auth/youtube.upload\"]\nCLIENT_SECRETS_FILE = \"/Users/steven/Documents/python/Youtube/client_secrets.json\"  # Replace with your client secrets file\nAPI_SERVICE_NAME = \"youtube\"\nAPI_VERSION = \"v3\"\n\n\ndef get_authenticated_service():\n    \"\"\"Authenticates and returns the YouTube Data API service.\"\"\"\n    flow = google_auth_oauthlib.flow.InstalledAppFlow.from_client_secrets_file(\n        CLIENT_SECRETS_FILE, SCOPES\n    )\n    credentials = flow.run_local_server(port=0)\n    return googleapiclient.discovery.build(API_SERVICE_NAME, API_VERSION, credentials=credentials)",
    "last_modified": "2025-09-13T05:54:11.893226"
  },
  {
    "id": "1050",
    "name": "yt.py",
    "path": "02_media_processing/video_tools/yt.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 130,
    "size": 3726,
    "docstring": "",
    "keywords": [
      "web_tools"
    ],
    "functions": [
      "get_authenticated_service",
      "print_response",
      "build_resource",
      "remove_empty_kwargs",
      "comment_threads_insert",
      "scrape"
    ],
    "classes": [],
    "imports": [
      "os",
      "random",
      "google.oauth2.credentials",
      "google_auth_oauthlib.flow",
      "requests",
      "bs4",
      "google_auth_oauthlib.flow",
      "googleapiclient.discovery",
      "googleapiclient.errors"
    ],
    "preview": "import os\nimport random\n\nimport google.oauth2.credentials\nimport google_auth_oauthlib.flow\nimport requests\nfrom bs4 import BeautifulSoup\nfrom google_auth_oauthlib.flow import InstalledAppFlow\nfrom googleapiclient.discovery import build\nfrom googleapiclient.errors import HttpError\n\nCLIENT_SECRETS_FILE = \"client_secret.json\"\n\nSCOPES = [\"https://www.googleapis.com/auth/youtube.force-ssl\"]\nAPI_SERVICE_NAME = \"youtube\"\nAPI_VERSION = \"v3\"\n\n\ndef get_authenticated_service():\n    flow = InstalledAppFlow.from_client_secrets_file(CLIENT_SECRETS_FILE, SCOPES)",
    "last_modified": "2025-05-04T23:28:21.027324"
  },
  {
    "id": "1051",
    "name": "search_scope.py",
    "path": "02_media_processing/video_tools/search_scope.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 127,
    "size": 4581,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "create",
      "__init__",
      "get_formatted_locations",
      "get_index_urls_locations",
      "mkurl_pypi_url"
    ],
    "classes": [
      "SearchScope"
    ],
    "imports": [
      "itertools",
      "logging",
      "os",
      "posixpath",
      "urllib.parse",
      "typing",
      "pip._internal.models.index",
      "pip._internal.utils.compat",
      "pip._internal.utils.misc",
      "pip._vendor.packaging.utils"
    ],
    "preview": "import itertools\nimport logging\nimport os\nimport posixpath\nimport urllib.parse\nfrom typing import List\n\nfrom pip._internal.models.index import PyPI\nfrom pip._internal.utils.compat import has_tls\nfrom pip._internal.utils.misc import normalize_path, redact_auth_from_url\nfrom pip._vendor.packaging.utils import canonicalize_name\n\nlogger = logging.getLogger(__name__)\n\n\nclass SearchScope:\n    \"\"\"\n    Encapsulates the locations that pip is configured to search.\n    \"\"\"\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1052",
    "name": "example.py",
    "path": "02_media_processing/video_tools/example.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 11,
    "size": 316,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "twitchtube.video"
    ],
    "preview": "from twitchtube.video import make_video\n\nmake_video(\n    data=[\"c xQcOW\", \"game Just Chatting\"],\n    client_id=\"1hq8ektpki36w5kn37mluioungyqjo\",  # example client id (fake)\n    oauth_token=\"9f5einm9qtp0bj4m9l1ykevpwdn98o\",  # example token (fake)\n    video_length=10.5,\n    resolution=(1080, 1920),\n    frames=60,\n)\n",
    "last_modified": "2025-03-28T18:37:10.655899"
  },
  {
    "id": "1053",
    "name": "NewUpload_20250607131235.py",
    "path": "02_media_processing/video_tools/NewUpload_20250607131235.py",
    "category": "02_media_processing",
    "type": "youtube",
    "lines": 78,
    "size": 3078,
    "docstring": "",
    "keywords": [
      "youtube",
      "web_tools"
    ],
    "functions": [
      "get_authenticated_service",
      "upload_video"
    ],
    "classes": [],
    "imports": [
      "os",
      "google.auth.transport.requests",
      "google_auth_oauthlib.flow",
      "googleapiclient.discovery",
      "googleapiclient.errors"
    ],
    "preview": "import os\n\nimport google.auth.transport.requests\nimport google_auth_oauthlib.flow\nimport googleapiclient.discovery\nimport googleapiclient.errors\n\nSCOPES = [\"https://www.googleapis.com/auth/youtube.upload\"]\nCLIENT_SECRETS_FILE = \"/Users/steven/Documents/python/Youtube/client_secrets.json\"  # Replace with your client secrets file\nAPI_SERVICE_NAME = \"youtube\"\nAPI_VERSION = \"v3\"\n\n\ndef get_authenticated_service():\n    \"\"\"Authenticates and returns the YouTube Data API service.\"\"\"\n    flow = google_auth_oauthlib.flow.InstalledAppFlow.from_client_secrets_file(\n        CLIENT_SECRETS_FILE, SCOPES\n    )\n    credentials = flow.run_local_server(port=0)\n    return googleapiclient.discovery.build(API_SERVICE_NAME, API_VERSION, credentials=credentials)",
    "last_modified": "2025-09-13T05:54:12.145314"
  },
  {
    "id": "1054",
    "name": "yt_studio.py",
    "path": "02_media_processing/video_tools/yt_studio.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 90,
    "size": 3177,
    "docstring": "",
    "keywords": [
      "youtube"
    ],
    "functions": [
      "main",
      "__init__",
      "get_authenticated_service",
      "upload_video"
    ],
    "classes": [
      "YouTubeUploader"
    ],
    "imports": [
      "logging",
      "datetime",
      "google.oauth2.credentials",
      "googleapiclient.discovery",
      "googleapiclient.http",
      "utilities.const"
    ],
    "preview": "import logging\nfrom datetime import datetime, timedelta\n\nimport google.oauth2.credentials\nfrom googleapiclient.discovery import build\nfrom googleapiclient.http import MediaFileUpload\nfrom utilities.const import CHANNEL_ID, LOG_PATH, SCOPES, YT_SECRET_FILE\n\n# Configure logging\n\nlogging.basicConfig(level=logging.INFO, format=\"[%(levelname)s] %(message)s\", filename=LOG_PATH)\n\n\nclass YouTubeUploader:\n    def __init__(self, video_file, channel_id, _YT_SECRET_FILE, _SCOPES):\n        self.video_file = video_file\n        self.channel_id = channel_id\n        self.CLIENT_SECRET_FILE = _YT_SECRET_FILE\n        self.SCOPES = SCOPES\n",
    "last_modified": "2025-09-13T05:53:28.865830"
  },
  {
    "id": "1055",
    "name": "NewUpload_20250607131201.py",
    "path": "02_media_processing/video_tools/NewUpload_20250607131201.py",
    "category": "02_media_processing",
    "type": "youtube",
    "lines": 95,
    "size": 4012,
    "docstring": "",
    "keywords": [
      "video_processing",
      "youtube",
      "web_tools"
    ],
    "functions": [
      "get_authenticated_service",
      "upload_video"
    ],
    "classes": [],
    "imports": [
      "os",
      "google.auth.transport.requests",
      "google_auth_oauthlib.flow",
      "googleapiclient.discovery",
      "googleapiclient.errors"
    ],
    "preview": "import os\n\nimport google.auth.transport.requests\nimport google_auth_oauthlib.flow\nimport googleapiclient.discovery\nimport googleapiclient.errors\n\nSCOPES = [\"https://www.googleapis.com/auth/youtube.upload\"]\nCLIENT_SECRETS_FILE = \"/Users/steven/Documents/python/Youtube/client_secrets.json\"  # Replace with your client secrets file\nAPI_SERVICE_NAME = \"youtube\"\nAPI_VERSION = \"v3\"\n\ndef get_authenticated_service():\n    \"\"\"Authenticates and returns the YouTube Data API service.\"\"\"\n    flow = google_auth_oauthlib.flow.InstalledAppFlow.from_client_secrets_file(\n        CLIENT_SECRETS_FILE, SCOPES)\n    credentials = flow.run_local_server(port=0)\n    return googleapiclient.discovery.build(API_SERVICE_NAME, API_VERSION,\n                                            credentials=credentials)\n",
    "last_modified": "2025-09-06T12:24:11.162545"
  },
  {
    "id": "1056",
    "name": "debug.py",
    "path": "02_media_processing/video_tools/debug.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 195,
    "size": 6708,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "show_value",
      "show_sys_implementation",
      "create_vendor_txt_map",
      "get_module_from_module_name",
      "get_vendor_version_from_module",
      "show_actual_vendor_versions",
      "show_vendor_versions",
      "show_tags",
      "ca_bundle_info",
      "add_options"
    ],
    "classes": [
      "DebugCommand"
    ],
    "imports": [
      "importlib.resources",
      "locale",
      "logging",
      "os",
      "sys",
      "optparse",
      "types",
      "typing",
      "pip._vendor",
      "pip._internal.cli"
    ],
    "preview": "import importlib.resources\nimport locale\nimport logging\nimport os\nimport sys\nfrom optparse import Values\nfrom types import ModuleType\nfrom typing import Any, Dict, List, Optional\n\nimport pip._vendor\nfrom pip._internal.cli import cmdoptions\nfrom pip._internal.cli.base_command import Command\nfrom pip._internal.cli.cmdoptions import make_target_python\nfrom pip._internal.cli.status_codes import SUCCESS\nfrom pip._internal.configuration import Configuration\nfrom pip._internal.metadata import get_environment\nfrom pip._internal.utils.logging import indent_log\nfrom pip._internal.utils.misc import get_pip_version\nfrom pip._vendor.certifi import where\nfrom pip._vendor.packaging.version import parse as parse_version",
    "last_modified": "2025-09-13T05:54:25.400175"
  },
  {
    "id": "1057",
    "name": "entrypoints.py",
    "path": "02_media_processing/video_tools/entrypoints.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 84,
    "size": 3056,
    "docstring": "",
    "keywords": [],
    "functions": [
      "_wrapper",
      "get_best_invocation_for_this_pip",
      "get_best_invocation_for_this_python"
    ],
    "classes": [],
    "imports": [
      "itertools",
      "os",
      "shutil",
      "sys",
      "typing",
      "pip._internal.cli.main",
      "pip._internal.utils.compat"
    ],
    "preview": "import itertools\nimport os\nimport shutil\nimport sys\nfrom typing import List, Optional\n\nfrom pip._internal.cli.main import main\nfrom pip._internal.utils.compat import WINDOWS\n\n_EXECUTABLE_NAMES = [\n    \"pip\",\n    f\"pip{sys.version_info.major}\",\n    f\"pip{sys.version_info.major}.{sys.version_info.minor}\",\n]\nif WINDOWS:\n    _allowed_extensions = {\"\", \".exe\"}\n    _EXECUTABLE_NAMES = [\n        \"\".join(parts) for parts in itertools.product(_EXECUTABLE_NAMES, _allowed_extensions)\n    ]\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1058",
    "name": "ytdlconfiguration 2.py",
    "path": "02_media_processing/video_tools/ytdlconfiguration 2.py",
    "category": "02_media_processing",
    "type": "setup",
    "lines": 83,
    "size": 3383,
    "docstring": "Config",
    "keywords": [],
    "functions": [
      "__init__",
      "load",
      "is_valid"
    ],
    "classes": [
      "Ytdlconfiguration"
    ],
    "imports": [
      "configparser",
      "os.path",
      "ytdl.oshelper"
    ],
    "preview": "\"Config\"\n\nimport configparser\nfrom os.path import exists, expanduser\n\nfrom ytdl.oshelper import join_paths, mkdir\n\n\nclass Ytdlconfiguration(object):\n    \"Ytdl Configuration\"\n\n    def __init__(self):\n        self.__home_path__ = expanduser(\"~\")\n        self._ytdl_home_path_ = join_paths(self.__home_path__, \".ytdl\")\n        self.listener_time_file_path = join_paths(self._ytdl_home_path_, \"listener-timestamp.txt\")\n        self.config_file_path = join_paths(self._ytdl_home_path_, \"config.ini\")\n        self.download_folder = join_paths(self._ytdl_home_path_, \"downloads\")\n        self.log_folder = join_paths(self._ytdl_home_path_, \"logs\")\n\n        self.googleplay_credential_file = \"\"",
    "last_modified": "2025-09-13T05:54:11.201560"
  },
  {
    "id": "1059",
    "name": "upload_video.py",
    "path": "02_media_processing/video_tools/upload_video.py",
    "category": "02_media_processing",
    "type": "youtube",
    "lines": 208,
    "size": 7655,
    "docstring": "",
    "keywords": [],
    "functions": [
      "get_authenticated_service",
      "initialize_upload",
      "resumable_upload",
      "simple_upload"
    ],
    "classes": [],
    "imports": [
      "os",
      "random",
      "sys",
      "time",
      "httplib2",
      "apiclient.discovery",
      "apiclient.errors",
      "apiclient.http",
      "oauth2client.client",
      "oauth2client.file"
    ],
    "preview": "#!/usr/bin/python\n\nimport os\nimport random\nimport sys\nimport time\n\nimport httplib2\nfrom apiclient.discovery import build\nfrom apiclient.errors import HttpError\nfrom apiclient.http import MediaFileUpload\nfrom oauth2client.client import flow_from_clientsecrets\nfrom oauth2client.file import Storage\nfrom oauth2client.tools import argparser, run_flow\n\n# Explicitly tell the underlying HTTP transport library not to retry, since\n# we are handling retry logic ourselves.\nhttplib2.RETRIES = 1\n\n# Maximum number of times to retry before giving up.",
    "last_modified": "2025-09-13T05:53:42.902193"
  },
  {
    "id": "1060",
    "name": "bbcode.py",
    "path": "02_media_processing/video_tools/bbcode.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 109,
    "size": 3294,
    "docstring": "pygments.formatters.bbcode\n~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nBBcode formatter.\n\n:copyright: Copyright 2006-2023 by the Pygments team, see AUTHORS.\n:license: BSD, see LICENSE for details.",
    "keywords": [],
    "functions": [
      "__init__",
      "_make_styles",
      "format_unencoded"
    ],
    "classes": [
      "BBCodeFormatter"
    ],
    "imports": [
      "pip._vendor.pygments.formatter",
      "pip._vendor.pygments.util"
    ],
    "preview": "\"\"\"\npygments.formatters.bbcode\n~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nBBcode formatter.\n\n:copyright: Copyright 2006-2023 by the Pygments team, see AUTHORS.\n:license: BSD, see LICENSE for details.\n\"\"\"\n\nfrom pip._vendor.pygments.formatter import Formatter\nfrom pip._vendor.pygments.util import get_bool_opt\n\n__all__ = [\"BBCodeFormatter\"]\n\n\nclass BBCodeFormatter(Formatter):\n    \"\"\"\n    Format tokens with BBcodes. These formatting codes are used by many\n    bulletin boards, so you can highlight your sourcecode with pygments before",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1061",
    "name": "askreddit.py",
    "path": "02_media_processing/video_tools/askreddit.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 49,
    "size": 1430,
    "docstring": "",
    "keywords": [],
    "functions": [
      "get_askreddit_threads"
    ],
    "classes": [],
    "imports": [
      "os",
      "random",
      "praw",
      "dotenv",
      "utils.console"
    ],
    "preview": "import os\nimport random\n\nimport praw\nfrom dotenv import load_dotenv\n\nfrom utils.console import print_markdown, print_step, print_substep\n\n\ndef get_askreddit_threads():\n    \"\"\"\n    Returns a list of threads from the AskReddit subreddit.\n    \"\"\"\n\n    print_step(\"Getting AskReddit threads...\")\n\n    content = {}\n    load_dotenv()\n    reddit = praw.Reddit(\n        client_id=os.getenv(\"REDDIT_CLIENT_ID\"),",
    "last_modified": "2025-09-11T13:27:05.507412"
  },
  {
    "id": "1062",
    "name": "main_parser.py",
    "path": "02_media_processing/video_tools/main_parser.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 131,
    "size": 4299,
    "docstring": "A single place for constructing and exposing the main parser",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "create_main_parser",
      "identify_python_interpreter",
      "parse_command"
    ],
    "classes": [],
    "imports": [
      "os",
      "subprocess",
      "sys",
      "typing",
      "pip._internal.build_env",
      "pip._internal.cli",
      "pip._internal.cli.parser",
      "pip._internal.commands",
      "pip._internal.exceptions",
      "pip._internal.utils.misc"
    ],
    "preview": "\"\"\"A single place for constructing and exposing the main parser\"\"\"\n\nimport os\nimport subprocess\nimport sys\nfrom typing import List, Optional, Tuple\n\nfrom pip._internal.build_env import get_runnable_pip\nfrom pip._internal.cli import cmdoptions\nfrom pip._internal.cli.parser import ConfigOptionParser, UpdatingDefaultsHelpFormatter\nfrom pip._internal.commands import commands_dict, get_similar_commands\nfrom pip._internal.exceptions import CommandError\nfrom pip._internal.utils.misc import get_pip_version, get_prog\n\n__all__ = [\"create_main_parser\", \"parse_command\"]\n\n\ndef create_main_parser() -> ConfigOptionParser:\n    \"\"\"Creates and returns the main parser for pip's CLI\"\"\"\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1063",
    "name": "download_user_videos.py",
    "path": "02_media_processing/video_tools/download_user_videos.py",
    "category": "02_media_processing",
    "type": "youtube",
    "lines": 49,
    "size": 1269,
    "docstring": "Download a user videos:\nThis script could be very useful to download a users videos.\n\nDependencies:\n    pip install -U instabot\nRun:\n  python download_user_videos.py -u username -p password -user user\n\nNotes:\n    You can change file and add there your comments.\n\nDeveloped by:\n    Steffan Jensen\n    http://www.instabotai.com",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "argparse",
      "os",
      "sys",
      "instabot"
    ],
    "preview": "\"\"\"\nDownload a user videos:\nThis script could be very useful to download a users videos.\n\nDependencies:\n    pip install -U instabot\nRun:\n  python download_user_videos.py -u username -p password -user user\n\nNotes:\n    You can change file and add there your comments.\n\nDeveloped by:\n    Steffan Jensen\n    http://www.instabotai.com\n\"\"\"\n\nimport argparse\nimport os\nimport sys",
    "last_modified": "2025-05-04T23:28:20"
  },
  {
    "id": "1064",
    "name": "youtube2.py",
    "path": "02_media_processing/video_tools/youtube2.py",
    "category": "02_media_processing",
    "type": "youtube",
    "lines": 88,
    "size": 2683,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "googleapiclient.discovery",
      "csv"
    ],
    "preview": "from googleapiclient.discovery import build\n\n# Set up YouTube Data API\nyoutube = build(\"youtube\", \"v3\", developerKey=\"AIzaSyC08MXHwy-tkAwAhvW0TumdKJmSfOJYFqw\")\n\n# Retrieve channel's videos\nvideos = []\nnext_page_token = None\nwhile True:\n    request = youtube.search().list(\n        part=\"snippet\",\n        channelId=\"UCDl7VmS3gD2BQBVZUlL21-A\",\n        maxResults=550,  # Max allowed value\n        pageToken=next_page_token,\n    )\n    response = request.execute()\n    videos.extend(response[\"items\"])\n    next_page_token = response.get(\"nextPageToken\")\n    if not next_page_token:\n        break",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1065",
    "name": "video-downloader.py",
    "path": "02_media_processing/video_tools/video-downloader.py",
    "category": "02_media_processing",
    "type": "youtube",
    "lines": 26,
    "size": 722,
    "docstring": "This script downloads videos from pexels.com. Uses this to get videos background\nin case you need more.",
    "keywords": [
      "web_tools"
    ],
    "functions": [
      "getVideo"
    ],
    "classes": [],
    "imports": [
      "requests",
      "pexelsPy",
      "config"
    ],
    "preview": "\"\"\"\nThis script downloads videos from pexels.com. Uses this to get videos background\nin case you need more.\n\"\"\"\n\nimport requests\nfrom pexelsPy import API\n\nimport config\n\nPEXEL_API_KEY = config.pexelKey\n\napi = API(PEXEL_API_KEY)\n\n\ndef getVideo(num):\n    api.search_videos(\"drone shot of the sea\", orientation=\"portrait\", page=1, results_per_page=num)\n    videos = api.get_videos()\n\n    for video in videos:",
    "last_modified": "2025-09-13T05:53:29.707716"
  },
  {
    "id": "1066",
    "name": "dedupe_python.py",
    "path": "02_media_processing/video_tools/dedupe_python.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 42,
    "size": 1702,
    "docstring": "",
    "keywords": [],
    "functions": [
      "similarity",
      "find_similar_files",
      "keep_latest_file"
    ],
    "classes": [],
    "imports": [
      "os",
      "difflib",
      "pathlib"
    ],
    "preview": "import os\nfrom difflib import SequenceMatcher\nfrom pathlib import Path\n\ndef similarity(a, b):\n    return SequenceMatcher(None, a, b).ratio()\n\ndef find_similar_files(directory, name_threshold=0.9, content_threshold=0.8):\n    py_files = [f for f in Path(directory).rglob(\"*.py\")]\n    similar_files = []\n    for i, file1 in enumerate(py_files):\n        for file2 in py_files[i + 1:]:\n            name_sim = similarity(file1.stem.lower(), file2.stem.lower())\n            if name_sim > name_threshold:\n                with open(file1, 'r', encoding='utf-8') as f1, open(file2, 'r', encoding='utf-8') as f2:\n                    content1 = f1.read()\n                    content2 = f2.read()\n                    content_sim = similarity(content1, content2)\n                    if content_sim > content_threshold:\n                        similar_files.append((str(file1), str(file2), name_sim, content_sim))",
    "last_modified": "2025-10-08T06:38:21"
  },
  {
    "id": "1067",
    "name": "NewUpload_20250607131149.py",
    "path": "02_media_processing/video_tools/NewUpload_20250607131149.py",
    "category": "02_media_processing",
    "type": "youtube",
    "lines": 113,
    "size": 5445,
    "docstring": "",
    "keywords": [
      "youtube",
      "web_tools"
    ],
    "functions": [
      "get_authenticated_service",
      "upload_video"
    ],
    "classes": [],
    "imports": [
      "os",
      "google.auth.transport.requests",
      "google_auth_oauthlib.flow",
      "googleapiclient.discovery",
      "googleapiclient.errors"
    ],
    "preview": "import os\n\nimport google.auth.transport.requests\nimport google_auth_oauthlib.flow\nimport googleapiclient.discovery\nimport googleapiclient.errors\n\n# --- Analysis of the code structure and logic ---\n\n# 1. Imports:\n#    - The script imports necessary modules for Google API client, authentication, and OS operations.\n#    - It assumes the googleapiclient and google_auth_oauthlib libraries are installed.\n\n# 2. Constants:\n#    - SCOPES: Defines the OAuth scope for uploading to YouTube.\n#    - CLIENT_SECRETS_FILE: Path to the OAuth client secrets JSON file.\n#    - API_SERVICE_NAME and API_VERSION: Used to build the YouTube API client.\n\n\n# 3. Authentication Function:",
    "last_modified": "2025-09-13T05:54:12.042617"
  },
  {
    "id": "1068",
    "name": "download_driver.py",
    "path": "02_media_processing/video_tools/download_driver.py",
    "category": "02_media_processing",
    "type": "youtube",
    "lines": 155,
    "size": 5109,
    "docstring": "MIT License\n\nCopyright (c) 2021-2022 MShawon\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.",
    "keywords": [
      "youtube"
    ],
    "functions": [
      "download_driver",
      "copy_drivers"
    ],
    "classes": [],
    "imports": [
      "platform",
      "shutil",
      "subprocess",
      "sys",
      "undetected_chromedriver._compat",
      "colors"
    ],
    "preview": "\"\"\"\nMIT License\n\nCopyright (c) 2021-2022 MShawon\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,",
    "last_modified": "2025-09-13T05:54:11.690616"
  },
  {
    "id": "1069",
    "name": "login.py",
    "path": "02_media_processing/video_tools/login.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 51,
    "size": 1926,
    "docstring": "",
    "keywords": [],
    "functions": [
      "domain_to_url",
      "login_using_cookie_file",
      "confirm_logged_in"
    ],
    "classes": [],
    "imports": [
      "json",
      "typing",
      "selenium.webdriver.common.by",
      "selenium.webdriver.remote.webdriver",
      "selenium.webdriver.support",
      "selenium.webdriver.support.ui"
    ],
    "preview": "import json\nfrom typing import Dict, List\n\nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.remote.webdriver import WebDriver\nfrom selenium.webdriver.support import expected_conditions as EC\nfrom selenium.webdriver.support.ui import WebDriverWait\n\n\"\"\" Login module \"\"\"\n\n\ndef domain_to_url(domain: str) -> str:\n    \"\"\"Converts a (partial) domain to valid URL\"\"\"\n    if domain.startswith(\".\"):\n        domain = \"www\" + domain\n    return \"http://\" + domain\n\n\ndef login_using_cookie_file(driver: WebDriver, cookie_file: str):\n    \"\"\"Restore auth cookies from a file. Does not guarantee that the user is logged in afterwards.",
    "last_modified": "2025-09-13T05:53:29.286205"
  },
  {
    "id": "1070",
    "name": "SendNotification.py",
    "path": "02_media_processing/video_tools/SendNotification.py",
    "category": "02_media_processing",
    "type": "youtube",
    "lines": 30,
    "size": 684,
    "docstring": "4. Send tweet when video is published",
    "keywords": [],
    "functions": [
      "send_tweet",
      "OAuth"
    ],
    "classes": [],
    "imports": [
      "tweepy"
    ],
    "preview": "\"\"\"\n4. Send tweet when video is published\n\"\"\"\n\nimport tweepy\n\nconsumer_key = \"consumer key\"\nconsumer_secret = \"consumer secret key\"\naccess_token = \"access token\"\naccess_token_secret = \"access token secret\"\n\n\ndef send_tweet(video_title, video_desc):\n    def OAuth():\n        try:\n            auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n            auth.set_access_token(access_token, access_token_secret)\n        except Exception as e:\n            return None\n",
    "last_modified": "2025-03-28T18:35:49"
  },
  {
    "id": "1071",
    "name": "NewUpload_20250607124913.py",
    "path": "02_media_processing/video_tools/NewUpload_20250607124913.py",
    "category": "02_media_processing",
    "type": "youtube",
    "lines": 78,
    "size": 2579,
    "docstring": "",
    "keywords": [
      "youtube",
      "web_tools"
    ],
    "functions": [
      "get_authenticated_service",
      "upload_video"
    ],
    "classes": [],
    "imports": [
      "os",
      "google.auth.transport.requests",
      "google_auth_oauthlib.flow",
      "googleapiclient.discovery",
      "googleapiclient.errors"
    ],
    "preview": "import os\n\nimport google.auth.transport.requests\nimport google_auth_oauthlib.flow\nimport googleapiclient.discovery\nimport googleapiclient.errors\n\nSCOPES = [\"https://www.googleapis.com/auth/youtube.upload\"]\nCLIENT_SECRETS_FILE = \"client_secrets.json\"  # Replace with your client secrets file\nAPI_SERVICE_NAME = \"youtube\"\nAPI_VERSION = \"v3\"\n\n\ndef get_authenticated_service():\n    \"\"\"Authenticates and returns the YouTube Data API service.\"\"\"\n    flow = google_auth_oauthlib.flow.InstalledAppFlow.from_client_secrets_file(\n        CLIENT_SECRETS_FILE, SCOPES\n    )\n    credentials = flow.run_local_server(port=0)\n    return googleapiclient.discovery.build(API_SERVICE_NAME, API_VERSION, credentials=credentials)",
    "last_modified": "2025-09-13T05:54:11.787231"
  },
  {
    "id": "1072",
    "name": "bot_unfollow.py",
    "path": "02_media_processing/video_tools/bot_unfollow.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 120,
    "size": 4880,
    "docstring": "",
    "keywords": [],
    "functions": [
      "unfollow",
      "unfollow_users",
      "unfollow_non_followers",
      "unfollow_everyone"
    ],
    "classes": [],
    "imports": [
      "time",
      "tqdm"
    ],
    "preview": "import time\n\nfrom tqdm import tqdm\n\n\ndef unfollow(self, user_id):\n    user_id = self.convert_to_user_id(user_id)\n    user_info = self.get_user_info(user_id)\n\n    if not user_info:\n        self.logger.info(\"Can't get user_id=%s info\" % str(user_id))\n        return False  # No user_info\n\n    username = user_info.get(\"username\")\n\n    if self.log_follow_unfollow:\n        msg = \"Going to unfollow `user_id` {} with username {}.\".format(user_id, username)\n        self.logger.info(msg)\n    else:\n        self.console_print(",
    "last_modified": "2025-09-13T05:54:58.068242"
  },
  {
    "id": "1073",
    "name": "scrape-youtube-channel-videos-url.py",
    "path": "02_media_processing/video_tools/scrape-youtube-channel-videos-url.py",
    "category": "02_media_processing",
    "type": "youtube",
    "lines": 54,
    "size": 1611,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "datetime",
      "sys",
      "time",
      "unittest",
      "urllib.error",
      "urllib.parse",
      "urllib.request",
      "selenium",
      "selenium.common.exceptions",
      "selenium.webdriver.common.by"
    ],
    "preview": "# scrape-youtube-channel-videos-url.py\n# _*_coding: utf-8_*_\n\nimport datetime\nimport sys\nimport time\nimport unittest\nimport urllib.error\nimport urllib.parse\nimport urllib.request\n\nfrom selenium import webdriver\nfrom selenium.common.exceptions import InvalidArgumentException\nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.support import expected_conditions as EC\nfrom selenium.webdriver.support.ui import WebDriverWait\n\nurl = sys.argv[1]\nchannelid = url.split(\"/\")[4]\n# driver = webdriver.Firefox()",
    "last_modified": "2025-05-04T23:28:25.507626"
  },
  {
    "id": "1074",
    "name": "yt_upload 2.py",
    "path": "02_media_processing/video_tools/yt_upload 2.py",
    "category": "02_media_processing",
    "type": "youtube",
    "lines": 209,
    "size": 7218,
    "docstring": "",
    "keywords": [
      "youtube"
    ],
    "functions": [
      "get_authenticated_service",
      "initialize_upload",
      "resumable_upload",
      "upload_video"
    ],
    "classes": [],
    "imports": [
      "http.client",
      "os",
      "random",
      "sys",
      "time",
      "types",
      "httplib2",
      "apiclient.discovery",
      "apiclient.errors",
      "apiclient.http"
    ],
    "preview": "import http.client as httplib\nimport os\nimport random\nimport sys\nimport time\nfrom types import SimpleNamespace\n\nimport httplib2\nfrom apiclient.discovery import build\nfrom apiclient.errors import HttpError\nfrom apiclient.http import MediaFileUpload\nfrom oauth2client.client import flow_from_clientsecrets\nfrom oauth2client.file import Storage\nfrom oauth2client.tools import argparser, run_flow\n\n# Explicitly tell the underlying HTTP transport library not to retry, since\n# we are handling retry logic ourselves.\nhttplib2.RETRIES = 1\n\n# Maximum number of times to retry before giving up.",
    "last_modified": "2025-09-13T05:54:10.988541"
  },
  {
    "id": "1075",
    "name": "user_agents.py",
    "path": "02_media_processing/video_tools/user_agents.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 912,
    "size": 101590,
    "docstring": "",
    "keywords": [],
    "functions": [
      "get_user_agent"
    ],
    "classes": [],
    "imports": [
      "random"
    ],
    "preview": "# coding=utf-8\n#!/usr/bin/env python3\n\nimport random\n\n\ndef get_user_agent():\n    return random.choice(user_agents)\n\n\nuser_agents = [\n    \"Mozilla/1.22 (compatible; MSIE 10.0; Windows 3.1)\",\n    \"Mozilla/4.0 (compatible; MSIE 10.0; Windows NT 6.1; Trident/5.0)\",\n    \"Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 6.0; tr) Opera 10.10\",\n    \"Mozilla/4.0 (compatible; MSIE 6.0; X11; Linux i686; de) Opera 10.10\",\n    \"Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 6.1; Trident/5.0; SLCC2; .NET CLR 2.0.50727; .NET CLR 3.5.30729; .NET CLR 3.0.30729; Media Center PC 6.0; FDM; .NET CLR 1.1.4322; .NET4.0C; .NET4.0E; Tablet PC 2.0)\",\n    \"Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 6.1; WOW64; Trident/5.0; SLCC2; .NET CLR 2.0.50727; .NET CLR 3.5.30729; .NET CLR 3.0.30729; InfoPath.2; .NET4.0C; .NET4.0E)\",\n    \"Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 6.1; WOW64; Trident/5.0; SLCC2; .NET CLR 2.0.50727; .NET CLR 3.5.30729; .NET CLR 3.0.30729; Media Center PC 6.0; .NET4.0C; .NET4.0E; AskTB5.5)\",\n    \"Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 6.1; Win64; x64; Trident/5.0; .NET CLR 2.0.50727; SLCC2; .NET CLR 3.5.30729; .NET CLR 3.0.30729; Media Center PC 6.0; InfoPath.3; .NET4.0C)\",\n    \"Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 7.1; Trident/5.0; .NET CLR 2.0.50727; SLCC2; .NET CLR 3.5.30729; .NET CLR 3.0.30729; Media Center PC 6.0; InfoPath.3; .NET4.0C)\",",
    "last_modified": "2025-03-28T18:35:46"
  },
  {
    "id": "1076",
    "name": "AskRedditBot.py",
    "path": "02_media_processing/video_tools/AskRedditBot.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 10,
    "size": 136,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "time",
      "AskReddit"
    ],
    "preview": "import time\n\nfrom AskReddit import gen_video_from_hot\n\ndelay = 60 * 60 * 12\n\nwhile True:\n    gen_video_from_hot()\n    time.sleep(delay)\n",
    "last_modified": "2025-05-04T23:28:22.830880"
  },
  {
    "id": "1077",
    "name": "NewUpload_20250607131039.py",
    "path": "02_media_processing/video_tools/NewUpload_20250607131039.py",
    "category": "02_media_processing",
    "type": "youtube",
    "lines": 113,
    "size": 5445,
    "docstring": "",
    "keywords": [
      "youtube",
      "web_tools"
    ],
    "functions": [
      "get_authenticated_service",
      "upload_video"
    ],
    "classes": [],
    "imports": [
      "os",
      "google.auth.transport.requests",
      "google_auth_oauthlib.flow",
      "googleapiclient.discovery",
      "googleapiclient.errors"
    ],
    "preview": "import os\n\nimport google.auth.transport.requests\nimport google_auth_oauthlib.flow\nimport googleapiclient.discovery\nimport googleapiclient.errors\n\n# --- Analysis of the code structure and logic ---\n\n# 1. Imports:\n#    - The script imports necessary modules for Google API client, authentication, and OS operations.\n#    - It assumes the googleapiclient and google_auth_oauthlib libraries are installed.\n\n# 2. Constants:\n#    - SCOPES: Defines the OAuth scope for uploading to YouTube.\n#    - CLIENT_SECRETS_FILE: Path to the OAuth client secrets JSON file.\n#    - API_SERVICE_NAME and API_VERSION: Used to build the YouTube API client.\n\n\n# 3. Authentication Function:",
    "last_modified": "2025-09-13T05:54:11.995510"
  },
  {
    "id": "1078",
    "name": "Clip.py",
    "path": "02_media_processing/video_tools/Clip.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 23,
    "size": 397,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [
      "Clip"
    ],
    "imports": [
      "dataclasses"
    ],
    "preview": "from dataclasses import dataclass\n\n\n@dataclass\nclass Clip:\n    id: str\n    url: str\n    embed_url: str\n    broadcaster_id: str\n    broadcaster_name: str\n    creator_id: str\n    creator_name: str\n    video_id: str\n    game_id: str\n    language: str\n    title: str\n    view_count: int\n    created_at: str\n    thumbnail_url: str\n    duration: float = 0.0",
    "last_modified": "2025-03-28T18:37:12.017791"
  },
  {
    "id": "1079",
    "name": "codingstatemachinedict.py",
    "path": "02_media_processing/video_tools/codingstatemachinedict.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 20,
    "size": 542,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [
      "CodingStateMachineDict"
    ],
    "imports": [
      "typing",
      "typing"
    ],
    "preview": "from typing import TYPE_CHECKING, Tuple\n\nif TYPE_CHECKING:\n    # TypedDict was introduced in Python 3.8.\n    #\n    # TODO: Remove the else block and TYPE_CHECKING check when dropping support\n    # for Python 3.7.\n    from typing import TypedDict\n\n    class CodingStateMachineDict(TypedDict, total=False):\n        class_table: Tuple[int, ...]\n        class_factor: int\n        state_table: Tuple[int, ...]\n        char_len_table: Tuple[int, ...]\n        name: str\n        language: str  # Optional key\n\nelse:\n    CodingStateMachineDict = dict\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1080",
    "name": "bot_checkpoint.py",
    "path": "02_media_processing/video_tools/bot_checkpoint.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 68,
    "size": 2024,
    "docstring": "Instabot Checkpoint methods.",
    "keywords": [],
    "functions": [
      "save_checkpoint",
      "load_checkpoint",
      "__init__",
      "fill_following",
      "fill_followers",
      "dump"
    ],
    "classes": [
      "Checkpoint"
    ],
    "imports": [
      "os",
      "pickle",
      "datetime"
    ],
    "preview": "\"\"\"\nInstabot Checkpoint methods.\n\"\"\"\n\nimport os\nimport pickle\nfrom datetime import datetime\n\nCHECKPOINT_PATH = \"{fname}.checkpoint\"\n\n\nclass Checkpoint(object):\n    \"\"\"\n    Checkpoint for instabot.Bot class which can store:\n        .total[<name>] - all Bot's counters\n        .blocked_actions[<name>] - Bot's blocked actions\n        .following (list of user_ids)\n        .followers (list of user_ids)\n        .date (of checkpoint creation)\n    \"\"\"",
    "last_modified": "2025-03-28T18:35:47"
  },
  {
    "id": "1081",
    "name": "help.py",
    "path": "02_media_processing/video_tools/help.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 79,
    "size": 2863,
    "docstring": "",
    "keywords": [],
    "functions": [
      "help_msg"
    ],
    "classes": [],
    "imports": [
      "os",
      "webbrowser",
      "colorama",
      "libs.logo",
      "libs.utils"
    ],
    "preview": "import os\nimport webbrowser\n\nfrom colorama import Back, Fore, Style\nfrom libs.logo import print_logo\nfrom libs.utils import clearConsole\n\n\ndef help_msg():\n    print(Style.RESET_ALL)\n    que = print(\n        \"\"\"\n${Fore.GREEN}============================== HELP  ==============================\n\n    [1] Connection Error\n    [2] Not banning account\n    [3] More help\n    [4] Exit\n    \"\"\"\n    )",
    "last_modified": "2025-05-04T23:28:20"
  },
  {
    "id": "1082",
    "name": "listen.py",
    "path": "02_media_processing/video_tools/listen.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 85,
    "size": 2246,
    "docstring": "listen",
    "keywords": [],
    "functions": [
      "configure_loggers",
      "handle_exception",
      "remove_old_log_files",
      "listen"
    ],
    "classes": [],
    "imports": [
      "datetime",
      "logging",
      "os",
      "sys",
      "time",
      "ytdl.oshelper",
      "ytdl.playlistlistener",
      "ytdl.ytdlconfiguration"
    ],
    "preview": "\"listen\"\n\nimport datetime\nimport logging\nimport os\nimport sys\nimport time\n\nfrom ytdl.oshelper import absolute_files, mkdir, remove\nfrom ytdl.playlistlistener import Playlistlistener\nfrom ytdl.ytdlconfiguration import Ytdlconfiguration\n\n\ndef configure_loggers(config):\n    \"Configure logger\"\n\n    logging.getLogger(\"\").handlers = []\n\n    mkdir(config.log_folder)\n    logs_file_name = os.path.join(config.log_folder, str(datetime.date.today()) + \"listen.log\")",
    "last_modified": "2025-09-13T05:54:15.047189"
  },
  {
    "id": "1083",
    "name": "yt 2.py",
    "path": "02_media_processing/video_tools/yt 2.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 130,
    "size": 3726,
    "docstring": "",
    "keywords": [
      "web_tools"
    ],
    "functions": [
      "get_authenticated_service",
      "print_response",
      "build_resource",
      "remove_empty_kwargs",
      "comment_threads_insert",
      "scrape"
    ],
    "classes": [],
    "imports": [
      "os",
      "random",
      "google.oauth2.credentials",
      "google_auth_oauthlib.flow",
      "requests",
      "bs4",
      "google_auth_oauthlib.flow",
      "googleapiclient.discovery",
      "googleapiclient.errors"
    ],
    "preview": "import os\nimport random\n\nimport google.oauth2.credentials\nimport google_auth_oauthlib.flow\nimport requests\nfrom bs4 import BeautifulSoup\nfrom google_auth_oauthlib.flow import InstalledAppFlow\nfrom googleapiclient.discovery import build\nfrom googleapiclient.errors import HttpError\n\nCLIENT_SECRETS_FILE = \"client_secret.json\"\n\nSCOPES = [\"https://www.googleapis.com/auth/youtube.force-ssl\"]\nAPI_SERVICE_NAME = \"youtube\"\nAPI_VERSION = \"v3\"\n\n\ndef get_authenticated_service():\n    flow = InstalledAppFlow.from_client_secrets_file(CLIENT_SECRETS_FILE, SCOPES)",
    "last_modified": "2025-08-06T13:42:30.011436"
  },
  {
    "id": "1084",
    "name": "googleapi-upload.py",
    "path": "02_media_processing/video_tools/googleapi-upload.py",
    "category": "02_media_processing",
    "type": "youtube",
    "lines": 135,
    "size": 4888,
    "docstring": "",
    "keywords": [
      "youtube"
    ],
    "functions": [
      "get_authenticated_service",
      "upload_video",
      "main"
    ],
    "classes": [],
    "imports": [
      "os",
      "google.oauth2.credentials",
      "google_auth_oauthlib.flow",
      "googleapiclient.discovery",
      "googleapiclient.errors",
      "googleapiclient.http"
    ],
    "preview": "import os\n\nimport google.oauth2.credentials\nimport google_auth_oauthlib.flow\nimport googleapiclient.discovery\nimport googleapiclient.errors\nfrom googleapiclient.http import MediaFileUpload\n\n# --- Configuration ---\nCLIENT_SECRETS_FILE = \"/Users/steven/Movies/PROJECt2025-DoMinIon/mp4/client_secret.json\"  # Path to your downloaded credentials.json\nSCOPES = [\"https://www.googleapis.com/auth/youtube.upload\"]  # YouTube Upload Scope\nAPI_SERVICE_NAME = \"youtube\"\nAPI_VERSION = \"v3\"\n\n\ndef get_authenticated_service():\n    \"\"\"\n    Authenticates and authorizes the user. Returns the YouTube Data API service object.\n    \"\"\"\n    # 1. Load existing credentials, if they exist",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1085",
    "name": "test_savify.py",
    "path": "02_media_processing/video_tools/test_savify.py",
    "category": "02_media_processing",
    "type": "video_processing",
    "lines": 18,
    "size": 401,
    "docstring": "Tests for `savify` package.",
    "keywords": [
      "testing"
    ],
    "functions": [
      "test_command_line_interface"
    ],
    "classes": [],
    "imports": [
      "pytest",
      "click.testing",
      "savify.cli",
      "savify"
    ],
    "preview": "#!/usr/bin/env python\n\n\"\"\"Tests for `savify` package.\"\"\"\n\nimport pytest\nfrom click.testing import CliRunner\n\nimport savify.cli as cli\nfrom savify import savify\n\n\ndef test_command_line_interface():\n    \"\"\"Test the CLI.\"\"\"\n    runner = CliRunner()\n    help_result = runner.invoke(cli.main, [\"--help\"])\n    assert help_result.exit_code == 0\n    assert \"Show this message and exit.\" in help_result.output\n",
    "last_modified": "2025-09-11T13:27:06.393552"
  },
  {
    "id": "1086",
    "name": "ytdlconfiguration.py",
    "path": "02_media_processing/video_tools/ytdlconfiguration.py",
    "category": "02_media_processing",
    "type": "setup",
    "lines": 83,
    "size": 3383,
    "docstring": "Config",
    "keywords": [],
    "functions": [
      "__init__",
      "load",
      "is_valid"
    ],
    "classes": [
      "Ytdlconfiguration"
    ],
    "imports": [
      "configparser",
      "os.path",
      "ytdl.oshelper"
    ],
    "preview": "\"Config\"\n\nimport configparser\nfrom os.path import exists, expanduser\n\nfrom ytdl.oshelper import join_paths, mkdir\n\n\nclass Ytdlconfiguration(object):\n    \"Ytdl Configuration\"\n\n    def __init__(self):\n        self.__home_path__ = expanduser(\"~\")\n        self._ytdl_home_path_ = join_paths(self.__home_path__, \".ytdl\")\n        self.listener_time_file_path = join_paths(self._ytdl_home_path_, \"listener-timestamp.txt\")\n        self.config_file_path = join_paths(self._ytdl_home_path_, \"config.ini\")\n        self.download_folder = join_paths(self._ytdl_home_path_, \"downloads\")\n        self.log_folder = join_paths(self._ytdl_home_path_, \"logs\")\n\n        self.googleplay_credential_file = \"\"",
    "last_modified": "2025-09-13T05:54:15.256480"
  },
  {
    "id": "1087",
    "name": "GUI.py",
    "path": "02_media_processing/video_tools/GUI.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 110,
    "size": 2994,
    "docstring": "",
    "keywords": [
      "video_processing"
    ],
    "functions": [
      "after_request",
      "index",
      "backgrounds",
      "background_add",
      "background_delete",
      "settings",
      "videos_json",
      "backgrounds_json",
      "results",
      "voices"
    ],
    "classes": [],
    "imports": [
      "webbrowser",
      "pathlib",
      "tomlkit",
      "flask",
      "utils.gui_utils"
    ],
    "preview": "import webbrowser\nfrom pathlib import Path\n\n# Used \"tomlkit\" instead of \"toml\" because it doesn't change formatting on \"dump\"\nimport tomlkit\nfrom flask import Flask, redirect, render_template, request, send_from_directory, url_for\n\nimport utils.gui_utils as gui\n\n# Set the hostname\nHOST = \"localhost\"\n# Set the port number\nPORT = 4000\n\n# Configure application\napp = Flask(__name__, template_folder=\"GUI\")\n\n# Configure secret key only to use 'flash'\napp.secret_key = b'_5#y2L\"F4Q8z\\n\\xec]/'\n",
    "last_modified": "2025-09-13T05:53:59.497943"
  },
  {
    "id": "1088",
    "name": "__main__.py",
    "path": "02_media_processing/video_tools/__main__.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 13,
    "size": 265,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "sys",
      "savify",
      "os.path"
    ],
    "preview": "import sys\n\nif __package__ is None and not hasattr(sys, \"frozen\"):\n    import os.path\n\n    path = os.path.realpath(os.path.abspath(__file__))\n    sys.path.insert(0, os.path.dirname(os.path.dirname(path)))\n\nimport savify\n\nif __name__ == \"__main__\":\n    savify.cli()\n",
    "last_modified": "2025-05-04T23:27:53.534935"
  },
  {
    "id": "1089",
    "name": "NewUpload_20250607131028.py",
    "path": "02_media_processing/video_tools/NewUpload_20250607131028.py",
    "category": "02_media_processing",
    "type": "youtube",
    "lines": 112,
    "size": 4689,
    "docstring": "",
    "keywords": [
      "video_processing",
      "youtube",
      "web_tools"
    ],
    "functions": [
      "get_authenticated_service",
      "upload_video"
    ],
    "classes": [],
    "imports": [
      "os",
      "google.auth.transport.requests",
      "google_auth_oauthlib.flow",
      "googleapiclient.discovery",
      "googleapiclient.errors"
    ],
    "preview": "import os\n\nimport google.auth.transport.requests\nimport google_auth_oauthlib.flow\nimport googleapiclient.discovery\nimport googleapiclient.errors\n\n# --- Analysis of the code structure and logic ---\n\n# 1. Imports:\n#    - The script imports necessary modules for Google API client, authentication, and OS operations.\n#    - It assumes the googleapiclient and google_auth_oauthlib libraries are installed.\n\n# 2. Constants:\n#    - SCOPES: Defines the OAuth scope for uploading to YouTube.\n#    - CLIENT_SECRETS_FILE: Path to the OAuth client secrets JSON file.\n#    - API_SERVICE_NAME and API_VERSION: Used to build the YouTube API client.\n\n# 3. Authentication Function:\ndef get_authenticated_service():",
    "last_modified": "2025-09-06T12:24:11.176493"
  },
  {
    "id": "1090",
    "name": "dupes.py",
    "path": "02_media_processing/video_tools/dupes.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 38,
    "size": 1220,
    "docstring": "",
    "keywords": [],
    "functions": [
      "parse_duplicates_file",
      "keep_latest_file"
    ],
    "classes": [],
    "imports": [
      "os",
      "pathlib"
    ],
    "preview": "import os\nfrom pathlib import Path\n\ndef parse_duplicates_file(file_path):\n    groups = []\n    current_group = []\n    with open(file_path, 'r') as f:\n        for line in f:\n            line = line.strip()\n            if line.endswith(\".md\"):\n                current_group.append(line)\n            elif line == \"\" and current_group:\n                groups.append(current_group)\n                current_group = []\n        if current_group:\n            groups.append(current_group)\n    return groups\n\ndef keep_latest_file(group):\n    # Keep the file with the most recent timestamp or simplest name",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1091",
    "name": "yt 3.py",
    "path": "02_media_processing/video_tools/yt 3.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 130,
    "size": 3726,
    "docstring": "",
    "keywords": [
      "web_tools"
    ],
    "functions": [
      "get_authenticated_service",
      "print_response",
      "build_resource",
      "remove_empty_kwargs",
      "comment_threads_insert",
      "scrape"
    ],
    "classes": [],
    "imports": [
      "os",
      "random",
      "google.oauth2.credentials",
      "google_auth_oauthlib.flow",
      "requests",
      "bs4",
      "google_auth_oauthlib.flow",
      "googleapiclient.discovery",
      "googleapiclient.errors"
    ],
    "preview": "import os\nimport random\n\nimport google.oauth2.credentials\nimport google_auth_oauthlib.flow\nimport requests\nfrom bs4 import BeautifulSoup\nfrom google_auth_oauthlib.flow import InstalledAppFlow\nfrom googleapiclient.discovery import build\nfrom googleapiclient.errors import HttpError\n\nCLIENT_SECRETS_FILE = \"client_secret.json\"\n\nSCOPES = [\"https://www.googleapis.com/auth/youtube.force-ssl\"]\nAPI_SERVICE_NAME = \"youtube\"\nAPI_VERSION = \"v3\"\n\n\ndef get_authenticated_service():\n    flow = InstalledAppFlow.from_client_secrets_file(CLIENT_SECRETS_FILE, SCOPES)",
    "last_modified": "2025-08-06T14:25:20.073684"
  },
  {
    "id": "1092",
    "name": "prepare.py",
    "path": "02_media_processing/video_tools/prepare.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 103,
    "size": 3509,
    "docstring": "",
    "keywords": [],
    "functions": [
      "get_credential_file",
      "add_credentials",
      "get_credentials",
      "check_secret",
      "delete_credentials"
    ],
    "classes": [],
    "imports": [
      "getpass",
      "os",
      "sys"
    ],
    "preview": "#!/usr/bin/env python\n\nimport getpass\nimport os\nimport sys\n\nDEFAULT_SECRET_DIR = os.path.abspath(os.getcwd())\n\n\ndef get_credential_file(base_path=DEFAULT_SECRET_DIR):\n    return base_path + \"/config/secret.txt\"\n\n\ndef add_credentials(base_path):\n    SECRET_FILE = get_credential_file(base_path)\n    with open(SECRET_FILE, \"a\") as f:\n        print(\"Enter your login: \")\n        f.write(str(sys.stdin.readline().strip()) + \":\")\n        print(\n            \"Enter your password: (it will not be shown due to security \"",
    "last_modified": "2025-03-28T18:35:47"
  },
  {
    "id": "1093",
    "name": "scrape.py",
    "path": "02_media_processing/video_tools/scrape.py",
    "category": "02_media_processing",
    "type": "web_tools",
    "lines": 66,
    "size": 2157,
    "docstring": "",
    "keywords": [
      "web_tools",
      "organization"
    ],
    "functions": [
      "clean_string",
      "read_subreddit_list",
      "scrape_reddit_text"
    ],
    "classes": [],
    "imports": [
      "re",
      "time",
      "praw",
      "readability",
      "config"
    ],
    "preview": "import re\nfrom time import sleep, time\n\nimport praw\nfrom readability import parse_url\n\nfrom config import parse_config\n\nconfig = parse_config(\"local\")\n\n\ndef clean_string(string_to_clean):\n    \"\"\"Method to remove punctuation and numbers from a string\"\"\"\n    return re.sub(r\"[^\\sa-zA-Z0-9]\", \"\", string_to_clean).lower().strip()\n\n\ndef read_subreddit_list():\n    subreddit_list_path = config[\"subreddit_list_path\"]\n    subreddit_limit_list = []\n    with open(subreddit_list_path) as f:",
    "last_modified": "2025-09-13T05:53:51.245013"
  },
  {
    "id": "1094",
    "name": "ReportBot.py",
    "path": "02_media_processing/video_tools/ReportBot.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 298,
    "size": 14481,
    "docstring": "",
    "keywords": [
      "video_processing",
      "web_tools"
    ],
    "functions": [
      "chunks",
      "profile_attack_process",
      "video_attack_process",
      "video_attack",
      "profile_attack",
      "unlock",
      "database",
      "main",
      "report"
    ],
    "classes": [],
    "imports": [
      "os",
      "time",
      "webbrowser",
      "multiprocessing",
      "os",
      "os",
      "sys",
      "firebase_admin",
      "requests",
      "about"
    ],
    "preview": "#!/usr/bin/env python3\nimport os  # line:26\nimport time  # line:22\nimport webbrowser  # line:27\nfrom multiprocessing import Process  # line:4\nfrom os import _exit  # line:25\nfrom os import path  # line:20\nfrom sys import exit  # line:24\n\nimport firebase_admin  # line:29\nimport requests  # line:21\nfrom about import about_msg  # line:5\nfrom colorama import Back, Fore, Style  # line:3\nfrom dotenv import load_dotenv  # line:33\nfrom firebase_admin import credentials  # line:30\nfrom firebase_admin import db  # line:31\nfrom firebase_admin import firestore  # line:32\nfrom libs.animation import animation_bar  # line:10\nfrom libs.animation import colorText  # line:7\nfrom libs.animation import load_animation  # line:9",
    "last_modified": "2025-09-13T05:53:30.454456"
  },
  {
    "id": "1095",
    "name": "self_outdated_check.py",
    "path": "02_media_processing/video_tools/self_outdated_check.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 248,
    "size": 8377,
    "docstring": "",
    "keywords": [],
    "functions": [
      "_get_statefile_name",
      "_convert_date",
      "was_installed_by_pip",
      "_get_current_remote_pip_version",
      "_self_version_check_logic",
      "pip_self_version_check",
      "__init__",
      "key",
      "get",
      "set"
    ],
    "classes": [
      "SelfCheckState",
      "UpgradePrompt"
    ],
    "imports": [
      "datetime",
      "functools",
      "hashlib",
      "json",
      "logging",
      "optparse",
      "os.path",
      "sys",
      "dataclasses",
      "typing"
    ],
    "preview": "import datetime\nimport functools\nimport hashlib\nimport json\nimport logging\nimport optparse\nimport os.path\nimport sys\nfrom dataclasses import dataclass\nfrom typing import Any, Callable, Dict, Optional\n\nfrom pip._internal.index.collector import LinkCollector\nfrom pip._internal.index.package_finder import PackageFinder\nfrom pip._internal.metadata import get_default_environment\nfrom pip._internal.metadata.base import DistributionVersion\nfrom pip._internal.models.selection_prefs import SelectionPreferences\nfrom pip._internal.network.session import PipSession\nfrom pip._internal.utils.compat import WINDOWS\nfrom pip._internal.utils.entrypoints import (\n    get_best_invocation_for_this_pip,",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1096",
    "name": "videos.py",
    "path": "02_media_processing/video_tools/videos.py",
    "category": "02_media_processing",
    "type": "youtube",
    "lines": 61,
    "size": 2162,
    "docstring": "",
    "keywords": [],
    "functions": [
      "check_done",
      "save_data"
    ],
    "classes": [],
    "imports": [
      "json",
      "time",
      "praw.models",
      "utils",
      "utils.console"
    ],
    "preview": "import json\nimport time\n\nfrom praw.models import Submission\n\nfrom utils import settings\nfrom utils.console import print_step\n\n\ndef check_done(\n    redditobj: Submission,\n) -> Submission:\n    # don't set this to be run anyplace that isn't subreddit.py bc of inspect stack\n    \"\"\"Checks if the chosen post has already been generated\n\n    Args:\n        redditobj (Submission): Reddit object gotten from reddit/subreddit.py\n\n    Returns:\n        Submission|None: Reddit object in args",
    "last_modified": "2025-09-13T05:54:00.533460"
  },
  {
    "id": "1097",
    "name": "yt_upload 3.py",
    "path": "02_media_processing/video_tools/yt_upload 3.py",
    "category": "02_media_processing",
    "type": "youtube",
    "lines": 209,
    "size": 7218,
    "docstring": "",
    "keywords": [
      "youtube"
    ],
    "functions": [
      "get_authenticated_service",
      "initialize_upload",
      "resumable_upload",
      "upload_video"
    ],
    "classes": [],
    "imports": [
      "http.client",
      "os",
      "random",
      "sys",
      "time",
      "types",
      "httplib2",
      "apiclient.discovery",
      "apiclient.errors",
      "apiclient.http"
    ],
    "preview": "import http.client as httplib\nimport os\nimport random\nimport sys\nimport time\nfrom types import SimpleNamespace\n\nimport httplib2\nfrom apiclient.discovery import build\nfrom apiclient.errors import HttpError\nfrom apiclient.http import MediaFileUpload\nfrom oauth2client.client import flow_from_clientsecrets\nfrom oauth2client.file import Storage\nfrom oauth2client.tools import argparser, run_flow\n\n# Explicitly tell the underlying HTTP transport library not to retry, since\n# we are handling retry logic ourselves.\nhttplib2.RETRIES = 1\n\n# Maximum number of times to retry before giving up.",
    "last_modified": "2025-09-13T05:54:11.059864"
  },
  {
    "id": "1098",
    "name": "NewUpload_20250607125040.py",
    "path": "02_media_processing/video_tools/NewUpload_20250607125040.py",
    "category": "02_media_processing",
    "type": "youtube",
    "lines": 78,
    "size": 2705,
    "docstring": "",
    "keywords": [
      "youtube",
      "web_tools"
    ],
    "functions": [
      "get_authenticated_service",
      "upload_video"
    ],
    "classes": [],
    "imports": [
      "os",
      "google.auth.transport.requests",
      "google_auth_oauthlib.flow",
      "googleapiclient.discovery",
      "googleapiclient.errors"
    ],
    "preview": "import os\n\nimport google.auth.transport.requests\nimport google_auth_oauthlib.flow\nimport googleapiclient.discovery\nimport googleapiclient.errors\n\nSCOPES = [\"https://www.googleapis.com/auth/youtube.upload\"]\nCLIENT_SECRETS_FILE = \"/Users/steven/Documents/python/Youtube/client_secrets.json\"  # Replace with your client secrets file\nAPI_SERVICE_NAME = \"youtube\"\nAPI_VERSION = \"v3\"\n\n\ndef get_authenticated_service():\n    \"\"\"Authenticates and returns the YouTube Data API service.\"\"\"\n    flow = google_auth_oauthlib.flow.InstalledAppFlow.from_client_secrets_file(\n        CLIENT_SECRETS_FILE, SCOPES\n    )\n    credentials = flow.run_local_server(port=0)\n    return googleapiclient.discovery.build(API_SERVICE_NAME, API_VERSION, credentials=credentials)",
    "last_modified": "2025-09-13T05:54:11.860951"
  },
  {
    "id": "1099",
    "name": "setup.py.py",
    "path": "02_media_processing/setup.py_consolidated/setup.py.py",
    "category": "02_media_processing",
    "type": "setup",
    "lines": 25,
    "size": 556,
    "docstring": "Setup class",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "distutils.core"
    ],
    "preview": "\"Setup class\"\n\nfrom distutils.core import setup\n\nsetup(\n    name=\"ytdl\",\n    version=\"1.0.30\",\n    packages=[\"ytdl\"],\n    url=\"https://github.com/develohpanda/ytdl\",\n    description=\"YT download and upload to GMusic\",\n    install_requires=[\n        \"youtube-dl\",\n        \"gmusicapi==10.1.2.rc1\",\n        \"boto3\",\n        \"mutagen\",\n        \"configparser\",\n        \"image\",\n        \"google-api-python-client\",\n        \"pytz\",\n    ],",
    "last_modified": "2025-05-04T23:27:55"
  },
  {
    "id": "1100",
    "name": "setup.py_02.py",
    "path": "02_media_processing/setup.py_consolidated/setup.py_02.py",
    "category": "02_media_processing",
    "type": "setup",
    "lines": 62,
    "size": 1617,
    "docstring": "The setup script.",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "setuptools"
    ],
    "preview": "#!/usr/bin/env python\n\n\"\"\"The setup script.\"\"\"\n\nfrom setuptools import Extension, find_packages, setup\n\nwith open(\"README.rst\") as readme_file:\n    readme = readme_file.read()\n\nrequirements = [\n    \"ffmpy>=0.3.0\",\n    \"spotipy>=2.16.1\",\n    \"tldextract>=3.1.0\",\n    \"validators>=0.18.2\",\n    \"youtube-dl>=2021.6.6\",\n    \"requests>=2.25.1\",\n    \"click>=7.1.2\",\n]\n\nsetup_requirements = [",
    "last_modified": "2025-05-04T23:28:25.577200"
  },
  {
    "id": "1101",
    "name": "setup.py_03.py",
    "path": "02_media_processing/setup.py_consolidated/setup.py_03.py",
    "category": "02_media_processing",
    "type": "youtube",
    "lines": 33,
    "size": 1150,
    "docstring": "Upload videos to Youtube.",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "distutils.core"
    ],
    "preview": "#!/usr/bin/python\n\"\"\"Upload videos to Youtube.\"\"\"\nfrom distutils.core import setup\n\nsetup_kwargs = {\n    \"name\": \"youtube-upload\",\n    \"version\": \"0.8.0\",\n    \"description\": \"Upload videos to Youtube\",\n    \"author\": \"Arnau Sanchez\",\n    \"author_email\": \"pyarnau@gmail.com\",\n    \"url\": \"https://github.com/tokland/youtube-upload\",\n    \"packages\": [\"youtube_upload/\", \"youtube_upload/auth\"],\n    \"scripts\": [\"bin/youtube-upload\"],\n    \"license\": \"GNU Public License v3.0\",\n    \"long_description\": \" \".join(__doc__.strip().splitlines()),\n    \"classifiers\": [\n        \"Development Status :: 4 - Beta\",\n        \"Intended Audience :: End Users/Desktop\",\n        \"License :: OSI Approved :: GNU General Public License (GPL)\",\n        \"Natural Language :: English\",",
    "last_modified": "2025-03-28T18:37:08"
  },
  {
    "id": "1102",
    "name": "utils.py_02.py",
    "path": "02_media_processing/utils.py_consolidated/utils.py_02.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 103,
    "size": 2778,
    "docstring": "",
    "keywords": [
      "youtube",
      "analysis",
      "organization"
    ],
    "functions": [
      "clean",
      "create_dir",
      "check_ffmpeg",
      "check_env",
      "check_file",
      "safe_path_string",
      "__init__",
      "get_download_dir",
      "get_temp_dir",
      "download_file"
    ],
    "classes": [
      "PathHolder"
    ],
    "imports": [
      "os",
      "re",
      "pathlib",
      "shutil",
      "sys",
      "urllib.request",
      "uuid",
      "shutil",
      "os"
    ],
    "preview": "import os\nimport re\nfrom pathlib import Path\nfrom shutil import rmtree\nfrom sys import platform\nfrom urllib.request import urlretrieve\nfrom uuid import uuid1\n\n__all__ = [\"PathHolder\"]\n\n\ndef clean(path) -> None:\n    for file in os.listdir(path):\n        file_path = os.path.join(path, file)\n        try:\n            if os.path.isfile(file_path) or os.path.islink(file_path):\n                os.unlink(file_path)\n            elif os.path.isdir(file_path):\n                rmtree(file_path)\n",
    "last_modified": "2025-05-04T23:28:25.609507"
  },
  {
    "id": "1103",
    "name": "utils.py_03.py",
    "path": "02_media_processing/utils.py_consolidated/utils.py_03.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 131,
    "size": 3734,
    "docstring": "",
    "keywords": [
      "analysis",
      "web_tools"
    ],
    "functions": [
      "get_date",
      "get_path",
      "get_description",
      "get_current_version",
      "create_video_config",
      "get_category",
      "get_category_and_name",
      "name_to_ids",
      "remove_blacklisted",
      "format_blacklist"
    ],
    "classes": [],
    "imports": [
      "datetime",
      "random",
      "string",
      "requests",
      "api",
      "config",
      "exceptions"
    ],
    "preview": "from datetime import date\nfrom random import choice\nfrom string import ascii_lowercase, digits\n\nimport requests\n\nfrom .api import get\nfrom .config import CLIP_PATH\nfrom .exceptions import InvalidCategory\n\n\ndef get_date() -> str:\n    \"\"\"\n    Gets the current date and returns the date as a string.\n    \"\"\"\n    return date.today().strftime(\"%b-%d-%Y\")\n\n\ndef get_path() -> str:\n    return CLIP_PATH.format(",
    "last_modified": "2025-09-13T05:53:56.311605"
  },
  {
    "id": "1104",
    "name": "utils.py.py",
    "path": "02_media_processing/utils.py_consolidated/utils.py.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 147,
    "size": 4838,
    "docstring": "",
    "keywords": [
      "organization"
    ],
    "functions": [
      "get_date_string",
      "time_plus",
      "get_start_end_time",
      "get_valid_file_name",
      "get_game_path",
      "get_previous_path",
      "make_dirs",
      "clean_directory",
      "load_txt_file",
      "load_json_file"
    ],
    "classes": [],
    "imports": [
      "json",
      "logging",
      "os",
      "re",
      "shutil",
      "datetime",
      "typing",
      "src.APIHandler",
      "config"
    ],
    "preview": "import json\nimport logging\nimport os\nimport re\nimport shutil\nfrom datetime import datetime, time, timedelta, timezone\nfrom typing import Union\n\nfrom src.APIHandler import APIHandler\n\nimport config\n\nFOLDER_TIMESTAMP = datetime.now(timezone.utc).astimezone().strftime(\"%Y_%m_%d_%H_%M_%S\")\n\n\ndef get_date_string(date: datetime) -> str:\n    return date.strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n\n\n# Adds a specific amount of seconds ontop of the given time under consideration of all time/date rules",
    "last_modified": "2025-09-13T05:53:45.738170"
  },
  {
    "id": "1105",
    "name": "logging.py",
    "path": "06_development_tools/testing_framework/logging.py",
    "category": "06_development_tools",
    "type": "utility",
    "lines": 53,
    "size": 1195,
    "docstring": "",
    "keywords": [],
    "functions": [
      "write_to_logs",
      "log",
      "info",
      "error",
      "warn",
      "clip",
      "debug"
    ],
    "classes": [
      "Log"
    ],
    "imports": [
      "datetime",
      "colorama",
      "colorama",
      "config",
      "utils"
    ],
    "preview": "from datetime import datetime\n\nfrom colorama import Fore as f\nfrom colorama import init\n\nfrom .config import DEBUG\nfrom .utils import get_date\n\ninit()\n\n\ndef write_to_logs(text: str) -> None:\n    with open(\"twitchtube/files/logs.txt\", \"a\") as f:\n        f.write(\"\\n\" + text)\n\n\ndef log(color: int, sort: str, text: str) -> None:\n    \"\"\"\n    Used for colored printing, does not return anything.\n    \"\"\"",
    "last_modified": "2025-09-13T05:53:56.248094"
  },
  {
    "id": "1106",
    "name": "isatty_test.py",
    "path": "06_development_tools/testing_framework/isatty_test.py",
    "category": "06_development_tools",
    "type": "testing",
    "lines": 59,
    "size": 1867,
    "docstring": "",
    "keywords": [
      "testing"
    ],
    "functions": [
      "is_a_tty",
      "test_TTY",
      "test_nonTTY",
      "test_withPycharm",
      "test_withPycharmTTYOverride",
      "test_withPycharmNonTTYOverride",
      "test_withPycharmNoneOverride",
      "test_withPycharmStreamWrapped"
    ],
    "classes": [
      "IsattyTest"
    ],
    "imports": [
      "sys",
      "unittest",
      "ansitowin32",
      "utils"
    ],
    "preview": "# Copyright Jonathan Hartley 2013. BSD 3-Clause license, see LICENSE file.\nimport sys\nfrom unittest import TestCase, main\n\nfrom ..ansitowin32 import AnsiToWin32, StreamWrapper\nfrom .utils import StreamNonTTY, StreamTTY, pycharm, replace_by, replace_original_by\n\n\ndef is_a_tty(stream):\n    return StreamWrapper(stream, None).isatty()\n\n\nclass IsattyTest(TestCase):\n\n    def test_TTY(self):\n        tty = StreamTTY()\n        self.assertTrue(is_a_tty(tty))\n        with pycharm():\n            self.assertTrue(is_a_tty(tty))\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1107",
    "name": "utf1632prober.py",
    "path": "06_development_tools/testing_framework/utf1632prober.py",
    "category": "06_development_tools",
    "type": "utility",
    "lines": 222,
    "size": 8457,
    "docstring": "",
    "keywords": [],
    "functions": [
      "__init__",
      "reset",
      "charset_name",
      "language",
      "approx_32bit_chars",
      "approx_16bit_chars",
      "is_likely_utf32be",
      "is_likely_utf32le",
      "is_likely_utf16be",
      "is_likely_utf16le"
    ],
    "classes": [
      "UTF1632Prober"
    ],
    "imports": [
      "typing",
      "charsetprober",
      "enums"
    ],
    "preview": "######################## BEGIN LICENSE BLOCK ########################\n#\n# Contributor(s):\n#   Jason Zavaglia\n#\n# This library is free software; you can redistribute it and/or\n# modify it under the terms of the GNU Lesser General Public\n# License as published by the Free Software Foundation; either\n# version 2.1 of the License, or (at your option) any later version.\n#\n# This library is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU\n# Lesser General Public License for more details.\n#\n# You should have received a copy of the GNU Lesser General Public\n# License along with this library; if not, write to the Free Software\n# Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA\n# 02110-1301  USA\n######################### END LICENSE BLOCK #########################",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1108",
    "name": "initialise_test.py",
    "path": "06_development_tools/testing_framework/initialise_test.py",
    "category": "06_development_tools",
    "type": "testing",
    "lines": 183,
    "size": 6625,
    "docstring": "",
    "keywords": [
      "testing"
    ],
    "functions": [
      "setUp",
      "tearDown",
      "assertWrapped",
      "assertNotWrapped",
      "testInitWrapsOnWindows",
      "testInitDoesntWrapOnEmulatedWindows",
      "testInitDoesntWrapOnNonWindows",
      "testInitDoesntWrapIfNone",
      "testInitAutoresetOnWrapsOnAllPlatforms",
      "testInitWrapOffDoesntWrapOnWindows"
    ],
    "classes": [
      "InitTest",
      "JustFixWindowsConsoleTest"
    ],
    "imports": [
      "sys",
      "unittest",
      "ansitowin32",
      "initialise",
      "utils",
      "unittest.mock",
      "mock"
    ],
    "preview": "# Copyright Jonathan Hartley 2013. BSD 3-Clause license, see LICENSE file.\nimport sys\nfrom unittest import TestCase, main, skipUnless\n\ntry:\n    from unittest.mock import Mock, patch\nexcept ImportError:\n    from mock import patch, Mock\n\nfrom ..ansitowin32 import StreamWrapper\nfrom ..initialise import _wipe_internal_state_for_tests, init, just_fix_windows_console\nfrom .utils import osname, replace_by\n\norig_stdout = sys.stdout\norig_stderr = sys.stderr\n\n\nclass InitTest(TestCase):\n\n    @skipUnless(sys.stdout.isatty(), \"sys.stdout is not a tty\")",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1109",
    "name": "test_quantum_chaos.py",
    "path": "06_development_tools/testing_framework/test_quantum_chaos.py",
    "category": "06_development_tools",
    "type": "testing",
    "lines": 9,
    "size": 233,
    "docstring": "",
    "keywords": [
      "testing"
    ],
    "functions": [
      "test_chaos_failure"
    ],
    "classes": [],
    "imports": [
      "pytest",
      "src.chaos_scheduler"
    ],
    "preview": "import pytest\nfrom src.chaos_scheduler import ChaosScheduler\n\n\ndef test_chaos_failure():\n    scheduler = ChaosScheduler(seed=42)\n    with pytest.raises(RuntimeError):\n        scheduler.schedule_operation(lambda: None, criticality=5)\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1110",
    "name": "test_uploader_factory.py",
    "path": "06_development_tools/testing_framework/test_uploader_factory.py",
    "category": "06_development_tools",
    "type": "youtube",
    "lines": 20,
    "size": 646,
    "docstring": "",
    "keywords": [
      "testing"
    ],
    "functions": [
      "test_get_uploader"
    ],
    "classes": [
      "UploaderFactoryTestCase"
    ],
    "imports": [
      "unittest",
      "simplegallery.common",
      "simplegallery.upload.uploader_factory",
      "simplegallery.upload.variants.aws_uploader",
      "simplegallery.upload.variants.netlify_uploader"
    ],
    "preview": "import unittest\n\nimport simplegallery.common as spg_common\nfrom simplegallery.upload.uploader_factory import get_uploader\nfrom simplegallery.upload.variants.aws_uploader import AWSUploader\nfrom simplegallery.upload.variants.netlify_uploader import NetlifyUploader\n\n\nclass UploaderFactoryTestCase(unittest.TestCase):\n    def test_get_uploader(self):\n        self.assertIs(AWSUploader, get_uploader(\"aws\").__class__)\n        self.assertIs(NetlifyUploader, get_uploader(\"netlify\").__class__)\n\n        with self.assertRaises(spg_common.SPGException):\n            get_uploader(\"non_existing_uploader\")\n\n\nif __name__ == \"__main__\":\n    unittest.main()\n",
    "last_modified": "2025-05-04T23:28:22"
  },
  {
    "id": "1111",
    "name": "reporter.py",
    "path": "06_development_tools/testing_framework/reporter.py",
    "category": "06_development_tools",
    "type": "utility",
    "lines": 81,
    "size": 3100,
    "docstring": "",
    "keywords": [],
    "functions": [
      "__init__",
      "rejecting_candidate",
      "starting",
      "starting_round",
      "ending_round",
      "ending",
      "adding_requirement",
      "rejecting_candidate",
      "pinning"
    ],
    "classes": [
      "PipReporter",
      "PipDebuggingReporter"
    ],
    "imports": [
      "collections",
      "logging",
      "typing",
      "pip._vendor.resolvelib.reporters",
      "base"
    ],
    "preview": "from collections import defaultdict\nfrom logging import getLogger\nfrom typing import Any, DefaultDict\n\nfrom pip._vendor.resolvelib.reporters import BaseReporter\n\nfrom .base import Candidate, Requirement\n\nlogger = getLogger(__name__)\n\n\nclass PipReporter(BaseReporter):\n    def __init__(self) -> None:\n        self.reject_count_by_package: DefaultDict[str, int] = defaultdict(int)\n\n        self._messages_at_reject_count = {\n            1: (\n                \"pip is looking at multiple versions of {package_name} to \"\n                \"determine which version is compatible with other \"\n                \"requirements. This could take a while.\"",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1112",
    "name": "ansi_test.py",
    "path": "06_development_tools/testing_framework/ansi_test.py",
    "category": "06_development_tools",
    "type": "testing",
    "lines": 74,
    "size": 2836,
    "docstring": "",
    "keywords": [
      "testing"
    ],
    "functions": [
      "setUp",
      "tearDown",
      "testForeAttributes",
      "testBackAttributes",
      "testStyleAttributes"
    ],
    "classes": [
      "AnsiTest"
    ],
    "imports": [
      "sys",
      "unittest",
      "ansi",
      "ansitowin32"
    ],
    "preview": "# Copyright Jonathan Hartley 2013. BSD 3-Clause license, see LICENSE file.\nimport sys\nfrom unittest import TestCase, main\n\nfrom ..ansi import Back, Fore, Style\nfrom ..ansitowin32 import AnsiToWin32\n\nstdout_orig = sys.stdout\nstderr_orig = sys.stderr\n\n\nclass AnsiTest(TestCase):\n\n    def setUp(self):\n        # sanity check: stdout should be a file or StringIO object.\n        # It will only be AnsiToWin32 if init() has previously wrapped it\n        self.assertNotEqual(type(sys.stdout), AnsiToWin32)\n        self.assertNotEqual(type(sys.stderr), AnsiToWin32)\n\n    def tearDown(self):",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1113",
    "name": "winterm_test.py",
    "path": "06_development_tools/testing_framework/winterm_test.py",
    "category": "06_development_tools",
    "type": "testing",
    "lines": 130,
    "size": 3683,
    "docstring": "",
    "keywords": [
      "testing"
    ],
    "functions": [
      "testInit",
      "testGetAttrs",
      "testResetAll",
      "testFore",
      "testBack",
      "testStyle",
      "testSetConsole",
      "testSetConsoleOnStderr"
    ],
    "classes": [
      "WinTermTest"
    ],
    "imports": [
      "sys",
      "unittest",
      "winterm",
      "unittest.mock",
      "mock"
    ],
    "preview": "# Copyright Jonathan Hartley 2013. BSD 3-Clause license, see LICENSE file.\nimport sys\nfrom unittest import TestCase, main, skipUnless\n\ntry:\n    from unittest.mock import Mock, patch\nexcept ImportError:\n    from mock import Mock, patch\n\nfrom ..winterm import WinColor, WinStyle, WinTerm\n\n\nclass WinTermTest(TestCase):\n\n    @patch(\"colorama.winterm.win32\")\n    def testInit(self, mockWin32):\n        mockAttr = Mock()\n        mockAttr.wAttributes = 7 + 6 * 16 + 8\n        mockWin32.GetConsoleScreenBufferInfo.return_value = mockAttr\n        term = WinTerm()",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1114",
    "name": "integration_example_test.py",
    "path": "06_development_tools/testing_framework/integration_example_test.py",
    "category": "06_development_tools",
    "type": "testing",
    "lines": 5,
    "size": 129,
    "docstring": "",
    "keywords": [
      "testing"
    ],
    "functions": [
      "test_integration_example"
    ],
    "classes": [],
    "imports": [],
    "preview": "def test_integration_example() -> None:\n    string = \"integration_test_example\"\n\n    assert string == \"integration_test_example\"\n",
    "last_modified": "2025-05-04T22:47:11.574892"
  },
  {
    "id": "1115",
    "name": "unit_example_test.py",
    "path": "06_development_tools/testing_framework/unit_example_test.py",
    "category": "06_development_tools",
    "type": "testing",
    "lines": 5,
    "size": 108,
    "docstring": "",
    "keywords": [
      "testing"
    ],
    "functions": [
      "test_unit_example"
    ],
    "classes": [],
    "imports": [],
    "preview": "def test_unit_example() -> None:\n    string = \"unit_test_example\"\n\n    assert string == \"unit_test_example\"\n",
    "last_modified": "2025-05-04T22:47:11.575669"
  },
  {
    "id": "1116",
    "name": "test_bot_filter.py",
    "path": "06_development_tools/testing_framework/test_bot_filter.py",
    "category": "06_development_tools",
    "type": "testing",
    "lines": 55,
    "size": 1604,
    "docstring": "",
    "keywords": [
      "testing"
    ],
    "functions": [
      "test_check_user"
    ],
    "classes": [
      "TestBotFilter"
    ],
    "imports": [
      "pytest",
      "responses",
      "instabot.api.config",
      "test_bot",
      "test_variables",
      "unittest.mock",
      "mock"
    ],
    "preview": "import pytest\nimport responses\nfrom instabot.api.config import API_URL\n\nfrom .test_bot import TestBot\nfrom .test_variables import TEST_USERNAME_INFO_ITEM\n\ntry:\n    from unittest.mock import patch\nexcept ImportError:\n    from mock import patch\n\n\nclass TestBotFilter(TestBot):\n    @pytest.mark.parametrize(\n        \"filter_users,filter_business_accounts,\" + \"filter_verified_accounts,expected\",\n        [\n            (False, False, False, True),\n            (True, False, False, True),\n            (True, True, False, False),",
    "last_modified": "2025-05-04T23:28:20"
  },
  {
    "id": "1117",
    "name": "tests.py",
    "path": "06_development_tools/testing_framework/tests.py",
    "category": "06_development_tools",
    "type": "web_tools",
    "lines": 169,
    "size": 6599,
    "docstring": "webencodings.tests\n~~~~~~~~~~~~~~~~~~\n\nA basic test suite for Encoding.\n\n:copyright: Copyright 2012 by Simon Sapin\n:license: BSD, see LICENSE for details.",
    "keywords": [
      "testing"
    ],
    "functions": [
      "assert_raises",
      "test_labels",
      "test_all_labels",
      "test_invalid_label",
      "test_decode",
      "test_encode",
      "test_iter_decode",
      "test_iter_encode",
      "test_x_user_defined",
      "iter_decode_to_string"
    ],
    "classes": [],
    "imports": [
      "__future__"
    ],
    "preview": "# coding: utf-8\n\"\"\"\n\nwebencodings.tests\n~~~~~~~~~~~~~~~~~~\n\nA basic test suite for Encoding.\n\n:copyright: Copyright 2012 by Simon Sapin\n:license: BSD, see LICENSE for details.\n\n\"\"\"\n\nfrom __future__ import unicode_literals\n\nfrom . import (\n    LABELS,\n    UTF8,\n    IncrementalDecoder,\n    IncrementalEncoder,",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1118",
    "name": "simple_docs_generator.py",
    "path": "06_development_tools/documentation_tools/simple_docs_generator.py",
    "category": "06_development_tools",
    "type": "web_tools",
    "lines": 1092,
    "size": 33794,
    "docstring": "Simple HTML Documentation Generator\nCreates a comprehensive HTML documentation website without Sphinx dependencies",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "main",
      "__init__",
      "create_directory_structure",
      "load_script_data",
      "create_css",
      "create_javascript",
      "create_index_html",
      "create_category_pages",
      "create_tutorial_pages",
      "create_readme"
    ],
    "classes": [
      "SimpleDocsGenerator"
    ],
    "imports": [
      "os",
      "json",
      "pathlib",
      "datetime"
    ],
    "preview": "#!/usr/bin/env python3\n\"\"\"\nSimple HTML Documentation Generator\nCreates a comprehensive HTML documentation website without Sphinx dependencies\n\"\"\"\n\nimport os\nimport json\nfrom pathlib import Path\nfrom datetime import datetime\n\nclass SimpleDocsGenerator:\n    def __init__(self, base_path=\"/Users/steven/Documents/python\"):\n        self.base_path = Path(base_path)\n        self.docs_path = self.base_path / \"docs\"\n        self.html_path = self.docs_path / \"html\"\n        \n    def create_directory_structure(self):\n        \"\"\"Create the documentation directory structure.\"\"\"\n        print(\"\ud83d\udcc1 Creating documentation structure...\")",
    "last_modified": "2025-10-09T06:28:52.311926"
  },
  {
    "id": "1119",
    "name": "serve_docs.py",
    "path": "06_development_tools/documentation_tools/serve_docs.py",
    "category": "06_development_tools",
    "type": "utility",
    "lines": 49,
    "size": 1340,
    "docstring": "Simple Documentation Server\nServes the HTML documentation locally",
    "keywords": [],
    "functions": [
      "serve_documentation"
    ],
    "classes": [],
    "imports": [
      "http.server",
      "socketserver",
      "webbrowser",
      "os",
      "pathlib",
      "sys"
    ],
    "preview": "#!/usr/bin/env python3\n\"\"\"\nSimple Documentation Server\nServes the HTML documentation locally\n\"\"\"\n\nimport http.server\nimport socketserver\nimport webbrowser\nimport os\nfrom pathlib import Path\n\ndef serve_documentation(port=8000):\n    \"\"\"Serve the documentation on the specified port.\"\"\"\n    docs_path = Path(\"/Users/steven/Documents/python/docs/html\")\n    \n    if not docs_path.exists():\n        print(\"\u274c Documentation not found. Run simple_docs_generator.py first.\")\n        return\n    ",
    "last_modified": "2025-10-09T06:29:04.777885"
  },
  {
    "id": "1120",
    "name": "setup_sphinx_docs_uv.py",
    "path": "06_development_tools/documentation_tools/setup_sphinx_docs_uv.py",
    "category": "06_development_tools",
    "type": "setup",
    "lines": 798,
    "size": 25935,
    "docstring": "Sphinx Documentation Setup Script (UV Compatible)\nCreates comprehensive documentation for all Python projects",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "main",
      "__init__",
      "check_dependencies",
      "create_directory_structure",
      "initialize_sphinx",
      "create_conf_py",
      "create_index_rst",
      "create_overview_rst",
      "create_category_pages",
      "create_api_documentation"
    ],
    "classes": [
      "SphinxDocSetup"
    ],
    "imports": [
      "os",
      "subprocess",
      "sys",
      "pathlib",
      "json"
    ],
    "preview": "#!/usr/bin/env python3\n\"\"\"\nSphinx Documentation Setup Script (UV Compatible)\nCreates comprehensive documentation for all Python projects\n\"\"\"\n\nimport os\nimport subprocess\nimport sys\nfrom pathlib import Path\nimport json\n\nclass SphinxDocSetup:\n    def __init__(self, base_path=\"/Users/steven/Documents/python\"):\n        self.base_path = Path(base_path)\n        self.docs_path = self.base_path / \"docs\"\n        self.sphinx_path = self.docs_path / \"sphinx\"\n        \n    def check_dependencies(self):\n        \"\"\"Check if required packages are installed.\"\"\"",
    "last_modified": "2025-10-09T06:26:52.989487"
  },
  {
    "id": "1121",
    "name": "setup_sphinx_docs.py",
    "path": "06_development_tools/documentation_tools/setup_sphinx_docs.py",
    "category": "06_development_tools",
    "type": "setup",
    "lines": 786,
    "size": 25105,
    "docstring": "Sphinx Documentation Setup Script\nCreates comprehensive documentation for all Python projects",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "main",
      "__init__",
      "check_dependencies",
      "create_directory_structure",
      "initialize_sphinx",
      "create_conf_py",
      "create_index_rst",
      "create_overview_rst",
      "create_category_pages",
      "create_api_documentation"
    ],
    "classes": [
      "SphinxDocSetup"
    ],
    "imports": [
      "os",
      "subprocess",
      "sys",
      "pathlib",
      "json"
    ],
    "preview": "#!/usr/bin/env python3\n\"\"\"\nSphinx Documentation Setup Script\nCreates comprehensive documentation for all Python projects\n\"\"\"\n\nimport os\nimport subprocess\nimport sys\nfrom pathlib import Path\nimport json\n\nclass SphinxDocSetup:\n    def __init__(self, base_path=\"/Users/steven/Documents/python\"):\n        self.base_path = Path(base_path)\n        self.docs_path = self.base_path / \"docs\"\n        self.sphinx_path = self.docs_path / \"sphinx\"\n        \n    def check_dependencies(self):\n        \"\"\"Check if required packages are installed.\"\"\"",
    "last_modified": "2025-10-09T06:25:27.317940"
  },
  {
    "id": "1122",
    "name": "serve_code_browser.py",
    "path": "06_development_tools/documentation_tools/serve_code_browser.py",
    "category": "06_development_tools",
    "type": "utility",
    "lines": 49,
    "size": 1341,
    "docstring": "Code Browser Server\nServes the visual code browser locally",
    "keywords": [],
    "functions": [
      "serve_code_browser"
    ],
    "classes": [],
    "imports": [
      "http.server",
      "socketserver",
      "webbrowser",
      "os",
      "pathlib",
      "sys"
    ],
    "preview": "#!/usr/bin/env python3\n\"\"\"\nCode Browser Server\nServes the visual code browser locally\n\"\"\"\n\nimport http.server\nimport socketserver\nimport webbrowser\nimport os\nfrom pathlib import Path\n\ndef serve_code_browser(port=8001):\n    \"\"\"Serve the code browser on the specified port.\"\"\"\n    browser_path = Path(\"/Users/steven/Documents/python/code_browser\")\n    \n    if not browser_path.exists():\n        print(\"\u274c Code browser not found. Run create_code_browser.py first.\")\n        return\n    ",
    "last_modified": "2025-10-09T06:34:54.183931"
  },
  {
    "id": "1123",
    "name": "find_script.py",
    "path": "06_development_tools/analysis_tools/find_script.py",
    "category": "06_development_tools",
    "type": "video_processing",
    "lines": 263,
    "size": 10847,
    "docstring": "Python Script Finder and Navigator\nHelps you locate any Python script in your organized structure",
    "keywords": [],
    "functions": [
      "main",
      "__init__",
      "build_index",
      "find_script",
      "find_by_functionality",
      "show_script_location",
      "show_directory_structure",
      "show_category_contents",
      "interactive_search",
      "print_tree"
    ],
    "classes": [
      "ScriptFinder"
    ],
    "imports": [
      "os",
      "re",
      "pathlib",
      "collections"
    ],
    "preview": "#!/usr/bin/env python3\n\"\"\"\nPython Script Finder and Navigator\nHelps you locate any Python script in your organized structure\n\"\"\"\n\nimport os\nimport re\nfrom pathlib import Path\nfrom collections import defaultdict\n\nclass ScriptFinder:\n    def __init__(self, base_path=\"/Users/steven/Documents/python\"):\n        self.base_path = Path(base_path)\n        self.script_index = {}\n        self.build_index()\n    \n    def build_index(self):\n        \"\"\"Build an index of all Python scripts for fast searching.\"\"\"\n        print(\"\ud83d\udd0d Building script index...\")",
    "last_modified": "2025-10-09T06:21:35.267534"
  },
  {
    "id": "1124",
    "name": "whereis.py",
    "path": "06_development_tools/analysis_tools/whereis.py",
    "category": "06_development_tools",
    "type": "utility",
    "lines": 98,
    "size": 3461,
    "docstring": "Quick Script Locator - whereis.py\nSimple command-line tool to find Python scripts\nUsage: python whereis.py <script_name>",
    "keywords": [],
    "functions": [
      "find_script",
      "show_categories",
      "main"
    ],
    "classes": [],
    "imports": [
      "sys",
      "os",
      "pathlib"
    ],
    "preview": "#!/usr/bin/env python3\n\"\"\"\nQuick Script Locator - whereis.py\nSimple command-line tool to find Python scripts\nUsage: python whereis.py <script_name>\n\"\"\"\n\nimport sys\nimport os\nfrom pathlib import Path\n\ndef find_script(script_name):\n    \"\"\"Find a script by name.\"\"\"\n    base_path = Path(\"/Users/steven/Documents/python\")\n    \n    # Search for the script\n    matches = list(base_path.rglob(f\"*{script_name}*\"))\n    py_matches = [f for f in matches if f.suffix == '.py']\n    \n    if not py_matches:",
    "last_modified": "2025-10-09T06:22:21.085725"
  },
  {
    "id": "1125",
    "name": "script_map.py",
    "path": "06_development_tools/analysis_tools/script_map.py",
    "category": "06_development_tools",
    "type": "utility",
    "lines": 246,
    "size": 11337,
    "docstring": "Python Script Map Generator\nCreates a comprehensive map of all Python scripts and their locations",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "main",
      "__init__",
      "generate_complete_map",
      "get_category_description",
      "get_subcategory_description",
      "map_by_functionality",
      "save_map",
      "save_human_readable_map",
      "print_quick_reference"
    ],
    "classes": [
      "ScriptMapper"
    ],
    "imports": [
      "os",
      "json",
      "pathlib",
      "collections"
    ],
    "preview": "#!/usr/bin/env python3\n\"\"\"\nPython Script Map Generator\nCreates a comprehensive map of all Python scripts and their locations\n\"\"\"\n\nimport os\nimport json\nfrom pathlib import Path\nfrom collections import defaultdict\n\nclass ScriptMapper:\n    def __init__(self, base_path=\"/Users/steven/Documents/python\"):\n        self.base_path = Path(base_path)\n        self.script_map = {}\n        self.category_map = {}\n        self.functionality_map = {}\n        \n    def generate_complete_map(self):\n        \"\"\"Generate a complete map of all Python scripts.\"\"\"",
    "last_modified": "2025-10-09T06:22:08.506094"
  },
  {
    "id": "1126",
    "name": "create_code_browser.py",
    "path": "06_development_tools/analysis_tools/create_code_browser.py",
    "category": "06_development_tools",
    "type": "utility",
    "lines": 1311,
    "size": 39318,
    "docstring": "Code Browser Generator\nCreates a visual code browser similar to avatararts.org/dalle.html\nDisplays Python scripts as interactive cards with code previews",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "main",
      "__init__",
      "create_directory_structure",
      "scan_python_files",
      "extract_file_metadata",
      "extract_docstring",
      "extract_imports",
      "extract_functions",
      "extract_classes",
      "determine_file_type"
    ],
    "classes": [
      "CodeBrowserGenerator"
    ],
    "imports": [
      "os",
      "json",
      "base64",
      "pathlib",
      "datetime",
      "re"
    ],
    "preview": "#!/usr/bin/env python3\n\"\"\"\nCode Browser Generator\nCreates a visual code browser similar to avatararts.org/dalle.html\nDisplays Python scripts as interactive cards with code previews\n\"\"\"\n\nimport os\nimport json\nimport base64\nfrom pathlib import Path\nfrom datetime import datetime\nimport re\n\nclass CodeBrowserGenerator:\n    def __init__(self, base_path=\"/Users/steven/Documents/python\"):\n        self.base_path = Path(base_path)\n        self.browser_path = self.base_path / \"code_browser\"\n        self.html_path = self.browser_path / \"index.html\"\n        ",
    "last_modified": "2025-10-09T06:34:34.070147"
  },
  {
    "id": "1127",
    "name": "content_based_migration.py",
    "path": "06_development_tools/analysis_tools/content_based_migration.py",
    "category": "06_development_tools",
    "type": "analysis",
    "lines": 323,
    "size": 13232,
    "docstring": "Content-Based Migration Script\nReorganizes files based on deep content analysis rather than filename patterns",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "main",
      "__init__",
      "load_analysis_results",
      "create_content_based_structure",
      "migrate_files_by_content",
      "consolidate_similar_files",
      "get_base_name",
      "consolidate_file_group",
      "create_shared_libraries",
      "generate_migration_report"
    ],
    "classes": [
      "ContentBasedMigrator"
    ],
    "imports": [
      "os",
      "re",
      "shutil",
      "json",
      "pathlib",
      "collections"
    ],
    "preview": "#!/usr/bin/env python3\n\"\"\"\nContent-Based Migration Script\nReorganizes files based on deep content analysis rather than filename patterns\n\"\"\"\n\nimport os\nimport re\nimport shutil\nimport json\nfrom pathlib import Path\nfrom collections import defaultdict\n\nclass ContentBasedMigrator:\n    def __init__(self, base_path=\"/Users/steven/Documents/python\"):\n        self.base_path = Path(base_path)\n        self.migration_log = []\n        \n    def load_analysis_results(self):\n        \"\"\"Load the content analysis results.\"\"\"",
    "last_modified": "2025-10-09T06:16:58.776702"
  },
  {
    "id": "1128",
    "name": "main.py.py",
    "path": "06_development_tools/main.py_consolidated/main.py.py",
    "category": "06_development_tools",
    "type": "utility",
    "lines": 31,
    "size": 810,
    "docstring": "",
    "keywords": [],
    "functions": [
      "Test"
    ],
    "classes": [],
    "imports": [
      "os",
      "re",
      "google_url_scrapper",
      "rank_provider"
    ],
    "preview": "import os\nimport re\n\nfrom google_url_scrapper import google_url_scrapper\nfrom rank_provider import AlexaTrafficRank, GooglePageRank, RankProvider\n\n\ndef Test():\n    keyword = raw_input(\"Prompt :\")\n    G = google_url_scrapper()\n    urls = G.scrape(keyword)\n    for purl in urls:\n        url = purl[0]\n        Title = purl[1]\n        results = G.MajesticSEO_API(url)\n        print(\"Traffic stats for: %s\" % (url))\n        print(\"Title: %s\" % (Title))\n        print(\"ACRank: %s\" % (results[\"ACRank\"]))\n        print(\"ExtBackLinks: %s\" % (results[\"ExtBackLinks\"]))\n        providers = (",
    "last_modified": "2025-05-04T23:28:20.717386"
  },
  {
    "id": "1129",
    "name": "main.py_02.py",
    "path": "06_development_tools/main.py_consolidated/main.py_02.py",
    "category": "06_development_tools",
    "type": "utility",
    "lines": 9,
    "size": 104,
    "docstring": "",
    "keywords": [],
    "functions": [
      "h"
    ],
    "classes": [],
    "imports": [
      "fastapi"
    ],
    "preview": "from fastapi import FastAPI\n\napp = FastAPI()\n\n\n@app.get(\"/api/health\")\ndef h():\n    return {\"ok\": True}\n",
    "last_modified": "2025-09-11T13:24:01.259289"
  },
  {
    "id": "1130",
    "name": "deepseek_python_20250608130224.py",
    "path": "06_development_tools/development_utilities/deepseek_python_20250608130224.py",
    "category": "06_development_tools",
    "type": "utility",
    "lines": 2,
    "size": 7,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [],
    "preview": "format\n",
    "last_modified": "2025-09-13T05:53:27.386086"
  },
  {
    "id": "1131",
    "name": "worker.py",
    "path": "06_development_tools/development_utilities/worker.py",
    "category": "06_development_tools",
    "type": "utility",
    "lines": 2,
    "size": 16,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [],
    "preview": "print(\"worker\")\n",
    "last_modified": "2025-09-11T13:24:01.247951"
  },
  {
    "id": "1132",
    "name": "config_20250430201612.py",
    "path": "06_development_tools/development_utilities/config_20250430201612.py",
    "category": "06_development_tools",
    "type": "setup",
    "lines": 3,
    "size": 48,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [],
    "preview": "# config.py\nSOURCE_DIRECTORY = \"/Users/steven/\"\n",
    "last_modified": "2025-04-30T20:16:12.706453"
  },
  {
    "id": "1133",
    "name": "ptt.py",
    "path": "06_development_tools/development_utilities/ptt.py",
    "category": "06_development_tools",
    "type": "utility",
    "lines": 11,
    "size": 241,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "pyttsx3"
    ],
    "preview": "import pyttsx3\n\nengine = pyttsx3.init()\nvoices = engine.getProperty(\"voices\")\nfor voice in voices:\n    print(voice, voice.id)\n    engine.setProperty(\"voice\", voice.id)\n    engine.say(\"Hello World!\")\n    engine.runAndWait()\n    engine.stop()\n",
    "last_modified": "2025-05-04T22:47:11.885485"
  },
  {
    "id": "1134",
    "name": "selection_prefs.py",
    "path": "06_development_tools/development_utilities/selection_prefs.py",
    "category": "06_development_tools",
    "type": "utility",
    "lines": 52,
    "size": 1907,
    "docstring": "",
    "keywords": [],
    "functions": [
      "__init__"
    ],
    "classes": [
      "SelectionPreferences"
    ],
    "imports": [
      "typing",
      "pip._internal.models.format_control"
    ],
    "preview": "from typing import Optional\n\nfrom pip._internal.models.format_control import FormatControl\n\n\nclass SelectionPreferences:\n    \"\"\"\n    Encapsulates the candidate selection preferences for downloading\n    and installing files.\n    \"\"\"\n\n    __slots__ = [\n        \"allow_yanked\",\n        \"allow_all_prereleases\",\n        \"format_control\",\n        \"prefer_binary\",\n        \"ignore_requires_python\",\n    ]\n\n    # Don't include an allow_yanked default value to make sure each call",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1135",
    "name": "before.py",
    "path": "06_development_tools/development_utilities/before.py",
    "category": "06_development_tools",
    "type": "utility",
    "lines": 49,
    "size": 1568,
    "docstring": "",
    "keywords": [],
    "functions": [
      "before_nothing",
      "before_log",
      "log_it"
    ],
    "classes": [],
    "imports": [
      "typing",
      "pip._vendor.tenacity",
      "logging",
      "pip._vendor.tenacity"
    ],
    "preview": "# Copyright 2016 Julien Danjou\n# Copyright 2016 Joshua Harlow\n# Copyright 2013-2014 Ray Holder\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport typing\n\nfrom pip._vendor.tenacity import _utils\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1136",
    "name": "console.py",
    "path": "06_development_tools/development_utilities/console.py",
    "category": "06_development_tools",
    "type": "utility",
    "lines": 78,
    "size": 1681,
    "docstring": "pygments.console\n~~~~~~~~~~~~~~~~\n\nFormat colored console output.\n\n:copyright: Copyright 2006-2023 by the Pygments team, see AUTHORS.\n:license: BSD, see LICENSE for details.",
    "keywords": [],
    "functions": [
      "reset_color",
      "colorize",
      "ansiformat"
    ],
    "classes": [],
    "imports": [],
    "preview": "\"\"\"\npygments.console\n~~~~~~~~~~~~~~~~\n\nFormat colored console output.\n\n:copyright: Copyright 2006-2023 by the Pygments team, see AUTHORS.\n:license: BSD, see LICENSE for details.\n\"\"\"\n\nesc = \"\\x1b[\"\n\ncodes = {}\ncodes[\"\"] = \"\"\ncodes[\"reset\"] = esc + \"39;49;00m\"\n\ncodes[\"bold\"] = esc + \"01m\"\ncodes[\"faint\"] = esc + \"02m\"\ncodes[\"standout\"] = esc + \"03m\"\ncodes[\"underline\"] = esc + \"04m\"",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1137",
    "name": "vid.py",
    "path": "06_development_tools/development_utilities/vid.py",
    "category": "06_development_tools",
    "type": "utility",
    "lines": 17,
    "size": 237,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [],
    "preview": "(\n    yt\n    - dlp\n    - a / Users / steven / Downloads / Misc / Thumbnails / vid.txt\n    - -write\n    - annotations\n    - -write\n    - description\n    - -write\n    - info\n    - json\n    - -write\n    - sub\n    - -write\n    - thumbnail\n)\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1138",
    "name": "ollama-run.py",
    "path": "06_development_tools/development_utilities/ollama-run.py",
    "category": "06_development_tools",
    "type": "utility",
    "lines": 40,
    "size": 1009,
    "docstring": "",
    "keywords": [],
    "functions": [
      "execute_command"
    ],
    "classes": [],
    "imports": [
      "os",
      "tkinter",
      "subprocess",
      "tkinter"
    ],
    "preview": "import os\nimport tkinter as tk\nfrom subprocess import PIPE, STDOUT, Popen\nfrom tkinter import scrolledtext\n\n\ndef execute_command():\n    # Get the command entered by the user\n    command = command_entry.get()\n\n    # Execute the command using subprocess\n    process = Popen(command, shell=True, stdout=PIPE, stderr=STDOUT)\n    output, _ = process.communicate()\n\n    # Display the output in the text area\n    output_text.delete(1.0, tk.END)\n    output_text.insert(tk.END, output.decode(\"utf-8\"))\n\n\n# GUI setup",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1139",
    "name": "before_sleep.py",
    "path": "06_development_tools/development_utilities/before_sleep.py",
    "category": "06_development_tools",
    "type": "utility",
    "lines": 73,
    "size": 2384,
    "docstring": "",
    "keywords": [],
    "functions": [
      "before_sleep_nothing",
      "before_sleep_log",
      "log_it"
    ],
    "classes": [],
    "imports": [
      "typing",
      "pip._vendor.tenacity",
      "logging",
      "pip._vendor.tenacity"
    ],
    "preview": "# Copyright 2016 Julien Danjou\n# Copyright 2016 Joshua Harlow\n# Copyright 2013-2014 Ray Holder\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport typing\n\nfrom pip._vendor.tenacity import _utils\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1140",
    "name": "mpvListener.py",
    "path": "06_development_tools/development_utilities/mpvListener.py",
    "category": "06_development_tools",
    "type": "utility",
    "lines": 5,
    "size": 45,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "mpv"
    ],
    "preview": "import mpv\n\nplayer = mpv.MPV()\nplayer.stop()\n",
    "last_modified": "2025-03-28T18:35:47.779566"
  },
  {
    "id": "1141",
    "name": "enums.py",
    "path": "06_development_tools/development_utilities/enums.py",
    "category": "06_development_tools",
    "type": "utility",
    "lines": 86,
    "size": 1683,
    "docstring": "All of the Enums that are used throughout the chardet package.\n\n:author: Dan Blanchard (dan.blanchard@gmail.com)",
    "keywords": [],
    "functions": [
      "get_num_categories"
    ],
    "classes": [
      "InputState",
      "LanguageFilter",
      "ProbingState",
      "MachineState",
      "SequenceLikelihood",
      "CharacterCategory"
    ],
    "imports": [
      "enum"
    ],
    "preview": "\"\"\"\nAll of the Enums that are used throughout the chardet package.\n\n:author: Dan Blanchard (dan.blanchard@gmail.com)\n\"\"\"\n\nfrom enum import Enum, Flag\n\n\nclass InputState:\n    \"\"\"\n    This enum represents the different states a universal detector can be in.\n    \"\"\"\n\n    PURE_ASCII = 0\n    ESC_ASCII = 1\n    HIGH_BYTE = 2\n\n\nclass LanguageFilter(Flag):",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1142",
    "name": "ml_service.py",
    "path": "06_development_tools/development_utilities/ml_service.py",
    "category": "06_development_tools",
    "type": "utility",
    "lines": 7,
    "size": 206,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "llm_engineering.infrastructure.inference_pipeline_api",
      "uvicorn"
    ],
    "preview": "from llm_engineering.infrastructure.inference_pipeline_api import app  # noqa\n\nif __name__ == \"__main__\":\n    import uvicorn\n\n    uvicorn.run(\"tools.ml_service:app\", host=\"0.0.0.0\", port=8000, reload=True)\n",
    "last_modified": "2025-05-04T22:47:11.576891"
  },
  {
    "id": "1143",
    "name": "package_data.py",
    "path": "06_development_tools/development_utilities/package_data.py",
    "category": "06_development_tools",
    "type": "analysis",
    "lines": 2,
    "size": 20,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [],
    "preview": "__version__ = \"3.4\"\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1144",
    "name": "playwright.py",
    "path": "06_development_tools/development_utilities/playwright.py",
    "category": "06_development_tools",
    "type": "utility",
    "lines": 6,
    "size": 253,
    "docstring": "",
    "keywords": [],
    "functions": [
      "clear_cookie_by_name"
    ],
    "classes": [],
    "imports": [],
    "preview": "def clear_cookie_by_name(context, cookie_cleared_name):\n    cookies = context.cookies()\n    filtered_cookies = [cookie for cookie in cookies if cookie[\"name\"] != cookie_cleared_name]\n    context.clear_cookies()\n    context.add_cookies(filtered_cookies)\n",
    "last_modified": "2025-09-13T05:54:00.328852"
  },
  {
    "id": "1145",
    "name": "config.py",
    "path": "06_development_tools/development_utilities/config.py",
    "category": "06_development_tools",
    "type": "setup",
    "lines": 3,
    "size": 47,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [],
    "preview": "# config.py\nSOURCE_DIRECTORY = \"/Volumes/Pics\"\n",
    "last_modified": "2025-05-04T22:47:12"
  },
  {
    "id": "1146",
    "name": "version.py",
    "path": "06_development_tools/development_utilities/version.py",
    "category": "06_development_tools",
    "type": "setup",
    "lines": 10,
    "size": 244,
    "docstring": "This module exists only to simplify retrieving the version number of chardet\nfrom within setuptools and from chardet subpackages.\n\n:author: Dan Blanchard (dan.blanchard@gmail.com)",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [],
    "preview": "\"\"\"\nThis module exists only to simplify retrieving the version number of chardet\nfrom within setuptools and from chardet subpackages.\n\n:author: Dan Blanchard (dan.blanchard@gmail.com)\n\"\"\"\n\n__version__ = \"5.1.0\"\nVERSION = __version__.split(\".\")\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1147",
    "name": "NewUpload_20250607124910.py",
    "path": "06_development_tools/development_utilities/NewUpload_20250607124910.py",
    "category": "06_development_tools",
    "type": "youtube",
    "lines": 1,
    "size": 0,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [],
    "preview": "",
    "last_modified": "2025-06-07T12:49:13.216444"
  },
  {
    "id": "1148",
    "name": "models.py",
    "path": "06_development_tools/development_utilities/models.py",
    "category": "06_development_tools",
    "type": "utility",
    "lines": 39,
    "size": 1192,
    "docstring": "Utilities for defining models",
    "keywords": [],
    "functions": [
      "__init__",
      "__hash__",
      "__lt__",
      "__le__",
      "__gt__",
      "__ge__",
      "__eq__",
      "_compare"
    ],
    "classes": [
      "KeyBasedCompareMixin"
    ],
    "imports": [
      "operator",
      "typing"
    ],
    "preview": "\"\"\"Utilities for defining models\"\"\"\n\nimport operator\nfrom typing import Any, Callable, Type\n\n\nclass KeyBasedCompareMixin:\n    \"\"\"Provides comparison capabilities that is based on a key\"\"\"\n\n    __slots__ = [\"_compare_key\", \"_defining_class\"]\n\n    def __init__(self, key: Any, defining_class: Type[\"KeyBasedCompareMixin\"]) -> None:\n        self._compare_key = key\n        self._defining_class = defining_class\n\n    def __hash__(self) -> int:\n        return hash(self._compare_key)\n\n    def __lt__(self, other: Any) -> bool:\n        return self._compare(other, operator.__lt__)",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1149",
    "name": "update_github_repo.py",
    "path": "06_development_tools/development_utilities/update_github_repo.py",
    "category": "06_development_tools",
    "type": "utility",
    "lines": 1007,
    "size": 31927,
    "docstring": "GitHub Repository Update Script\nUpdates your GitHub Python repository with all the new tools and documentation",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "main",
      "__init__",
      "create_github_structure",
      "create_readme",
      "copy_documentation",
      "copy_tools",
      "copy_scripts",
      "copy_scripts_safe",
      "copy_examples",
      "create_github_files"
    ],
    "classes": [
      "GitHubRepoUpdater"
    ],
    "imports": [
      "os",
      "subprocess",
      "shutil",
      "pathlib",
      "datetime"
    ],
    "preview": "#!/usr/bin/env python3\n\"\"\"\nGitHub Repository Update Script\nUpdates your GitHub Python repository with all the new tools and documentation\n\"\"\"\n\nimport os\nimport subprocess\nimport shutil\nfrom pathlib import Path\nfrom datetime import datetime\n\nclass GitHubRepoUpdater:\n    def __init__(self, base_path=\"/Users/steven/Documents/python\"):\n        self.base_path = Path(base_path)\n        self.github_path = self.base_path / \"github_repo\"\n        \n    def create_github_structure(self):\n        \"\"\"Create the GitHub repository structure.\"\"\"\n        print(\"\ud83d\udcc1 Creating GitHub repository structure...\")",
    "last_modified": "2025-10-09T06:38:11.538675"
  },
  {
    "id": "1150",
    "name": "terminal.py",
    "path": "06_development_tools/development_utilities/terminal.py",
    "category": "06_development_tools",
    "type": "utility",
    "lines": 134,
    "size": 4357,
    "docstring": "pygments.formatters.terminal\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nFormatter for terminal output with ANSI sequences.\n\n:copyright: Copyright 2006-2023 by the Pygments team, see AUTHORS.\n:license: BSD, see LICENSE for details.",
    "keywords": [],
    "functions": [
      "__init__",
      "format",
      "_write_lineno",
      "_get_color",
      "format_unencoded"
    ],
    "classes": [
      "TerminalFormatter"
    ],
    "imports": [
      "pip._vendor.pygments.console",
      "pip._vendor.pygments.formatter",
      "pip._vendor.pygments.token",
      "pip._vendor.pygments.util"
    ],
    "preview": "\"\"\"\npygments.formatters.terminal\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nFormatter for terminal output with ANSI sequences.\n\n:copyright: Copyright 2006-2023 by the Pygments team, see AUTHORS.\n:license: BSD, see LICENSE for details.\n\"\"\"\n\nfrom pip._vendor.pygments.console import ansiformat\nfrom pip._vendor.pygments.formatter import Formatter\nfrom pip._vendor.pygments.token import (\n    Comment,\n    Error,\n    Generic,\n    Keyword,\n    Name,\n    Number,\n    Operator,",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1151",
    "name": "credentials.py",
    "path": "06_development_tools/development_utilities/credentials.py",
    "category": "06_development_tools",
    "type": "utility",
    "lines": 3,
    "size": 42,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [],
    "preview": "username = \"xxxxxxx\"\npassword = \"xxxxxxx\"\n",
    "last_modified": "2025-09-13T05:53:38.642643"
  },
  {
    "id": "1152",
    "name": "config_20250430201608.py",
    "path": "06_development_tools/development_utilities/config_20250430201608.py",
    "category": "06_development_tools",
    "type": "setup",
    "lines": 3,
    "size": 48,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [],
    "preview": "# config.py\nSOURCE_DIRECTORY = \"/Users/steven/\"\n",
    "last_modified": "2025-04-30T20:16:08.984808"
  },
  {
    "id": "1153",
    "name": "diag_20250530223526.py",
    "path": "06_development_tools/development_utilities/diag_20250530223526.py",
    "category": "06_development_tools",
    "type": "utility",
    "lines": 1,
    "size": 0,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [],
    "preview": "",
    "last_modified": "2025-05-30T22:35:32.974215"
  },
  {
    "id": "1154",
    "name": "base_uploader.py",
    "path": "06_development_tools/development_utilities/base_uploader.py",
    "category": "06_development_tools",
    "type": "youtube",
    "lines": 21,
    "size": 713,
    "docstring": "",
    "keywords": [
      "youtube"
    ],
    "functions": [
      "check_location",
      "upload_gallery"
    ],
    "classes": [
      "BaseUploader"
    ],
    "imports": [],
    "preview": "class BaseUploader:\n    \"\"\"\n    Base class defining the interface to a remote uploader.\n    \"\"\"\n\n    def check_location(self, location):\n        \"\"\"\n        Checks if the provided location for the upload is valid or not\n        :param location: location where the gallery will be uploaded (uploader specific)\n        :return: True if the location is valid, False otherwise\n        \"\"\"\n        pass\n\n    def upload_gallery(self, location, gallery_path):\n        \"\"\"\n        Upload the gallery to the specified location\n        :param location: location where the gallery will be uploaded (uploader specific)\n        :param gallery_path: path to the root of the public files of the gallery\n        \"\"\"\n        pass",
    "last_modified": "2025-05-04T22:47:13"
  },
  {
    "id": "1155",
    "name": "config_20250329125101.py",
    "path": "06_development_tools/development_utilities/config_20250329125101.py",
    "category": "06_development_tools",
    "type": "setup",
    "lines": 3,
    "size": 56,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [],
    "preview": "# config.py\nSOURCE_DIRECTORY = \"/Volumes/oG-bAk/steven\"\n",
    "last_modified": "2025-04-30T20:16:01.517106"
  },
  {
    "id": "1156",
    "name": "config 3.py",
    "path": "06_development_tools/development_utilities/config 3.py",
    "category": "06_development_tools",
    "type": "setup",
    "lines": 4,
    "size": 89,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [],
    "preview": "assemblyai = \"ASSEMBLY_API_KEY\"\npexelKey = \"PEXELS_KEY\"\nelevenLabsKey = \"ELEVENLABS_KEY\"\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1157",
    "name": "txt-csv (1).py",
    "path": "06_development_tools/development_utilities/txt-csv (1).py",
    "category": "06_development_tools",
    "type": "utility",
    "lines": 34,
    "size": 1062,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "csv"
    ],
    "preview": "import csv\n\n# Input and output file paths\ninput_file = \"/Users/steven/dalle/trashy.txt\"\noutput_file = \"/Users/steven/dalle/trashy_output.txt\"\n\n# Open the input file for reading\nwith open(input_file, \"r\") as file:\n    lines = file.readlines()\n\n# Open the output file for writing\nwith open(output_file, \"w\", newline=\"\") as csvfile:\n    writer = csv.writer(csvfile)\n    writer.writerow([\"Url\", \"Prompt\"])  # Write the CSV header\n\n    url = None\n    prompt = \"\"\n\n    for line in lines:\n        line = line.strip()",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1158",
    "name": "api_env_setup.py",
    "path": "06_development_tools/development_utilities/api_env_setup.py",
    "category": "06_development_tools",
    "type": "web_tools",
    "lines": 1,
    "size": 0,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [],
    "preview": "",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1159",
    "name": "stop.py",
    "path": "06_development_tools/development_utilities/stop.py",
    "category": "06_development_tools",
    "type": "utility",
    "lines": 104,
    "size": 3086,
    "docstring": "",
    "keywords": [],
    "functions": [
      "__call__",
      "__and__",
      "__or__",
      "__init__",
      "__call__",
      "__init__",
      "__call__",
      "__call__",
      "__init__",
      "__call__"
    ],
    "classes": [
      "stop_base",
      "stop_any",
      "stop_all",
      "_stop_never",
      "stop_when_event_set",
      "stop_after_attempt",
      "stop_after_delay"
    ],
    "imports": [
      "abc",
      "typing",
      "pip._vendor.tenacity",
      "threading",
      "pip._vendor.tenacity"
    ],
    "preview": "# Copyright 2016\u20132021 Julien Danjou\n# Copyright 2016 Joshua Harlow\n# Copyright 2013-2014 Ray Holder\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nimport abc\nimport typing\n\nfrom pip._vendor.tenacity import _utils\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1160",
    "name": "irc.py",
    "path": "06_development_tools/development_utilities/irc.py",
    "category": "06_development_tools",
    "type": "utility",
    "lines": 164,
    "size": 4712,
    "docstring": "pygments.formatters.irc\n~~~~~~~~~~~~~~~~~~~~~~~\n\nFormatter for IRC output\n\n:copyright: Copyright 2006-2023 by the Pygments team, see AUTHORS.\n:license: BSD, see LICENSE for details.",
    "keywords": [],
    "functions": [
      "ircformat",
      "__init__",
      "_write_lineno",
      "format_unencoded"
    ],
    "classes": [
      "IRCFormatter"
    ],
    "imports": [
      "pip._vendor.pygments.formatter",
      "pip._vendor.pygments.token",
      "pip._vendor.pygments.util"
    ],
    "preview": "\"\"\"\npygments.formatters.irc\n~~~~~~~~~~~~~~~~~~~~~~~\n\nFormatter for IRC output\n\n:copyright: Copyright 2006-2023 by the Pygments team, see AUTHORS.\n:license: BSD, see LICENSE for details.\n\"\"\"\n\nfrom pip._vendor.pygments.formatter import Formatter\nfrom pip._vendor.pygments.token import (\n    Comment,\n    Error,\n    Generic,\n    Keyword,\n    Name,\n    Number,\n    Operator,\n    String,",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1161",
    "name": "leodown_20250102105144.py",
    "path": "06_development_tools/development_utilities/leodown_20250102105144.py",
    "category": "06_development_tools",
    "type": "utility",
    "lines": 1,
    "size": 0,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [],
    "preview": "",
    "last_modified": "2025-03-28T18:37:01.034000"
  },
  {
    "id": "1162",
    "name": "x_user_defined.py",
    "path": "06_development_tools/development_utilities/x_user_defined.py",
    "category": "06_development_tools",
    "type": "web_tools",
    "lines": 326,
    "size": 4287,
    "docstring": "webencodings.x_user_defined\n~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nAn implementation of the x-user-defined encoding.\n\n:copyright: Copyright 2012 by Simon Sapin\n:license: BSD, see LICENSE for details.",
    "keywords": [],
    "functions": [
      "encode",
      "decode",
      "encode",
      "decode"
    ],
    "classes": [
      "Codec",
      "IncrementalEncoder",
      "IncrementalDecoder",
      "StreamWriter",
      "StreamReader"
    ],
    "imports": [
      "__future__",
      "codecs"
    ],
    "preview": "# coding: utf-8\n\"\"\"\n\nwebencodings.x_user_defined\n~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nAn implementation of the x-user-defined encoding.\n\n:copyright: Copyright 2012 by Simon Sapin\n:license: BSD, see LICENSE for details.\n\n\"\"\"\n\nfrom __future__ import unicode_literals\n\nimport codecs\n\n### Codec APIs\n\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1163",
    "name": "datetime_local.py",
    "path": "06_development_tools/development_utilities/datetime_local.py",
    "category": "06_development_tools",
    "type": "utility",
    "lines": 11,
    "size": 241,
    "docstring": "For when pip wants to check the date or time.",
    "keywords": [],
    "functions": [
      "today_is_later_than"
    ],
    "classes": [],
    "imports": [
      "datetime"
    ],
    "preview": "\"\"\"For when pip wants to check the date or time.\"\"\"\n\nimport datetime\n\n\ndef today_is_later_than(year: int, month: int, day: int) -> bool:\n    today = datetime.date.today()\n    given = datetime.date(year, month, day)\n\n    return today > given\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1164",
    "name": "customerrors.py",
    "path": "06_development_tools/development_utilities/customerrors.py",
    "category": "06_development_tools",
    "type": "utility",
    "lines": 55,
    "size": 1260,
    "docstring": "Errors used in this application",
    "keywords": [],
    "functions": [
      "__init__",
      "__init__",
      "__init__",
      "__init__",
      "__init__"
    ],
    "classes": [
      "Error",
      "AuthError",
      "DirectoryNotFoundError",
      "FileNotFoundError",
      "InvalidConfig"
    ],
    "imports": [],
    "preview": "\"\"\"Errors used in this application\"\"\"\n\n\nclass Error(Exception):\n    \"\"\"Base class for exceptions in this module\"\"\"\n\n    def __init__(self, message):\n        super(Error, self).__init__()\n        self.message = message\n\n\nclass AuthError(Error):\n    \"\"\"Exception raised for authentication errors\n\n    Attributes:\n        message -- explanation of the error\n    \"\"\"\n\n    def __init__(self, message):\n        super(AuthError, self).__init__(message)",
    "last_modified": "2025-05-04T23:27:55"
  },
  {
    "id": "1165",
    "name": "__init__.py",
    "path": "06_development_tools/development_utilities/__init__.py",
    "category": "06_development_tools",
    "type": "setup",
    "lines": 2,
    "size": 22,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [],
    "preview": "__version__ = \"0.1.0\"\n",
    "last_modified": "2025-09-11T13:24:00.164291"
  },
  {
    "id": "1166",
    "name": "notify.py",
    "path": "06_development_tools/development_utilities/notify.py",
    "category": "06_development_tools",
    "type": "utility",
    "lines": 38,
    "size": 1022,
    "docstring": "Responsible for sending a notification",
    "keywords": [],
    "functions": [
      "__init__",
      "send"
    ],
    "classes": [
      "Iftttnotify"
    ],
    "imports": [
      "http.client",
      "json",
      "logging"
    ],
    "preview": "\"Responsible for sending a notification\"\n\nimport http.client\nimport json\nimport logging\n\n\nclass Iftttnotify(object):\n    \"Sends notification through IFTTT\"\n\n    def __init__(self, ytdl_config):\n        self.logger = logging.getLogger(__name__)\n        self.ytdl_config = ytdl_config\n\n    def send(self, value1=\"\", value2=\"\", value3=\"\"):\n        \"Sends value1, value2 and value3\"\n\n        try:\n            conn = http.client.HTTPSConnection(\"maker.ifttt.com\")\n",
    "last_modified": "2025-05-04T23:28:25"
  },
  {
    "id": "1167",
    "name": "status_codes.py",
    "path": "06_development_tools/development_utilities/status_codes.py",
    "category": "06_development_tools",
    "type": "utility",
    "lines": 7,
    "size": 116,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [],
    "preview": "SUCCESS = 0\nERROR = 1\nUNKNOWN_ERROR = 2\nVIRTUALENV_NOT_FOUND = 3\nPREVIOUS_BUILD_DIR_ERROR = 4\nNO_MATCHES_FOUND = 23\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1168",
    "name": "pdfcsv.py",
    "path": "06_development_tools/development_utilities/pdfcsv.py",
    "category": "06_development_tools",
    "type": "utility",
    "lines": 7,
    "size": 156,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "tabula"
    ],
    "preview": "import tabula\n\nfilename = input(\"Enter File Path: \")\ndf = tabula.read_pdf(filename, encoding=\"utf-8\", spreadsheet=True, pages=\"1\")\n\ndf.to_csv(\"output.csv\")\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1169",
    "name": "expand_prompts_by_style_20250530223226.py",
    "path": "06_development_tools/development_utilities/expand_prompts_by_style_20250530223226.py",
    "category": "06_development_tools",
    "type": "utility",
    "lines": 1,
    "size": 0,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [],
    "preview": "",
    "last_modified": "2025-05-30T22:32:33.583994"
  },
  {
    "id": "1170",
    "name": "logo.py",
    "path": "06_development_tools/development_utilities/logo.py",
    "category": "06_development_tools",
    "type": "utility",
    "lines": 25,
    "size": 3459,
    "docstring": "",
    "keywords": [],
    "functions": [
      "print_logo"
    ],
    "classes": [],
    "imports": [
      "libs.animation"
    ],
    "preview": "# coding=utf-8\n#!/usr/bin/env python3\n\nfrom libs.animation import colorText\n\nlogo = \"\"\"\n\n[[black-bright-background]][[yellow]] \u2588\u2588\u2593 [[yellow]]\u2588\u2588\u2588\u2584    \u2588  [[yellow]] \u2588\u2588\u2588\u2588\u2588\u2588 [[yellow]]\u2584\u2584\u2584\u2588\u2588\u2588\u2588\u2588\u2593 [[yellow]]\u2584\u2584\u2584         [[yellow]] \u2588\u2588\u2580\u2588\u2588\u2588  [[yellow]]\u2593\u2588\u2588\u2588\u2588\u2588 [[yellow]] \u2588\u2588\u2593\u2588\u2588\u2588   [[yellow]]\u2592\u2588\u2588\u2588\u2588\u2588   [[yellow]]\u2588\u2588\u2580\u2588\u2588\u2588  [[yellow]]\u2584\u2584\u2584\u2588\u2588\u2588\u2588\u2588\u2593[[reset]]\n[[black-bright-background]][[yellow]]\u2593\u2588\u2588\u2592 [[yellow]]\u2588\u2588 \u2580\u2588   \u2588 \u2592[[yellow]]\u2588\u2588    \u2592 [[yellow]]\u2593  \u2588\u2588\u2592 \u2593\u2592\u2592[[yellow]]\u2588\u2588\u2588\u2588\u2584       [[yellow]]\u2593\u2588\u2588 \u2592 \u2588\u2588\u2592[[yellow]]\u2593\u2588   \u2580 [[yellow]]\u2593\u2588\u2588\u2591  \u2588\u2588\u2592\u2592[[yellow]]\u2588\u2588\u2592  \u2588\u2588\u2592\u2593[[yellow]]\u2588\u2588 \u2592 \u2588\u2588\u2592[[yellow]]\u2593  \u2588\u2588\u2592 \u2593\u2592[[reset]]\n[[black-bright-background]][[yellow]]\u2592\u2588\u2588\u2592\u2593[[yellow]]\u2588\u2588  \u2580\u2588 \u2588\u2588\u2592\u2591[[yellow]] \u2593\u2588\u2588\u2584   [[yellow]]\u2592 \u2593\u2588\u2588\u2591 \u2592\u2591\u2592[[yellow]]\u2588\u2588  \u2580\u2588\u2584     [[yellow]]\u2593\u2588\u2588 \u2591\u2584\u2588 \u2592[[yellow]]\u2592\u2588\u2588\u2588   [[yellow]]\u2593\u2588\u2588\u2591 \u2588\u2588\u2593\u2592\u2592[[yellow]]\u2588\u2588\u2591  \u2588\u2588\u2592\u2593[[yellow]]\u2588\u2588 \u2591\u2584\u2588 \u2592[[yellow]]\u2592 \u2593\u2588\u2588\u2591 \u2592\u2591[[reset]]\n[[black-bright-background]][[yellow]]\u2591\u2588\u2588\u2591\u2593[[yellow]]\u2588\u2588\u2592  \u2590\u258c\u2588\u2588\u2592 [[yellow]] \u2592   \u2588\u2588\u2592[[yellow]]\u2591 \u2593\u2588\u2588\u2593 \u2591 \u2591[[yellow]]\u2588\u2588\u2584\u2584\u2584\u2584\u2588\u2588    [[yellow]]\u2592\u2588\u2588\u2580\u2580\u2588\u2584  [[yellow]]\u2592\u2593\u2588  \u2584 [[yellow]]\u2592\u2588\u2588\u2584\u2588\u2593\u2592 \u2592\u2592[[yellow]]\u2588\u2588   \u2588\u2588\u2591\u2592[[yellow]]\u2588\u2588\u2580\u2580\u2588\u2584  [[yellow]]\u2591 \u2593\u2588\u2588\u2593 \u2591 [[reset]]\n[[black-bright-background]][[yellow]]\u2591\u2588\u2588\u2591\u2592[[yellow]]\u2588\u2588\u2591   \u2593\u2588\u2588\u2591\u2592[[yellow]]\u2588\u2588\u2588\u2588\u2588\u2588\u2592\u2592[[yellow]]  \u2592\u2588\u2588\u2592 \u2591  [[yellow]]\u2593\u2588   \u2593\u2588\u2588\u2592   [[yellow]]\u2591\u2588\u2588\u2593 \u2592\u2588\u2588\u2592[[yellow]]\u2591\u2592\u2588\u2588\u2588\u2588\u2592[[yellow]]\u2592\u2588\u2588\u2592 \u2591  \u2591\u2591[[yellow]] \u2588\u2588\u2588\u2588\u2593\u2592\u2591\u2591[[yellow]]\u2588\u2588\u2593 \u2592\u2588\u2588\u2592[[yellow]]  \u2592\u2588\u2588\u2592 \u2591 [[reset]]\n[[black-bright-background]][[yellow]]\u2591\u2593  \u2591[[yellow]] \u2592\u2591   \u2592 \u2592 \u2592[[yellow]] \u2592\u2593\u2592 \u2592 \u2591[[yellow]]  \u2592 \u2591\u2591    [[yellow]]\u2592\u2592   \u2593\u2592\u2588\u2591   [[yellow]]\u2591 \u2592\u2593 \u2591\u2592\u2593\u2591[[yellow]]\u2591\u2591 \u2592\u2591 \u2591[[yellow]]\u2592\u2593\u2592\u2591 \u2591  \u2591\u2591[[yellow]] \u2592\u2591\u2592\u2591\u2592\u2591 \u2591[[yellow]] \u2592\u2593 \u2591\u2592\u2593\u2591[[yellow]]  \u2592 \u2591\u2591   [[reset]]\n[[black-bright-background]][[yellow]] \u2592 \u2591\u2591[[yellow]] \u2591\u2591   \u2591 \u2592\u2591\u2591[[yellow]] \u2591\u2592  \u2591 \u2591[[yellow]]    \u2591     [[yellow]] \u2592   \u2592\u2592 \u2591   [[yellow]]  \u2591\u2592 \u2591 \u2592\u2591[[yellow]] \u2591 \u2591  \u2591[[yellow]]\u2591\u2592 \u2591      [[yellow]] \u2591 \u2592 \u2592\u2591  [[yellow]] \u2591\u2592 \u2591 \u2592\u2591[[yellow]]    \u2591    [[reset]]\n[[black-bright-background]][[yellow]] \u2592 \u2591 [[yellow]]  \u2591   \u2591 \u2591 \u2591[[yellow]]  \u2591  \u2591  [[yellow]]  \u2591       [[yellow]] \u2591   \u2592      [[yellow]]  \u2591\u2591   \u2591 [[yellow]]   \u2591   [[yellow]]\u2591\u2591       \u2591[[yellow]] \u2591 \u2591 \u2592   [[yellow]] \u2591\u2591   \u2591 [[yellow]]  \u2591      [[reset]]\n[[black-bright-background]][[yellow]] \u2591   [[yellow]]        \u2591  [[yellow]]     \u2591  [[yellow]]          [[yellow]]     \u2591  \u2591   [[yellow]]   \u2591     [[yellow]]   \u2591  \u2591[[yellow]]          [[yellow]]   \u2591 \u2591   [[yellow]]  \u2591     [[yellow]]         [[reset]]\n                                                                                                  \n\n                                           \n\"\"\"",
    "last_modified": "2025-03-28T18:35:46"
  },
  {
    "id": "1171",
    "name": "config (2).py",
    "path": "06_development_tools/development_utilities/config (2).py",
    "category": "06_development_tools",
    "type": "setup",
    "lines": 3,
    "size": 47,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [],
    "preview": "# config.py\nSOURCE_DIRECTORY = \"/Users/steven\"\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1172",
    "name": "hash.py",
    "path": "06_development_tools/development_utilities/hash.py",
    "category": "06_development_tools",
    "type": "utility",
    "lines": 56,
    "size": 1643,
    "docstring": "",
    "keywords": [],
    "functions": [
      "_hash_of_file",
      "add_options",
      "run"
    ],
    "classes": [
      "HashCommand"
    ],
    "imports": [
      "hashlib",
      "logging",
      "sys",
      "optparse",
      "typing",
      "pip._internal.cli.base_command",
      "pip._internal.cli.status_codes",
      "pip._internal.utils.hashes",
      "pip._internal.utils.misc"
    ],
    "preview": "import hashlib\nimport logging\nimport sys\nfrom optparse import Values\nfrom typing import List\n\nfrom pip._internal.cli.base_command import Command\nfrom pip._internal.cli.status_codes import ERROR, SUCCESS\nfrom pip._internal.utils.hashes import FAVORITE_HASH, STRONG_HASHES\nfrom pip._internal.utils.misc import read_chunks, write_output\n\nlogger = logging.getLogger(__name__)\n\n\nclass HashCommand(Command):\n    \"\"\"\n    Compute a hash of a local package archive.\n\n    These can be used with --hash in a requirements file to do repeatable\n    installs.",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1173",
    "name": "pyttsx.py",
    "path": "06_development_tools/development_utilities/pyttsx.py",
    "category": "06_development_tools",
    "type": "utility",
    "lines": 43,
    "size": 1201,
    "docstring": "",
    "keywords": [],
    "functions": [
      "__init__",
      "run",
      "randomvoice"
    ],
    "classes": [
      "pyttsx"
    ],
    "imports": [
      "random",
      "pyttsx3",
      "utils"
    ],
    "preview": "import random\n\nimport pyttsx3\n\nfrom utils import settings\n\n\nclass pyttsx:\n    def __init__(self):\n        self.max_chars = 5000\n        self.voices = []\n\n    def run(\n        self,\n        text: str,\n        filepath: str,\n        random_voice=False,\n    ):\n        voice_id = settings.config[\"settings\"][\"tts\"][\"python_voice\"]\n        voice_num = settings.config[\"settings\"][\"tts\"][\"py_voice_num\"]",
    "last_modified": "2025-09-13T05:53:59.748700"
  },
  {
    "id": "1174",
    "name": "config 5.py",
    "path": "06_development_tools/development_utilities/config 5.py",
    "category": "06_development_tools",
    "type": "setup",
    "lines": 3,
    "size": 56,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [],
    "preview": "# config.py\nSOURCE_DIRECTORY = \"/Volumes/oG-bAk/steven\"\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1175",
    "name": "load_files.py",
    "path": "06_development_tools/development_utilities/load_files.py",
    "category": "06_development_tools",
    "type": "organization",
    "lines": 66,
    "size": 2093,
    "docstring": "MIT License\n\nCopyright (c) 2021-2022 MShawon\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.",
    "keywords": [],
    "functions": [
      "load_url",
      "load_search",
      "get_hash"
    ],
    "classes": [],
    "imports": [
      "hashlib",
      "random",
      "colors"
    ],
    "preview": "\"\"\"\nMIT License\n\nCopyright (c) 2021-2022 MShawon\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,",
    "last_modified": "2025-09-13T05:54:11.711565"
  },
  {
    "id": "1176",
    "name": "fdupes.py",
    "path": "06_development_tools/development_utilities/fdupes.py",
    "category": "06_development_tools",
    "type": "utility",
    "lines": 18,
    "size": 542,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "csv"
    ],
    "preview": "import csv\n\n# Read the output from the fdupes results\nwith open(\"duplicates.txt\", \"r\") as infile, open(\"duplicates.csv\", \"w\", newline=\"\") as outfile:\n    writer = csv.writer(outfile)\n    writer.writerow([\"File1\", \"File2\"])  # Write the CSV header\n\n    files = []\n    for line in infile:\n        line = line.strip()\n        if line:\n            files.append(line)\n        else:\n            # Write pairs of duplicate files\n            for i in range(1, len(files)):\n                writer.writerow([files[0], files[i]])\n            files = []\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1177",
    "name": "config (1).py",
    "path": "06_development_tools/development_utilities/config (1).py",
    "category": "06_development_tools",
    "type": "setup",
    "lines": 3,
    "size": 47,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [],
    "preview": "# config.py\nSOURCE_DIRECTORY = \"/Users/steven\"\n",
    "last_modified": "2025-05-04T22:47:12.597946"
  },
  {
    "id": "1178",
    "name": "launch.py",
    "path": "06_development_tools/development_utilities/launch.py",
    "category": "06_development_tools",
    "type": "utility",
    "lines": 5,
    "size": 67,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "src.main"
    ],
    "preview": "from src.main import start\n\nif __name__ == \"__main__\":\n    start()\n",
    "last_modified": "2025-05-04T22:47:12"
  },
  {
    "id": "1179",
    "name": "bot.py",
    "path": "06_development_tools/development_utilities/bot.py",
    "category": "06_development_tools",
    "type": "web_tools",
    "lines": 129,
    "size": 3693,
    "docstring": "Before changing the program and publishing it somewhere, please\nPlease note that this program is under GPLv3 license.\nMore information:\nhttps://tr.wikipedia.org/wiki/gnu_genel_kamu_lisans%c4%b1\nhttps://www.gnu.org/licenses/quick-guide-gplv3.html",
    "keywords": [],
    "functions": [
      "MultiThread",
      "NoMultiThread"
    ],
    "classes": [],
    "imports": [
      "multiprocessing",
      "random",
      "time",
      "libs.instaclient",
      "libs.utils"
    ],
    "preview": "# coding=utf-8\n#!/usr/bin/env python3\n\n\"\"\"\nBefore changing the program and publishing it somewhere, please\nPlease note that this program is under GPLv3 license.\nMore information:\nhttps://tr.wikipedia.org/wiki/gnu_genel_kamu_lisans%c4%b1\nhttps://www.gnu.org/licenses/quick-guide-gplv3.html\n\"\"\"\n\n__author__ = \"Marwan 007 : @mrwn.007\"\n__license__ = \"GPLv3\"\n__version__ = \"0.1\"\n__status__ = \"being developed\"\n\n\nfrom multiprocessing import Process\nfrom random import choice\nfrom time import sleep, time",
    "last_modified": "2025-09-13T05:53:27.928570"
  },
  {
    "id": "1180",
    "name": "merge-pdfs.py",
    "path": "06_development_tools/development_utilities/merge-pdfs.py",
    "category": "06_development_tools",
    "type": "utility",
    "lines": 22,
    "size": 528,
    "docstring": "",
    "keywords": [],
    "functions": [
      "merge_pdfs"
    ],
    "classes": [],
    "imports": [
      "PyPDF2"
    ],
    "preview": "from PyPDF2 import PdfReader, PdfWriter\n\n\ndef merge_pdfs(toc_path, main_pdf_path, output_path):\n    toc_reader = PdfReader(toc_path)\n    main_reader = PdfReader(main_pdf_path)\n    writer = PdfWriter()\n\n    # Add TOC pages\n    for page in toc_reader.pages:\n        writer.add_page(page)\n\n    # Add main PDF pages\n    for page in main_reader.pages:\n        writer.add_page(page)\n\n    with open(output_path, \"wb\") as output_file:\n        writer.write(output_file)\n\n",
    "last_modified": "2025-05-04T22:47:13.331849"
  },
  {
    "id": "1181",
    "name": "uploader_factory.py",
    "path": "06_development_tools/development_utilities/uploader_factory.py",
    "category": "06_development_tools",
    "type": "youtube",
    "lines": 22,
    "size": 777,
    "docstring": "",
    "keywords": [],
    "functions": [
      "get_uploader"
    ],
    "classes": [],
    "imports": [
      "simplegallery.common",
      "simplegallery.upload.variants.aws_uploader",
      "simplegallery.upload.variants.netlify_uploader"
    ],
    "preview": "import simplegallery.common as spg_common\nfrom simplegallery.upload.variants.aws_uploader import AWSUploader\nfrom simplegallery.upload.variants.netlify_uploader import NetlifyUploader\n\n\ndef get_uploader(hosting_type):\n    \"\"\"\n    Factory function that returns an object of a class derived from BaseUploader based on the provided hosting type.\n    Supported uploaders:\n    - AWSUploader - uploader for AWS S3\n    - Netlify - uploader for Netlify\n\n    :param hosting_type: name of the hosting provider (aws or netlify)\n    :return: uploader object\n    \"\"\"\n    if hosting_type == \"aws\":\n        return AWSUploader()\n    elif hosting_type == \"netlify\":\n        return NetlifyUploader()\n    else:",
    "last_modified": "2025-05-04T22:47:13"
  },
  {
    "id": "1182",
    "name": "bot_archive.py",
    "path": "06_development_tools/development_utilities/bot_archive.py",
    "category": "06_development_tools",
    "type": "utility",
    "lines": 44,
    "size": 1469,
    "docstring": "",
    "keywords": [],
    "functions": [
      "archive",
      "archive_medias",
      "unarchive_medias"
    ],
    "classes": [],
    "imports": [
      "tqdm"
    ],
    "preview": "from tqdm import tqdm\n\n\ndef archive(self, media_id, undo=False):\n    self.small_delay()\n    media = self.get_media_info(media_id)\n    media = media[0] if isinstance(media, list) else media\n    if self.api.archive_media(media, undo):\n        self.total[\"archived\"] += int(not undo)\n        self.total[\"unarchived\"] += int(undo)\n        return True\n    self.logger.info(\"Media id %s is not %s.\", media_id, \"unarchived\" if undo else \"archived\")\n    return False\n\n\ndef archive_medias(self, medias):\n    broken_items = []\n    if not medias:\n        self.logger.info(\"Nothing to archive.\")\n        return broken_items",
    "last_modified": "2025-09-13T05:54:57.086581"
  },
  {
    "id": "1183",
    "name": "inference_pipeline_api.py",
    "path": "06_development_tools/development_utilities/inference_pipeline_api.py",
    "category": "06_development_tools",
    "type": "web_tools",
    "lines": 67,
    "size": 1921,
    "docstring": "",
    "keywords": [],
    "functions": [
      "call_llm_service",
      "rag"
    ],
    "classes": [
      "QueryRequest",
      "QueryResponse"
    ],
    "imports": [
      "opik",
      "fastapi",
      "llm_engineering",
      "llm_engineering.application.rag.retriever",
      "llm_engineering.application.utils",
      "llm_engineering.domain.embedded_chunks",
      "llm_engineering.infrastructure.opik_utils",
      "llm_engineering.model.inference",
      "opik",
      "pydantic"
    ],
    "preview": "import opik\nfrom fastapi import FastAPI, HTTPException\nfrom llm_engineering import settings\nfrom llm_engineering.application.rag.retriever import ContextRetriever\nfrom llm_engineering.application.utils import misc\nfrom llm_engineering.domain.embedded_chunks import EmbeddedChunk\nfrom llm_engineering.infrastructure.opik_utils import configure_opik\nfrom llm_engineering.model.inference import InferenceExecutor, LLMInferenceSagemakerEndpoint\nfrom opik import opik_context\nfrom pydantic import BaseModel\n\nconfigure_opik()\n\napp = FastAPI()\n\n\nclass QueryRequest(BaseModel):\n    query: str\n\n",
    "last_modified": "2025-09-13T05:53:42.104751"
  },
  {
    "id": "1184",
    "name": "analyze_all_images_20250530220426.py",
    "path": "06_development_tools/development_utilities/analyze_all_images_20250530220426.py",
    "category": "06_development_tools",
    "type": "analysis",
    "lines": 1,
    "size": 0,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [],
    "preview": "",
    "last_modified": "2025-05-30T22:04:35.765492"
  },
  {
    "id": "1185",
    "name": "bot_delete.py",
    "path": "06_development_tools/development_utilities/bot_delete.py",
    "category": "06_development_tools",
    "type": "utility",
    "lines": 35,
    "size": 1116,
    "docstring": "",
    "keywords": [],
    "functions": [
      "delete_media",
      "delete_medias",
      "delete_comment"
    ],
    "classes": [],
    "imports": [
      "tqdm"
    ],
    "preview": "from tqdm import tqdm\n\n\ndef delete_media(self, media_id):\n    self.small_delay()\n    media = self.get_media_info(media_id)\n    media = media[0] if isinstance(media, list) else media\n    if self.api.delete_media(media):\n        return True\n    self.logger.info(\"Media with {} is not {}.\".format(media.get(\"id\"), \"deleted\"))\n    return False\n\n\ndef delete_medias(self, medias):\n    broken_items = []\n    if not medias:\n        self.logger.info(\"Nothing to delete.\")\n        return broken_items\n    self.logger.info(\"Going to delete %d medias.\" % (len(medias)))\n    for media in tqdm(medias):",
    "last_modified": "2025-09-13T05:54:57.225078"
  },
  {
    "id": "1186",
    "name": "uninstall.py",
    "path": "06_development_tools/development_utilities/uninstall.py",
    "category": "06_development_tools",
    "type": "setup",
    "lines": 109,
    "size": 3833,
    "docstring": "",
    "keywords": [],
    "functions": [
      "add_options",
      "run"
    ],
    "classes": [
      "UninstallCommand"
    ],
    "imports": [
      "logging",
      "optparse",
      "typing",
      "pip._internal.cli",
      "pip._internal.cli.base_command",
      "pip._internal.cli.req_command",
      "pip._internal.cli.status_codes",
      "pip._internal.exceptions",
      "pip._internal.req",
      "pip._internal.req.constructors"
    ],
    "preview": "import logging\nfrom optparse import Values\nfrom typing import List\n\nfrom pip._internal.cli import cmdoptions\nfrom pip._internal.cli.base_command import Command\nfrom pip._internal.cli.req_command import SessionCommandMixin, warn_if_run_as_root\nfrom pip._internal.cli.status_codes import SUCCESS\nfrom pip._internal.exceptions import InstallationError\nfrom pip._internal.req import parse_requirements\nfrom pip._internal.req.constructors import (\n    install_req_from_line,\n    install_req_from_parsed_requirement,\n)\nfrom pip._internal.utils.misc import (\n    check_externally_managed,\n    protect_pip_from_modification_on_windows,\n)\nfrom pip._vendor.packaging.utils import canonicalize_name\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1187",
    "name": "aws_uploader.py",
    "path": "06_development_tools/development_utilities/aws_uploader.py",
    "category": "06_development_tools",
    "type": "youtube",
    "lines": 53,
    "size": 1727,
    "docstring": "",
    "keywords": [
      "youtube"
    ],
    "functions": [
      "check_location",
      "upload_gallery"
    ],
    "classes": [
      "AWSUploader"
    ],
    "imports": [
      "subprocess",
      "simplegallery.common",
      "simplegallery.upload.base_uploader"
    ],
    "preview": "import subprocess\n\nimport simplegallery.common as spg_common\nfrom simplegallery.upload.base_uploader import BaseUploader\n\n\nclass AWSUploader(BaseUploader):\n    def check_location(self, location):\n        \"\"\"\n        Checks if the location is empty or not\n        :param location: S3 bucket where the gallery should be uploaded\n        :return: True if the location is not empty, False otherwise\n        \"\"\"\n        if not location:\n            spg_common.log(\"Location cannot be empty when uploading to AWS\")\n\n        return bool(location)\n\n    def upload_gallery(self, location, gallery_path):\n        \"\"\"",
    "last_modified": "2025-09-13T05:53:53.263033"
  },
  {
    "id": "1188",
    "name": "autofill_20250530225744.py",
    "path": "06_development_tools/development_utilities/autofill_20250530225744.py",
    "category": "06_development_tools",
    "type": "utility",
    "lines": 1,
    "size": 0,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [],
    "preview": "",
    "last_modified": "2025-05-30T22:57:52.525018"
  },
  {
    "id": "1189",
    "name": "chardetect.py",
    "path": "06_development_tools/development_utilities/chardetect.py",
    "category": "06_development_tools",
    "type": "organization",
    "lines": 104,
    "size": 3153,
    "docstring": "Script which takes one or more file paths and reports on their detected\nencodings\n\nExample::\n\n    % chardetect somefile someotherfile\n    somefile: windows-1252 with confidence 0.5\n    someotherfile: ascii with confidence 1.0\n\nIf no paths are provided, it takes its input from stdin.",
    "keywords": [],
    "functions": [
      "description_of",
      "main"
    ],
    "classes": [],
    "imports": [
      "argparse",
      "sys",
      "typing",
      "universaldetector"
    ],
    "preview": "\"\"\"\nScript which takes one or more file paths and reports on their detected\nencodings\n\nExample::\n\n    % chardetect somefile someotherfile\n    somefile: windows-1252 with confidence 0.5\n    someotherfile: ascii with confidence 1.0\n\nIf no paths are provided, it takes its input from stdin.\n\n\"\"\"\n\nimport argparse\nimport sys\nfrom typing import Iterable, List, Optional\n\nfrom .. import __version__\nfrom ..universaldetector import UniversalDetector",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1190",
    "name": "singleton.py",
    "path": "06_development_tools/development_utilities/singleton.py",
    "category": "06_development_tools",
    "type": "utility",
    "lines": 9,
    "size": 240,
    "docstring": "",
    "keywords": [],
    "functions": [
      "__call__"
    ],
    "classes": [
      "Singleton"
    ],
    "imports": [],
    "preview": "class Singleton(type):\n\n    _instances = {}\n\n    def __call__(cls, *args, **kwargs):\n        if cls not in cls._instances:\n            cls._instances[cls] = super(Singleton, cls).__call__(*args, **kwargs)\n        return cls._instances[cls]\n",
    "last_modified": "2025-03-28T18:35:47"
  },
  {
    "id": "1191",
    "name": "plugin.py",
    "path": "06_development_tools/development_utilities/plugin.py",
    "category": "06_development_tools",
    "type": "analysis",
    "lines": 89,
    "size": 2491,
    "docstring": "pygments.plugin\n~~~~~~~~~~~~~~~\n\nPygments plugin interface. By default, this tries to use\n``importlib.metadata``, which is in the Python standard\nlibrary since Python 3.8, or its ``importlib_metadata``\nbackport for earlier versions of Python. It falls back on\n``pkg_resources`` if not found. Finally, if ``pkg_resources``\nis not found either, no plugins are loaded at all.\n\nlexer plugins::\n\n    [pygments.lexers]\n    yourlexer = yourmodule:YourLexer\n\nformatter plugins::\n\n    [pygments.formatters]\n    yourformatter = yourformatter:YourFormatter\n    /.ext = yourformatter:YourFormatter\n\nAs you can see, you can define extensions for the formatter\nwith a leading slash.\n\nsyntax plugins::\n\n    [pygments.styles]\n    yourstyle = yourstyle:YourStyle\n\nfilter plugin::\n\n    [pygments.filter]\n    yourfilter = yourfilter:YourFilter\n\n\n:copyright: Copyright 2006-2023 by the Pygments team, see AUTHORS.\n:license: BSD, see LICENSE for details.",
    "keywords": [],
    "functions": [
      "iter_entry_points",
      "find_plugin_lexers",
      "find_plugin_formatters",
      "find_plugin_styles",
      "find_plugin_filters"
    ],
    "classes": [],
    "imports": [
      "importlib.metadata",
      "importlib_metadata",
      "pip._vendor.pkg_resources"
    ],
    "preview": "\"\"\"\npygments.plugin\n~~~~~~~~~~~~~~~\n\nPygments plugin interface. By default, this tries to use\n``importlib.metadata``, which is in the Python standard\nlibrary since Python 3.8, or its ``importlib_metadata``\nbackport for earlier versions of Python. It falls back on\n``pkg_resources`` if not found. Finally, if ``pkg_resources``\nis not found either, no plugins are loaded at all.\n\nlexer plugins::\n\n    [pygments.lexers]\n    yourlexer = yourmodule:YourLexer\n\nformatter plugins::\n\n    [pygments.formatters]\n    yourformatter = yourformatter:YourFormatter",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1192",
    "name": "tts_test.py",
    "path": "06_development_tools/development_utilities/tts_test.py",
    "category": "06_development_tools",
    "type": "testing",
    "lines": 3,
    "size": 22,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "*",
      "clips"
    ],
    "preview": "import *\nimport clips\n",
    "last_modified": "2025-05-04T23:28:22.826071"
  },
  {
    "id": "1193",
    "name": "filetypes.py",
    "path": "06_development_tools/development_utilities/filetypes.py",
    "category": "06_development_tools",
    "type": "organization",
    "lines": 27,
    "size": 715,
    "docstring": "Filetype information.",
    "keywords": [],
    "functions": [
      "is_archive_file"
    ],
    "classes": [],
    "imports": [
      "typing",
      "pip._internal.utils.misc"
    ],
    "preview": "\"\"\"Filetype information.\"\"\"\n\nfrom typing import Tuple\n\nfrom pip._internal.utils.misc import splitext\n\nWHEEL_EXTENSION = \".whl\"\nBZ2_EXTENSIONS: Tuple[str, ...] = (\".tar.bz2\", \".tbz\")\nXZ_EXTENSIONS: Tuple[str, ...] = (\n    \".tar.xz\",\n    \".txz\",\n    \".tlz\",\n    \".tar.lz\",\n    \".tar.lzma\",\n)\nZIP_EXTENSIONS: Tuple[str, ...] = (\".zip\", WHEEL_EXTENSION)\nTAR_EXTENSIONS: Tuple[str, ...] = (\".tar.gz\", \".tgz\", \".tar\")\nARCHIVE_EXTENSIONS = ZIP_EXTENSIONS + BZ2_EXTENSIONS + TAR_EXTENSIONS + XZ_EXTENSIONS\n\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1194",
    "name": "exceptions.py",
    "path": "06_development_tools/development_utilities/exceptions.py",
    "category": "06_development_tools",
    "type": "utility",
    "lines": 49,
    "size": 1081,
    "docstring": "",
    "keywords": [],
    "functions": [
      "__init__",
      "__str__"
    ],
    "classes": [
      "UnpackException",
      "BufferFull",
      "OutOfData",
      "FormatError",
      "StackError",
      "ExtraData"
    ],
    "imports": [],
    "preview": "class UnpackException(Exception):\n    \"\"\"Base class for some exceptions raised while unpacking.\n\n    NOTE: unpack may raise exception other than subclass of\n    UnpackException.  If you want to catch all error, catch\n    Exception instead.\n    \"\"\"\n\n\nclass BufferFull(UnpackException):\n    pass\n\n\nclass OutOfData(UnpackException):\n    pass\n\n\nclass FormatError(ValueError, UnpackException):\n    \"\"\"Invalid msgpack format\"\"\"\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1195",
    "name": "config_20250430201604.py",
    "path": "06_development_tools/development_utilities/config_20250430201604.py",
    "category": "06_development_tools",
    "type": "setup",
    "lines": 3,
    "size": 40,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [],
    "preview": "# config.py\nSOURCE_DIRECTORY = \"/Users\"\n",
    "last_modified": "2025-04-30T20:16:04.178949"
  },
  {
    "id": "1196",
    "name": "process_leonardo_20250102104737.py",
    "path": "06_development_tools/development_utilities/process_leonardo_20250102104737.py",
    "category": "06_development_tools",
    "type": "utility",
    "lines": 1,
    "size": 0,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [],
    "preview": "",
    "last_modified": "2025-03-28T18:37:01.038000"
  },
  {
    "id": "1197",
    "name": "candidate.py",
    "path": "06_development_tools/development_utilities/candidate.py",
    "category": "06_development_tools",
    "type": "utility",
    "lines": 30,
    "size": 930,
    "docstring": "",
    "keywords": [],
    "functions": [
      "__init__",
      "__repr__",
      "__str__"
    ],
    "classes": [
      "InstallationCandidate"
    ],
    "imports": [
      "pip._internal.models.link",
      "pip._internal.utils.models",
      "pip._vendor.packaging.version"
    ],
    "preview": "from pip._internal.models.link import Link\nfrom pip._internal.utils.models import KeyBasedCompareMixin\nfrom pip._vendor.packaging.version import parse as parse_version\n\n\nclass InstallationCandidate(KeyBasedCompareMixin):\n    \"\"\"Represents a potential \"candidate\" for installation.\"\"\"\n\n    __slots__ = [\"name\", \"version\", \"link\"]\n\n    def __init__(self, name: str, version: str, link: Link) -> None:\n        self.name = name\n        self.version = parse_version(version)\n        self.link = link\n\n        super().__init__(\n            key=(self.name, self.version, self.link),\n            defining_class=InstallationCandidate,\n        )\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1198",
    "name": "pangomarkup.py",
    "path": "06_development_tools/development_utilities/pangomarkup.py",
    "category": "06_development_tools",
    "type": "utility",
    "lines": 83,
    "size": 2191,
    "docstring": "pygments.formatters.pangomarkup\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nFormatter for Pango markup output.\n\n:copyright: Copyright 2006-2023 by the Pygments team, see AUTHORS.\n:license: BSD, see LICENSE for details.",
    "keywords": [],
    "functions": [
      "escape_special_chars",
      "__init__",
      "format_unencoded"
    ],
    "classes": [
      "PangoMarkupFormatter"
    ],
    "imports": [
      "pip._vendor.pygments.formatter"
    ],
    "preview": "\"\"\"\npygments.formatters.pangomarkup\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nFormatter for Pango markup output.\n\n:copyright: Copyright 2006-2023 by the Pygments team, see AUTHORS.\n:license: BSD, see LICENSE for details.\n\"\"\"\n\nfrom pip._vendor.pygments.formatter import Formatter\n\n__all__ = [\"PangoMarkupFormatter\"]\n\n\n_escape_table = {\n    ord(\"&\"): \"&amp;\",\n    ord(\"<\"): \"&lt;\",\n}\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1199",
    "name": "format_control.py",
    "path": "06_development_tools/development_utilities/format_control.py",
    "category": "06_development_tools",
    "type": "utility",
    "lines": 76,
    "size": 2455,
    "docstring": "",
    "keywords": [],
    "functions": [
      "__init__",
      "__eq__",
      "__repr__",
      "handle_mutual_excludes",
      "get_allowed_formats",
      "disallow_binaries"
    ],
    "classes": [
      "FormatControl"
    ],
    "imports": [
      "typing",
      "pip._internal.exceptions",
      "pip._vendor.packaging.utils"
    ],
    "preview": "from typing import FrozenSet, Optional, Set\n\nfrom pip._internal.exceptions import CommandError\nfrom pip._vendor.packaging.utils import canonicalize_name\n\n\nclass FormatControl:\n    \"\"\"Helper for managing formats from which a package can be installed.\"\"\"\n\n    __slots__ = [\"no_binary\", \"only_binary\"]\n\n    def __init__(\n        self,\n        no_binary: Optional[Set[str]] = None,\n        only_binary: Optional[Set[str]] = None,\n    ) -> None:\n        if no_binary is None:\n            no_binary = set()\n        if only_binary is None:\n            only_binary = set()",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1200",
    "name": "datetime.py",
    "path": "06_development_tools/development_utilities/datetime.py",
    "category": "06_development_tools",
    "type": "utility",
    "lines": 11,
    "size": 241,
    "docstring": "For when pip wants to check the date or time.",
    "keywords": [],
    "functions": [
      "today_is_later_than"
    ],
    "classes": [],
    "imports": [
      "datetime"
    ],
    "preview": "\"\"\"For when pip wants to check the date or time.\"\"\"\n\nimport datetime\n\n\ndef today_is_later_than(year: int, month: int, day: int) -> bool:\n    today = datetime.date.today()\n    given = datetime.date(year, month, day)\n\n    return today > given\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1201",
    "name": "help.py",
    "path": "06_development_tools/development_utilities/help.py",
    "category": "06_development_tools",
    "type": "utility",
    "lines": 38,
    "size": 1083,
    "docstring": "",
    "keywords": [],
    "functions": [
      "run"
    ],
    "classes": [
      "HelpCommand"
    ],
    "imports": [
      "optparse",
      "typing",
      "pip._internal.cli.base_command",
      "pip._internal.cli.status_codes",
      "pip._internal.exceptions",
      "pip._internal.commands"
    ],
    "preview": "from optparse import Values\nfrom typing import List\n\nfrom pip._internal.cli.base_command import Command\nfrom pip._internal.cli.status_codes import SUCCESS\nfrom pip._internal.exceptions import CommandError\n\n\nclass HelpCommand(Command):\n    \"\"\"Show help for commands\"\"\"\n\n    usage = \"\"\"\n      %prog <command>\"\"\"\n    ignore_require_venv = True\n\n    def run(self, options: Values, args: List[str]) -> int:\n        from pip._internal.commands import commands_dict, create_command, get_similar_commands\n\n        try:\n            # 'pip help' with no args is handled by pip.__init__.parseopt()",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1202",
    "name": "installed.py",
    "path": "06_development_tools/development_utilities/installed.py",
    "category": "06_development_tools",
    "type": "setup",
    "lines": 30,
    "size": 842,
    "docstring": "",
    "keywords": [],
    "functions": [
      "build_tracker_id",
      "get_metadata_distribution",
      "prepare_distribution_metadata"
    ],
    "classes": [
      "InstalledDistribution"
    ],
    "imports": [
      "typing",
      "pip._internal.distributions.base",
      "pip._internal.index.package_finder",
      "pip._internal.metadata"
    ],
    "preview": "from typing import Optional\n\nfrom pip._internal.distributions.base import AbstractDistribution\nfrom pip._internal.index.package_finder import PackageFinder\nfrom pip._internal.metadata import BaseDistribution\n\n\nclass InstalledDistribution(AbstractDistribution):\n    \"\"\"Represents an installed package.\n\n    This does not need any preparation as the required information has already\n    been computed.\n    \"\"\"\n\n    @property\n    def build_tracker_id(self) -> Optional[str]:\n        return None\n\n    def get_metadata_distribution(self) -> BaseDistribution:\n        assert self.req.satisfied_by is not None, \"not actually installed\"",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1203",
    "name": "spotify.py",
    "path": "06_development_tools/development_utilities/spotify.py",
    "category": "06_development_tools",
    "type": "utility",
    "lines": 158,
    "size": 5122,
    "docstring": "",
    "keywords": [],
    "functions": [
      "_pack_album",
      "_pack_show",
      "_pack_playlist",
      "__init__",
      "search",
      "link",
      "_get_playlist_tracks",
      "_get_show_episodes",
      "_get_artist_albums",
      "_get_artist_top"
    ],
    "classes": [
      "Spotify"
    ],
    "imports": [
      "spotipy",
      "spotipy.oauth2",
      "track",
      "types"
    ],
    "preview": "import spotipy\nfrom spotipy.oauth2 import SpotifyClientCredentials\n\nfrom .track import Track\nfrom .types import Type\n\n\nclass Spotify:\n    def __init__(self, api_credentials=None) -> None:\n        if api_credentials is None:\n            self.sp = spotipy.Spotify(client_credentials_manager=SpotifyClientCredentials())\n        else:\n            client_id, client_secret = api_credentials\n            self.sp = spotipy.Spotify(\n                client_credentials_manager=SpotifyClientCredentials(\n                    client_id=client_id, client_secret=client_secret\n                )\n            )\n\n    def search(self, query, query_type=Type.TRACK, artist_albums: bool = False) -> list:",
    "last_modified": "2025-09-13T05:55:15.849584"
  },
  {
    "id": "1204",
    "name": "clean_test.py",
    "path": "06_development_tools/development_utilities/clean_test.py",
    "category": "06_development_tools",
    "type": "testing",
    "lines": 4,
    "size": 43,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "clips"
    ],
    "preview": "from clips import clean_temp\n\nclean_temp()\n",
    "last_modified": "2025-05-04T23:27:53.300828"
  },
  {
    "id": "1205",
    "name": "emport_prompts_20250530225118.py",
    "path": "06_development_tools/development_utilities/emport_prompts_20250530225118.py",
    "category": "06_development_tools",
    "type": "utility",
    "lines": 1,
    "size": 0,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [],
    "preview": "",
    "last_modified": "2025-05-30T22:51:21.768054"
  },
  {
    "id": "1206",
    "name": "update-file-organization-in-organize_albums1.py",
    "path": "06_development_tools/development_utilities/update-file-organization-in-organize_albums1.py",
    "category": "06_development_tools",
    "type": "organization",
    "lines": 2,
    "size": 41,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [],
    "preview": "45b437252556203cd3a4431bbc2f5c6592e2bc58\n",
    "last_modified": "2025-09-13T04:20:03.057518"
  },
  {
    "id": "1207",
    "name": "devices.py",
    "path": "06_development_tools/development_utilities/devices.py",
    "category": "06_development_tools",
    "type": "utility",
    "lines": 123,
    "size": 3529,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "random"
    ],
    "preview": "import random\n\nAPP_VERSION = \"136.0.0.34.124\"\nVERSION_CODE = \"208061712\"\nDEVICES = {\n    \"one_plus_7\": {\n        \"app_version\": APP_VERSION,\n        \"android_version\": \"29\",\n        \"android_release\": \"10.0\",\n        \"dpi\": \"420dpi\",\n        \"resolution\": \"1080x2340\",\n        \"manufacturer\": \"OnePlus\",\n        \"device\": \"GM1903\",\n        \"model\": \"OnePlus7\",\n        \"cpu\": \"qcom\",\n        \"version_code\": VERSION_CODE,\n    },\n    \"one_plus_3\": {\n        \"app_version\": APP_VERSION,\n        \"android_version\": \"28\",",
    "last_modified": "2025-03-28T18:35:47"
  },
  {
    "id": "1208",
    "name": "backup_installations.py",
    "path": "06_development_tools/development_utilities/backup_installations.py",
    "category": "06_development_tools",
    "type": "setup",
    "lines": 45,
    "size": 1307,
    "docstring": "",
    "keywords": [],
    "functions": [
      "run_command"
    ],
    "classes": [],
    "imports": [
      "csv",
      "os",
      "subprocess"
    ],
    "preview": "import csv\nimport os\nimport subprocess\n\n\ndef run_command(command):\n    try:\n        result = subprocess.check_output(command, shell=True, universal_newlines=True)\n        return result.strip().split(\"\\n\")\n    except subprocess.CalledProcessError as e:\n        return [str(e)]\n\n\n# Define the commands to gather information\ncommands = {\n    \"Python Installations\": [\n        \"ls /usr/local/bin/python*\",\n        \"ls /usr/bin/python*\",\n        \"ls /Library/Frameworks/Python.framework/Versions/\",\n    ],",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1209",
    "name": "config_20250430201601.py",
    "path": "06_development_tools/development_utilities/config_20250430201601.py",
    "category": "06_development_tools",
    "type": "setup",
    "lines": 3,
    "size": 35,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [],
    "preview": "# config.py\nSOURCE_DIRECTORY = \"/\"\n",
    "last_modified": "2025-04-30T20:16:01.553254"
  },
  {
    "id": "1210",
    "name": "redis_cache.py",
    "path": "06_development_tools/development_utilities/redis_cache.py",
    "category": "06_development_tools",
    "type": "utility",
    "lines": 46,
    "size": 1371,
    "docstring": "",
    "keywords": [],
    "functions": [
      "__init__",
      "get",
      "set",
      "delete",
      "clear",
      "close"
    ],
    "classes": [
      "RedisCache"
    ],
    "imports": [
      "__future__",
      "datetime",
      "typing",
      "pip._vendor.cachecontrol.cache",
      "redis"
    ],
    "preview": "# SPDX-FileCopyrightText: 2015 Eric Larson\n#\n# SPDX-License-Identifier: Apache-2.0\nfrom __future__ import annotations\n\nfrom datetime import datetime, timezone\nfrom typing import TYPE_CHECKING\n\nfrom pip._vendor.cachecontrol.cache import BaseCache\n\nif TYPE_CHECKING:\n    from redis import Redis\n\n\nclass RedisCache(BaseCache):\n    def __init__(self, conn: Redis[bytes]) -> None:\n        self.conn = conn\n\n    def get(self, key: str) -> bytes | None:\n        return self.conn.get(key)",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1211",
    "name": "scheme.py",
    "path": "06_development_tools/development_utilities/scheme.py",
    "category": "06_development_tools",
    "type": "web_tools",
    "lines": 31,
    "size": 737,
    "docstring": "For types associated with installation schemes.\n\nFor a general overview of available schemes and their context, see\nhttps://docs.python.org/3/install/index.html#alternate-installation.",
    "keywords": [],
    "functions": [
      "__init__"
    ],
    "classes": [
      "Scheme"
    ],
    "imports": [],
    "preview": "\"\"\"\nFor types associated with installation schemes.\n\nFor a general overview of available schemes and their context, see\nhttps://docs.python.org/3/install/index.html#alternate-installation.\n\"\"\"\n\nSCHEME_KEYS = [\"platlib\", \"purelib\", \"headers\", \"scripts\", \"data\"]\n\n\nclass Scheme:\n    \"\"\"A Scheme holds paths which are used as the base directories for\n    artifacts associated with a Python package.\n    \"\"\"\n\n    __slots__ = SCHEME_KEYS\n\n    def __init__(\n        self,\n        platlib: str,",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1212",
    "name": "base.py",
    "path": "06_development_tools/development_utilities/base.py",
    "category": "06_development_tools",
    "type": "utility",
    "lines": 17,
    "size": 563,
    "docstring": "",
    "keywords": [],
    "functions": [
      "resolve",
      "get_installation_order"
    ],
    "classes": [
      "BaseResolver"
    ],
    "imports": [
      "typing",
      "pip._internal.req.req_install",
      "pip._internal.req.req_set"
    ],
    "preview": "from typing import Callable, List, Optional\n\nfrom pip._internal.req.req_install import InstallRequirement\nfrom pip._internal.req.req_set import RequirementSet\n\nInstallRequirementProvider = Callable[[str, Optional[InstallRequirement]], InstallRequirement]\n\n\nclass BaseResolver:\n    def resolve(\n        self, root_reqs: List[InstallRequirement], check_supported_wheels: bool\n    ) -> RequirementSet:\n        raise NotImplementedError()\n\n    def get_installation_order(self, req_set: RequirementSet) -> List[InstallRequirement]:\n        raise NotImplementedError()\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1213",
    "name": "quickstart.py",
    "path": "06_development_tools/development_utilities/quickstart.py",
    "category": "06_development_tools",
    "type": "utility",
    "lines": 37,
    "size": 1028,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "instapy"
    ],
    "preview": "# imports\nfrom instapy import InstaPy, smart_run\n\n# login credentials\ninsta_username = \"\"\ninsta_password = \"\"\n\ncomments = [\n    \"Nice shot! @{}\",\n    \"I love your profile! @{}\",\n    \"Your feed is an inspiration :thumbsup:\",\n    \"Just incredible :open_mouth:\",\n    \"What camera did you use @{}?\",\n    \"Love your posts @{}\",\n    \"Looks awesome @{}\",\n    \"Getting inspired by you @{}\",\n    \":raised_hands: Yes!\",\n    \"I can feel your passion @{} :muscle:\",\n]\n",
    "last_modified": "2025-09-13T05:53:49.242462"
  },
  {
    "id": "1214",
    "name": "deepseek_python_20250608130223.py",
    "path": "06_development_tools/development_utilities/deepseek_python_20250608130223.py",
    "category": "06_development_tools",
    "type": "utility",
    "lines": 2,
    "size": 7,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [],
    "preview": "format\n",
    "last_modified": "2025-09-13T05:53:27.376531"
  },
  {
    "id": "1215",
    "name": "ext.py",
    "path": "06_development_tools/development_utilities/ext.py",
    "category": "06_development_tools",
    "type": "utility",
    "lines": 185,
    "size": 5972,
    "docstring": "",
    "keywords": [],
    "functions": [
      "__new__",
      "__init__",
      "__repr__",
      "__eq__",
      "__ne__",
      "__hash__",
      "from_bytes",
      "to_bytes",
      "from_unix",
      "to_unix"
    ],
    "classes": [
      "ExtType",
      "Timestamp"
    ],
    "imports": [
      "datetime",
      "struct",
      "sys",
      "collections"
    ],
    "preview": "# coding: utf-8\nimport datetime\nimport struct\nimport sys\nfrom collections import namedtuple\n\nPY2 = sys.version_info[0] == 2\n\nif PY2:\n    int_types = (int, long)\n    _utc = None\nelse:\n    int_types = int\n    try:\n        _utc = datetime.timezone.utc\n    except AttributeError:\n        _utc = datetime.timezone(datetime.timedelta(0))\n\n\nclass ExtType(namedtuple(\"ExtType\", \"code data\")):",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1216",
    "name": "colors.py",
    "path": "06_development_tools/development_utilities/colors.py",
    "category": "06_development_tools",
    "type": "utility",
    "lines": 81,
    "size": 1418,
    "docstring": "",
    "keywords": [],
    "functions": [
      "randomize",
      "randomize1",
      "randomize2",
      "randomize3",
      "yellow",
      "cyan",
      "red",
      "white",
      "green",
      "magento"
    ],
    "classes": [
      "get_colors"
    ],
    "imports": [
      "random"
    ],
    "preview": "#!/usr/bin/python3\n# Created By ybenel\nfrom random import randint\n\nlist = [\n    \"\\033[1;33m\",\n    \"\\033[1;34m\",\n    \"\\033[1;30m\",\n    \"\\033[1;36m\",\n    \"\\033[1;31m\",\n    \"\\033[35m\",\n    \"\\033[95m\",\n    \"\\033[96m\",\n    \"\\033[39m\",\n    \"\\033[38;5;82m\",\n    \"\\033[38;5;198m\",\n    \"\\033[38;5;208m\",\n    \"\\033[38;5;167m\",\n    \"\\033[38;5;91m\",\n    \"\\033[38;5;210m\",",
    "last_modified": "2025-03-28T18:35:48.836972"
  },
  {
    "id": "1217",
    "name": "auth.py",
    "path": "03_automation_platforms/youtube_automation/auth.py",
    "category": "03_automation_platforms",
    "type": "utility",
    "lines": 35,
    "size": 953,
    "docstring": "",
    "keywords": [],
    "functions": [
      "get_authenticated_service"
    ],
    "classes": [],
    "imports": [
      "httplib2",
      "googleapiclient.discovery",
      "oauth2client.client",
      "oauth2client.file",
      "oauth2client.tools",
      "constants",
      "utils"
    ],
    "preview": "import httplib2\nfrom googleapiclient.discovery import Resource, build\nfrom oauth2client.client import flow_from_clientsecrets\nfrom oauth2client.file import Storage\nfrom oauth2client.tools import run_flow\n\nfrom .constants import (\n    CLIENT_SECRETS_FILE,\n    MISSING_CLIENT_SECRETS_MESSAGE,\n    YOUTUBE_API_SCOPES,\n    YOUTUBE_API_SERVICE_NAME,\n    YOUTUBE_API_VERSION,\n)\nfrom .utils import get_local_path\n\n\ndef get_authenticated_service() -> Resource:\n    flow = flow_from_clientsecrets(\n        get_local_path(CLIENT_SECRETS_FILE),\n        scope=YOUTUBE_API_SCOPES,",
    "last_modified": "2025-09-13T05:53:47.043447"
  },
  {
    "id": "1218",
    "name": "set_github_vars.py",
    "path": "03_automation_platforms/youtube_automation/set_github_vars.py",
    "category": "03_automation_platforms",
    "type": "utility",
    "lines": 44,
    "size": 1141,
    "docstring": "",
    "keywords": [],
    "functions": [
      "get_os",
      "get_cpu_architecture",
      "extract_app_version",
      "write_to_github_env"
    ],
    "classes": [],
    "imports": [
      "os",
      "platform",
      "subprocess"
    ],
    "preview": "import os\nimport platform\nimport subprocess\n\n\ndef get_os():\n    return platform.system()\n\n\ndef get_cpu_architecture():\n    # e.g. x86_64, arm64\n    return platform.machine()\n\n\ndef extract_app_version():\n    result = subprocess.run([\"youtube-bulk-upload\", \"--version\"], capture_output=True, text=True)\n    version = result.stdout.strip().split()[-1]\n    return version\n\n",
    "last_modified": "2025-09-13T05:53:46.179727"
  },
  {
    "id": "1219",
    "name": "test_auth.py",
    "path": "03_automation_platforms/youtube_automation/test_auth.py",
    "category": "03_automation_platforms",
    "type": "testing",
    "lines": 21,
    "size": 636,
    "docstring": "",
    "keywords": [
      "testing"
    ],
    "functions": [
      "test_authentication"
    ],
    "classes": [],
    "imports": [
      "google_auth_oauthlib.flow"
    ],
    "preview": "import google_auth_oauthlib.flow\n\nCLIENT_SECRETS_FILE = \"client_secret.json\"  # Ensure this file is in the same folder\nSCOPES = [\"https://www.googleapis.com/auth/youtube.upload\"]\n\n\ndef test_authentication():\n    \"\"\"Tests OAuth authentication with YouTube.\"\"\"\n    try:\n        flow = google_auth_oauthlib.flow.InstalledAppFlow.from_client_secrets_file(\n            CLIENT_SECRETS_FILE, SCOPES\n        )\n        credentials = flow.run_local_server(port=0)\n        print(\"\u2705 Authentication successful!\")\n    except Exception as e:\n        print(\"\u274c Authentication failed:\", str(e))\n\n\nif __name__ == \"__main__\":\n    test_authentication()",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1220",
    "name": "bypass.py",
    "path": "03_automation_platforms/youtube_automation/bypass.py",
    "category": "03_automation_platforms",
    "type": "organization",
    "lines": 127,
    "size": 4551,
    "docstring": "MIT License\n\nCopyright (c) 2021-2022 MShawon\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.",
    "keywords": [],
    "functions": [
      "ensure_click",
      "personalization",
      "bypass_consent",
      "click_popup",
      "bypass_popup",
      "bypass_other_popup"
    ],
    "classes": [],
    "imports": [
      "random",
      "time",
      "selenium",
      "selenium.common.exceptions",
      "selenium.webdriver.chrome.service",
      "selenium.webdriver.common.by",
      "selenium.webdriver.common.keys",
      "selenium.webdriver.support",
      "selenium.webdriver.support.ui"
    ],
    "preview": "\"\"\"\nMIT License\n\nCopyright (c) 2021-2022 MShawon\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,",
    "last_modified": "2025-09-13T05:54:11.615935"
  },
  {
    "id": "1221",
    "name": "APIHandler.py",
    "path": "03_automation_platforms/youtube_automation/APIHandler.py",
    "category": "03_automation_platforms",
    "type": "web_tools",
    "lines": 59,
    "size": 2259,
    "docstring": "",
    "keywords": [
      "web_tools"
    ],
    "functions": [
      "get_yt_playlist_size",
      "get_new_twitch_token",
      "get_twitch_game_id"
    ],
    "classes": [
      "APIHandler"
    ],
    "imports": [
      "logging",
      "requests",
      "src.utils",
      "config"
    ],
    "preview": "import logging\n\nimport requests\nimport src.utils as utils\n\nimport config\n\n\nclass APIHandler:\n    @staticmethod\n    def get_yt_playlist_size(playlist_id: str) -> int:\n        logging.info(\"Getting amount of playlist items\")\n        try:\n            config.YT_API_KEY\n        except AttributeError:\n            logging.warning(f\"No YT_API_KEY provided in config.py -> return playlist_size of 0\")\n            return 0\n        url = f\"https://www.googleapis.com/youtube/v3/playlistItems\"\n        payload = {\"part\": \"id\", \"playlistId\": playlist_id, \"key\": config.YT_API_KEY}\n        resp = requests.get(url, params=payload, headers={})",
    "last_modified": "2025-09-13T05:53:45.320054"
  },
  {
    "id": "1222",
    "name": "constants.py",
    "path": "03_automation_platforms/youtube_automation/constants.py",
    "category": "03_automation_platforms",
    "type": "utility",
    "lines": 66,
    "size": 2318,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "os",
      "httplib2"
    ],
    "preview": "import os\n\nimport httplib2\n\n# Explicitly tell the underlying HTTP transport library not to retry, since\n# we are handling retry logic ourselves.\nhttplib2.RETRIES = 1\n\n# Maximum number of times to retry before giving up.\nMAX_RETRIES = 10\n\n# Always retry when these exceptions are raised.\nRETRIABLE_EXCEPTIONS = (httplib2.HttpLib2Error, IOError)\n\n# Always retry when an apiclient.errors.HttpError with one of these status\n# codes is raised.\nRETRIABLE_STATUS_CODES = [500, 502, 503, 504]\n\n# The CLIENT_SECRETS_FILE variable specifies the name of a file that contains\n# the OAuth 2.0 information for this application, including its client_id and",
    "last_modified": "2025-05-04T22:47:12"
  },
  {
    "id": "1223",
    "name": "yt_plists_to_csv.py",
    "path": "03_automation_platforms/youtube_automation/yt_plists_to_csv.py",
    "category": "03_automation_platforms",
    "type": "utility",
    "lines": 49,
    "size": 1083,
    "docstring": "",
    "keywords": [],
    "functions": [
      "main"
    ],
    "classes": [],
    "imports": [
      "csv",
      "os",
      "re",
      "sys"
    ],
    "preview": "#!/usr/bin/python\n# Copyright 2018 stringCode ltd.\n# Licensed under the Apache License, Version 2.0\n# http://www.apache.org/licenses/LICENSE-2.0\n\nimport csv\nimport os\nimport re\nimport sys\n\n\"\"\"YouTube playlist name & url extractor\n\nExtracts names and urls form YouTube playlist html page. Writes them to csv.\n\"\"\"\n\ndef main():\n    # Handle args\n    args = sys.argv[1:]\n    if not args:\n        print 'usage: file'",
    "last_modified": "2025-03-28T18:36:55"
  },
  {
    "id": "1224",
    "name": "secret.py",
    "path": "03_automation_platforms/youtube_automation/secret.py",
    "category": "03_automation_platforms",
    "type": "utility",
    "lines": 4,
    "size": 176,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [],
    "preview": "SPOTIFY_CLIENT_ID = \"28b20556906f4b75874c4ae98320c81d\"\nSPOTIFY_CLIENT_SECRET = \"c7033fd14e1247cfb9eef73874dd2365\"\nYOUTUBE_CLIENT_ID = \"AIzaSyAAWcCkndgVzQBgbV0hk2UbRE0a1uO-5H8\"\n",
    "last_modified": "2022-12-30T18:09:58"
  },
  {
    "id": "1225",
    "name": "proxies.py",
    "path": "03_automation_platforms/youtube_automation/proxies.py",
    "category": "03_automation_platforms",
    "type": "organization",
    "lines": 148,
    "size": 4259,
    "docstring": "MIT License\n\nCopyright (c) 2021-2022 MShawon\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.",
    "keywords": [
      "web_tools"
    ],
    "functions": [
      "gather_proxy",
      "load_proxy",
      "scrape_api",
      "check_proxy"
    ],
    "classes": [],
    "imports": [
      "sys",
      "random",
      "requests",
      "colors"
    ],
    "preview": "\"\"\"\nMIT License\n\nCopyright (c) 2021-2022 MShawon\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,",
    "last_modified": "2025-05-04T23:27:45"
  },
  {
    "id": "1226",
    "name": "yt_plists_to_csv 2.py",
    "path": "03_automation_platforms/youtube_automation/yt_plists_to_csv 2.py",
    "category": "03_automation_platforms",
    "type": "utility",
    "lines": 49,
    "size": 1083,
    "docstring": "",
    "keywords": [],
    "functions": [
      "main"
    ],
    "classes": [],
    "imports": [
      "csv",
      "os",
      "re",
      "sys"
    ],
    "preview": "#!/usr/bin/python\n# Copyright 2018 stringCode ltd.\n# Licensed under the Apache License, Version 2.0\n# http://www.apache.org/licenses/LICENSE-2.0\n\nimport csv\nimport os\nimport re\nimport sys\n\n\"\"\"YouTube playlist name & url extractor\n\nExtracts names and urls form YouTube playlist html page. Writes them to csv.\n\"\"\"\n\ndef main():\n    # Handle args\n    args = sys.argv[1:]\n    if not args:\n        print 'usage: file'",
    "last_modified": "2025-08-06T14:07:36.329219"
  },
  {
    "id": "1227",
    "name": "proxy_harvester.py_02.py",
    "path": "03_automation_platforms/proxy_harvester.py_consolidated/proxy_harvester.py_02.py",
    "category": "03_automation_platforms",
    "type": "utility",
    "lines": 39,
    "size": 1025,
    "docstring": "",
    "keywords": [
      "web_tools"
    ],
    "functions": [
      "find_proxies"
    ],
    "classes": [],
    "imports": [
      "asyncio",
      "libs.utils",
      "proxybroker",
      "requests"
    ],
    "preview": "import asyncio\n\nfrom libs.utils import ask_question, print_error, print_status, print_success\nfrom proxybroker import Broker\nfrom requests import get\n\n\nasync def show(proxies, proxy_list):\n    while len(proxy_list) < 50:\n        proxy = await proxies.get()\n        if proxy is None:\n            break\n\n        print_success(\n            \"[\" + str(len(proxy_list) + 1) + \"/50]\",\n            \"Proxy found:\",\n            proxy.as_json()[\"host\"] + \":\" + str(proxy.as_json()[\"port\"]),\n        )\n\n        proxy_list.append(proxy.as_json()[\"host\"] + \":\" + str(proxy.as_json()[\"port\"]))",
    "last_modified": "2025-09-13T05:53:40.916815"
  },
  {
    "id": "1228",
    "name": "proxy_harvester.py.py",
    "path": "03_automation_platforms/proxy_harvester.py_consolidated/proxy_harvester.py.py",
    "category": "03_automation_platforms",
    "type": "utility",
    "lines": 151,
    "size": 6694,
    "docstring": "",
    "keywords": [
      "web_tools"
    ],
    "functions": [
      "find_proxies"
    ],
    "classes": [],
    "imports": [
      "os",
      "random",
      "time",
      "os",
      "requests",
      "colorama",
      "colorama",
      "libs.animation"
    ],
    "preview": "#!/usr/bin/env python3\nimport os  # line:4\nimport random  # line:8\nimport time  # line:7\nfrom os import _exit  # line:5\n\nimport requests  # line:2\nfrom colorama import *  # line:3\nfrom colorama import Back, Fore, Style  # line:6\nfrom libs.animation import colorText  # line:9\n\ninit(convert=True)  # line:10\nos.system(\"cls\" if os.name == \"nt\" else \"clear\")  # line:11\nrhttps = requests.get(\n    \"https://api.proxyscrape.com/?request=displayproxies&proxytype=https&timeout=7000&country=ALL&anonymity=elite&ssl=no\"\n)  # line:12\nrhttp = requests.get(\n    \"https://api.proxyscrape.com/?request=displayproxies&proxytype=http&timeout=7000&country=ALL&anonymity=elite&ssl=no\"\n)  # line:13\nrs4 = requests.get(\"https://www.proxy-list.download/api/v1/get?type=socks4\")  # line:14",
    "last_modified": "2025-05-04T23:28:20"
  },
  {
    "id": "1229",
    "name": "config.py.py",
    "path": "03_automation_platforms/config.py_consolidated/config.py.py",
    "category": "03_automation_platforms",
    "type": "setup",
    "lines": 36,
    "size": 1320,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "json",
      "loguru",
      "llm_engineering.settings",
      "sagemaker.compute_resource_requirements.resource_requirements"
    ],
    "preview": "import json\n\nfrom loguru import logger\n\ntry:\n    from sagemaker.compute_resource_requirements.resource_requirements import ResourceRequirements\nexcept ModuleNotFoundError:\n    logger.warning(\n        \"Couldn't load SageMaker imports. Run 'poetry install --with aws' to support AWS.\"\n    )\n\nfrom llm_engineering.settings import settings\n\nhugging_face_deploy_config = {\n    \"HF_MODEL_ID\": settings.HF_MODEL_ID,\n    \"HUGGING_FACE_HUB_TOKEN\": settings.HUGGINGFACE_ACCESS_TOKEN,\n    \"SM_NUM_GPUS\": json.dumps(settings.SM_NUM_GPUS),  # Number of GPU used per replica\n    \"MAX_INPUT_LENGTH\": json.dumps(settings.MAX_INPUT_LENGTH),  # Max length of input text\n    \"MAX_TOTAL_TOKENS\": json.dumps(\n        settings.MAX_TOTAL_TOKENS",
    "last_modified": "2025-09-13T05:53:41.920509"
  },
  {
    "id": "1230",
    "name": "config.py_02.py",
    "path": "03_automation_platforms/config.py_consolidated/config.py_02.py",
    "category": "03_automation_platforms",
    "type": "setup",
    "lines": 6,
    "size": 307,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [],
    "preview": "# Obtain Praw credentails and add your client id and secret below in their respected fields\n# For help try visiting: https://www.jcchouinard.com/get-reddit-api-credentials-with-praw/\n#              and: reddit.com/prefs/apps\n\nPRAW_CONFIG = {\"client_id\": \"\", \"client_secret\": \"\", \"user_agent\": \"user-agent\"}\n",
    "last_modified": "2025-09-13T05:53:51.719499"
  },
  {
    "id": "1231",
    "name": "like_timeline_feed.py",
    "path": "03_automation_platforms/api_integrations/like_timeline_feed.py",
    "category": "03_automation_platforms",
    "type": "utility",
    "lines": 17,
    "size": 242,
    "docstring": "instabot example\n\nWorkflow:\n    Like rescent medias from your timeline feed.",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "os",
      "sys",
      "instabot"
    ],
    "preview": "\"\"\"\ninstabot example\n\nWorkflow:\n    Like rescent medias from your timeline feed.\n\"\"\"\n\nimport os\nimport sys\n\nsys.path.append(os.path.join(sys.path[0], \"../\"))\nfrom instabot import Bot  # noqa: E402\n\nbot = Bot()\nbot.login()\nbot.like_timeline()\n",
    "last_modified": "2025-05-04T23:28:20"
  },
  {
    "id": "1232",
    "name": "like_medias_by_location.py",
    "path": "03_automation_platforms/api_integrations/like_medias_by_location.py",
    "category": "03_automation_platforms",
    "type": "utility",
    "lines": 99,
    "size": 3008,
    "docstring": "instabot example\n\nWorkflow:\n    Like medias by location.",
    "keywords": [],
    "functions": [
      "like_location_feed"
    ],
    "classes": [],
    "imports": [
      "argparse",
      "codecs",
      "os",
      "sys",
      "tqdm",
      "instabot"
    ],
    "preview": "# coding=utf-8\n\"\"\"\ninstabot example\n\nWorkflow:\n    Like medias by location.\n\"\"\"\n\nimport argparse\nimport codecs\nimport os\nimport sys\n\nfrom tqdm import tqdm\n\nstdout = sys.stdout\nsys.stdout = codecs.getwriter(\"utf8\")(sys.stdout)\n\nsys.path.append(os.path.join(sys.path[0], \"../\"))\nfrom instabot import Bot  # noqa: E402",
    "last_modified": "2025-09-13T05:54:55.301798"
  },
  {
    "id": "1233",
    "name": "chaos_scheduler.py",
    "path": "03_automation_platforms/api_integrations/chaos_scheduler.py",
    "category": "03_automation_platforms",
    "type": "utility",
    "lines": 20,
    "size": 640,
    "docstring": "",
    "keywords": [],
    "functions": [
      "__init__",
      "_generate_quantum_seed",
      "schedule_operation"
    ],
    "classes": [
      "ChaosScheduler"
    ],
    "imports": [
      "hashlib",
      "random",
      "datetime"
    ],
    "preview": "import hashlib\nimport random\nfrom datetime import datetime\n\n\nclass ChaosScheduler:\n    def __init__(self, seed=None):\n        self.seed = seed or self._generate_quantum_seed()\n\n    def _generate_quantum_seed(self):\n        ts = str(datetime.now().timestamp()).encode()\n        return int(hashlib.sha3_256(ts).hexdigest(), 16) % 2**32\n\n    def schedule_operation(self, operation, criticality=3):\n        random.seed(self.seed)\n        chaos_level = (criticality * 0.15) + (random.random() * 0.35)\n        if random.random() < chaos_level:\n            raise RuntimeError(f\"Chaos failure (Level {chaos_level:.2f})\")\n        return operation()\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1234",
    "name": "bot_cache.py",
    "path": "03_automation_platforms/api_integrations/bot_cache.py",
    "category": "03_automation_platforms",
    "type": "utility",
    "lines": 15,
    "size": 347,
    "docstring": "",
    "keywords": [],
    "functions": [
      "__init__",
      "__repr__"
    ],
    "classes": [
      "BotCache"
    ],
    "imports": [
      "instabot.singleton"
    ],
    "preview": "from instabot.singleton import Singleton\n\n\nclass BotCache(object):\n    __metaclass__ = Singleton\n\n    def __init__(self):\n        self.following = None\n        self.followers = None\n        self.user_infos = {}  # User info cache\n        self.usernames = {}  # `username` to `user_id` mapping\n\n    def __repr__(self):\n        return self.__dict__\n",
    "last_modified": "2025-03-28T18:35:47"
  },
  {
    "id": "1235",
    "name": "labels.py",
    "path": "03_automation_platforms/api_integrations/labels.py",
    "category": "03_automation_platforms",
    "type": "web_tools",
    "lines": 232,
    "size": 6833,
    "docstring": "webencodings.labels\n~~~~~~~~~~~~~~~~~~~\n\nMap encoding labels to their name.\n\n:copyright: Copyright 2012 by Simon Sapin\n:license: BSD, see LICENSE for details.",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [],
    "preview": "\"\"\"\n\nwebencodings.labels\n~~~~~~~~~~~~~~~~~~~\n\nMap encoding labels to their name.\n\n:copyright: Copyright 2012 by Simon Sapin\n:license: BSD, see LICENSE for details.\n\n\"\"\"\n\n# XXX Do not edit!\n# This file is automatically generated by mklabels.py\n\nLABELS = {\n    \"unicode-1-1-utf-8\": \"utf-8\",\n    \"utf-8\": \"utf-8\",\n    \"utf8\": \"utf-8\",\n    \"866\": \"ibm866\",",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1236",
    "name": "like_users.py",
    "path": "03_automation_platforms/api_integrations/like_users.py",
    "category": "03_automation_platforms",
    "type": "utility",
    "lines": 28,
    "size": 710,
    "docstring": "instabot example\n\nWorkflow:\n    Like last medias by users.",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "argparse",
      "os",
      "sys",
      "instabot"
    ],
    "preview": "\"\"\"\ninstabot example\n\nWorkflow:\n    Like last medias by users.\n\"\"\"\n\nimport argparse\nimport os\nimport sys\n\nsys.path.append(os.path.join(sys.path[0], \"../\"))\nfrom instabot import Bot  # noqa: E402\n\nparser = argparse.ArgumentParser(add_help=True)\nparser.add_argument(\"-u\", type=str, help=\"username\")\nparser.add_argument(\"-p\", type=str, help=\"password\")\nparser.add_argument(\"-l\", type=int, help=\"limit\")\nparser.add_argument(\"-proxy\", type=str, help=\"proxy\")\nparser.add_argument(\"users\", type=str, nargs=\"+\", help=\"users\")",
    "last_modified": "2025-03-28T18:35:47"
  },
  {
    "id": "1237",
    "name": "like_and_follow_media_likers.py",
    "path": "03_automation_platforms/api_integrations/like_and_follow_media_likers.py",
    "category": "03_automation_platforms",
    "type": "utility",
    "lines": 44,
    "size": 1041,
    "docstring": "instabot example\n\nWorkflow:\n    Like and follow you last media likers.",
    "keywords": [],
    "functions": [
      "like_and_follow",
      "like_and_follow_media_likers"
    ],
    "classes": [],
    "imports": [
      "argparse",
      "os",
      "random",
      "sys",
      "time",
      "tqdm",
      "instabot"
    ],
    "preview": "\"\"\"\ninstabot example\n\nWorkflow:\n    Like and follow you last media likers.\n\"\"\"\n\nimport argparse\nimport os\nimport random\nimport sys\nimport time\n\nfrom tqdm import tqdm\n\nsys.path.append(os.path.join(sys.path[0], \"../\"))\nfrom instabot import Bot  # noqa: E402\n\n\ndef like_and_follow(bot, user_id, nlikes=3):",
    "last_modified": "2025-03-28T18:35:47"
  },
  {
    "id": "1238",
    "name": "interact_DM.py",
    "path": "03_automation_platforms/api_integrations/interact_DM.py",
    "category": "03_automation_platforms",
    "type": "utility",
    "lines": 58,
    "size": 1493,
    "docstring": "instabot example\n\nWorkflow:\n    read and reply your DM",
    "keywords": [],
    "functions": [
      "choice"
    ],
    "classes": [],
    "imports": [
      "argparse",
      "os",
      "sys",
      "instabot"
    ],
    "preview": "# -*- coding: utf-8 -*-\n\"\"\"\ninstabot example\n\nWorkflow:\n    read and reply your DM\n\"\"\"\n\nimport argparse\nimport os\nimport sys\n\nsys.path.append(os.path.join(sys.path[0], \"../\"))\nfrom instabot import Bot  # noqa: E402\n\ntry:\n    input = raw_input\nexcept NameError:\n    pass\n",
    "last_modified": "2025-03-28T18:35:47"
  },
  {
    "id": "1239",
    "name": "like_and_follow_your_last_media_likers.py",
    "path": "03_automation_platforms/api_integrations/like_and_follow_your_last_media_likers.py",
    "category": "03_automation_platforms",
    "type": "utility",
    "lines": 49,
    "size": 1229,
    "docstring": "instabot example\n\nWorkflow:\n    Like and follow likers of last medias from your timeline feed.",
    "keywords": [],
    "functions": [
      "like_and_follow",
      "like_and_follow_media_likers",
      "like_and_follow_your_feed_likers"
    ],
    "classes": [],
    "imports": [
      "argparse",
      "os",
      "random",
      "sys",
      "time",
      "tqdm",
      "instabot"
    ],
    "preview": "\"\"\"\ninstabot example\n\nWorkflow:\n    Like and follow likers of last medias from your timeline feed.\n\"\"\"\n\nimport argparse\nimport os\nimport random\nimport sys\nimport time\n\nfrom tqdm import tqdm\n\nsys.path.append(os.path.join(sys.path[0], \"../\"))\nfrom instabot import Bot  # noqa: E402\n\n\ndef like_and_follow(bot, user_id, nlikes=3):",
    "last_modified": "2025-03-28T18:35:47"
  },
  {
    "id": "1240",
    "name": "like_example.py",
    "path": "03_automation_platforms/api_integrations/like_example.py",
    "category": "03_automation_platforms",
    "type": "utility",
    "lines": 35,
    "size": 555,
    "docstring": "instabot example\n\nWorkflow:\n1) likes your timeline feed\n2) likes user's feed\n\nNotes:\n1) You should pass user_id, not username",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "os",
      "sys",
      "instabot"
    ],
    "preview": "\"\"\"\ninstabot example\n\nWorkflow:\n1) likes your timeline feed\n2) likes user's feed\n\nNotes:\n1) You should pass user_id, not username\n\"\"\"\n\nimport os\nimport sys\n\nsys.path.append(os.path.join(sys.path[0], \"../\"))\nfrom instabot import Bot  # noqa: E402\n\nbot = Bot()\nbot.login()\n",
    "last_modified": "2025-03-28T18:35:47"
  },
  {
    "id": "1241",
    "name": "super_simple_setting_for_crontab.py",
    "path": "03_automation_platforms/api_integrations/super_simple_setting_for_crontab.py",
    "category": "03_automation_platforms",
    "type": "utility",
    "lines": 66,
    "size": 1518,
    "docstring": "This template is written by @Edhim\n\nWhat does this quickstart script aim to do?\n- I am using simple settings for my personal account with a crontab each 3H,\nit's been working since 5 months with no problem.",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "instapy"
    ],
    "preview": "\"\"\"\nThis template is written by @Edhim\n\nWhat does this quickstart script aim to do?\n- I am using simple settings for my personal account with a crontab each 3H,\nit's been working since 5 months with no problem.\n\"\"\"\n\nfrom instapy import InstaPy, smart_run\n\n# get a session!\nsession = InstaPy(username=\"\", password=\"\")\n\n# let's go! :>\nwith smart_run(session):\n    # settings\n    session.set_relationship_bounds(\n        enabled=False,\n        potency_ratio=-1.21,\n        delimit_by_numbers=True,",
    "last_modified": "2025-03-28T18:36:56"
  },
  {
    "id": "1242",
    "name": "save_users_followers_into_file.py",
    "path": "03_automation_platforms/api_integrations/save_users_followers_into_file.py",
    "category": "03_automation_platforms",
    "type": "organization",
    "lines": 30,
    "size": 780,
    "docstring": "instabot example\n\nWorkflow:\n    Save users' followers into a file.",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "argparse",
      "os",
      "sys",
      "instabot"
    ],
    "preview": "\"\"\"\ninstabot example\n\nWorkflow:\n    Save users' followers into a file.\n\"\"\"\n\nimport argparse\nimport os\nimport sys\n\nsys.path.append(os.path.join(sys.path[0], \"../\"))\nfrom instabot import Bot, utils  # noqa: E402\n\nparser = argparse.ArgumentParser(add_help=False)\nparser.add_argument(\"-u\", type=str, help=\"username\")\nparser.add_argument(\"-p\", type=str, help=\"password\")\nparser.add_argument(\"-proxy\", type=str, help=\"proxy\")\nparser.add_argument(\"filename\", type=str, help=\"filename\")\nparser.add_argument(\"users\", type=str, nargs=\"+\", help=\"users\")",
    "last_modified": "2025-03-28T18:35:47"
  },
  {
    "id": "1243",
    "name": "help_text.py",
    "path": "03_automation_platforms/api_integrations/help_text.py",
    "category": "03_automation_platforms",
    "type": "utility",
    "lines": 85,
    "size": 2549,
    "docstring": "",
    "keywords": [],
    "functions": [
      "GetExpiryDate"
    ],
    "classes": [],
    "imports": [
      "logging",
      "os",
      "sqlite3",
      "pyrogram",
      "translation",
      "helper_funcs.chat_base",
      "sample_config",
      "config"
    ],
    "preview": "import logging\n\nlogging.basicConfig(\n    level=logging.DEBUG, format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\n)\nlogger = logging.getLogger(__name__)\n\nimport os\nimport sqlite3\n\n# the secret configuration specific things\nif bool(os.environ.get(\"WEBHOOK\", False)):\n    from sample_config import Config\nelse:\n    from config import Config\n\n# the Strings used for this \"thing\"\nimport pyrogram\nfrom translation import Translation\n",
    "last_modified": "2025-05-06T04:35:15.024583"
  },
  {
    "id": "1244",
    "name": "like_your_last_media_likers.py",
    "path": "03_automation_platforms/api_integrations/like_your_last_media_likers.py",
    "category": "03_automation_platforms",
    "type": "utility",
    "lines": 37,
    "size": 899,
    "docstring": "instabot example\nWorkflow:\n    Like likers of last medias from your timeline feed.",
    "keywords": [],
    "functions": [
      "like_media_likers",
      "like_your_feed_likers"
    ],
    "classes": [],
    "imports": [
      "argparse",
      "os",
      "sys",
      "tqdm",
      "instabot"
    ],
    "preview": "\"\"\"\ninstabot example\nWorkflow:\n    Like likers of last medias from your timeline feed.\n\"\"\"\n\nimport argparse\nimport os\nimport sys\n\nfrom tqdm import tqdm\n\nsys.path.append(os.path.join(sys.path[0], \"../\"))\nfrom instabot import Bot  # noqa: E402\n\n\ndef like_media_likers(bot, media, nlikes=3):\n    for user in tqdm(bot.get_media_likers(media), desc=\"Media likers\"):\n        bot.like_user(user, nlikes)\n    return True",
    "last_modified": "2025-03-28T18:35:47"
  },
  {
    "id": "1245",
    "name": "save_unfollowers_into_file.py",
    "path": "03_automation_platforms/api_integrations/save_unfollowers_into_file.py",
    "category": "03_automation_platforms",
    "type": "organization",
    "lines": 38,
    "size": 893,
    "docstring": "instabot example\n\nWorkflow:\n    Save user' unfollowers into a file.",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "argparse",
      "os",
      "sys",
      "instabot"
    ],
    "preview": "\"\"\"\ninstabot example\n\nWorkflow:\n    Save user' unfollowers into a file.\n\"\"\"\n\nimport argparse\nimport os\nimport sys\n\nsys.path.append(os.path.join(sys.path[0], \"../\"))\nfrom instabot import Bot, utils  # noqa: E402\n\nparser = argparse.ArgumentParser(add_help=False)\nparser.add_argument(\"-u\", type=str, help=\"username\")\nparser.add_argument(\"-p\", type=str, help=\"password\")\nparser.add_argument(\"-proxy\", type=str, help=\"proxy\")\nargs = parser.parse_args()\n",
    "last_modified": "2025-03-28T18:35:47"
  },
  {
    "id": "1246",
    "name": "follow_users_from_file.py",
    "path": "03_automation_platforms/api_integrations/follow_users_from_file.py",
    "category": "03_automation_platforms",
    "type": "organization",
    "lines": 33,
    "size": 845,
    "docstring": "instabot example\n\nWorkflow:\n    Take users from input file and follow them.\n    The file should contain one username per line!",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "argparse",
      "os",
      "sys",
      "instabot"
    ],
    "preview": "\"\"\"\ninstabot example\n\nWorkflow:\n    Take users from input file and follow them.\n    The file should contain one username per line!\n\"\"\"\n\nimport argparse\nimport os\nimport sys\n\nsys.path.append(os.path.join(sys.path[0], \"../\"))\nfrom instabot import Bot  # noqa: E402\n\nparser = argparse.ArgumentParser(add_help=True)\nparser.add_argument(\"-u\", type=str, help=\"username\")\nparser.add_argument(\"-p\", type=str, help=\"password\")\nparser.add_argument(\"-proxy\", type=str, help=\"proxy\")\nparser.add_argument(\"filepath\", type=str, help=\"filepath\")",
    "last_modified": "2025-03-28T18:35:47"
  },
  {
    "id": "1247",
    "name": "whitelist_generator.py",
    "path": "03_automation_platforms/api_integrations/whitelist_generator.py",
    "category": "03_automation_platforms",
    "type": "utility",
    "lines": 37,
    "size": 967,
    "docstring": "instabot example\n\nWhitelist generator: generates a list of users which\nwill not be unfollowed.",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "os",
      "random",
      "sys",
      "instabot"
    ],
    "preview": "\"\"\"\ninstabot example\n\nWhitelist generator: generates a list of users which\nwill not be unfollowed.\n\"\"\"\n\nimport os\nimport random\nimport sys\n\nsys.path.append(os.path.join(sys.path[0], \"../../\"))\nfrom instabot import Bot  # noqa: E402\n\nbot = Bot()\nbot.login()\n\nprint(\n    \"This script will generate whitelist.txt file with users\"\n    \"who will not be unfollowed by bot. \"",
    "last_modified": "2025-03-28T18:35:47"
  },
  {
    "id": "1248",
    "name": "conf.py",
    "path": "03_automation_platforms/api_integrations/conf.py",
    "category": "03_automation_platforms",
    "type": "utility",
    "lines": 38,
    "size": 1204,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [],
    "preview": "# Configuration file for the Sphinx documentation builder.\n#\n# For the full list of built-in configuration values, see the documentation:\n# https://www.sphinx-doc.org/en/master/usage/configuration.html\n\n# -- Project information -----------------------------------------------------\n# https://www.sphinx-doc.org/en/master/usage/configuration.html#project-information\n\nproject = 'Creative Automation Docs'\ncopyright = '2025, Steven'\nauthor = 'Steven'\nrelease = '1'\n\n# -- General configuration ---------------------------------------------------\n# https://www.sphinx-doc.org/en/master/usage/configuration.html#general-configuration\n\nextensions = [\n    'myst_parser',  # For Markdown\n    'sphinx.ext.autodoc',  # For Python code\n    'sphinx.ext.napoleon',  # For Google-style docstrings",
    "last_modified": "2025-10-08T06:41:23"
  },
  {
    "id": "1249",
    "name": "ollama_gui.py",
    "path": "03_automation_platforms/api_integrations/ollama_gui.py",
    "category": "03_automation_platforms",
    "type": "utility",
    "lines": 115,
    "size": 4130,
    "docstring": "",
    "keywords": [],
    "functions": [
      "run_command",
      "__init__",
      "on_run_clicked"
    ],
    "classes": [
      "OllamaGUI"
    ],
    "imports": [
      "os",
      "shlex",
      "tkinter",
      "subprocess",
      "tkinter"
    ],
    "preview": "#!/usr/bin/env python3\nimport os\nimport shlex\nimport tkinter as tk\nfrom subprocess import PIPE, STDOUT, CalledProcessError, Popen\nfrom tkinter import messagebox, scrolledtext, ttk\n\n\n# --------------------------------------------------\n# 1. Helper Function: Run a shell command & capture output\n# --------------------------------------------------\ndef run_command(cmd: str) -> str:\n    \"\"\"\n    Execute the given shell command string and return combined stdout/stderr.\n    If the command fails, return the error message.\n    \"\"\"\n    try:\n        # Use shlex.split to handle quoted arguments safely\n        process = Popen(shlex.split(cmd), stdout=PIPE, stderr=STDOUT, text=True)\n        output, _ = process.communicate()",
    "last_modified": "2025-09-06T12:24:11.827019"
  },
  {
    "id": "1250",
    "name": "block_bots.py",
    "path": "03_automation_platforms/api_integrations/block_bots.py",
    "category": "03_automation_platforms",
    "type": "utility",
    "lines": 35,
    "size": 904,
    "docstring": "instabot example\n\nWorkflow:\n    Block bots. That makes them unfollow you -> You have clear account.",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "argparse",
      "os",
      "sys",
      "instabot"
    ],
    "preview": "\"\"\"\ninstabot example\n\nWorkflow:\n    Block bots. That makes them unfollow you -> You have clear account.\n\"\"\"\n\nimport argparse\nimport os\nimport sys\n\nsys.path.append(os.path.join(sys.path[0], \"../\"))\nfrom instabot import Bot  # noqa: E402\n\nparser = argparse.ArgumentParser(add_help=True)\nparser.add_argument(\"-u\", type=str, help=\"username\")\nparser.add_argument(\"-p\", type=str, help=\"password\")\nparser.add_argument(\"-proxy\", type=str, help=\"proxy\")\nargs = parser.parse_args()\n",
    "last_modified": "2025-03-28T18:35:47"
  },
  {
    "id": "1251",
    "name": "sphinxext.py",
    "path": "03_automation_platforms/api_integrations/sphinxext.py",
    "category": "03_automation_platforms",
    "type": "utility",
    "lines": 236,
    "size": 7140,
    "docstring": "pygments.sphinxext\n~~~~~~~~~~~~~~~~~~\n\nSphinx extension to generate automatic documentation of lexers,\nformatters and filters.\n\n:copyright: Copyright 2006-2023 by the Pygments team, see AUTHORS.\n:license: BSD, see LICENSE for details.",
    "keywords": [],
    "functions": [
      "setup",
      "run",
      "document_lexers_overview",
      "document_lexers",
      "document_formatters",
      "document_filters",
      "format_link",
      "write_row",
      "write_seperator"
    ],
    "classes": [
      "PygmentsDoc"
    ],
    "imports": [
      "sys",
      "docutils",
      "docutils.parsers.rst",
      "docutils.statemachine",
      "sphinx.util.nodes",
      "pip._vendor.pygments.lexers",
      "pip._vendor.pygments.lexers._mapping",
      "pip._vendor.pygments.lexers._mapping",
      "pip._vendor.pygments.formatters",
      "pip._vendor.pygments.filters"
    ],
    "preview": "\"\"\"\npygments.sphinxext\n~~~~~~~~~~~~~~~~~~\n\nSphinx extension to generate automatic documentation of lexers,\nformatters and filters.\n\n:copyright: Copyright 2006-2023 by the Pygments team, see AUTHORS.\n:license: BSD, see LICENSE for details.\n\"\"\"\n\nimport sys\n\nfrom docutils import nodes\nfrom docutils.parsers.rst import Directive\nfrom docutils.statemachine import ViewList\nfrom sphinx.util.nodes import nested_parse_with_titles\n\nMODULEDOC = \"\"\"\n.. module:: %s",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1252",
    "name": "archive_medias.py",
    "path": "03_automation_platforms/api_integrations/archive_medias.py",
    "category": "03_automation_platforms",
    "type": "utility",
    "lines": 35,
    "size": 749,
    "docstring": "instabot example\n\nWorkflow:\n    Archive medias.",
    "keywords": [],
    "functions": [
      "archive_medias"
    ],
    "classes": [],
    "imports": [
      "argparse",
      "os",
      "sys",
      "tqdm",
      "instabot"
    ],
    "preview": "\"\"\"\ninstabot example\n\nWorkflow:\n    Archive medias.\n\"\"\"\n\nimport argparse\nimport os\nimport sys\n\nfrom tqdm import tqdm\n\nsys.path.append(os.path.join(sys.path[0], \"../\"))\nfrom instabot import Bot  # noqa: E402\n\n\ndef archive_medias(bot, medias):\n    for media in tqdm(medias, desc=\"Medias\"):\n        bot.archive(media)",
    "last_modified": "2025-03-28T18:35:47"
  },
  {
    "id": "1253",
    "name": "infinity_feedliker.py",
    "path": "03_automation_platforms/api_integrations/infinity_feedliker.py",
    "category": "03_automation_platforms",
    "type": "setup",
    "lines": 30,
    "size": 623,
    "docstring": "instabot example\n\nWorkflow:\n    Like rescent medias from your timeline feed.",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "argparse",
      "os",
      "sys",
      "time",
      "instabot"
    ],
    "preview": "\"\"\"\ninstabot example\n\nWorkflow:\n    Like rescent medias from your timeline feed.\n\"\"\"\n\nimport argparse\nimport os\nimport sys\nimport time\n\nsys.path.append(os.path.join(sys.path[0], \"../\"))\nfrom instabot import Bot  # noqa: E402\n\nparser = argparse.ArgumentParser(add_help=True)\nparser.add_argument(\"-u\", type=str, help=\"username\")\nparser.add_argument(\"-p\", type=str, help=\"password\")\nparser.add_argument(\"-proxy\", type=str, help=\"proxy\")\nargs = parser.parse_args()",
    "last_modified": "2025-03-28T18:35:47"
  },
  {
    "id": "1254",
    "name": "save_users_following_into_file.py",
    "path": "03_automation_platforms/api_integrations/save_users_following_into_file.py",
    "category": "03_automation_platforms",
    "type": "organization",
    "lines": 30,
    "size": 780,
    "docstring": "instabot example\n\nWorkflow:\n    Save users' following into a file.",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "argparse",
      "os",
      "sys",
      "instabot"
    ],
    "preview": "\"\"\"\ninstabot example\n\nWorkflow:\n    Save users' following into a file.\n\"\"\"\n\nimport argparse\nimport os\nimport sys\n\nsys.path.append(os.path.join(sys.path[0], \"../\"))\nfrom instabot import Bot, utils  # noqa: E402\n\nparser = argparse.ArgumentParser(add_help=False)\nparser.add_argument(\"-u\", type=str, help=\"username\")\nparser.add_argument(\"-p\", type=str, help=\"password\")\nparser.add_argument(\"-proxy\", type=str, help=\"proxy\")\nparser.add_argument(\"filename\", type=str, help=\"filename\")\nparser.add_argument(\"users\", type=str, nargs=\"+\", help=\"users\")",
    "last_modified": "2025-03-28T18:35:47"
  },
  {
    "id": "1255",
    "name": "mistral_ai_api_quickstart copy.py",
    "path": "03_automation_platforms/api_integrations/mistral_ai_api_quickstart copy.py",
    "category": "03_automation_platforms",
    "type": "web_tools",
    "lines": 38,
    "size": 1038,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "mistralai"
    ],
    "preview": "# -*- coding: utf-8 -*-\n\"\"\"Mistral AI API quickstart.ipynb\n\nAutomatically generated by Colab.\n\nOriginal file is located at\n    https://colab.research.google.com/drive/1lh2Uc6h2BRkSVXhzyewGHRjhXWvkz9Rq\n\n# Getting started with Mistral AI API\n\"\"\"\n\n! pip install mistralai\n\n\"\"\"Our API is currently available through [La Plateforme](https://console.mistral.ai/). You need to activate payments on your account to enable your API keys. After a few moments, you will be able to use our `chat` endpoint:\"\"\"\n\nfrom mistralai import Mistral\napi_key = \"z7opEhiZF9aPgHsKCgMW9G3axhFq5qiO\"\nmodel = \"mistral-large-latest\"\n\nclient = Mistral(api_key=api_key)",
    "last_modified": "2025-09-07T01:16:31.677939"
  },
  {
    "id": "1256",
    "name": "__init__.py",
    "path": "03_automation_platforms/api_integrations/__init__.py",
    "category": "03_automation_platforms",
    "type": "setup",
    "lines": 4,
    "size": 40,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "bot"
    ],
    "preview": "from .bot import Bot\n\n__all__ = [\"Bot\"]\n",
    "last_modified": "2022-11-05T04:33:59"
  },
  {
    "id": "1257",
    "name": "comment_hashtags.py",
    "path": "03_automation_platforms/api_integrations/comment_hashtags.py",
    "category": "03_automation_platforms",
    "type": "organization",
    "lines": 34,
    "size": 793,
    "docstring": "instabot example\n\nDependencies:\n    You must have a file with comments to post.\n    The file should have one comment per line.\n\nNotes:\n    You can change file and add there your comments.",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "os",
      "sys",
      "instabot"
    ],
    "preview": "\"\"\"\ninstabot example\n\nDependencies:\n    You must have a file with comments to post.\n    The file should have one comment per line.\n\nNotes:\n    You can change file and add there your comments.\n\"\"\"\n\nimport os\nimport sys\n\nsys.path.append(os.path.join(sys.path[0], \"../../\"))\nfrom instabot import Bot  # noqa: E402\n\nif len(sys.argv) < 3:\n    print(\"USAGE: Pass a path to the file with comments \" \"and a hashtag to comment\")\n    print(\"Example: %s comments_emoji.txt dog cat\" % sys.argv[0])",
    "last_modified": "2025-03-28T18:35:47"
  },
  {
    "id": "1258",
    "name": "get_hashtags_from_keywords.py",
    "path": "03_automation_platforms/api_integrations/get_hashtags_from_keywords.py",
    "category": "03_automation_platforms",
    "type": "utility",
    "lines": 68,
    "size": 1591,
    "docstring": "",
    "keywords": [],
    "functions": [
      "dispositions"
    ],
    "classes": [],
    "imports": [
      "__future__",
      "os",
      "sys",
      "tqdm",
      "instabot",
      "itertools"
    ],
    "preview": "from __future__ import unicode_literals\n\nimport os\nimport sys\n\nfrom tqdm import tqdm\n\nsys.path.append(os.path.join(sys.path[0], \"../\"))\nfrom instabot import Bot  # noqa: E402\n\nif len(sys.argv) < 2:\n    print(\n        \"Please provide keywords separated by space\\n\"\n        \"Usage:\\n\"\n        \"python get_hashtags_from_keywords.py keyword1 keyword2 etc\"\n    )\n    exit()\n\nbot = Bot()\nbot.login()",
    "last_modified": "2025-03-28T18:35:47"
  },
  {
    "id": "1259",
    "name": "mistral_ai_api_quickstart.py",
    "path": "03_automation_platforms/api_integrations/mistral_ai_api_quickstart.py",
    "category": "03_automation_platforms",
    "type": "web_tools",
    "lines": 38,
    "size": 1038,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "mistralai"
    ],
    "preview": "# -*- coding: utf-8 -*-\n\"\"\"Mistral AI API quickstart.ipynb\n\nAutomatically generated by Colab.\n\nOriginal file is located at\n    https://colab.research.google.com/drive/1lh2Uc6h2BRkSVXhzyewGHRjhXWvkz9Rq\n\n# Getting started with Mistral AI API\n\"\"\"\n\n! pip install mistralai\n\n\"\"\"Our API is currently available through [La Plateforme](https://console.mistral.ai/). You need to activate payments on your account to enable your API keys. After a few moments, you will be able to use our `chat` endpoint:\"\"\"\n\nfrom mistralai import Mistral\napi_key = \"z7opEhiZF9aPgHsKCgMW9G3axhFq5qiO\"\nmodel = \"mistral-large-latest\"\n\nclient = Mistral(api_key=api_key)",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1260",
    "name": "unarchive_your_medias.py",
    "path": "03_automation_platforms/api_integrations/unarchive_your_medias.py",
    "category": "03_automation_platforms",
    "type": "utility",
    "lines": 19,
    "size": 269,
    "docstring": "instabot example\n\nWorkflow:\n1) Unarchives your last medias",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "os",
      "sys",
      "instabot"
    ],
    "preview": "\"\"\"\ninstabot example\n\nWorkflow:\n1) Unarchives your last medias\n\n\"\"\"\n\nimport os\nimport sys\n\nsys.path.append(os.path.join(sys.path[0], \"../\"))\nfrom instabot import Bot  # noqa: E402\n\nbot = Bot()\nbot.login()\nmedias = bot.get_archived_medias()\nbot.unarchive_medias(medias)\n",
    "last_modified": "2025-03-28T18:35:47"
  },
  {
    "id": "1261",
    "name": "ultimate.py",
    "path": "03_automation_platforms/api_integrations/ultimate.py",
    "category": "03_automation_platforms",
    "type": "analysis",
    "lines": 44,
    "size": 1383,
    "docstring": "ULTIMATE SCRIPT\n\nIt uses data written in files:\n    * follow_followers.txt\n    * follow_following.txt\n    * like_hashtags.txt\n    * like_users.txt\nand do the job. This bot can be run 24/7.",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "os",
      "sys",
      "instabot"
    ],
    "preview": "\"\"\"\nULTIMATE SCRIPT\n\nIt uses data written in files:\n    * follow_followers.txt\n    * follow_following.txt\n    * like_hashtags.txt\n    * like_users.txt\nand do the job. This bot can be run 24/7.\n\"\"\"\n\nimport os\nimport sys\n\nsys.path.append(os.path.join(sys.path[0], \"../../\"))\nfrom instabot import Bot  # noqa: E402\n\nbot = Bot()\nbot.login()\n",
    "last_modified": "2025-03-28T18:35:47"
  },
  {
    "id": "1262",
    "name": "botInstagram.py",
    "path": "03_automation_platforms/api_integrations/botInstagram.py",
    "category": "03_automation_platforms",
    "type": "web_tools",
    "lines": 53,
    "size": 1198,
    "docstring": "Created in 07/2020\n@Author: Paulo https://github.com/alpdias",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "os",
      "platform",
      "art",
      "botComment",
      "botDraw",
      "botLike",
      "botStories"
    ],
    "preview": "# -*- coding: utf-8 -*-\n\n\"\"\"\nCreated in 07/2020\n@Author: Paulo https://github.com/alpdias\n\"\"\"\n\n# imported libraries\nimport os\nimport platform\n\nimport art\nimport botComment\nimport botDraw\nimport botLike\nimport botStories\n\nmySystem = platform.system()  # which operating system is running\n\nwhile True:",
    "last_modified": "2025-09-13T05:54:06.948910"
  },
  {
    "id": "1263",
    "name": "animation.py",
    "path": "03_automation_platforms/api_integrations/animation.py",
    "category": "03_automation_platforms",
    "type": "utility",
    "lines": 103,
    "size": 4311,
    "docstring": "",
    "keywords": [],
    "functions": [
      "load_animation",
      "animation_bar",
      "starting_bot",
      "colorText"
    ],
    "classes": [],
    "imports": [
      "os",
      "sys",
      "time"
    ],
    "preview": "import os  # line:3\nimport sys  # line:2\nimport time  # line:1\n\n\ndef load_animation():  # line:6\n    OOO0OOOO0O00O0OOO = \"starting your console application...\"  # line:7\n    OOO0O0000O0OOO00O = len(OOO0OOOO0O00O0OOO)  # line:8\n    O0OO0OO00O0OOOOO0 = \"|/-\\\\\"  # line:10\n    O00OO00000O000O0O = 0  # line:11\n    OOO0O0O00OOO0OOO0 = 0  # line:12\n    OO0OOO00O00OO00O0 = 0  # line:13\n    while OOO0O0O00OOO0OOO0 != 100:  # line:15\n        time.sleep(0.075)  # line:17\n        O0O000O0O0O0O0OOO = list(OOO0OOOO0O00O0OOO)  # line:18\n        OOOOO00O0OO00OOO0 = ord(O0O000O0O0O0O0OOO[OO0OOO00O00OO00O0])  # line:19\n        OOOO0O00O00O0000O = 0  # line:20\n        if OOOOO00O0OO00OOO0 != 32 and OOOOO00O0OO00OOO0 != 46:  # line:21\n            if OOOOO00O0OO00OOO0 > 90:  # line:22\n                OOOO0O00O00O0000O = OOOOO00O0OO00OOO0 - 32  # line:23",
    "last_modified": "2025-09-13T05:53:30.538659"
  },
  {
    "id": "1264",
    "name": "filter_private_profiles.py",
    "path": "03_automation_platforms/api_integrations/filter_private_profiles.py",
    "category": "03_automation_platforms",
    "type": "organization",
    "lines": 19,
    "size": 447,
    "docstring": "instabot filtering private users example\n\nWorkflow:\n    Try to follow a private user with the bot and see how it\n    filters that user out.",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "os",
      "sys",
      "instabot"
    ],
    "preview": "\"\"\"\ninstabot filtering private users example\n\nWorkflow:\n    Try to follow a private user with the bot and see how it\n    filters that user out.\n\"\"\"\n\nimport os\nimport sys\n\nsys.path.append(os.path.join(sys.path[0], \"../\"))\nfrom instabot import Bot  # noqa: E402\n\nbot = Bot(filter_users=True, filter_private_users=True)\nbot.login()\nprivate_user_input = input(\"\\n Enter a private user: \")\nbot.follow(bot.get_user_id_from_username(private_user_input))\n",
    "last_modified": "2025-05-04T23:28:20"
  },
  {
    "id": "1265",
    "name": "delete_sagemaker_endpoint.py",
    "path": "03_automation_platforms/api_integrations/delete_sagemaker_endpoint.py",
    "category": "03_automation_platforms",
    "type": "utility",
    "lines": 77,
    "size": 2403,
    "docstring": "",
    "keywords": [],
    "functions": [
      "delete_endpoint_and_config"
    ],
    "classes": [],
    "imports": [
      "loguru",
      "llm_engineering.settings",
      "boto3",
      "botocore.exceptions"
    ],
    "preview": "from loguru import logger\n\ntry:\n    import boto3\n    from botocore.exceptions import ClientError\nexcept ModuleNotFoundError:\n    logger.warning(\n        \"Couldn't load AWS or SageMaker imports. Run 'poetry install --with aws' to support AWS.\"\n    )\n\n\nfrom llm_engineering.settings import settings\n\n\ndef delete_endpoint_and_config(endpoint_name) -> None:\n    \"\"\"\n    Deletes an AWS SageMaker endpoint and its associated configuration.\n    Args:\n    endpoint_name (str): The name of the SageMaker endpoint to delete.\n    Returns:",
    "last_modified": "2025-09-13T05:53:41.894767"
  },
  {
    "id": "1266",
    "name": "unfollow_everyone.py",
    "path": "03_automation_platforms/api_integrations/unfollow_everyone.py",
    "category": "03_automation_platforms",
    "type": "utility",
    "lines": 24,
    "size": 541,
    "docstring": "instabot example\n\nWorkflow:\n    1) unfollows every from your account.",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "argparse",
      "os",
      "sys",
      "instabot"
    ],
    "preview": "\"\"\"\ninstabot example\n\nWorkflow:\n    1) unfollows every from your account.\n\"\"\"\n\nimport argparse\nimport os\nimport sys\n\nsys.path.append(os.path.join(sys.path[0], \"../\"))\nfrom instabot import Bot  # noqa: E402\n\nparser = argparse.ArgumentParser(add_help=True)\nparser.add_argument(\"-u\", type=str, help=\"username\")\nparser.add_argument(\"-p\", type=str, help=\"password\")\nparser.add_argument(\"-proxy\", type=str, help=\"proxy\")\nargs = parser.parse_args()\n",
    "last_modified": "2025-05-04T23:28:20"
  },
  {
    "id": "1267",
    "name": "awsqueue.py",
    "path": "03_automation_platforms/api_integrations/awsqueue.py",
    "category": "03_automation_platforms",
    "type": "utility",
    "lines": 36,
    "size": 1046,
    "docstring": "Talks to AWS",
    "keywords": [],
    "functions": [
      "__init__",
      "get_messages",
      "send_message"
    ],
    "classes": [
      "Awsqueue"
    ],
    "imports": [
      "json",
      "logging",
      "boto3"
    ],
    "preview": "\"Talks to AWS\"\n\nimport json\nimport logging\n\nimport boto3\n\n\nclass Awsqueue(object):\n    \"Calling an aws queue\"\n\n    def __init__(self, queue_url):\n        self.queue_url = queue_url\n        self.logger = logging.getLogger(__name__)\n\n    def get_messages(self):\n        \"Gets the next few messages\"\n        self.logger.info(\"Loading messages\")\n        try:\n            sqs = boto3.resource(\"sqs\")",
    "last_modified": "2025-05-04T23:27:55"
  },
  {
    "id": "1268",
    "name": "follow_user_followers.py",
    "path": "03_automation_platforms/api_integrations/follow_user_followers.py",
    "category": "03_automation_platforms",
    "type": "utility",
    "lines": 33,
    "size": 807,
    "docstring": "instabot example\n\nWorkflow:\n    Follow user's followers by username.",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "argparse",
      "os",
      "sys",
      "instabot"
    ],
    "preview": "\"\"\"\ninstabot example\n\nWorkflow:\n    Follow user's followers by username.\n\"\"\"\n\nimport argparse\nimport os\nimport sys\n\nsys.path.append(os.path.join(sys.path[0], \"../\"))\nfrom instabot import Bot  # noqa: E402\n\nparser = argparse.ArgumentParser(add_help=True)\nparser.add_argument(\"-u\", type=str, help=\"username\")\nparser.add_argument(\"-p\", type=str, help=\"password\")\nparser.add_argument(\"-proxy\", type=str, help=\"proxy\")\nparser.add_argument(\"users\", type=str, nargs=\"+\", help=\"users\")\nargs = parser.parse_args()",
    "last_modified": "2025-03-28T18:35:47"
  },
  {
    "id": "1269",
    "name": "bot_state.py",
    "path": "03_automation_platforms/api_integrations/bot_state.py",
    "category": "03_automation_platforms",
    "type": "utility",
    "lines": 69,
    "size": 1587,
    "docstring": "",
    "keywords": [],
    "functions": [
      "__init__",
      "__repr__"
    ],
    "classes": [
      "BotState"
    ],
    "imports": [
      "datetime",
      "instabot.singleton"
    ],
    "preview": "import datetime\n\nfrom instabot.singleton import Singleton\n\n\nclass BotState(object):\n    __metaclass__ = Singleton\n\n    def __init__(self):\n        self.start_time = datetime.datetime.now()\n        self.total = dict.fromkeys(\n            [\n                \"likes\",\n                \"unlikes\",\n                \"follows\",\n                \"unfollows\",\n                \"comments\",\n                \"blocks\",\n                \"unblocks\",\n                \"messages\",",
    "last_modified": "2025-03-28T18:35:47"
  },
  {
    "id": "1270",
    "name": "bot.py",
    "path": "03_automation_platforms/api_integrations/bot.py",
    "category": "03_automation_platforms",
    "type": "utility",
    "lines": 35,
    "size": 840,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "logging",
      "os",
      "pyrogram",
      "sample_config",
      "config"
    ],
    "preview": "import logging\n\nlogging.basicConfig(\n    level=logging.DEBUG, format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\n)\nlogger = logging.getLogger(__name__)\n\nimport os\n\n# the secret configuration specific things\nif bool(os.environ.get(\"WEBHOOK\", False)):\n    from sample_config import Config\nelse:\n    from config import Config\n\nimport pyrogram\n\nlogging.getLogger(\"pyrogram\").setLevel(logging.WARNING)\n\n",
    "last_modified": "2025-05-06T04:35:15.015944"
  },
  {
    "id": "1271",
    "name": "like_and_follow_last_user_media_likers.py",
    "path": "03_automation_platforms/api_integrations/like_and_follow_last_user_media_likers.py",
    "category": "03_automation_platforms",
    "type": "utility",
    "lines": 34,
    "size": 896,
    "docstring": "instabot example\n\nWorkflow:\n    Like and follow users who liked the last media of input users.",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "argparse",
      "os",
      "sys",
      "tqdm",
      "instabot"
    ],
    "preview": "\"\"\"\ninstabot example\n\nWorkflow:\n    Like and follow users who liked the last media of input users.\n\"\"\"\n\nimport argparse\nimport os\nimport sys\n\nfrom tqdm import tqdm\n\nsys.path.append(os.path.join(sys.path[0], \"../\"))\nfrom instabot import Bot  # noqa: E402\n\nparser = argparse.ArgumentParser(add_help=True)\nparser.add_argument(\"-u\", type=str, help=\"username\")\nparser.add_argument(\"-p\", type=str, help=\"password\")\nparser.add_argument(\"-proxy\", type=str, help=\"proxy\")",
    "last_modified": "2025-03-28T18:35:47"
  },
  {
    "id": "1272",
    "name": "reply_to_media_comments.py",
    "path": "03_automation_platforms/api_integrations/reply_to_media_comments.py",
    "category": "03_automation_platforms",
    "type": "utility",
    "lines": 92,
    "size": 3008,
    "docstring": "instabot example\n\nWorkflow:\n    If media is commented, reply to comments\n    if you didn't reply yet to that user.",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "__future__",
      "argparse",
      "os",
      "sys",
      "tqdm",
      "instabot"
    ],
    "preview": "\"\"\"\ninstabot example\n\nWorkflow:\n    If media is commented, reply to comments\n    if you didn't reply yet to that user.\n\"\"\"\n\nfrom __future__ import unicode_literals\n\nimport argparse\nimport os\nimport sys\n\nfrom tqdm import tqdm\n\nsys.path.append(os.path.join(sys.path[0], \"../../\"))\nfrom instabot import Bot  # noqa: E402\n\nparser = argparse.ArgumentParser(add_help=True)",
    "last_modified": "2025-09-13T05:54:54.980397"
  },
  {
    "id": "1273",
    "name": "like_user_followers.py",
    "path": "03_automation_platforms/api_integrations/like_user_followers.py",
    "category": "03_automation_platforms",
    "type": "utility",
    "lines": 27,
    "size": 657,
    "docstring": "instabot example\n\nWorkflow:\n    Like user's, follower's media by user_id.",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "argparse",
      "os",
      "sys",
      "instabot"
    ],
    "preview": "\"\"\"\ninstabot example\n\nWorkflow:\n    Like user's, follower's media by user_id.\n\"\"\"\n\nimport argparse\nimport os\nimport sys\n\nsys.path.append(os.path.join(sys.path[0], \"../\"))\nfrom instabot import Bot  # noqa: E402\n\nparser = argparse.ArgumentParser(add_help=True)\nparser.add_argument(\"-u\", type=str, help=\"username\")\nparser.add_argument(\"-p\", type=str, help=\"password\")\nparser.add_argument(\"-proxy\", type=str, help=\"proxy\")\nparser.add_argument(\"users\", type=str, nargs=\"+\", help=\"users\")\nargs = parser.parse_args()",
    "last_modified": "2025-03-28T18:35:47"
  },
  {
    "id": "1274",
    "name": "build_site.py",
    "path": "03_automation_platforms/api_integrations/build_site.py",
    "category": "03_automation_platforms",
    "type": "utility",
    "lines": 88,
    "size": 2772,
    "docstring": "",
    "keywords": [],
    "functions": [
      "load_index",
      "build_html",
      "main"
    ],
    "classes": [],
    "imports": [
      "__future__",
      "json",
      "datetime",
      "pathlib",
      "typing"
    ],
    "preview": "from __future__ import annotations\n\nimport json\nfrom datetime import datetime\nfrom pathlib import Path\nfrom typing import List, Dict, Any\n\nROOT = Path(__file__).resolve().parents[1]\n\n\ndef load_index() -> List[Dict[str, Any]]:\n    data = json.loads((ROOT / \"python_index.json\").read_text(encoding=\"utf-8\"))\n    # sort by project then name\n    data.sort(key=lambda d: (d.get(\"project\", \"\"), d.get(\"path\", \"\")))\n    return data\n\n\ndef build_html(items: List[Dict[str, Any]]) -> str:\n    ts = datetime.now().strftime(\"%Y-%m-%d %H:%M\")\n    rows = []",
    "last_modified": "2025-09-13T06:01:14.790179"
  },
  {
    "id": "1275",
    "name": "collect_stats.py",
    "path": "03_automation_platforms/api_integrations/collect_stats.py",
    "category": "03_automation_platforms",
    "type": "organization",
    "lines": 32,
    "size": 789,
    "docstring": "instabot example\n\nCollects the information about your account\nevery hour in username.tsv file.",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "argparse",
      "os",
      "sys",
      "time",
      "instabot"
    ],
    "preview": "\"\"\"\ninstabot example\n\nCollects the information about your account\nevery hour in username.tsv file.\n\"\"\"\n\nimport argparse\nimport os\nimport sys\nimport time\n\nsys.path.append(os.path.join(sys.path[0], \"../\"))\nfrom instabot import Bot  # noqa: E402\n\nparser = argparse.ArgumentParser(add_help=True)\nparser.add_argument(\"-u\", type=str, help=\"username\")\nparser.add_argument(\"-p\", type=str, help=\"password\")\nparser.add_argument(\"-proxy\", type=str, help=\"proxy\")\nparser.add_argument(\"user\", type=str, nargs=\"*\", help=\"user\")",
    "last_modified": "2025-03-28T18:35:47"
  },
  {
    "id": "1276",
    "name": "welcome_message.py",
    "path": "03_automation_platforms/api_integrations/welcome_message.py",
    "category": "03_automation_platforms",
    "type": "utility",
    "lines": 82,
    "size": 2289,
    "docstring": "instabot example\nWorkflow:\nWelcome message for new followers.",
    "keywords": [],
    "functions": [
      "get_recent_followers",
      "main"
    ],
    "classes": [],
    "imports": [
      "argparse",
      "datetime",
      "os",
      "sys",
      "time",
      "instabot"
    ],
    "preview": "\"\"\"\ninstabot example\nWorkflow:\nWelcome message for new followers.\n\"\"\"\n\nimport argparse\nimport datetime\nimport os\nimport sys\nimport time\n\nsys.path.append(os.path.join(sys.path[0], \"../\"))\nfrom instabot import Bot  # noqa: E402\n\nRETRY_DELAY = 60\nDELAY = 30 * 60\n\n\ndef get_recent_followers(bot, from_time):",
    "last_modified": "2025-09-13T05:54:56.030467"
  },
  {
    "id": "1277",
    "name": "infinity_hashtags_follower.py",
    "path": "03_automation_platforms/api_integrations/infinity_hashtags_follower.py",
    "category": "03_automation_platforms",
    "type": "setup",
    "lines": 33,
    "size": 780,
    "docstring": "instabot example\n\nWorkflow:\n    Follow users who post medias with hashtag.",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "argparse",
      "os",
      "sys",
      "time",
      "instabot"
    ],
    "preview": "\"\"\"\ninstabot example\n\nWorkflow:\n    Follow users who post medias with hashtag.\n\"\"\"\n\nimport argparse\nimport os\nimport sys\nimport time\n\nsys.path.append(os.path.join(sys.path[0], \"../\"))\nfrom instabot import Bot  # noqa: E402\n\nparser = argparse.ArgumentParser(add_help=True)\nparser.add_argument(\"-u\", type=str, help=\"username\")\nparser.add_argument(\"-p\", type=str, help=\"password\")\nparser.add_argument(\"-proxy\", type=str, help=\"proxy\")\nparser.add_argument(\"hashtags\", type=str, nargs=\"+\", help=\"hashtags\")",
    "last_modified": "2025-03-28T18:35:47"
  },
  {
    "id": "1278",
    "name": "follow_users_by_hashtag.py",
    "path": "03_automation_platforms/api_integrations/follow_users_by_hashtag.py",
    "category": "03_automation_platforms",
    "type": "utility",
    "lines": 28,
    "size": 694,
    "docstring": "instabot example\n\nWorkflow:\n    Follow users who post medias with hashtag.",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "argparse",
      "os",
      "sys",
      "instabot"
    ],
    "preview": "\"\"\"\ninstabot example\n\nWorkflow:\n    Follow users who post medias with hashtag.\n\"\"\"\n\nimport argparse\nimport os\nimport sys\n\nsys.path.append(os.path.join(sys.path[0], \"../\"))\nfrom instabot import Bot  # noqa: E402\n\nparser = argparse.ArgumentParser(add_help=True)\nparser.add_argument(\"-u\", type=str, help=\"username\")\nparser.add_argument(\"-p\", type=str, help=\"password\")\nparser.add_argument(\"-proxy\", type=str, help=\"proxy\")\nparser.add_argument(\"hashtags\", type=str, nargs=\"+\", help=\"hashtags\")\nargs = parser.parse_args()",
    "last_modified": "2025-03-28T18:35:47"
  },
  {
    "id": "1279",
    "name": "black_white_lists.py",
    "path": "03_automation_platforms/api_integrations/black_white_lists.py",
    "category": "03_automation_platforms",
    "type": "organization",
    "lines": 38,
    "size": 969,
    "docstring": "instabot example\n\nWorkflow:\n    1) Reads user_ids from blacklist and whitelist\n    2) likes several last medias by users in your timeline\n\nNotes:\n    blacklist and whitelist files should contain user_ids - each one on the\n    separate line.\n    Example:\n        1234125\n        1234124512",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "argparse",
      "os",
      "sys",
      "tqdm",
      "instabot"
    ],
    "preview": "\"\"\"\ninstabot example\n\nWorkflow:\n    1) Reads user_ids from blacklist and whitelist\n    2) likes several last medias by users in your timeline\n\nNotes:\n    blacklist and whitelist files should contain user_ids - each one on the\n    separate line.\n    Example:\n        1234125\n        1234124512\n\"\"\"\n\nimport argparse\nimport os\nimport sys\n\nfrom tqdm import tqdm",
    "last_modified": "2025-03-28T18:35:47"
  },
  {
    "id": "1280",
    "name": "extract_streams.py",
    "path": "03_automation_platforms/api_integrations/extract_streams.py",
    "category": "03_automation_platforms",
    "type": "utility",
    "lines": 76,
    "size": 2493,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "logging",
      "asyncio",
      "os",
      "time",
      "pyrogram",
      "translation",
      "helper_funcs.chat_base",
      "helper_funcs.display_progress",
      "sample_config",
      "config"
    ],
    "preview": "import logging\n\nlogging.basicConfig(\n    level=logging.DEBUG, format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\n)\nlogger = logging.getLogger(__name__)\n\nimport asyncio\nimport os\nimport time\n\n# the secret configuration specific things\nif bool(os.environ.get(\"WEBHOOK\", False)):\n    from sample_config import Config\nelse:\n    from config import Config\n\n# the Strings used for this \"thing\"\nimport pyrogram\nfrom translation import Translation",
    "last_modified": "2025-05-06T04:35:15.023095"
  },
  {
    "id": "1281",
    "name": "unfollow_non_followers.py",
    "path": "03_automation_platforms/api_integrations/unfollow_non_followers.py",
    "category": "03_automation_platforms",
    "type": "utility",
    "lines": 24,
    "size": 550,
    "docstring": "instabot example\n\nWorkflow:\n    1) unfollows users that don't follow you.",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "argparse",
      "os",
      "sys",
      "instabot"
    ],
    "preview": "\"\"\"\ninstabot example\n\nWorkflow:\n    1) unfollows users that don't follow you.\n\"\"\"\n\nimport argparse\nimport os\nimport sys\n\nsys.path.append(os.path.join(sys.path[0], \"../\"))\nfrom instabot import Bot  # noqa: E402\n\nparser = argparse.ArgumentParser(add_help=True)\nparser.add_argument(\"-u\", type=str, help=\"username\")\nparser.add_argument(\"-p\", type=str, help=\"password\")\nparser.add_argument(\"-proxy\", type=str, help=\"proxy\")\nargs = parser.parse_args()\n",
    "last_modified": "2025-05-04T23:28:20"
  },
  {
    "id": "1282",
    "name": "follow_last_user_media_likers.py",
    "path": "03_automation_platforms/api_integrations/follow_last_user_media_likers.py",
    "category": "03_automation_platforms",
    "type": "utility",
    "lines": 33,
    "size": 878,
    "docstring": "instabot example\n\nWorkflow:\n    Follow users who liked the last media of input users.",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "argparse",
      "os",
      "sys",
      "tqdm",
      "instabot"
    ],
    "preview": "\"\"\"\ninstabot example\n\nWorkflow:\n    Follow users who liked the last media of input users.\n\"\"\"\n\nimport argparse\nimport os\nimport sys\n\nfrom tqdm import tqdm\n\nsys.path.append(os.path.join(sys.path[0], \"../\"))\nfrom instabot import Bot  # noqa: E402\n\nparser = argparse.ArgumentParser(add_help=True)\nparser.add_argument(\"-u\", type=str, help=\"username\")\nparser.add_argument(\"-p\", type=str, help=\"password\")\nparser.add_argument(\"-proxy\", type=str, help=\"proxy\")",
    "last_modified": "2025-03-28T18:35:47"
  },
  {
    "id": "1283",
    "name": "follow_user_following.py",
    "path": "03_automation_platforms/api_integrations/follow_user_following.py",
    "category": "03_automation_platforms",
    "type": "utility",
    "lines": 27,
    "size": 644,
    "docstring": "instabot example\n\nWorkflow:\n    Follow user's following by username.",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "argparse",
      "os",
      "sys",
      "instabot"
    ],
    "preview": "\"\"\"\ninstabot example\n\nWorkflow:\n    Follow user's following by username.\n\"\"\"\n\nimport argparse\nimport os\nimport sys\n\nsys.path.append(os.path.join(sys.path[0], \"../\"))\nfrom instabot import Bot  # noqa: E402\n\nparser = argparse.ArgumentParser(add_help=True)\nparser.add_argument(\"-u\", type=str, help=\"username\")\nparser.add_argument(\"-p\", type=str, help=\"password\")\nparser.add_argument(\"-proxy\", type=str, help=\"proxy\")\nparser.add_argument(\"users\", type=str, nargs=\"+\", help=\"users\")\nargs = parser.parse_args()",
    "last_modified": "2025-03-28T18:35:47"
  },
  {
    "id": "1284",
    "name": "like_users_from_file.py",
    "path": "03_automation_platforms/api_integrations/like_users_from_file.py",
    "category": "03_automation_platforms",
    "type": "organization",
    "lines": 34,
    "size": 889,
    "docstring": "instabot example\n\nWorkflow:\n    Take users from input file and like them.\n    The file should contain one username per line!",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "argparse",
      "os",
      "sys",
      "instabot"
    ],
    "preview": "\"\"\"\ninstabot example\n\nWorkflow:\n    Take users from input file and like them.\n    The file should contain one username per line!\n\"\"\"\n\nimport argparse\nimport os\nimport sys\n\nsys.path.append(os.path.join(sys.path[0], \"../\"))\nfrom instabot import Bot  # noqa: E402\n\nparser = argparse.ArgumentParser(add_help=True)\nparser.add_argument(\"-u\", type=str, help=\"username\")\nparser.add_argument(\"-p\", type=str, help=\"password\")\nparser.add_argument(\"-proxy\", type=str, help=\"proxy\")\nparser.add_argument(\"filepath\", type=str, help=\"filepath\")",
    "last_modified": "2025-03-28T18:35:47"
  },
  {
    "id": "1285",
    "name": "filter_blacklist_hashtag_medias.py",
    "path": "03_automation_platforms/api_integrations/filter_blacklist_hashtag_medias.py",
    "category": "03_automation_platforms",
    "type": "utility",
    "lines": 27,
    "size": 660,
    "docstring": "instabot filters out the media with your set blacklist hashtags\n\nWorkflow:\n    Try to follow a media with your blacklist hashtag in the\n    description and see how bot filters it out.",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "os",
      "sys",
      "instabot"
    ],
    "preview": "\"\"\"\ninstabot filters out the media with your set blacklist hashtags\n\nWorkflow:\n    Try to follow a media with your blacklist hashtag in the\n    description and see how bot filters it out.\n\"\"\"\n\nimport os\nimport sys\n\nsys.path.append(os.path.join(sys.path[0], \"../\"))\nfrom instabot import Bot  # noqa: E402\n\nblacklist_hashtag_input = input(\"\\n Enter a blacklist hashtag: \")\n\nbot = Bot(\n    filter_users=True,\n    filter_private_users=True,\n    filter_previously_followed=True,",
    "last_modified": "2025-03-28T18:35:47"
  },
  {
    "id": "1286",
    "name": "download_stories.py",
    "path": "03_automation_platforms/api_integrations/download_stories.py",
    "category": "03_automation_platforms",
    "type": "youtube",
    "lines": 19,
    "size": 553,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "argparse",
      "os",
      "sys",
      "instabot"
    ],
    "preview": "import argparse\nimport os\nimport sys\n\nsys.path.append(os.path.join(sys.path[0], \"../\"))\nfrom instabot import Bot  # noqa: E402\n\nparser = argparse.ArgumentParser(add_help=True)\nparser.add_argument(\"-u\", type=str, help=\"username\")\nparser.add_argument(\"-p\", type=str, help=\"password\")\nparser.add_argument(\"-proxy\", type=str, help=\"proxy\")\nparser.add_argument(\"-story_username\", type=str, help=\"story_username\")\nargs = parser.parse_args()\n\nbot = Bot()\nbot.login(username=args.u, password=args.p, proxy=args.proxy)\n\nbot.download_stories(args.story_username)\n",
    "last_modified": "2025-03-28T18:35:47"
  },
  {
    "id": "1287",
    "name": "delete_all_posts.py",
    "path": "03_automation_platforms/api_integrations/delete_all_posts.py",
    "category": "03_automation_platforms",
    "type": "organization",
    "lines": 25,
    "size": 582,
    "docstring": "instabot example\n\nWorkflow:\n    delete all posts in profile.",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "argparse",
      "os",
      "sys",
      "instabot"
    ],
    "preview": "\"\"\"\ninstabot example\n\nWorkflow:\n    delete all posts in profile.\n\"\"\"\n\nimport argparse\nimport os\nimport sys\n\nsys.path.append(os.path.join(sys.path[0], \"../\"))\nfrom instabot import Bot  # noqa: E402\n\nparser = argparse.ArgumentParser(add_help=True)\nparser.add_argument(\"-u\", type=str, help=\"username\")\nparser.add_argument(\"-p\", type=str, help=\"password\")\nparser.add_argument(\"-proxy\", type=str, help=\"proxy\")\nargs = parser.parse_args()\n",
    "last_modified": "2025-03-28T18:35:47"
  },
  {
    "id": "1288",
    "name": "brand.py",
    "path": "03_automation_platforms/api_integrations/brand.py",
    "category": "03_automation_platforms",
    "type": "utility",
    "lines": 26,
    "size": 689,
    "docstring": "",
    "keywords": [],
    "functions": [
      "load"
    ],
    "classes": [
      "BrandTemplate"
    ],
    "imports": [
      "__future__",
      "dataclasses",
      "json",
      "typing"
    ],
    "preview": "from __future__ import annotations\n\nimport dataclasses\nimport json\nfrom typing import Any, Dict\n\n\n@dataclasses.dataclass\nclass BrandTemplate:\n    font: str = \"Arial\"\n    caption_case: str = \"sentence\"  # sentence|upper|lower|title\n    color: str = \"#FFFFFF\"\n    stroke_color: str = \"#000000\"\n    stroke_width: int = 2\n    position: str = \"bottom\"  # top|bottom\n    margin_px: int = 40\n    safe_area_pct: float = 0.08\n\n    @staticmethod\n    def load(path: str) -> \"BrandTemplate\":",
    "last_modified": "2025-09-11T13:27:06.809466"
  },
  {
    "id": "1289",
    "name": "hashes.py",
    "path": "03_automation_platforms/api_integrations/hashes.py",
    "category": "03_automation_platforms",
    "type": "utility",
    "lines": 152,
    "size": 5118,
    "docstring": "",
    "keywords": [],
    "functions": [
      "__init__",
      "__and__",
      "digest_count",
      "is_hash_allowed",
      "check_against_chunks",
      "_raise",
      "check_against_file",
      "check_against_path",
      "has_one_of",
      "__bool__"
    ],
    "classes": [
      "Hashes",
      "MissingHashes"
    ],
    "imports": [
      "hashlib",
      "typing",
      "pip._internal.exceptions",
      "pip._internal.utils.misc",
      "hashlib",
      "typing"
    ],
    "preview": "import hashlib\nfrom typing import TYPE_CHECKING, BinaryIO, Dict, Iterable, List, Optional\n\nfrom pip._internal.exceptions import HashMismatch, HashMissing, InstallationError\nfrom pip._internal.utils.misc import read_chunks\n\nif TYPE_CHECKING:\n    from hashlib import _Hash\n\n    # NoReturn introduced in 3.6.2; imported only for type checking to maintain\n    # pip compatibility with older patch versions of Python 3.6\n    from typing import NoReturn\n\n\n# The recommended hash algo of the moment. Change this whenever the state of\n# the art changes; it won't hurt backward compatibility.\nFAVORITE_HASH = \"sha256\"\n\n\n# Names of hashlib algorithms allowed by the --hash option and ``pip hash``",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1290",
    "name": "unlike_users.py",
    "path": "03_automation_platforms/api_integrations/unlike_users.py",
    "category": "03_automation_platforms",
    "type": "utility",
    "lines": 27,
    "size": 632,
    "docstring": "instabot example\n\nWorkflow:\n    Unlike last media's of users.",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "argparse",
      "os",
      "sys",
      "instabot"
    ],
    "preview": "\"\"\"\ninstabot example\n\nWorkflow:\n    Unlike last media's of users.\n\"\"\"\n\nimport argparse\nimport os\nimport sys\n\nsys.path.append(os.path.join(sys.path[0], \"../\"))\nfrom instabot import Bot  # noqa: E402\n\nparser = argparse.ArgumentParser(add_help=True)\nparser.add_argument(\"-u\", type=str, help=\"username\")\nparser.add_argument(\"-p\", type=str, help=\"password\")\nparser.add_argument(\"-proxy\", type=str, help=\"proxy\")\nparser.add_argument(\"users\", type=str, nargs=\"+\", help=\"users\")\nargs = parser.parse_args()",
    "last_modified": "2025-03-28T18:35:47"
  },
  {
    "id": "1291",
    "name": "like_location_feed.py",
    "path": "03_automation_platforms/api_integrations/like_location_feed.py",
    "category": "03_automation_platforms",
    "type": "utility",
    "lines": 20,
    "size": 613,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "argparse",
      "os",
      "sys",
      "instabot"
    ],
    "preview": "import argparse\nimport os\nimport sys\n\nsys.path.append(os.path.join(sys.path[0], \"../\"))\nfrom instabot import Bot  # noqa: E402\n\nparser = argparse.ArgumentParser(add_help=True)\nparser.add_argument(\"-u\", type=str, help=\"username\")\nparser.add_argument(\"-p\", type=str, help=\"password\")\nparser.add_argument(\"-proxy\", type=str, help=\"proxy\")\nparser.add_argument(\"-location\", type=str, help=\"location\")\nparser.add_argument(\"-amount\", type=str, help=\"amount\")\nargs = parser.parse_args()\n\nbot = Bot()\nbot.login(username=args.u, password=args.p, proxy=args.proxy)\n\nbot.like_location_feed(args.location, amount=args.amount)\n",
    "last_modified": "2025-03-28T18:35:47"
  },
  {
    "id": "1292",
    "name": "like_user_following.py",
    "path": "03_automation_platforms/api_integrations/like_user_following.py",
    "category": "03_automation_platforms",
    "type": "utility",
    "lines": 27,
    "size": 658,
    "docstring": "instabot example\n\nWorkflow:\n    Like user's, following's media by user_id.",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "argparse",
      "os",
      "sys",
      "instabot"
    ],
    "preview": "\"\"\"\ninstabot example\n\nWorkflow:\n    Like user's, following's media by user_id.\n\"\"\"\n\nimport argparse\nimport os\nimport sys\n\nsys.path.append(os.path.join(sys.path[0], \"../\"))\nfrom instabot import Bot  # noqa: E402\n\nparser = argparse.ArgumentParser(add_help=True)\nparser.add_argument(\"-u\", type=str, help=\"username\")\nparser.add_argument(\"-p\", type=str, help=\"password\")\nparser.add_argument(\"-proxy\", type=str, help=\"proxy\")\nparser.add_argument(\"users\", type=str, nargs=\"+\", help=\"users\")\nargs = parser.parse_args()",
    "last_modified": "2025-03-28T18:35:47"
  },
  {
    "id": "1293",
    "name": "comment_medias_by_location.py",
    "path": "03_automation_platforms/api_integrations/comment_medias_by_location.py",
    "category": "03_automation_platforms",
    "type": "utility",
    "lines": 95,
    "size": 3017,
    "docstring": "instabot example\n\nWorkflow:\n    Comment medias by location.",
    "keywords": [],
    "functions": [
      "comment_location_feed"
    ],
    "classes": [],
    "imports": [
      "argparse",
      "codecs",
      "os",
      "sys",
      "tqdm",
      "instabot"
    ],
    "preview": "# coding=utf-8\n\"\"\"\ninstabot example\n\nWorkflow:\n    Comment medias by location.\n\"\"\"\n\nimport argparse\nimport codecs\nimport os\nimport sys\n\nfrom tqdm import tqdm\n\nstdout = sys.stdout\nsys.stdout = codecs.getwriter(\"utf8\")(sys.stdout)\n\nsys.path.append(os.path.join(sys.path[0], \"../\"))\nfrom instabot import Bot  # noqa: E402",
    "last_modified": "2025-09-13T05:54:55.039536"
  },
  {
    "id": "1294",
    "name": "comment_your_feed.py",
    "path": "03_automation_platforms/api_integrations/comment_your_feed.py",
    "category": "03_automation_platforms",
    "type": "organization",
    "lines": 36,
    "size": 817,
    "docstring": "instabot example\n\nDependencies:\n    You must have a file with comments to post.\n    The file should have one comment per line.\n\nWorkflow:\n    1) Get your timeline medias\n    2) Comment them with random comments from file.\n\nNotes:\n    You can change file and add there your comments.",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "os",
      "sys",
      "instabot"
    ],
    "preview": "\"\"\"\ninstabot example\n\nDependencies:\n    You must have a file with comments to post.\n    The file should have one comment per line.\n\nWorkflow:\n    1) Get your timeline medias\n    2) Comment them with random comments from file.\n\nNotes:\n    You can change file and add there your comments.\n\"\"\"\n\nimport os\nimport sys\n\nsys.path.append(os.path.join(sys.path[0], \"../../\"))\nfrom instabot import Bot  # noqa: E402",
    "last_modified": "2025-03-28T18:35:47"
  },
  {
    "id": "1295",
    "name": "get_followers_or_followings_to_file.py",
    "path": "03_automation_platforms/api_integrations/get_followers_or_followings_to_file.py",
    "category": "03_automation_platforms",
    "type": "organization",
    "lines": 91,
    "size": 2451,
    "docstring": "instabot example\nWorkflow:\n    Get total or filtered followers or followings to file.",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "argparse",
      "os",
      "sys",
      "instabot"
    ],
    "preview": "\"\"\"\ninstabot example\nWorkflow:\n    Get total or filtered followers or followings to file.\n\"\"\"\n\nimport argparse\nimport os\nimport sys\n\nsys.path.append(os.path.join(sys.path[0], \"../\"))\nfrom instabot import Bot  # noqa: E402\n\nparser = argparse.ArgumentParser(add_help=True)\n# login arguments\nparser.add_argument(\"-u\", type=str, help=\"username\")\nparser.add_argument(\"-p\", type=str, help=\"password\")\nparser.add_argument(\"-proxy\", type=str, help=\"proxy\")\n# required arguments\nparser.add_argument(",
    "last_modified": "2025-03-28T18:35:47"
  },
  {
    "id": "1296",
    "name": "logo.py_02.py",
    "path": "03_automation_platforms/logo.py_consolidated/logo.py_02.py",
    "category": "03_automation_platforms",
    "type": "utility",
    "lines": 24,
    "size": 2236,
    "docstring": "",
    "keywords": [],
    "functions": [
      "print_logo"
    ],
    "classes": [],
    "imports": [
      "os",
      "random",
      "colorama"
    ],
    "preview": "import os\nfrom random import choice\n\nfrom colorama import Back, Fore, Style\n\nlogo = \"\"\"\n\n        \u2588\u2588\u2557 \u2588\u2588\u2588\u2588\u2588\u2588\u2557     \u2588\u2588\u2588\u2557   \u2588\u2588\u2588\u2557 \u2588\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557    \u2588\u2588\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2588\u2588\u2588\u2588\u2557  \u2588\u2588\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\n        \u2588\u2588\u2551\u2588\u2588\u2554\u2550\u2550\u2550\u2550\u255d     \u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2588\u2588\u2551\u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2557\u2588\u2588\u2554\u2550\u2550\u2550\u2550\u255d\u2588\u2588\u2554\u2550\u2550\u2550\u2550\u255d    \u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2557\u2588\u2588\u2554\u2550\u2550\u2550\u2550\u255d\u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2557\u2588\u2588\u2554\u2550\u2550\u2550\u2588\u2588\u2557\u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2557\u255a\u2550\u2550\u2588\u2588\u2554\u2550\u2550\u255d\n        \u2588\u2588\u2551\u2588\u2588\u2551  \u2588\u2588\u2588\u2557    \u2588\u2588\u2554\u2588\u2588\u2588\u2588\u2554\u2588\u2588\u2551\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2551\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557    \u2588\u2588\u2588\u2588\u2588\u2588\u2554\u255d\u2588\u2588\u2588\u2588\u2588\u2557  \u2588\u2588\u2588\u2588\u2588\u2588\u2554\u255d\u2588\u2588\u2551   \u2588\u2588\u2551\u2588\u2588\u2588\u2588\u2588\u2588\u2554\u255d   \u2588\u2588\u2551   \n        \u2588\u2588\u2551\u2588\u2588\u2551   \u2588\u2588\u2551    \u2588\u2588\u2551\u255a\u2588\u2588\u2554\u255d\u2588\u2588\u2551\u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2551\u255a\u2550\u2550\u2550\u2550\u2588\u2588\u2551\u255a\u2550\u2550\u2550\u2550\u2588\u2588\u2551    \u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2557\u2588\u2588\u2554\u2550\u2550\u255d  \u2588\u2588\u2554\u2550\u2550\u2550\u255d \u2588\u2588\u2551   \u2588\u2588\u2551\u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2557   \u2588\u2588\u2551   \n        \u2588\u2588\u2551\u255a\u2588\u2588\u2588\u2588\u2588\u2588\u2554\u255d    \u2588\u2588\u2551 \u255a\u2550\u255d \u2588\u2588\u2551\u2588\u2588\u2551  \u2588\u2588\u2551\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2551\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2551    \u2588\u2588\u2551  \u2588\u2588\u2551\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2551     \u255a\u2588\u2588\u2588\u2588\u2588\u2588\u2554\u255d\u2588\u2588\u2551  \u2588\u2588\u2551   \u2588\u2588\u2551   \n        \u255a\u2550\u255d \u255a\u2550\u2550\u2550\u2550\u2550\u255d     \u255a\u2550\u255d     \u255a\u2550\u255d\u255a\u2550\u255d  \u255a\u2550\u255d\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u255d\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u255d    \u255a\u2550\u255d  \u255a\u2550\u255d\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u255d\u255a\u2550\u255d      \u255a\u2550\u2550\u2550\u2550\u2550\u255d \u255a\u2550\u255d  \u255a\u2550\u255d   \u255a\u2550\u255d   \n                                                                                                          \"\"\".replace(\n    \"\u2588\", f\"{Fore.WHITE}\u2588{Fore.BLACK}\"\n)\n\n\ndef print_logo():\n    os.system(f\"title Instagram Mass Report Bot Made by Rdimo#6969\")",
    "last_modified": "2025-05-04T23:28:20"
  },
  {
    "id": "1297",
    "name": "logo.py.py",
    "path": "03_automation_platforms/logo.py_consolidated/logo.py.py",
    "category": "03_automation_platforms",
    "type": "utility",
    "lines": 29,
    "size": 3654,
    "docstring": "",
    "keywords": [],
    "functions": [
      "print_logo"
    ],
    "classes": [],
    "imports": [
      "libs.animation"
    ],
    "preview": "# coding=utf-8\n#!/usr/bin/env python3\n\nfrom libs.animation import colorText\n\nlogo = \"\"\"\n\n[[black-bright-background]][[red]] \u2588\u2588\u2593 [[green]]\u2588\u2588\u2588\u2584    \u2588  [[blue]] \u2588\u2588\u2588\u2588\u2588\u2588 [[magenta]]\u2584\u2584\u2584\u2588\u2588\u2588\u2588\u2588\u2593 [[cyan]]\u2584\u2584\u2584         [[red]] \u2588\u2588\u2580\u2588\u2588\u2588  [[green]]\u2593\u2588\u2588\u2588\u2588\u2588 [[blue]] \u2588\u2588\u2593\u2588\u2588\u2588   [[magenta]]\u2592\u2588\u2588\u2588\u2588\u2588   [[cyan]]\u2588\u2588\u2580\u2588\u2588\u2588  [[yellow]]\u2584\u2584\u2584\u2588\u2588\u2588\u2588\u2588\u2593[[reset]]\n[[black-bright-background]][[red]]\u2593\u2588\u2588\u2592 [[green]]\u2588\u2588 \u2580\u2588   \u2588 \u2592[[blue]]\u2588\u2588    \u2592 [[magenta]]\u2593  \u2588\u2588\u2592 \u2593\u2592\u2592[[cyan]]\u2588\u2588\u2588\u2588\u2584       [[red]]\u2593\u2588\u2588 \u2592 \u2588\u2588\u2592[[green]]\u2593\u2588   \u2580 [[blue]]\u2593\u2588\u2588\u2591  \u2588\u2588\u2592\u2592[[magenta]]\u2588\u2588\u2592  \u2588\u2588\u2592\u2593[[cyan]]\u2588\u2588 \u2592 \u2588\u2588\u2592[[yellow]]\u2593  \u2588\u2588\u2592 \u2593\u2592[[reset]]\n[[black-bright-background]][[red]]\u2592\u2588\u2588\u2592\u2593[[green]]\u2588\u2588  \u2580\u2588 \u2588\u2588\u2592\u2591[[blue]] \u2593\u2588\u2588\u2584   [[magenta]]\u2592 \u2593\u2588\u2588\u2591 \u2592\u2591\u2592[[cyan]]\u2588\u2588  \u2580\u2588\u2584     [[red]]\u2593\u2588\u2588 \u2591\u2584\u2588 \u2592[[green]]\u2592\u2588\u2588\u2588   [[blue]]\u2593\u2588\u2588\u2591 \u2588\u2588\u2593\u2592\u2592[[magenta]]\u2588\u2588\u2591  \u2588\u2588\u2592\u2593[[cyan]]\u2588\u2588 \u2591\u2584\u2588 \u2592[[yellow]]\u2592 \u2593\u2588\u2588\u2591 \u2592\u2591[[reset]]\n[[black-bright-background]][[red]]\u2591\u2588\u2588\u2591\u2593[[green]]\u2588\u2588\u2592  \u2590\u258c\u2588\u2588\u2592 [[blue]] \u2592   \u2588\u2588\u2592[[magenta]]\u2591 \u2593\u2588\u2588\u2593 \u2591 \u2591[[cyan]]\u2588\u2588\u2584\u2584\u2584\u2584\u2588\u2588    [[red]]\u2592\u2588\u2588\u2580\u2580\u2588\u2584  [[green]]\u2592\u2593\u2588  \u2584 [[blue]]\u2592\u2588\u2588\u2584\u2588\u2593\u2592 \u2592\u2592[[magenta]]\u2588\u2588   \u2588\u2588\u2591\u2592[[cyan]]\u2588\u2588\u2580\u2580\u2588\u2584  [[yellow]]\u2591 \u2593\u2588\u2588\u2593 \u2591 [[reset]]\n[[black-bright-background]][[red]]\u2591\u2588\u2588\u2591\u2592[[green]]\u2588\u2588\u2591   \u2593\u2588\u2588\u2591\u2592[[blue]]\u2588\u2588\u2588\u2588\u2588\u2588\u2592\u2592[[magenta]]  \u2592\u2588\u2588\u2592 \u2591  [[cyan]]\u2593\u2588   \u2593\u2588\u2588\u2592   [[red]]\u2591\u2588\u2588\u2593 \u2592\u2588\u2588\u2592[[green]]\u2591\u2592\u2588\u2588\u2588\u2588\u2592[[blue]]\u2592\u2588\u2588\u2592 \u2591  \u2591\u2591[[magenta]] \u2588\u2588\u2588\u2588\u2593\u2592\u2591\u2591[[cyan]]\u2588\u2588\u2593 \u2592\u2588\u2588\u2592[[yellow]]  \u2592\u2588\u2588\u2592 \u2591 [[reset]]\n[[black-bright-background]][[red]]\u2591\u2593  \u2591[[green]] \u2592\u2591   \u2592 \u2592 \u2592[[blue]] \u2592\u2593\u2592 \u2592 \u2591[[magenta]]  \u2592 \u2591\u2591    [[cyan]]\u2592\u2592   \u2593\u2592\u2588\u2591   [[red]]\u2591 \u2592\u2593 \u2591\u2592\u2593\u2591[[green]]\u2591\u2591 \u2592\u2591 \u2591[[blue]]\u2592\u2593\u2592\u2591 \u2591  \u2591\u2591[[magenta]] \u2592\u2591\u2592\u2591\u2592\u2591 \u2591[[cyan]] \u2592\u2593 \u2591\u2592\u2593\u2591[[yellow]]  \u2592 \u2591\u2591   [[reset]]\n[[black-bright-background]][[red]] \u2592 \u2591\u2591[[green]] \u2591\u2591   \u2591 \u2592\u2591\u2591[[blue]] \u2591\u2592  \u2591 \u2591[[magenta]]    \u2591     [[cyan]] \u2592   \u2592\u2592 \u2591   [[red]]  \u2591\u2592 \u2591 \u2592\u2591[[green]] \u2591 \u2591  \u2591[[blue]]\u2591\u2592 \u2591      [[magenta]] \u2591 \u2592 \u2592\u2591  [[cyan]] \u2591\u2592 \u2591 \u2592\u2591[[yellow]]    \u2591    [[reset]]\n[[black-bright-background]][[red]] \u2592 \u2591 [[green]]  \u2591   \u2591 \u2591 \u2591[[blue]]  \u2591  \u2591  [[magenta]]  \u2591       [[cyan]] \u2591   \u2592      [[red]]  \u2591\u2591   \u2591 [[green]]   \u2591   [[blue]]\u2591\u2591       \u2591[[magenta]] \u2591 \u2591 \u2592   [[cyan]] \u2591\u2591   \u2591 [[yellow]]  \u2591      [[reset]]\n[[black-bright-background]][[red]] \u2591   [[green]]        \u2591  [[blue]]     \u2591  [[magenta]]          [[cyan]]     \u2591  \u2591   [[red]]   \u2591     [[green]]   \u2591  \u2591[[blue]]          [[magenta]]   \u2591 \u2591   [[cyan]]  \u2591     [[yellow]]         [[reset]]\n                                                                                                  \n\n                                           [[black-bright-background]][[white]]Codded By Crevil[[reset]]\n                                            [[black]]Version :- 2.01[[reset]]",
    "last_modified": "2025-03-28T18:35:46"
  },
  {
    "id": "1298",
    "name": "backlinker.py",
    "path": "03_automation_platforms/web_automation/backlinker.py",
    "category": "03_automation_platforms",
    "type": "utility",
    "lines": 42,
    "size": 1394,
    "docstring": "",
    "keywords": [
      "web_tools"
    ],
    "functions": [],
    "classes": [],
    "imports": [
      "json",
      "re",
      "sys",
      "requests"
    ],
    "preview": "import json\nimport re\nimport sys\n\nimport requests\n\ntry:\n    print(\n        \"\"\"\n            _ ____             _    _ _       _             \n           | |  _ \\           | |  | (_)     | |            \n _   _ _ __| | |_) | __ _  ___| | _| |_ _ __ | | _____ _ __ \n| | | | '__| |  _ < / _` |/ __| |/ / | | '_ \\| |/ / _ \\ '__|\n| |_| | |  | | |_) | (_| | (__|   <| | | | | |   <  __/ |   \n \\__,_|_|  |_|____/ \\__,_|\\___|_|\\_\\_|_|_| |_|_|\\_\\___|_|   \n                                              H4-cklinker - wmdark.com     \n  \"\"\"\n    )\n    if sys.version_info.major == 3:\n        site = input(\" => Backlink Kasilcak Site\\t: \")",
    "last_modified": "2025-09-13T05:53:29.852027"
  },
  {
    "id": "1299",
    "name": "myinfo.py",
    "path": "03_automation_platforms/web_automation/myinfo.py",
    "category": "03_automation_platforms",
    "type": "utility",
    "lines": 17,
    "size": 505,
    "docstring": "",
    "keywords": [
      "web_tools"
    ],
    "functions": [],
    "classes": [],
    "imports": [
      "json",
      "requests"
    ],
    "preview": "import json\n\nimport requests\n\nurl = \"https://cloud.leonardo.ai/api/rest/v1/generations/user/f7bb8476-e3f0-4f1f-9a06-4600866cc49c?offset=0&limit=1000\"\nheaders = {\n    \"accept\": \"application/json\",\n    \"authorization\": \"Bearer de7c9cb8-022f-42f8-8bf7-a8f9caadfaee\",\n}\n\nresponse = requests.get(url, headers=headers)\nif response.status_code == 200:\n    data = response.json()\n    print(json.dumps(data, indent=4))  # Pretty print the JSON data\nelse:\n    print(f\"Failed to fetch data: {response.status_code}\")\n",
    "last_modified": "2025-05-04T22:47:12.937472"
  },
  {
    "id": "1300",
    "name": "wrapper.py",
    "path": "03_automation_platforms/web_automation/wrapper.py",
    "category": "03_automation_platforms",
    "type": "utility",
    "lines": 43,
    "size": 1416,
    "docstring": "",
    "keywords": [],
    "functions": [
      "CacheControl"
    ],
    "classes": [],
    "imports": [
      "__future__",
      "typing",
      "pip._vendor.cachecontrol.adapter",
      "pip._vendor.cachecontrol.cache",
      "pip._vendor",
      "pip._vendor.cachecontrol.cache",
      "pip._vendor.cachecontrol.controller",
      "pip._vendor.cachecontrol.heuristics",
      "pip._vendor.cachecontrol.serialize"
    ],
    "preview": "# SPDX-FileCopyrightText: 2015 Eric Larson\n#\n# SPDX-License-Identifier: Apache-2.0\nfrom __future__ import annotations\n\nfrom typing import TYPE_CHECKING, Collection\n\nfrom pip._vendor.cachecontrol.adapter import CacheControlAdapter\nfrom pip._vendor.cachecontrol.cache import DictCache\n\nif TYPE_CHECKING:\n    from pip._vendor import requests\n    from pip._vendor.cachecontrol.cache import BaseCache\n    from pip._vendor.cachecontrol.controller import CacheController\n    from pip._vendor.cachecontrol.heuristics import BaseHeuristic\n    from pip._vendor.cachecontrol.serialize import Serializer\n\n\ndef CacheControl(\n    sess: requests.Session,",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1301",
    "name": "translate.py",
    "path": "03_automation_platforms/web_automation/translate.py",
    "category": "03_automation_platforms",
    "type": "utility",
    "lines": 223,
    "size": 6026,
    "docstring": "",
    "keywords": [],
    "functions": [
      "__init__",
      "translate",
      "__init__",
      "__repr__",
      "__init__",
      "detect"
    ],
    "classes": [
      "TranslationError(Exception)",
      "Translator(object)",
      "DetectionError(Exception)",
      "Language(object)",
      "LanguageDetector(object)"
    ],
    "imports": [
      "urllib",
      "browser",
      "json",
      "simplejson"
    ],
    "preview": "#!/usr/bin/python\n#\n# Peteris Krumins (peter@catonmat.net)\n# http://www.catonmat.net  --  good coders code, great reuse\n#\n# http://www.catonmat.net/blog/python-library-for-google-translate/\n#\n# Code is licensed under MIT license.\n#\n\nfrom urllib import quote_plus\n\nfrom browser import Browser, BrowserError\n\ntry:\n    import json\nexcept:\n    import simplejson as json\n\n",
    "last_modified": "2025-05-04T23:28:20.727612"
  },
  {
    "id": "1302",
    "name": "git_downloader.py",
    "path": "03_automation_platforms/web_automation/git_downloader.py",
    "category": "03_automation_platforms",
    "type": "youtube",
    "lines": 94,
    "size": 3128,
    "docstring": "",
    "keywords": [
      "youtube",
      "web_tools"
    ],
    "functions": [
      "download_repository",
      "download_files",
      "compare_files",
      "main"
    ],
    "classes": [],
    "imports": [
      "hashlib",
      "os",
      "requests",
      "git"
    ],
    "preview": "import hashlib\nimport os\n\nimport requests\nfrom git import Repo\n\n# Define the URL of the GitHub repository\nurl = \"https://github.com/ichoake/python\"\n\n\ndef download_repository(path):\n    \"\"\"Clone the repository using Git.\"\"\"\n    try:\n        Repo.clone_from(url, path)\n        print(\"Repository cloned successfully.\")\n        return True\n    except Exception as e:\n        print(f\"Error cloning repository: {e}\")\n        return False\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1303",
    "name": "compat.py",
    "path": "03_automation_platforms/web_automation/compat.py",
    "category": "03_automation_platforms",
    "type": "utility",
    "lines": 64,
    "size": 1884,
    "docstring": "Stuff that differs in different Python versions and platform\ndistributions.",
    "keywords": [],
    "functions": [
      "has_tls",
      "get_path_uid"
    ],
    "classes": [],
    "imports": [
      "logging",
      "os",
      "sys",
      "pip._vendor.urllib3.util",
      "_ssl"
    ],
    "preview": "\"\"\"Stuff that differs in different Python versions and platform\ndistributions.\"\"\"\n\nimport logging\nimport os\nimport sys\n\n__all__ = [\"get_path_uid\", \"stdlib_pkgs\", \"WINDOWS\"]\n\n\nlogger = logging.getLogger(__name__)\n\n\ndef has_tls() -> bool:\n    try:\n        import _ssl  # noqa: F401  # ignore unused\n\n        return True\n    except ImportError:\n        pass",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1304",
    "name": "filewrapper.py",
    "path": "03_automation_platforms/web_automation/filewrapper.py",
    "category": "03_automation_platforms",
    "type": "organization",
    "lines": 116,
    "size": 4240,
    "docstring": "",
    "keywords": [],
    "functions": [
      "__init__",
      "__getattr__",
      "__is_fp_closed",
      "_close",
      "read",
      "_safe_read"
    ],
    "classes": [
      "CallbackFileWrapper"
    ],
    "imports": [
      "__future__",
      "mmap",
      "tempfile",
      "typing",
      "http.client"
    ],
    "preview": "# SPDX-FileCopyrightText: 2015 Eric Larson\n#\n# SPDX-License-Identifier: Apache-2.0\nfrom __future__ import annotations\n\nimport mmap\nfrom tempfile import NamedTemporaryFile\nfrom typing import TYPE_CHECKING, Any, Callable\n\nif TYPE_CHECKING:\n    from http.client import HTTPResponse\n\n\nclass CallbackFileWrapper:\n    \"\"\"\n    Small wrapper around a fp object which will tee everything read into a\n    buffer, and when that file is closed it will execute a callback with the\n    contents of that buffer.\n\n    All attributes are proxied to the underlying file object.",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1305",
    "name": "follow_unfollow_and_send_telegram_msg.py",
    "path": "03_automation_platforms/web_automation/follow_unfollow_and_send_telegram_msg.py",
    "category": "03_automation_platforms",
    "type": "web_tools",
    "lines": 167,
    "size": 4805,
    "docstring": "This template is written by @Mehran\n\nWhat does this quickstart script aim to do?\n- My quickstart is just for follow/unfollow users.\n\nNOTES:\n- It uses schedulers to trigger activities in chosen hours and also, sends me\n  messages through Telegram API.",
    "keywords": [
      "web_tools"
    ],
    "functions": [
      "get_session",
      "follow",
      "unfollow",
      "xunfollow"
    ],
    "classes": [],
    "imports": [
      "time",
      "traceback",
      "datetime",
      "requests",
      "schedule",
      "instapy"
    ],
    "preview": "\"\"\"\nThis template is written by @Mehran\n\nWhat does this quickstart script aim to do?\n- My quickstart is just for follow/unfollow users.\n\nNOTES:\n- It uses schedulers to trigger activities in chosen hours and also, sends me\n  messages through Telegram API.\n\"\"\"\n\n# -*- coding: UTF-8 -*-\nimport time\nimport traceback\nfrom datetime import datetime\n\nimport requests\nimport schedule\nfrom instapy import InstaPy, smart_run\n",
    "last_modified": "2025-09-13T05:53:49.337466"
  },
  {
    "id": "1306",
    "name": "heuristics.py",
    "path": "03_automation_platforms/web_automation/heuristics.py",
    "category": "03_automation_platforms",
    "type": "utility",
    "lines": 155,
    "size": 4828,
    "docstring": "",
    "keywords": [],
    "functions": [
      "expire_after",
      "datetime_to_header",
      "warning",
      "update_headers",
      "apply",
      "update_headers",
      "__init__",
      "update_headers",
      "warning",
      "update_headers"
    ],
    "classes": [
      "BaseHeuristic",
      "OneDayCache",
      "ExpiresAfter",
      "LastModified"
    ],
    "imports": [
      "__future__",
      "calendar",
      "time",
      "datetime",
      "email.utils",
      "typing",
      "pip._vendor.urllib3"
    ],
    "preview": "# SPDX-FileCopyrightText: 2015 Eric Larson\n#\n# SPDX-License-Identifier: Apache-2.0\nfrom __future__ import annotations\n\nimport calendar\nimport time\nfrom datetime import datetime, timedelta, timezone\nfrom email.utils import formatdate, parsedate, parsedate_tz\nfrom typing import TYPE_CHECKING, Any, Mapping\n\nif TYPE_CHECKING:\n    from pip._vendor.urllib3 import HTTPResponse\n\nTIME_FMT = \"%a, %d %b %Y %H:%M:%S GMT\"\n\n\ndef expire_after(delta: timedelta, date: datetime | None = None) -> datetime:\n    date = date or datetime.now(timezone.utc)\n    return date + delta",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1307",
    "name": "xmlrpc.py",
    "path": "03_automation_platforms/web_automation/xmlrpc.py",
    "category": "03_automation_platforms",
    "type": "utility",
    "lines": 60,
    "size": 1823,
    "docstring": "xmlrpclib.Transport implementation",
    "keywords": [
      "web_tools"
    ],
    "functions": [
      "__init__",
      "request"
    ],
    "classes": [
      "PipXmlrpcTransport"
    ],
    "imports": [
      "logging",
      "urllib.parse",
      "xmlrpc.client",
      "typing",
      "pip._internal.exceptions",
      "pip._internal.network.session",
      "pip._internal.network.utils",
      "xmlrpc.client",
      "_typeshed"
    ],
    "preview": "\"\"\"xmlrpclib.Transport implementation\"\"\"\n\nimport logging\nimport urllib.parse\nimport xmlrpc.client\nfrom typing import TYPE_CHECKING, Tuple\n\nfrom pip._internal.exceptions import NetworkConnectionError\nfrom pip._internal.network.session import PipSession\nfrom pip._internal.network.utils import raise_for_status\n\nif TYPE_CHECKING:\n    from xmlrpc.client import _HostType, _Marshallable\n\n    from _typeshed import SizedBuffer\n\nlogger = logging.getLogger(__name__)\n\n\nclass PipXmlrpcTransport(xmlrpc.client.Transport):",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1308",
    "name": "mywork.py",
    "path": "03_automation_platforms/web_automation/mywork.py",
    "category": "03_automation_platforms",
    "type": "utility",
    "lines": 62,
    "size": 1904,
    "docstring": "",
    "keywords": [
      "web_tools"
    ],
    "functions": [],
    "classes": [],
    "imports": [
      "csv",
      "json",
      "requests"
    ],
    "preview": "import csv\nimport json\n\nimport requests\n\n# Define the URL and headers\nurl = \"https://cloud.leonardo.ai/api/rest/v1/generations/user/f7bb8476-e3f0-4f1f-9a06-4600866cc49c?offset=0&limit=1000\"\nheaders = {\n    \"accept\": \"application/json\",\n    \"authorization\": \"Bearer de7c9cb8-022f-42f8-8bf7-a8f9caadfaee\",\n}\n\ntry:\n    # Make the GET request\n    response = requests.get(url, headers=headers)\n    response.raise_for_status()  # Raises an HTTPError for bad responses\n\n    # Load JSON data from the response\n    data = response.json()\n",
    "last_modified": "2025-05-04T22:47:12.937776"
  },
  {
    "id": "1309",
    "name": "adapter.py",
    "path": "03_automation_platforms/web_automation/adapter.py",
    "category": "03_automation_platforms",
    "type": "utility",
    "lines": 156,
    "size": 6304,
    "docstring": "",
    "keywords": [
      "web_tools"
    ],
    "functions": [
      "__init__",
      "send",
      "build_response",
      "close",
      "_update_chunk_length"
    ],
    "classes": [
      "CacheControlAdapter"
    ],
    "imports": [
      "__future__",
      "functools",
      "types",
      "zlib",
      "typing",
      "pip._vendor.cachecontrol.cache",
      "pip._vendor.cachecontrol.controller",
      "pip._vendor.cachecontrol.filewrapper",
      "pip._vendor.requests.adapters",
      "pip._vendor.cachecontrol.cache"
    ],
    "preview": "# SPDX-FileCopyrightText: 2015 Eric Larson\n#\n# SPDX-License-Identifier: Apache-2.0\nfrom __future__ import annotations\n\nimport functools\nimport types\nimport zlib\nfrom typing import TYPE_CHECKING, Any, Collection, Mapping\n\nfrom pip._vendor.cachecontrol.cache import DictCache\nfrom pip._vendor.cachecontrol.controller import PERMANENT_REDIRECT_STATUSES, CacheController\nfrom pip._vendor.cachecontrol.filewrapper import CallbackFileWrapper\nfrom pip._vendor.requests.adapters import HTTPAdapter\n\nif TYPE_CHECKING:\n    from pip._vendor.cachecontrol.cache import BaseCache\n    from pip._vendor.cachecontrol.heuristics import BaseHeuristic\n    from pip._vendor.cachecontrol.serialize import Serializer\n    from pip._vendor.requests import PreparedRequest, Response",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1310",
    "name": "approve_thread_requests.py",
    "path": "03_automation_platforms/web_automation/approve_thread_requests.py",
    "category": "03_automation_platforms",
    "type": "web_tools",
    "lines": 24,
    "size": 555,
    "docstring": "instabot example\n\nWorkflow:\n    1) approve incoming message requests.",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "argparse",
      "os",
      "sys",
      "instabot"
    ],
    "preview": "\"\"\"\ninstabot example\n\nWorkflow:\n    1) approve incoming message requests.\n\"\"\"\n\nimport argparse\nimport os\nimport sys\n\nsys.path.append(os.path.join(sys.path[0], \"../\"))\nfrom instabot import Bot  # noqa: E402\n\nparser = argparse.ArgumentParser(add_help=True)\nparser.add_argument(\"-u\", type=str, help=\"username\")\nparser.add_argument(\"-p\", type=str, help=\"password\")\nparser.add_argument(\"-proxy\", type=str, help=\"proxy\")\nargs = parser.parse_args()\n",
    "last_modified": "2025-05-04T23:28:20"
  },
  {
    "id": "1311",
    "name": "check_modules.py",
    "path": "03_automation_platforms/web_automation/check_modules.py",
    "category": "03_automation_platforms",
    "type": "utility",
    "lines": 48,
    "size": 1173,
    "docstring": "",
    "keywords": [
      "web_tools"
    ],
    "functions": [
      "check_modules"
    ],
    "classes": [],
    "imports": [
      "sys",
      "warnings",
      "colorama",
      "requests",
      "colorama",
      "asyncio",
      "proxybroker",
      "warnings"
    ],
    "preview": "import sys\n\n\ndef check_modules():\n    try:\n        import requests\n    except:\n        print_error(\"'requests' module not found!\")\n        print_status(\"run install_requirements.bat to install the modules\")\n        sys.exiprint_errort(0)\n\n    try:\n        import colorama\n    except Exception as e:\n        print_error(\"'colorama' package not installed!\")\n        print_status(\"run install_requirements.bat to install the modules\")\n        print(e)\n        sys.exit(0)\n\n    try:",
    "last_modified": "2025-03-28T18:35:48"
  },
  {
    "id": "1312",
    "name": "sdist.py",
    "path": "03_automation_platforms/web_automation/sdist.py",
    "category": "03_automation_platforms",
    "type": "utility",
    "lines": 149,
    "size": 6603,
    "docstring": "",
    "keywords": [],
    "functions": [
      "build_tracker_id",
      "get_metadata_distribution",
      "prepare_distribution_metadata",
      "_prepare_build_backend",
      "_get_build_requires_wheel",
      "_get_build_requires_editable",
      "_install_build_reqs",
      "_raise_conflicts",
      "_raise_missing_reqs"
    ],
    "classes": [
      "SourceDistribution"
    ],
    "imports": [
      "logging",
      "typing",
      "pip._internal.build_env",
      "pip._internal.distributions.base",
      "pip._internal.exceptions",
      "pip._internal.index.package_finder",
      "pip._internal.metadata",
      "pip._internal.utils.subprocess"
    ],
    "preview": "import logging\nfrom typing import Iterable, Optional, Set, Tuple\n\nfrom pip._internal.build_env import BuildEnvironment\nfrom pip._internal.distributions.base import AbstractDistribution\nfrom pip._internal.exceptions import InstallationError\nfrom pip._internal.index.package_finder import PackageFinder\nfrom pip._internal.metadata import BaseDistribution\nfrom pip._internal.utils.subprocess import runner_with_spinner_message\n\nlogger = logging.getLogger(__name__)\n\n\nclass SourceDistribution(AbstractDistribution):\n    \"\"\"Represents a source distribution.\n\n    The preparation step for these needs metadata for the packages to be\n    generated, either using PEP 517 or using the legacy `setup.py egg_info`.\n    \"\"\"\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1313",
    "name": "dalle.py",
    "path": "03_automation_platforms/web_automation/dalle.py",
    "category": "03_automation_platforms",
    "type": "utility",
    "lines": 27,
    "size": 908,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "csv"
    ],
    "preview": "import csv\n\n# Define the input and output file paths\ninput_file_path = \"dalle.txt\"  # Make sure this points to your actual input file path\noutput_file_path = \"output_urls_info.csv\"  # The CSV file to save the output\n\n# Initialize lists to store URLs and descriptions\ndata = []\n\n# Open and read the input file\nwith open(input_file_path, \"r\") as file:\n    lines = file.readlines()\n\n# Iterate over the lines in the file\nfor i in range(0, len(lines), 2):  # Iterate by pairs (URL, description)\n    url = lines[i].strip()\n    description = lines[i + 1].strip() if i + 1 < len(lines) else \"\"\n    data.append([url, description])\n\n# Write the data to a CSV file",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1314",
    "name": "browser.py",
    "path": "03_automation_platforms/web_automation/browser.py",
    "category": "03_automation_platforms",
    "type": "utility",
    "lines": 107,
    "size": 4599,
    "docstring": "",
    "keywords": [],
    "functions": [
      "__init__",
      "connect",
      "http_open",
      "__init__",
      "get_page",
      "set_random_user_agent"
    ],
    "classes": [
      "BrowserError(Exception)",
      "PoolHTTPConnection(httplib.HTTPConnection)",
      "PoolHTTPHandler(urllib2.HTTPHandler)",
      "Browser(object)"
    ],
    "imports": [
      "random",
      "socket",
      "urllib",
      "httplib",
      "urllib2"
    ],
    "preview": "#!/usr/bin/python\n#\n# Peteris Krumins (peter@catonmat.net)\n# http://www.catonmat.net  --  good coders code, great reuse\n#\n# http://www.catonmat.net/blog/python-library-for-google-search/\n#\n# Code is licensed under MIT license.\n#\n\nimport random\nimport socket\nimport urllib\n\nimport httplib\nimport urllib2\n\nBROWSERS = (\n    # Top most popular browsers in my access.log on 2009.02.12\n    # tail -50000 access.log |",
    "last_modified": "2025-05-04T23:28:20.735922"
  },
  {
    "id": "1315",
    "name": "sufflecsv.py",
    "path": "03_automation_platforms/web_automation/sufflecsv.py",
    "category": "03_automation_platforms",
    "type": "utility",
    "lines": 34,
    "size": 860,
    "docstring": "",
    "keywords": [
      "web_tools"
    ],
    "functions": [
      "shuffle_csv"
    ],
    "classes": [],
    "imports": [
      "csv",
      "random",
      "io",
      "requests"
    ],
    "preview": "import csv\nimport random\nfrom io import StringIO\n\nimport requests\n\n\ndef shuffle_csv(url):\n    # Download the CSV data from the URL\n    response = requests.get(url)\n    response.raise_for_status()  # Check for errors\n    csv_data = response.text\n\n    # Read the CSV data\n    csv_file = StringIO(csv_data)\n    reader = csv.reader(csv_file)\n    data = list(reader)\n\n    # Shuffle the data\n    random.shuffle(data)",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1316",
    "name": "setup.py",
    "path": "03_automation_platforms/web_automation/setup.py",
    "category": "03_automation_platforms",
    "type": "setup",
    "lines": 6,
    "size": 194,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [],
    "preview": "# This is the file that will run the setup before we run the main program.\n# This has not been developed yet\n\n# TODO DOWNLOAD THE SELENIUM DRIVER FOR THE USER\n# ALONG WITH OTHER PYTHON PACKAGES\n",
    "last_modified": "2025-03-28T18:35:47.782369"
  },
  {
    "id": "1317",
    "name": "requirements.py",
    "path": "03_automation_platforms/web_automation/requirements.py",
    "category": "03_automation_platforms",
    "type": "utility",
    "lines": 147,
    "size": 4764,
    "docstring": "",
    "keywords": [],
    "functions": [
      "__init__",
      "__str__",
      "__repr__"
    ],
    "classes": [
      "InvalidRequirement",
      "Requirement"
    ],
    "imports": [
      "re",
      "string",
      "urllib.parse",
      "typing",
      "typing",
      "typing",
      "pip._vendor.pyparsing",
      "pip._vendor.pyparsing",
      "pip._vendor.pyparsing",
      "markers"
    ],
    "preview": "# This file is dual licensed under the terms of the Apache License, Version\n# 2.0, and the BSD License. See the LICENSE file in the root of this repository\n# for complete details.\n\nimport re\nimport string\nimport urllib.parse\nfrom typing import List\nfrom typing import Optional as TOptional\nfrom typing import Set\n\nfrom pip._vendor.pyparsing import Combine\nfrom pip._vendor.pyparsing import Literal as L  # noqa\nfrom pip._vendor.pyparsing import (\n    Optional,\n    ParseException,\n    Regex,\n    Word,\n    ZeroOrMore,\n    originalTextFor,",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1318",
    "name": "rank_provider.py",
    "path": "03_automation_platforms/web_automation/rank_provider.py",
    "category": "03_automation_platforms",
    "type": "utility",
    "lines": 257,
    "size": 7414,
    "docstring": "",
    "keywords": [],
    "functions": [
      "__init__",
      "get_rank",
      "__init__",
      "get_rank",
      "__init__",
      "get_rank",
      "_compute_ch_new",
      "_compute_ch",
      "_mix",
      "_wadd"
    ],
    "classes": [
      "RankProvider",
      "AlexaTrafficRank",
      "GooglePageRank"
    ],
    "imports": [
      "re",
      "struct",
      "sys",
      "urllib",
      "xml.etree.ElementTree",
      "httplib",
      "urllib2"
    ],
    "preview": "import re\nimport struct\nimport sys\nimport urllib\nimport xml.etree.ElementTree\n\nimport httplib\nimport urllib2\n\n\nclass RankProvider(object):\n    \"\"\"Abstract class for obtaining the page rank (popularity)\n    from a provider such as Google or Alexa.\n\n    \"\"\"\n\n    def __init__(self, host, proxy=None, timeout=30):\n        \"\"\"Keyword arguments:\n        host -- toolbar host address\n        proxy -- address of proxy server. Default: None",
    "last_modified": "2025-09-13T05:54:17.959882"
  },
  {
    "id": "1319",
    "name": "settings.py",
    "path": "03_automation_platforms/web_automation/settings.py",
    "category": "03_automation_platforms",
    "type": "utility",
    "lines": 175,
    "size": 5564,
    "docstring": "",
    "keywords": [],
    "functions": [
      "crawl",
      "check",
      "crawl_and_check",
      "check_vars",
      "check_toml",
      "get_check_value"
    ],
    "classes": [],
    "imports": [
      "re",
      "pathlib",
      "typing",
      "toml",
      "rich.console",
      "utils.console"
    ],
    "preview": "import re\nfrom pathlib import Path\nfrom typing import Dict, Tuple\n\nimport toml\nfrom rich.console import Console\n\nfrom utils.console import handle_input\n\nconsole = Console()\nconfig = dict  # autocomplete\n\n\ndef crawl(obj: dict, func=lambda x, y: print(x, y, end=\"\\n\"), path=None):\n    if path is None:  # path Default argument value is mutable\n        path = []\n    for key in obj.keys():\n        if type(obj[key]) is dict:\n            crawl(obj[key], func, path + [key])\n            continue",
    "last_modified": "2025-09-13T05:54:00.429473"
  },
  {
    "id": "1320",
    "name": "google_url_scrapper.py",
    "path": "03_automation_platforms/web_automation/google_url_scrapper.py",
    "category": "03_automation_platforms",
    "type": "utility",
    "lines": 54,
    "size": 1513,
    "docstring": "",
    "keywords": [
      "web_tools"
    ],
    "functions": [
      "__init__",
      "scrape",
      "MajesticSEO_API"
    ],
    "classes": [
      "google_url_scrapper"
    ],
    "imports": [
      "datetime",
      "os",
      "random",
      "sys",
      "time",
      "requests",
      "xgoogle.search"
    ],
    "preview": "import datetime\nimport os\nimport random\nimport sys\nimport time\n\nimport requests\nfrom xgoogle.search import GoogleSearch, SearchError\n\n\nclass google_url_scrapper:\n    def __init__(self):        \n        self.urls = []\n        self.seo = ''\n\n    def scrape(self, keyword, pages=2):\n        try:\n            gs = GoogleSearch(keyword)\n            gs.results_per_page = 10\n            gs.page = 0",
    "last_modified": "2025-05-04T23:28:20.714689"
  },
  {
    "id": "1321",
    "name": "lazy_wheel.py",
    "path": "03_automation_platforms/web_automation/lazy_wheel.py",
    "category": "03_automation_platforms",
    "type": "web_tools",
    "lines": 208,
    "size": 7623,
    "docstring": "Lazy ZIP over HTTP",
    "keywords": [
      "web_tools"
    ],
    "functions": [
      "dist_from_wheel_url",
      "__init__",
      "mode",
      "name",
      "seekable",
      "close",
      "closed",
      "read",
      "readable",
      "seek"
    ],
    "classes": [
      "HTTPRangeRequestUnsupported",
      "LazyZipOverHTTP"
    ],
    "imports": [
      "bisect",
      "contextlib",
      "tempfile",
      "typing",
      "zipfile",
      "pip._internal.metadata",
      "pip._internal.network.session",
      "pip._internal.network.utils",
      "pip._vendor.packaging.utils",
      "pip._vendor.requests.models"
    ],
    "preview": "\"\"\"Lazy ZIP over HTTP\"\"\"\n\n__all__ = [\"HTTPRangeRequestUnsupported\", \"dist_from_wheel_url\"]\n\nfrom bisect import bisect_left, bisect_right\nfrom contextlib import contextmanager\nfrom tempfile import NamedTemporaryFile\nfrom typing import Any, Dict, Generator, List, Optional, Tuple\nfrom zipfile import BadZipFile, ZipFile\n\nfrom pip._internal.metadata import BaseDistribution, MemoryWheel, get_wheel_distribution\nfrom pip._internal.network.session import PipSession\nfrom pip._internal.network.utils import HEADERS, raise_for_status, response_chunks\nfrom pip._vendor.packaging.utils import canonicalize_name\nfrom pip._vendor.requests.models import CONTENT_CHUNK_SIZE, Response\n\n\nclass HTTPRangeRequestUnsupported(Exception):\n    pass\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1322",
    "name": "main.py",
    "path": "03_automation_platforms/web_automation/main.py",
    "category": "03_automation_platforms",
    "type": "utility",
    "lines": 81,
    "size": 2388,
    "docstring": "",
    "keywords": [],
    "functions": [
      "doesnt_exist"
    ],
    "classes": [],
    "imports": [
      "os",
      "time",
      "selenium",
      "selenium.common.exceptions",
      "webdriver_manager.chrome"
    ],
    "preview": "import os\nimport time\n\nfrom selenium import webdriver\nfrom selenium.common.exceptions import NoSuchElementException\nfrom webdriver_manager.chrome import ChromeDriverManager as CM\n\nHOW_MANY = int(input(\"How many comments you want to like (0-20):\"))\n\nwhile HOW_MANY > 20:\n    print(\"Cant like more than 20 comments, please choose a smaller number!\")\n    HOW_MANY = int(input(\"How many comments you want to like (0-20):\"))\n\n\noptions = webdriver.ChromeOptions()\noptions.add_argument(\"--log-level=3\")\noptions.add_argument(f\"--user-data-dir={os.getcwd()}\\\\profile\")\nmobile_emulation = {\n    \"userAgent\": \"Mozilla/5.0 (Linux; Android 4.2.1; en-us; Nexus 5 Build/JOP40D) AppleWebKit/535.19 (KHTML, like Gecko) Chrome/90.0.1025.166 Mobile Safari/535.19\"\n}",
    "last_modified": "2025-05-04T23:28:20.677246"
  },
  {
    "id": "1323",
    "name": "urls.py",
    "path": "03_automation_platforms/web_automation/urls.py",
    "category": "03_automation_platforms",
    "type": "utility",
    "lines": 59,
    "size": 1723,
    "docstring": "",
    "keywords": [],
    "functions": [
      "get_url_scheme",
      "path_to_url",
      "url_to_path"
    ],
    "classes": [],
    "imports": [
      "os",
      "string",
      "urllib.parse",
      "urllib.request",
      "typing",
      "compat"
    ],
    "preview": "import os\nimport string\nimport urllib.parse\nimport urllib.request\nfrom typing import Optional\n\nfrom .compat import WINDOWS\n\n\ndef get_url_scheme(url: str) -> Optional[str]:\n    if \":\" not in url:\n        return None\n    return url.split(\":\", 1)[0].lower()\n\n\ndef path_to_url(path: str) -> str:\n    \"\"\"\n    Convert a path to a file: URL.  The path will be made absolute and have\n    quoted path parts.\n    \"\"\"",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1324",
    "name": "test_bot.py",
    "path": "03_automation_platforms/web_automation/test_bot.py",
    "category": "03_automation_platforms",
    "type": "testing",
    "lines": 140,
    "size": 4283,
    "docstring": "",
    "keywords": [
      "testing",
      "web_tools"
    ],
    "functions": [
      "setup",
      "prepare_api",
      "test_login",
      "test_generate_uuid",
      "test_set_user",
      "test_reset_counters",
      "mockreturn",
      "mockreturn_login"
    ],
    "classes": [
      "TestBot",
      "TestBotAPI"
    ],
    "imports": [
      "json",
      "requests",
      "instabot",
      "unittest.mock",
      "mock",
      "uuid"
    ],
    "preview": "import json\n\nimport requests\nfrom instabot import Bot\n\ntry:\n    from unittest.mock import Mock, patch\nexcept ImportError:\n    from mock import Mock, patch\n\n\nclass TestBot:\n    def setup(self):\n        self.USER_ID = 1234567\n        self.USERNAME = \"test_username\"\n        self.PASSWORD = \"test_password\"\n        self.FULLNAME = \"test_full_name\"\n        self.TOKEN = \"abcdef123456\"\n        self.bot = Bot(\n            max_likes_per_day=1000,",
    "last_modified": "2025-09-13T05:54:58.221758"
  },
  {
    "id": "1325",
    "name": "follow_requests.py",
    "path": "03_automation_platforms/web_automation/follow_requests.py",
    "category": "03_automation_platforms",
    "type": "web_tools",
    "lines": 25,
    "size": 741,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "argparse",
      "os",
      "sys",
      "instabot"
    ],
    "preview": "import argparse\nimport os\nimport sys\n\nsys.path.append(os.path.join(sys.path[0], \"../\"))\nfrom instabot import Bot  # noqa: E402\n\nparser = argparse.ArgumentParser(add_help=True)\nparser.add_argument(\"-u\", type=str, help=\"username\")\nparser.add_argument(\"-p\", type=str, help=\"password\")\nparser.add_argument(\"-proxy\", type=str, help=\"proxy\")\nparser.add_argument(\"-story_username\", type=str, help=\"story_username\")\nargs = parser.parse_args()\n\nbot = Bot()\nbot.login(username=args.u, password=args.p, proxy=args.proxy)\n\n# (The following functions apply if you have a private account)\n\n# Approve users that requested to follow you",
    "last_modified": "2025-03-28T18:35:47"
  },
  {
    "id": "1326",
    "name": "test_netlify_uploader.py",
    "path": "03_automation_platforms/web_automation/test_netlify_uploader.py",
    "category": "03_automation_platforms",
    "type": "youtube",
    "lines": 105,
    "size": 4005,
    "docstring": "",
    "keywords": [
      "testing"
    ],
    "functions": [
      "setUp",
      "test_netlify_without_location",
      "test_get_authorization_token",
      "test_get_netlify_site_id",
      "test_deploy_to_netlify",
      "test_upload_gallery"
    ],
    "classes": [
      "NetlifyUploaderTestCase"
    ],
    "imports": [
      "json",
      "os",
      "unittest",
      "unittest",
      "unittest.mock",
      "simplegallery.upload.variants.netlify_uploader",
      "simplegallery.upload.uploader_factory",
      "testfixtures"
    ],
    "preview": "import json\nimport os\nimport unittest\nfrom unittest import mock\nfrom unittest.mock import Mock\n\nimport simplegallery.upload.variants.netlify_uploader as netlify\nfrom simplegallery.upload.uploader_factory import get_uploader\nfrom testfixtures import TempDirectory\n\n\nclass NetlifyUploaderTestCase(unittest.TestCase):\n    def setUp(self) -> None:\n        self.uploader = get_uploader(\"netlify\")\n\n    def test_netlify_without_location(self):\n        self.assertTrue(self.uploader.check_location(\"\"))\n\n    @mock.patch(\"webbrowser.open\")\n    def test_get_authorization_token(self, webbrowser_open):",
    "last_modified": "2025-09-13T05:53:53.233150"
  },
  {
    "id": "1327",
    "name": "news.py",
    "path": "03_automation_platforms/web_automation/news.py",
    "category": "03_automation_platforms",
    "type": "utility",
    "lines": 33,
    "size": 990,
    "docstring": "",
    "keywords": [
      "web_tools"
    ],
    "functions": [
      "__init__",
      "getnews"
    ],
    "classes": [
      "NEWS"
    ],
    "imports": [
      "logging",
      "requests",
      "utilities.const"
    ],
    "preview": "import logging\n\nimport requests\nfrom utilities.const import LOG_PATH, NEWS_FETCH_LIMIT\n\n# Configure logging\nlogging.basicConfig(\n    level=logging.INFO,\n    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n    filename=LOG_PATH,\n)\n\n\nclass NEWS:\n    def __init__(self, news_url):\n        self.news_fetch_limit = NEWS_FETCH_LIMIT\n        self.url = news_url\n\n    def getnews(self):\n        try:",
    "last_modified": "2025-03-28T18:37:11.566049"
  },
  {
    "id": "1328",
    "name": "about.py.py",
    "path": "03_automation_platforms/about.py_consolidated/about.py.py",
    "category": "03_automation_platforms",
    "type": "utility",
    "lines": 36,
    "size": 1334,
    "docstring": "",
    "keywords": [],
    "functions": [
      "about_msg"
    ],
    "classes": [],
    "imports": [
      "os",
      "webbrowser",
      "libs.animation"
    ],
    "preview": "import os  # line:2\nimport webbrowser  # line:3\n\nfrom libs.animation import colorText  # line:1\n\n\ndef about_msg():  # line:4\n    print(\n        colorText(\n            \"\"\"\n\n[[red]] [ 1 ] [[reset]] [[cyan]] Github - https://github.com/crevils\n[[red]] [ 2 ] [[reset]] [[cyan]] Youtube - https://github.com/crevil\n[[red]] [ 3 ] [[reset]] [[cyan]] Telegram - https://t.me/HackerExploits\n[[red]] [ 4 ] [[reset]] [[cyan]] Instagram - https://instagram.com/_crevil\n[[red]] [ 5 ] [[reset]] [[cyan]] EXIT \n\n    \"\"\"\n        )\n    )  # line:13",
    "last_modified": "2025-09-13T05:53:30.471732"
  },
  {
    "id": "1329",
    "name": "about.py_02.py",
    "path": "03_automation_platforms/about.py_consolidated/about.py_02.py",
    "category": "03_automation_platforms",
    "type": "utility",
    "lines": 31,
    "size": 898,
    "docstring": "",
    "keywords": [],
    "functions": [
      "about_msg"
    ],
    "classes": [],
    "imports": [
      "os",
      "webbrowser",
      "libs.animation"
    ],
    "preview": "import os  # line:2\nimport webbrowser  # line:3\n\nfrom libs.animation import colorText  # line:1\n\n\ndef about_msg():  # line:4\n    print(\n        colorText(\n            \"\"\"\n\n\n    \"\"\"\n        )\n    )  # line:13\n    OOO000OO00000O00O = input(\" Select - \")  # line:14\n    if int(OOO000OO00000O00O) == 1:  # line:15\n        webbrowser.open(\"\")  # line:16\n        about_msg()  # line:17\n    if int(OOO000OO00000O00O) == 2:  # line:18",
    "last_modified": "2025-09-13T05:53:30.879501"
  },
  {
    "id": "1330",
    "name": "utils.py_02.py",
    "path": "03_automation_platforms/utils.py_consolidated/utils.py_02.py",
    "category": "03_automation_platforms",
    "type": "utility",
    "lines": 69,
    "size": 1660,
    "docstring": "",
    "keywords": [
      "web_tools"
    ],
    "functions": [
      "print_success",
      "print_error",
      "print_status",
      "ask_question",
      "parse_proxy_file"
    ],
    "classes": [],
    "imports": [
      "random",
      "re",
      "os",
      "sys",
      "colorama",
      "requests"
    ],
    "preview": "import random\nimport re\nfrom os import path\nfrom sys import exit\n\nfrom colorama import Back, Fore, Style\nfrom requests import get\n\n\ndef print_success(message, *argv):\n    print(Fore.GREEN + \"[OK] \" + Style.RESET_ALL + Style.BRIGHT, end=\"\")\n    print(message, end=\" \")\n    for arg in argv:\n        print(arg, end=\" \")\n    print(\"\")\n\n\ndef print_error(message, *argv):\n    print(Fore.RED + \"[ERR] \" + Style.RESET_ALL + Style.BRIGHT, end=\"\")\n    print(message, end=\" \")",
    "last_modified": "2025-05-04T23:28:20"
  },
  {
    "id": "1331",
    "name": "utils.py.py",
    "path": "03_automation_platforms/utils.py_consolidated/utils.py.py",
    "category": "03_automation_platforms",
    "type": "utility",
    "lines": 42,
    "size": 1566,
    "docstring": "",
    "keywords": [],
    "functions": [
      "__init__",
      "endpoint_config_exists",
      "endpoint_exists"
    ],
    "classes": [
      "ResourceManager"
    ],
    "imports": [
      "loguru",
      "llm_engineering.settings",
      "boto3",
      "botocore.exceptions"
    ],
    "preview": "from loguru import logger\n\ntry:\n    import boto3\n    from botocore.exceptions import ClientError\nexcept ModuleNotFoundError:\n    logger.warning(\n        \"Couldn't load AWS or SageMaker imports. Run 'poetry install --with aws' to support AWS.\"\n    )\n\nfrom llm_engineering.settings import settings\n\n\nclass ResourceManager:\n    def __init__(self) -> None:\n        self.sagemaker_client = boto3.client(\n            \"sagemaker\",\n            region_name=settings.AWS_REGION,\n            aws_access_key_id=settings.AWS_ACCESS_KEY,\n            aws_secret_access_key=settings.AWS_SECRET_KEY,",
    "last_modified": "2025-09-13T05:53:42.487557"
  },
  {
    "id": "1332",
    "name": "id.py",
    "path": "03_automation_platforms/social_media_automation/id.py",
    "category": "03_automation_platforms",
    "type": "utility",
    "lines": 13,
    "size": 290,
    "docstring": "",
    "keywords": [],
    "functions": [
      "id"
    ],
    "classes": [],
    "imports": [
      "re",
      "utils.console"
    ],
    "preview": "import re\n\nfrom utils.console import print_substep\n\n\ndef id(reddit_obj: dict):\n    \"\"\"\n    This function takes a reddit object and returns the post id\n    \"\"\"\n    id = re.sub(r\"[^\\w\\s-]\", \"\", reddit_obj[\"thread_id\"])\n    print_substep(f\"Thread ID is {id}\", style=\"bold blue\")\n    return id\n",
    "last_modified": "2025-05-04T22:47:11.890812"
  },
  {
    "id": "1333",
    "name": "test_bot_comment.py",
    "path": "03_automation_platforms/social_media_automation/test_bot_comment.py",
    "category": "03_automation_platforms",
    "type": "testing",
    "lines": 116,
    "size": 4117,
    "docstring": "",
    "keywords": [
      "testing"
    ],
    "functions": [
      "test_comment_feedback",
      "test_comment"
    ],
    "classes": [
      "TestBotGet"
    ],
    "imports": [
      "pytest",
      "responses",
      "instabot.api.config",
      "test_bot",
      "test_variables",
      "unittest.mock",
      "mock"
    ],
    "preview": "import pytest\nimport responses\nfrom instabot.api.config import API_URL\n\nfrom .test_bot import TestBot\nfrom .test_variables import TEST_CAPTION_ITEM, TEST_COMMENT_ITEM\n\ntry:\n    from unittest.mock import patch\nexcept ImportError:\n    from mock import patch\n\n\nclass TestBotGet(TestBot):\n    @responses.activate\n    @pytest.mark.parametrize(\n        \"blocked_actions_protection,blocked_actions\",\n        [(True, True), (True, False), (False, True), (False, False)],\n    )\n    @patch(\"time.sleep\", return_value=None)",
    "last_modified": "2025-09-13T05:54:58.284333"
  },
  {
    "id": "1334",
    "name": "database.py",
    "path": "03_automation_platforms/social_media_automation/database.py",
    "category": "03_automation_platforms",
    "type": "analysis",
    "lines": 300,
    "size": 9403,
    "docstring": "",
    "keywords": [],
    "functions": [
      "startDatabase",
      "initDatabase",
      "beginDatabaseConnection",
      "addFoundClip",
      "getFoundClips",
      "addFilter",
      "getAllSavedFilters",
      "getSavedFilterByName",
      "getFilterNames",
      "getFilterClipCount"
    ],
    "classes": [],
    "imports": [
      "pickle",
      "datetime",
      "mysql.connector",
      "settings",
      "mysql.connector"
    ],
    "preview": "import pickle\nfrom datetime import date\n\nimport mysql.connector\nimport settings\nfrom mysql.connector import pooling\n\ncurrent_date = date.today()\nconnection_pool = None\n\n\ndef startDatabase():\n    beginDatabaseConnection()\n    initDatabase()\n\n\ndef initDatabase():\n    global connection_pool\n    connection_object = connection_pool.get_connection()\n    cursor = connection_object.cursor()",
    "last_modified": "2025-09-13T05:53:32.019246"
  },
  {
    "id": "1335",
    "name": "multi_script_CLI.py",
    "path": "03_automation_platforms/social_media_automation/multi_script_CLI.py",
    "category": "03_automation_platforms",
    "type": "utility",
    "lines": 664,
    "size": 18308,
    "docstring": "",
    "keywords": [],
    "functions": [
      "initial_checker",
      "read_input",
      "setting_input",
      "parameter_setting",
      "username_adder",
      "get_adder",
      "hashtag_adder",
      "competitor_adder",
      "blacklist_adder",
      "whitelist_adder"
    ],
    "classes": [],
    "imports": [
      "__future__",
      "getpass",
      "os",
      "random",
      "sys",
      "time",
      "tqdm",
      "instabot"
    ],
    "preview": "#!/usr/bin/python\n# - * - coding: utf-8 - * -\nfrom __future__ import unicode_literals\n\nimport getpass\nimport os\nimport random\nimport sys\nimport time\n\nfrom tqdm import tqdm\n\nsys.path.append(os.path.join(sys.path[0], \"../\"))\nfrom instabot import Bot\n\n\ndef initial_checker():\n    files = [hashtag_file, users_file, whitelist, blacklist, comment, setting_file]\n    # files = [setting_file]\n    try:",
    "last_modified": "2025-09-13T05:54:55.647239"
  },
  {
    "id": "1336",
    "name": "config-example.py",
    "path": "03_automation_platforms/social_media_automation/config-example.py",
    "category": "03_automation_platforms",
    "type": "setup",
    "lines": 9,
    "size": 175,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [],
    "preview": "# Reddit Praw\npraw_client_id = \"xxxxxx\"\npraw_client_secret = \"xxxxxx\"\npraw_user_agent = \"xxxxxx\"\n\n# Amazon Polly\naws_access_key_id = \"xxxxxx\"\naws_secret_access_key = \"xxxxxx\"\n",
    "last_modified": "2025-03-28T18:35:46.755522"
  },
  {
    "id": "1337",
    "name": "from_link.py",
    "path": "03_automation_platforms/social_media_automation/from_link.py",
    "category": "03_automation_platforms",
    "type": "utility",
    "lines": 49,
    "size": 1203,
    "docstring": "",
    "keywords": [],
    "functions": [
      "main"
    ],
    "classes": [],
    "imports": [
      "argparse",
      "sys",
      "praw",
      "config"
    ],
    "preview": "# Program takes a reddit post link\n# Then returns the top 5 comments from the post\n#\n\n\nimport argparse  # command line argument parser\nimport sys\n\nimport praw\n\nsys.path.append(\"../\")\nimport config\n\n\ndef main() -> int:\n\n    # Creating an argument parser and an argument for the link\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"link\", help=\"the link of the post\")\n    args = parser.parse_args()  # Collecting arguments",
    "last_modified": "2025-05-04T23:28:21.790812"
  },
  {
    "id": "1338",
    "name": "cleanup.py",
    "path": "03_automation_platforms/social_media_automation/cleanup.py",
    "category": "03_automation_platforms",
    "type": "organization",
    "lines": 21,
    "size": 422,
    "docstring": "",
    "keywords": [
      "organization"
    ],
    "functions": [
      "_listdir",
      "cleanup"
    ],
    "classes": [],
    "imports": [
      "os",
      "shutil",
      "os.path"
    ],
    "preview": "import os\nimport shutil\nfrom os.path import exists\n\n\ndef _listdir(d):  # listdir with full path\n    return [os.path.join(d, f) for f in os.listdir(d)]\n\n\ndef cleanup(reddit_id) -> int:\n    \"\"\"Deletes all temporary assets in assets/temp\n\n    Returns:\n        int: How many files were deleted\n    \"\"\"\n    directory = f\"../assets/temp/{reddit_id}/\"\n    if exists(directory):\n        shutil.rmtree(directory)\n\n        return 1",
    "last_modified": "2025-05-04T22:47:11.888656"
  },
  {
    "id": "1339",
    "name": "instaclient.py",
    "path": "03_automation_platforms/social_media_automation/instaclient.py",
    "category": "03_automation_platforms",
    "type": "utility",
    "lines": 204,
    "size": 7337,
    "docstring": "",
    "keywords": [
      "web_tools"
    ],
    "functions": [
      "__init__",
      "SetDefaultHeaders",
      "IsCookiesOK",
      "GetAndUpdate",
      "PostAndUpdate",
      "Connect",
      "Login",
      "Spam"
    ],
    "classes": [
      "InstaClient"
    ],
    "imports": [
      "random",
      "libs.utils",
      "requests"
    ],
    "preview": "from random import choice\n\nfrom libs.utils import CheckPublicIP, IsProxyWorking, PrintError, PrintStatus, PrintSuccess\nfrom requests import Session\n\nUSER_AGENTS = [\n    \"Mozilla/5.0 (Android 4.4; Mobile; rv:41.0) Gecko/41.0 Firefox/41.0\",\n    \"Mozilla/5.0 (Android 4.4; Tablet; rv:41.0) Gecko/41.0 Firefox/41.0\",\n    \"Mozilla/5.0 (Windows NT x.y; rv:10.0) Gecko/20100101 Firefox/10.0\",\n    \"Mozilla/5.0 (X11; Linux i686; rv:10.0) Gecko/20100101 Firefox/10.0\",\n    \"Mozilla/5.0 (X11; Linux x86_64; rv:10.0) Gecko/20100101 Firefox/10.0\",\n    \"Mozilla/5.0 (Android 4.4; Mobile; rv:41.0) Gecko/41.0 Firefox/41.0\",\n]\n\n\nclass InstaClient:\n    def __init__(self, user, password, ip, port):\n        self.isproxyok = True\n        self.ip = ip\n        self.port = port",
    "last_modified": "2025-09-13T05:53:28.033763"
  },
  {
    "id": "1340",
    "name": "scrape100.py",
    "path": "03_automation_platforms/social_media_automation/scrape100.py",
    "category": "03_automation_platforms",
    "type": "web_tools",
    "lines": 16,
    "size": 365,
    "docstring": "",
    "keywords": [
      "web_tools"
    ],
    "functions": [],
    "classes": [],
    "imports": [
      "requests",
      "bs4"
    ],
    "preview": "import requests\nfrom bs4 import BeautifulSoup\n\nurl = requests.get(\"https://redditmetrics.com/top\")\n\nsoup = BeautifulSoup(url.text, \"html.parser\")\n\n\nwith open(\"sb.txt\", \"w\") as f:\n    for subreddit in soup.find_all(\"a\"):\n        try:\n            if \"/r\" in subreddit.string:\n                f.write(subreddit.string[3:] + \"\\n\")\n        except:\n            TypeError\n",
    "last_modified": "2025-05-04T23:28:25.649247"
  },
  {
    "id": "1341",
    "name": "autodownloader.py",
    "path": "03_automation_platforms/social_media_automation/autodownloader.py",
    "category": "03_automation_platforms",
    "type": "youtube",
    "lines": 67,
    "size": 2273,
    "docstring": "",
    "keywords": [
      "youtube"
    ],
    "functions": [
      "__init__",
      "startAutoMode",
      "startDownloading",
      "startFinding",
      "stop",
      "findClips",
      "downloadClips"
    ],
    "classes": [
      "AutoDownloader"
    ],
    "imports": [
      "threading",
      "time",
      "database",
      "tiktok"
    ],
    "preview": "from threading import Thread\nfrom time import sleep\n\nimport database\nimport tiktok\n\n\nclass AutoDownloader:\n    def __init__(self, window, downloadqueue):\n        self.window = window\n        self.autoDownloadQueue = downloadqueue\n        self.clipIndex = 0\n        self.auto = False\n\n    def startAutoMode(self):\n        self.auto = True\n        self.findClips()\n\n    def startDownloading(self):\n        self.downloadClips()",
    "last_modified": "2025-09-13T05:53:31.618362"
  },
  {
    "id": "1342",
    "name": "bot_block.py",
    "path": "03_automation_platforms/social_media_automation/bot_block.py",
    "category": "03_automation_platforms",
    "type": "utility",
    "lines": 67,
    "size": 2025,
    "docstring": "",
    "keywords": [],
    "functions": [
      "block",
      "unblock",
      "block_users",
      "unblock_users",
      "block_bots"
    ],
    "classes": [],
    "imports": [
      "random",
      "tqdm"
    ],
    "preview": "import random\n\nfrom tqdm import tqdm\n\n\ndef block(self, user_id):\n    user_id = self.convert_to_user_id(user_id)\n    if self.check_not_bot(user_id):\n        return True\n    if not self.reached_limit(\"blocks\"):\n        self.delay(\"block\")\n        if self.api.block(user_id):\n            self.total[\"blocks\"] += 1\n            return True\n    else:\n        self.logger.info(\"Out of blocks for today.\")\n    return False\n\n\ndef unblock(self, user_id):",
    "last_modified": "2025-09-13T05:54:57.121697"
  },
  {
    "id": "1343",
    "name": "Instagram Report Bot2.py",
    "path": "03_automation_platforms/social_media_automation/Instagram Report Bot2.py",
    "category": "03_automation_platforms",
    "type": "utility",
    "lines": 102,
    "size": 2267,
    "docstring": "",
    "keywords": [],
    "functions": [
      "getOptions"
    ],
    "classes": [],
    "imports": [
      "argparse",
      "sys",
      "time",
      "pyautogui",
      "webbot"
    ],
    "preview": "import argparse\nimport sys\nimport time\n\nimport pyautogui\nfrom webbot import *\n\n\n# To parse the arguments\ndef getOptions(args=sys.argv[1:]):\n\n    parser = argparse.ArgumentParser(\n        description=\"This bot helps users to mass report accounts with clickbaits or objectionable material.\"\n    )\n    parser.add_argument(\"-u\", \"--username\", type=str, default=\"\", help=\"Username to report.\")\n    parser.add_argument(\n        \"-f\",\n        \"--file\",\n        type=str,\n        default=\"acc.txt\",",
    "last_modified": "2025-09-13T05:53:30.312819"
  },
  {
    "id": "1344",
    "name": "reddit_scraper.py",
    "path": "03_automation_platforms/social_media_automation/reddit_scraper.py",
    "category": "03_automation_platforms",
    "type": "web_tools",
    "lines": 44,
    "size": 1182,
    "docstring": "",
    "keywords": [
      "youtube"
    ],
    "functions": [
      "download_vid",
      "reddit_scraper"
    ],
    "classes": [],
    "imports": [
      "os",
      "praw",
      "redvid",
      "config"
    ],
    "preview": "import os\n\nimport praw\nfrom redvid import Downloader\n\nimport config\n\n\ndef download_vid(url, directory):  # Download reddit vid given URL and directory\n    download = Downloader(url, max_q=True)\n    download.path = directory\n    download.download()\n    print(os.listdir(directory))\n\n\ndef reddit_scraper(subreddit):  # pulls out top reddit posts\n    print(\"Logging into Reddit...\")\n\n    red = praw.Reddit(\n        client_id=config.reddit_login[\"client_id\"],",
    "last_modified": "2025-09-11T13:27:03.985868"
  },
  {
    "id": "1345",
    "name": "art.py",
    "path": "03_automation_platforms/social_media_automation/art.py",
    "category": "03_automation_platforms",
    "type": "web_tools",
    "lines": 25,
    "size": 450,
    "docstring": "Created in 07/2020\n@Author: Paulo https://github.com/alpdias",
    "keywords": [],
    "functions": [
      "artName"
    ],
    "classes": [],
    "imports": [
      "time",
      "pyfiglet"
    ],
    "preview": "# -*- coding: utf-8 -*-\n\n\"\"\"\nCreated in 07/2020\n@Author: Paulo https://github.com/alpdias\n\"\"\"\n\n# imported libraries\nfrom time import sleep\n\nfrom pyfiglet import Figlet\n\n\ndef artName(timeSleep=0):\n    \"\"\"\n    -> function to print text in ascii art\\\n    \\n:param timeSleep: art loading time\\\n    \\n:return: ascii art\\\n    \"\"\"\n",
    "last_modified": "2025-05-04T23:28:20.977701"
  },
  {
    "id": "1346",
    "name": "ex.py",
    "path": "03_automation_platforms/social_media_automation/ex.py",
    "category": "03_automation_platforms",
    "type": "utility",
    "lines": 29,
    "size": 568,
    "docstring": "",
    "keywords": [],
    "functions": [
      "main"
    ],
    "classes": [],
    "imports": [
      "sys",
      "praw",
      "config"
    ],
    "preview": "import sys\n\nimport praw\n\nsys.path.append(\"../\")\nimport config\n\n\n# Main driver function for software\ndef main() -> int:\n\n    # Creating a reddit api instance\n    reddit = praw.Reddit(\n        client_id=config.PRAW_CONFIG[\"client_id\"],\n        client_secret=config.PRAW_CONFIG[\"client_secret\"],\n        user_agent=config.PRAW_CONFIG[\"user_agent\"],\n    )\n\n    # Looping a subreddit\n    for submission in reddit.subreddit(\"learnpython\").hot(limit=10):",
    "last_modified": "2025-05-04T23:28:21.795450"
  },
  {
    "id": "1347",
    "name": "GTTS.py",
    "path": "03_automation_platforms/social_media_automation/GTTS.py",
    "category": "03_automation_platforms",
    "type": "utility",
    "lines": 23,
    "size": 443,
    "docstring": "",
    "keywords": [],
    "functions": [
      "__init__",
      "run",
      "randomvoice"
    ],
    "classes": [
      "GTTS"
    ],
    "imports": [
      "random",
      "gtts",
      "utils"
    ],
    "preview": "import random\n\nfrom gtts import gTTS\n\nfrom utils import settings\n\n\nclass GTTS:\n    def __init__(self):\n        self.max_chars = 5000\n        self.voices = []\n\n    def run(self, text, filepath):\n        tts = gTTS(\n            text=text,\n            lang=settings.config[\"reddit\"][\"thread\"][\"post_lang\"] or \"en\",\n            slow=False,\n        )\n        tts.save(filepath)\n",
    "last_modified": "2025-09-11T13:27:01.858790"
  },
  {
    "id": "1348",
    "name": "organize_python.py",
    "path": "07_experimental/misc/organize_python.py",
    "category": "07_experimental",
    "type": "organization",
    "lines": 291,
    "size": 12765,
    "docstring": "Python Directory Organization Maintenance Script\n==============================================\n\nThis script helps maintain the organization of your Python projects directory.\nIt automatically categorizes and moves files to appropriate directories.\n\nUsage:\n    python organize_python.py [--dry-run] [--cleanup] [--report]\n\nOptions:\n    --dry-run    Show what would be moved without actually moving files\n    --cleanup    Clean up root directory by moving files to appropriate categories\n    --report     Generate a report of current organization status",
    "keywords": [
      "analysis",
      "organization"
    ],
    "functions": [
      "main",
      "__init__",
      "get_files_in_root",
      "find_target_directory",
      "create_target_directory",
      "move_file",
      "cleanup_root",
      "generate_report",
      "print_report"
    ],
    "classes": [
      "PythonOrganizer"
    ],
    "imports": [
      "os",
      "shutil",
      "argparse",
      "json",
      "pathlib",
      "datetime",
      "typing"
    ],
    "preview": "#!/usr/bin/env python3\n\"\"\"\nPython Directory Organization Maintenance Script\n==============================================\n\nThis script helps maintain the organization of your Python projects directory.\nIt automatically categorizes and moves files to appropriate directories.\n\nUsage:\n    python organize_python.py [--dry-run] [--cleanup] [--report]\n\nOptions:\n    --dry-run    Show what would be moved without actually moving files\n    --cleanup    Clean up root directory by moving files to appropriate categories\n    --report     Generate a report of current organization status\n\"\"\"\n\nimport os\nimport shutil\nimport argparse",
    "last_modified": "2025-10-09T06:54:57.774012"
  },
  {
    "id": "1349",
    "name": "simple_docs_generator.py",
    "path": "github_repo/tools/documentation/simple_docs_generator.py",
    "category": "07_experimental",
    "type": "web_tools",
    "lines": 1092,
    "size": 33794,
    "docstring": "Simple HTML Documentation Generator\nCreates a comprehensive HTML documentation website without Sphinx dependencies",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "main",
      "__init__",
      "create_directory_structure",
      "load_script_data",
      "create_css",
      "create_javascript",
      "create_index_html",
      "create_category_pages",
      "create_tutorial_pages",
      "create_readme"
    ],
    "classes": [
      "SimpleDocsGenerator"
    ],
    "imports": [
      "os",
      "json",
      "pathlib",
      "datetime"
    ],
    "preview": "#!/usr/bin/env python3\n\"\"\"\nSimple HTML Documentation Generator\nCreates a comprehensive HTML documentation website without Sphinx dependencies\n\"\"\"\n\nimport os\nimport json\nfrom pathlib import Path\nfrom datetime import datetime\n\nclass SimpleDocsGenerator:\n    def __init__(self, base_path=\"/Users/steven/Documents/python\"):\n        self.base_path = Path(base_path)\n        self.docs_path = self.base_path / \"docs\"\n        self.html_path = self.docs_path / \"html\"\n        \n    def create_directory_structure(self):\n        \"\"\"Create the documentation directory structure.\"\"\"\n        print(\"\ud83d\udcc1 Creating documentation structure...\")",
    "last_modified": "2025-10-09T06:28:52.311926"
  },
  {
    "id": "1350",
    "name": "serve_docs.py",
    "path": "github_repo/tools/documentation/serve_docs.py",
    "category": "07_experimental",
    "type": "utility",
    "lines": 49,
    "size": 1340,
    "docstring": "Simple Documentation Server\nServes the HTML documentation locally",
    "keywords": [],
    "functions": [
      "serve_documentation"
    ],
    "classes": [],
    "imports": [
      "http.server",
      "socketserver",
      "webbrowser",
      "os",
      "pathlib",
      "sys"
    ],
    "preview": "#!/usr/bin/env python3\n\"\"\"\nSimple Documentation Server\nServes the HTML documentation locally\n\"\"\"\n\nimport http.server\nimport socketserver\nimport webbrowser\nimport os\nfrom pathlib import Path\n\ndef serve_documentation(port=8000):\n    \"\"\"Serve the documentation on the specified port.\"\"\"\n    docs_path = Path(\"/Users/steven/Documents/python/docs/html\")\n    \n    if not docs_path.exists():\n        print(\"\u274c Documentation not found. Run simple_docs_generator.py first.\")\n        return\n    ",
    "last_modified": "2025-10-09T06:29:04.777885"
  },
  {
    "id": "1351",
    "name": "setup_sphinx_docs_uv.py",
    "path": "github_repo/tools/documentation/setup_sphinx_docs_uv.py",
    "category": "07_experimental",
    "type": "setup",
    "lines": 798,
    "size": 25935,
    "docstring": "Sphinx Documentation Setup Script (UV Compatible)\nCreates comprehensive documentation for all Python projects",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "main",
      "__init__",
      "check_dependencies",
      "create_directory_structure",
      "initialize_sphinx",
      "create_conf_py",
      "create_index_rst",
      "create_overview_rst",
      "create_category_pages",
      "create_api_documentation"
    ],
    "classes": [
      "SphinxDocSetup"
    ],
    "imports": [
      "os",
      "subprocess",
      "sys",
      "pathlib",
      "json"
    ],
    "preview": "#!/usr/bin/env python3\n\"\"\"\nSphinx Documentation Setup Script (UV Compatible)\nCreates comprehensive documentation for all Python projects\n\"\"\"\n\nimport os\nimport subprocess\nimport sys\nfrom pathlib import Path\nimport json\n\nclass SphinxDocSetup:\n    def __init__(self, base_path=\"/Users/steven/Documents/python\"):\n        self.base_path = Path(base_path)\n        self.docs_path = self.base_path / \"docs\"\n        self.sphinx_path = self.docs_path / \"sphinx\"\n        \n    def check_dependencies(self):\n        \"\"\"Check if required packages are installed.\"\"\"",
    "last_modified": "2025-10-09T06:26:52.989487"
  },
  {
    "id": "1352",
    "name": "create_code_browser.py",
    "path": "github_repo/tools/documentation/create_code_browser.py",
    "category": "07_experimental",
    "type": "utility",
    "lines": 1311,
    "size": 39318,
    "docstring": "Code Browser Generator\nCreates a visual code browser similar to avatararts.org/dalle.html\nDisplays Python scripts as interactive cards with code previews",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "main",
      "__init__",
      "create_directory_structure",
      "scan_python_files",
      "extract_file_metadata",
      "extract_docstring",
      "extract_imports",
      "extract_functions",
      "extract_classes",
      "determine_file_type"
    ],
    "classes": [
      "CodeBrowserGenerator"
    ],
    "imports": [
      "os",
      "json",
      "base64",
      "pathlib",
      "datetime",
      "re"
    ],
    "preview": "#!/usr/bin/env python3\n\"\"\"\nCode Browser Generator\nCreates a visual code browser similar to avatararts.org/dalle.html\nDisplays Python scripts as interactive cards with code previews\n\"\"\"\n\nimport os\nimport json\nimport base64\nfrom pathlib import Path\nfrom datetime import datetime\nimport re\n\nclass CodeBrowserGenerator:\n    def __init__(self, base_path=\"/Users/steven/Documents/python\"):\n        self.base_path = Path(base_path)\n        self.browser_path = self.base_path / \"code_browser\"\n        self.html_path = self.browser_path / \"index.html\"\n        ",
    "last_modified": "2025-10-09T06:34:34.070147"
  },
  {
    "id": "1353",
    "name": "serve_code_browser.py",
    "path": "github_repo/tools/documentation/serve_code_browser.py",
    "category": "07_experimental",
    "type": "utility",
    "lines": 49,
    "size": 1341,
    "docstring": "Code Browser Server\nServes the visual code browser locally",
    "keywords": [],
    "functions": [
      "serve_code_browser"
    ],
    "classes": [],
    "imports": [
      "http.server",
      "socketserver",
      "webbrowser",
      "os",
      "pathlib",
      "sys"
    ],
    "preview": "#!/usr/bin/env python3\n\"\"\"\nCode Browser Server\nServes the visual code browser locally\n\"\"\"\n\nimport http.server\nimport socketserver\nimport webbrowser\nimport os\nfrom pathlib import Path\n\ndef serve_code_browser(port=8001):\n    \"\"\"Serve the code browser on the specified port.\"\"\"\n    browser_path = Path(\"/Users/steven/Documents/python/code_browser\")\n    \n    if not browser_path.exists():\n        print(\"\u274c Code browser not found. Run create_code_browser.py first.\")\n        return\n    ",
    "last_modified": "2025-10-09T06:34:54.183931"
  },
  {
    "id": "1354",
    "name": "content_based_migration.py",
    "path": "github_repo/tools/analysis/content_based_migration.py",
    "category": "07_experimental",
    "type": "analysis",
    "lines": 323,
    "size": 13232,
    "docstring": "Content-Based Migration Script\nReorganizes files based on deep content analysis rather than filename patterns",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "main",
      "__init__",
      "load_analysis_results",
      "create_content_based_structure",
      "migrate_files_by_content",
      "consolidate_similar_files",
      "get_base_name",
      "consolidate_file_group",
      "create_shared_libraries",
      "generate_migration_report"
    ],
    "classes": [
      "ContentBasedMigrator"
    ],
    "imports": [
      "os",
      "re",
      "shutil",
      "json",
      "pathlib",
      "collections"
    ],
    "preview": "#!/usr/bin/env python3\n\"\"\"\nContent-Based Migration Script\nReorganizes files based on deep content analysis rather than filename patterns\n\"\"\"\n\nimport os\nimport re\nimport shutil\nimport json\nfrom pathlib import Path\nfrom collections import defaultdict\n\nclass ContentBasedMigrator:\n    def __init__(self, base_path=\"/Users/steven/Documents/python\"):\n        self.base_path = Path(base_path)\n        self.migration_log = []\n        \n    def load_analysis_results(self):\n        \"\"\"Load the content analysis results.\"\"\"",
    "last_modified": "2025-10-09T06:16:58.776702"
  },
  {
    "id": "1355",
    "name": "find_script.py",
    "path": "github_repo/tools/search/find_script.py",
    "category": "07_experimental",
    "type": "video_processing",
    "lines": 263,
    "size": 10847,
    "docstring": "Python Script Finder and Navigator\nHelps you locate any Python script in your organized structure",
    "keywords": [],
    "functions": [
      "main",
      "__init__",
      "build_index",
      "find_script",
      "find_by_functionality",
      "show_script_location",
      "show_directory_structure",
      "show_category_contents",
      "interactive_search",
      "print_tree"
    ],
    "classes": [
      "ScriptFinder"
    ],
    "imports": [
      "os",
      "re",
      "pathlib",
      "collections"
    ],
    "preview": "#!/usr/bin/env python3\n\"\"\"\nPython Script Finder and Navigator\nHelps you locate any Python script in your organized structure\n\"\"\"\n\nimport os\nimport re\nfrom pathlib import Path\nfrom collections import defaultdict\n\nclass ScriptFinder:\n    def __init__(self, base_path=\"/Users/steven/Documents/python\"):\n        self.base_path = Path(base_path)\n        self.script_index = {}\n        self.build_index()\n    \n    def build_index(self):\n        \"\"\"Build an index of all Python scripts for fast searching.\"\"\"\n        print(\"\ud83d\udd0d Building script index...\")",
    "last_modified": "2025-10-09T06:21:35.267534"
  },
  {
    "id": "1356",
    "name": "whereis.py",
    "path": "github_repo/tools/search/whereis.py",
    "category": "07_experimental",
    "type": "utility",
    "lines": 98,
    "size": 3461,
    "docstring": "Quick Script Locator - whereis.py\nSimple command-line tool to find Python scripts\nUsage: python whereis.py <script_name>",
    "keywords": [],
    "functions": [
      "find_script",
      "show_categories",
      "main"
    ],
    "classes": [],
    "imports": [
      "sys",
      "os",
      "pathlib"
    ],
    "preview": "#!/usr/bin/env python3\n\"\"\"\nQuick Script Locator - whereis.py\nSimple command-line tool to find Python scripts\nUsage: python whereis.py <script_name>\n\"\"\"\n\nimport sys\nimport os\nfrom pathlib import Path\n\ndef find_script(script_name):\n    \"\"\"Find a script by name.\"\"\"\n    base_path = Path(\"/Users/steven/Documents/python\")\n    \n    # Search for the script\n    matches = list(base_path.rglob(f\"*{script_name}*\"))\n    py_matches = [f for f in matches if f.suffix == '.py']\n    \n    if not py_matches:",
    "last_modified": "2025-10-09T06:22:21.085725"
  },
  {
    "id": "1357",
    "name": "script_map.py",
    "path": "github_repo/tools/search/script_map.py",
    "category": "07_experimental",
    "type": "utility",
    "lines": 246,
    "size": 11337,
    "docstring": "Python Script Map Generator\nCreates a comprehensive map of all Python scripts and their locations",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "main",
      "__init__",
      "generate_complete_map",
      "get_category_description",
      "get_subcategory_description",
      "map_by_functionality",
      "save_map",
      "save_human_readable_map",
      "print_quick_reference"
    ],
    "classes": [
      "ScriptMapper"
    ],
    "imports": [
      "os",
      "json",
      "pathlib",
      "collections"
    ],
    "preview": "#!/usr/bin/env python3\n\"\"\"\nPython Script Map Generator\nCreates a comprehensive map of all Python scripts and their locations\n\"\"\"\n\nimport os\nimport json\nfrom pathlib import Path\nfrom collections import defaultdict\n\nclass ScriptMapper:\n    def __init__(self, base_path=\"/Users/steven/Documents/python\"):\n        self.base_path = Path(base_path)\n        self.script_map = {}\n        self.category_map = {}\n        self.functionality_map = {}\n        \n    def generate_complete_map(self):\n        \"\"\"Generate a complete map of all Python scripts.\"\"\"",
    "last_modified": "2025-10-09T06:22:08.506094"
  },
  {
    "id": "1358",
    "name": "common_imports.py",
    "path": "github_repo/scripts/00_shared_libraries/common_imports.py",
    "category": "00_shared_libraries",
    "type": "utility",
    "lines": 24,
    "size": 640,
    "docstring": "Common imports used across projects",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [],
    "preview": "\"\"\"Common imports used across projects\"\"\"\n\n# Most frequently used imports\n# os (used in 843 files)\n# sys (used in 252 files)\n# csv (used in 219 files)\n# json (used in 202 files)\n# requests (used in 177 files)\n# time (used in 154 files)\n# load_dotenv (used in 144 files)\n# logging (used in 142 files)\n# Path (used in 130 files)\n# datetime (used in 130 files)\n# OpenAI (used in 127 files)\n# re (used in 120 files)\n# Image (used in 115 files)\n# subprocess (used in 114 files)\n# shutil (used in 100 files)\n# tqdm (used in 100 files)\n# pandas (used in 99 files)",
    "last_modified": "2025-10-09T06:17:16.281716"
  },
  {
    "id": "1359",
    "name": "utility_functions.py",
    "path": "github_repo/scripts/00_shared_libraries/utility_functions.py",
    "category": "00_shared_libraries",
    "type": "utility",
    "lines": 24,
    "size": 809,
    "docstring": "Common utility functions used across projects",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [],
    "preview": "\"\"\"Common utility functions used across projects\"\"\"\n\n# Most frequently used functions\n# __init__ (used in 234 files)\n# main (used in 182 files)\n# format_timestamp (used in 62 files)\n# analyze_text_for_section (used in 56 files)\n# transcribe_audio (used in 50 files)\n# upscale_image (used in 34 files)\n# get_authenticated_service (used in 31 files)\n# upload_video (used in 28 files)\n# process_audio_directory (used in 26 files)\n# run (used in 25 files)\n# organize_files (used in 25 files)\n# process_directory (used in 23 files)\n# process_images (used in 23 files)\n# __str__ (used in 23 files)\n# convert_and_upscale_images (used in 23 files)\n# parse_args (used in 21 files)\n# get_creation_date (used in 20 files)",
    "last_modified": "2025-10-09T06:17:16.282631"
  },
  {
    "id": "1360",
    "name": "cli.py_02.py",
    "path": "github_repo/scripts/01_core_ai_analysis/cli.py_consolidated/cli.py_02.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 32,
    "size": 856,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "cli",
      "process_image"
    ],
    "classes": [],
    "imports": [
      "click",
      "chaos_scheduler",
      "quantum_media_processor"
    ],
    "preview": "import click\n\nfrom .chaos_scheduler import ChaosScheduler\nfrom .quantum_media_processor import QuantumMediaProcessor\n\n\n@click.group()\ndef cli():\n    \"\"\"QuantumForge CLI - Where Order Meets Chaos\"\"\"\n    pass\n\n\n@cli.command()\n@click.option(\"--chaos\", default=0.07, help=\"Chaos factor (0.0-1.0)\")\n@click.argument(\"input_path\")\n@click.argument(\"output_path\")\ndef process_image(chaos, input_path, output_path):\n    \"\"\"Quantum-inspired image processing\"\"\"\n    qmp = QuantumMediaProcessor(chaos_factor=chaos)\n    scheduler = ChaosScheduler()",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1361",
    "name": "cli.py.py",
    "path": "github_repo/scripts/01_core_ai_analysis/cli.py_consolidated/cli.py.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 39,
    "size": 1197,
    "docstring": "",
    "keywords": [],
    "functions": [
      "main"
    ],
    "classes": [],
    "imports": [
      "__future__",
      "argparse",
      "json",
      "os",
      "sys",
      "pipeline"
    ],
    "preview": "from __future__ import annotations\n\nimport argparse\nimport json\nimport os\nimport sys\n\nfrom .pipeline import OpusClonePipeline\n\n\ndef main():\n    p = argparse.ArgumentParser(\n        description=\"Opus-style local video-to-shorts pipeline (open-source clone)\"\n    )\n    p.add_argument(\"--video\", required=True, help=\"Path to input video (mp4/mov/etc.)\")\n    p.add_argument(\"--out\", required=True, help=\"Output directory for clips\")\n    p.add_argument(\"--brand\", required=True, help=\"Path to brand.json template\")\n    p.add_argument(\"--k\", type=int, default=5, help=\"Number of clips to export\")\n    p.add_argument(\n        \"--whisper\",",
    "last_modified": "2025-09-11T13:27:06.797107"
  },
  {
    "id": "1362",
    "name": "audio_to_text.py",
    "path": "github_repo/scripts/01_core_ai_analysis/transcription/audio_to_text.py",
    "category": "01_core_ai_analysis",
    "type": "transcription",
    "lines": 753,
    "size": 28563,
    "docstring": "AudioToText.ipynb",
    "keywords": [
      "openai",
      "transcription"
    ],
    "functions": [
      "get_audio",
      "add_chunk",
      "raw_split",
      "write_result",
      "write_result"
    ],
    "classes": [
      "WriteText(WriteTXT)"
    ],
    "imports": [
      "subprocess",
      "sys",
      "platform",
      "io",
      "base64",
      "os.path",
      "ffmpeg",
      "numpy",
      "google.colab.output",
      "IPython.display"
    ],
    "preview": "# -*- coding: utf-8 -*-\n\"\"\"AudioToText.ipynb\n\nAutomatically generated by Colab.\n\nOriginal file is located at\n    https://colab.research.google.com/github/Carleslc/AudioToText/blob/master/AudioToText.ipynb\n\n# \ud83d\udde3\ufe0f [**AudioToText**](https://github.com/Carleslc/AudioToText)\n\n[![Donate](https://www.ko-fi.com/img/githubbutton_sm.svg)](https://ko-fi.com/carleslc)\n\n### \ud83d\udee0 [Whisper by OpenAI](https://github.com/openai/whisper)\n\n## [Step 1] \u2699\ufe0f Install the required libraries\n\nClick \u25b6\ufe0f button below to install the dependencies for this notebook.\n\"\"\"\n\n#@title { display-mode: \"form\" }",
    "last_modified": "2025-09-11T13:26:54.712610"
  },
  {
    "id": "1363",
    "name": "quiz-.py",
    "path": "github_repo/scripts/01_core_ai_analysis/transcription/quiz-.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 59,
    "size": 2128,
    "docstring": "",
    "keywords": [
      "image_processing",
      "youtube",
      "analysis",
      "web_tools",
      "openai"
    ],
    "functions": [
      "upscale_image",
      "generate_youtube_content",
      "analyze_and_generate"
    ],
    "classes": [],
    "imports": [
      "openai",
      "csv",
      "io",
      "requests",
      "PIL"
    ],
    "preview": "from openai import OpenAI\n\nclient = OpenAI(api_key='sk-acw35nWnLFMd23JUzcQoQ7QBhg4y5wLxoQJpR64ITBWHqBT7')\nimport csv\nfrom io import BytesIO\n\nimport requests\nfrom PIL import Image\n\n# Set your OpenAI API key here\n\ndef upscale_image(image_url):\n    # Fetch the image\n    response = requests.get(image_url)\n    image = Image.open(BytesIO(response.content))\n\n    # Calculate the new size, doubling the width and height\n    new_size = (image.width * 2, image.height * 2)\n\n    # Resize the image to the new size",
    "last_modified": "2025-05-04T22:47:13.043882"
  },
  {
    "id": "1364",
    "name": "tts-doc.py",
    "path": "github_repo/scripts/01_core_ai_analysis/transcription/tts-doc.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 220,
    "size": 7712,
    "docstring": "",
    "keywords": [
      "openai"
    ],
    "functions": [
      "load_home_env",
      "read_docx_text",
      "make_heading_regex",
      "slugify",
      "normalize_body",
      "extract_chapters",
      "ensure_voice",
      "openai_client",
      "synth_mp3_bytes",
      "write_jsonl"
    ],
    "classes": [],
    "imports": [
      "os",
      "re",
      "json",
      "argparse",
      "pathlib",
      "typing",
      "dotenv",
      "docx",
      "openai"
    ],
    "preview": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\nDOCX -> per-chapter MP3s using OpenAI TTS.\n- Keys from ~/.env  (OPENAI_API_KEY=sk-...)\n- Chapter detection via docx heading styles OR fallback regex for known titles\n- Voices: alloy, ash, ballad, coral, echo, fable, nova, onyx, sage, shimmer\n- Alias: cove -> sage\n\"\"\"\n\nimport os\nimport re\nimport json\nimport argparse\nfrom pathlib import Path\nfrom typing import List, Tuple, Iterable, Dict\n\n# -------- env loader (quiet) --------\ndef load_home_env():\n    try:",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1365",
    "name": "generate_screen_shot.py",
    "path": "github_repo/scripts/01_core_ai_analysis/transcription/generate_screen_shot.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 120,
    "size": 4242,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "logging",
      "os",
      "shutil",
      "time",
      "pyrogram",
      "translation",
      "helper_funcs.chat_base",
      "helper_funcs.display_progress",
      "helper_funcs.help_Nekmo_ffmpeg",
      "sample_config"
    ],
    "preview": "import logging\n\nlogging.basicConfig(\n    level=logging.DEBUG, format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\n)\nlogger = logging.getLogger(__name__)\n\nimport os\nimport shutil\nimport time\n\n# the secret configuration specific things\nif bool(os.environ.get(\"WEBHOOK\", False)):\n    from sample_config import Config\nelse:\n    from config import Config\n\n# the Strings used for this \"thing\"\nimport pyrogram\nfrom translation import Translation",
    "last_modified": "2025-09-13T05:53:44.156301"
  },
  {
    "id": "1366",
    "name": "ocr_gpt_renamer.py",
    "path": "github_repo/scripts/01_core_ai_analysis/transcription/ocr_gpt_renamer.py",
    "category": "01_core_ai_analysis",
    "type": "youtube",
    "lines": 422,
    "size": 13808,
    "docstring": "Recursive OCR + GPT-4o Renamer & Prompt Generator (Noise-Resistant)\n\n- Images: .jpg .jpeg .png .webp .tiff\n- Videos: .mp4 .webm .mov .mkv\n- Loads OPENAI_API_KEY from ~/env\n- Preview mode by default (no changes). Use --apply / -a to rename.\n- CSV (TAB-delimited): Old Path | New Path | Detected Title | File Type | Prompt\n\nmacOS setup:\n  brew install tesseract\n  pip install pillow pytesseract opencv-python python-dotenv openai\n\nRun (preview):\n  python ocr_gpt_renamer.py /path/to/folder\n\nApply:\n  python ocr_gpt_renamer.py /path/to/folder --apply",
    "keywords": [
      "openai",
      "opencv",
      "analysis"
    ],
    "functions": [
      "load_api_key_from_home_env",
      "_preprocess_for_ocr",
      "_ocr_data",
      "_clean_token",
      "_good_token",
      "_titlecase_join",
      "ocr_detect_title",
      "best_video_frame",
      "gpt_refine_and_describe",
      "process_media"
    ],
    "classes": [],
    "imports": [
      "argparse",
      "csv",
      "os",
      "re",
      "sys",
      "pathlib",
      "typing",
      "cv2",
      "openai",
      "pytesseract"
    ],
    "preview": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n\"\"\"\nRecursive OCR + GPT-4o Renamer & Prompt Generator (Noise-Resistant)\n\n- Images: .jpg .jpeg .png .webp .tiff\n- Videos: .mp4 .webm .mov .mkv\n- Loads OPENAI_API_KEY from ~/env\n- Preview mode by default (no changes). Use --apply / -a to rename.\n- CSV (TAB-delimited): Old Path | New Path | Detected Title | File Type | Prompt\n\nmacOS setup:\n  brew install tesseract\n  pip install pillow pytesseract opencv-python python-dotenv openai\n\nRun (preview):\n  python ocr_gpt_renamer.py /path/to/folder\n\nApply:",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1367",
    "name": "trek 2.py",
    "path": "github_repo/scripts/01_core_ai_analysis/transcription/trek 2.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 58,
    "size": 2102,
    "docstring": "",
    "keywords": [
      "analysis",
      "web_tools"
    ],
    "functions": [
      "generate_speech",
      "process_csv_and_generate_speech"
    ],
    "classes": [],
    "imports": [
      "csv",
      "os",
      "requests"
    ],
    "preview": "import csv\nimport os\n\nimport requests  # Make sure to install this package\n\n\ndef generate_speech(input_text, output_path, api_key):\n    \"\"\"\n    Generates speech from text using an API (e.g., OpenAI's text-to-speech API).\n    \"\"\"\n    url = \"https://api.openai.com/v1/audio/speech\"\n    headers = {\n        \"Authorization\": f\"Bearer {api_key}\",\n        \"Content-Type\": \"application/json\",\n    }\n    data = {\n        \"model\": \"tts-1\",  # Update this according to the specific model you're using\n        \"input\": input_text,\n        \"voice\": \"shimmer\",  # Update the voice parameter as needed\n    }",
    "last_modified": "2025-09-13T05:55:27.236819"
  },
  {
    "id": "1368",
    "name": "pythoncatorgize.py",
    "path": "github_repo/scripts/01_core_ai_analysis/transcription/pythoncatorgize.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 60,
    "size": 1966,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis"
    ],
    "functions": [
      "get_openai_category",
      "create_directory",
      "save_to_category",
      "categorize_files"
    ],
    "classes": [],
    "imports": [
      "os",
      "openai"
    ],
    "preview": "import os\n\nfrom openai import OpenAI\n\nclient = OpenAI(api_key=api_key)\n\n# Load the API key from the environment variable\napi_key = os.getenv(\"OPENAI_API_KEY\")\nif not api_key:\n    raise ValueError(\"API key not found. Please ensure it is set in your environment variables.\")\n\n\ndef get_openai_category(script_content):\n    try:\n        response = client.chat.completions.create(\n            model=\"gpt-3.5-turbo\",\n            messages=[\n                {\"role\": \"system\", \"content\": \"You are an expert Python programmer.\"},\n                {\n                    \"role\": \"user\",",
    "last_modified": "2025-09-13T05:55:12.859828"
  },
  {
    "id": "1369",
    "name": "song-process.py",
    "path": "github_repo/scripts/01_core_ai_analysis/transcription/song-process.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 179,
    "size": 6943,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis",
      "transcription"
    ],
    "functions": [
      "format_timestamp",
      "transcribe_audio",
      "analyze_text_for_section",
      "process_audio_file",
      "process_audio_directory"
    ],
    "classes": [],
    "imports": [
      "csv",
      "os",
      "sys",
      "dotenv",
      "openai",
      "termcolor"
    ],
    "preview": "import csv\nimport os\nimport sys\n\nfrom dotenv import load_dotenv\nfrom openai import OpenAI\nfrom termcolor import colored\n\n# Load environment variables from .env file\nload_dotenv(dotenv_path=\"/Users/steven/.env\")\n\n# Get API key and validate it\napi_key = os.getenv(\"OPENAI_API_KEY\")\nif not api_key:\n    raise EnvironmentError(\"\u274c OpenAI API key not found. Please check your .env file.\")\n\n# Initialize OpenAI client\nclient = OpenAI(api_key=api_key)\n\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1370",
    "name": "import.py",
    "path": "github_repo/scripts/01_core_ai_analysis/transcription/import.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 66,
    "size": 2512,
    "docstring": "",
    "keywords": [
      "data_processing",
      "openai",
      "analysis"
    ],
    "functions": [
      "generate_clickable_title",
      "generate_description",
      "generate_tags"
    ],
    "classes": [],
    "imports": [
      "pandas",
      "openai"
    ],
    "preview": "import pandas as pd\nfrom openai import OpenAI\n\nclient = OpenAI(api_key='your_openai_api_key')\n\n# Set your OpenAI API key\n\n Function to generate a clickable title\ndef generate_clickable_title(detail):\n    prompt = f\"Generate a catchy and clickable title for a Cork Back Coaster with the theme: '{detail}'. Maximum 50 characters. At the end of each title write Cork Back Coaster\"\n    response = client.chat.completions.create(model=\"gpt-3.5-turbo\", messages=[{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, {\"role\": \"user\", \"content\": prompt}])\n    clickable_title = response.choices[0].message.content.strip()\n    clickable_title = clickable_title.replace('\"', '')  # Remove double quotes\n    return clickable_title\n\n# Function to generate a description\ndef generate_description(detail):\n    prompt = f\"'{detail}'. Maximum 150 characters.\"\n    response = client.chat.completions.create(model=\"gpt-3.5-turbo\", messages=[{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, {\"role\": \"user\", \"content\": prompt}])\n    description = response.choices[0].message.content.strip()",
    "last_modified": "2025-05-04T22:47:11.601196"
  },
  {
    "id": "1371",
    "name": "analyze-mp3-transcript-prompts.py",
    "path": "github_repo/scripts/01_core_ai_analysis/transcription/analyze-mp3-transcript-prompts.py",
    "category": "01_core_ai_analysis",
    "type": "analysis",
    "lines": 110,
    "size": 4649,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis",
      "transcription"
    ],
    "functions": [
      "transcribe_audio",
      "format_timestamp",
      "analyze_text_for_section",
      "process_audio_directory"
    ],
    "classes": [],
    "imports": [
      "openai",
      "os",
      "dotenv"
    ],
    "preview": "from openai import OpenAI\n\nclient = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\nimport os\n\nfrom dotenv import load_dotenv\n\n# Load environment variables from .env (make sure your OPENAI_API_KEY is stored here)\nenv_path = \"/Users/steven/.env\"\nload_dotenv(dotenv_path=env_path)\n\n# Directory pathsp\nAUDIO_DIR = \"/Users/steven/Music/NocTurnE-meLoDieS/mp3\"  # Directory containing MP3 files\nTRANSCRIPT_DIR = \"/Users/steven/Music/NocTurnE-meLoDieS/song-video/analysis-transcript\"  # Directory to save transcripts\nANALYSIS_DIR = \"/Users/steven/Music/NocTurnE-meLoDieS/song-video/analysis-transcript\"  # Directory to save the analysis files\n\n# Create output directories if they don't exist\nos.makedirs(TRANSCRIPT_DIR, exist_ok=True)\nos.makedirs(ANALYSIS_DIR, exist_ok=True)\n",
    "last_modified": "2025-09-13T05:53:54.886407"
  },
  {
    "id": "1372",
    "name": "analyze-prompt-1.py",
    "path": "github_repo/scripts/01_core_ai_analysis/transcription/analyze-prompt-1.py",
    "category": "01_core_ai_analysis",
    "type": "analysis",
    "lines": 97,
    "size": 3941,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis",
      "transcription"
    ],
    "functions": [
      "transcribe_audio",
      "format_timestamp",
      "analyze_text_for_section",
      "process_audio_directory"
    ],
    "classes": [],
    "imports": [
      "openai",
      "os",
      "subprocess",
      "dotenv"
    ],
    "preview": "from openai import OpenAI\n\nclient = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\nimport os\nimport subprocess\n\nfrom dotenv import load_dotenv\n\n# Load environment variables from .env (make sure your OPENAI_API_KEY is stored here)\nenv_path = \"/Users/steven/.env\"\nload_dotenv(dotenv_path=env_path)\n\n# Directory paths\nAUDIO_DIR = \"/Users/steven/Music/shadow\"  # Directory containing MP3 files\nTRANSCRIPT_DIR = \"/Users/steven/Music/shadow/transcript\"  # Directory to save transcripts\nANALYSIS_DIR = \"/Users/steven/Music/shadow/analysis\"  # Directory to save the analysis files\n\n# Create output directories if they don't exist\nos.makedirs(TRANSCRIPT_DIR, exist_ok=True)\nos.makedirs(ANALYSIS_DIR, exist_ok=True)",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1373",
    "name": "docks.py",
    "path": "github_repo/scripts/01_core_ai_analysis/transcription/docks.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 115,
    "size": 3160,
    "docstring": "",
    "keywords": [],
    "functions": [
      "is_excluded",
      "get_category",
      "scan_directory",
      "save_to_csv"
    ],
    "classes": [],
    "imports": [
      "csv",
      "os",
      "re",
      "datetime"
    ],
    "preview": "# docs.py\nimport csv\nimport os\nimport re\nfrom datetime import datetime\n\n# === CONFIGURATION ===\nBASE_DIRECTORIES = [\n    \"/Users/steven/Documents/Python_backup\",\n    \"/Users/steven/Documents/Python\",\n    \"/Users/steven/Music/nocTurneMeLoDieS/lyrics-keys-indo\",\n    \"/Users/steven/Music/nocTurneMeLoDieS/mp3-analyze-transcribe\",\n]\n\n# === EXCLUDED PATTERNS (regex) ===\nEXCLUDED_PATTERNS = [\n    r\"\\/.?venv\\/\",\n    r\"\\/lib\\/\",\n    r\"\\/my_global_venv\\/\",\n    r\"\\/simplegallery\\/\",",
    "last_modified": "2025-09-13T05:53:47.869781"
  },
  {
    "id": "1374",
    "name": "batch_image_seo_pipeline_20250530220524.py",
    "path": "github_repo/scripts/01_core_ai_analysis/transcription/batch_image_seo_pipeline_20250530220524.py",
    "category": "01_core_ai_analysis",
    "type": "image_processing",
    "lines": 409,
    "size": 16734,
    "docstring": "",
    "keywords": [
      "openai"
    ],
    "functions": [
      "load_openai_client",
      "retry_on_api_error",
      "call_gpt4o",
      "get_image_tech_meta",
      "build_source_tag",
      "discover_images",
      "build_gpt_messages",
      "parse_args",
      "main"
    ],
    "classes": [],
    "imports": [
      "argparse",
      "csv",
      "json",
      "logging",
      "os",
      "sys",
      "time",
      "datetime",
      "pathlib",
      "typing"
    ],
    "preview": "import argparse\nimport csv\nimport json\nimport logging\nimport os\nimport sys\nimport time\nfrom datetime import datetime\nfrom pathlib import Path\nfrom typing import Any, Dict, List, Optional\n\nimport backoff  # pip install backoff\nfrom dotenv import load_dotenv\nfrom openai import OpenAI, OpenAIAPIError\nfrom PIL import Image, UnidentifiedImageError\nfrom tqdm import tqdm\n\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n# CONFIGURATION\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500",
    "last_modified": "2025-09-13T05:53:26.777321"
  },
  {
    "id": "1375",
    "name": "generate_speech 2.py",
    "path": "github_repo/scripts/01_core_ai_analysis/transcription/generate_speech 2.py",
    "category": "01_core_ai_analysis",
    "type": "transcription",
    "lines": 46,
    "size": 1449,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis"
    ],
    "functions": [
      "generate_speech",
      "main"
    ],
    "classes": [],
    "imports": [
      "csv",
      "openai"
    ],
    "preview": "import csv\n\nfrom openai import OpenAI\n\nclient = OpenAI(api_key='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')\n\n# Replace 'your_api_key_here' with your actual OpenAI API key\n\n\ndef generate_speech(text, voice='shimmer', output_path='speech.mp3'):\n    response = client.audio.create(model=\"tts-1-hd\",  # Ensure this model supports the audio creation\n                                   input=text,\n                                   voice=voice,\n                                   format=\"mp3\")\n    with open(output_path, \"wb\") as file:\n        file.write(response.content)\n\n\ndef main():\n    # Update this path to where your CSV is located",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1376",
    "name": "political-analysist-prompter.py",
    "path": "github_repo/scripts/01_core_ai_analysis/transcription/political-analysist-prompter.py",
    "category": "01_core_ai_analysis",
    "type": "analysis",
    "lines": 123,
    "size": 5917,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis",
      "transcription"
    ],
    "functions": [
      "format_timestamp",
      "transcribe_media",
      "analyze_text",
      "analyze_image",
      "process_media"
    ],
    "classes": [
      "Config"
    ],
    "imports": [
      "os",
      "argparse",
      "pathlib",
      "dotenv",
      "openai",
      "concurrent.futures",
      "logging",
      "base64",
      "subprocess"
    ],
    "preview": "import os\nimport argparse\nfrom pathlib import Path\nfrom dotenv import load_dotenv\nfrom openai import OpenAI\nfrom concurrent.futures import ThreadPoolExecutor\nimport logging\nimport base64\nimport subprocess\n\n# Config class with updated prompt for Christian Nationalism\nclass Config:\n    MODEL = \"gpt-4o\"\n    MAX_TOKENS = 1500\n    TEMPERATURE = 0.7\n    MAX_ATTEMPTS = 3\n    PROMPT_TYPE = \"christian_nationalism\"  # New prompt type\n\n# Setup logging\nlogging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")",
    "last_modified": "2025-10-09T02:38:03.660405"
  },
  {
    "id": "1377",
    "name": "vid-mp3-transcribe-analyze.py",
    "path": "github_repo/scripts/01_core_ai_analysis/transcription/vid-mp3-transcribe-analyze.py",
    "category": "01_core_ai_analysis",
    "type": "transcription",
    "lines": 137,
    "size": 6455,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis",
      "transcription"
    ],
    "functions": [
      "split_video_to_segments",
      "transcribe_video_segment",
      "format_timestamp",
      "analyze_text_for_section",
      "process_video_by_section"
    ],
    "classes": [],
    "imports": [
      "os",
      "subprocess",
      "openai",
      "dotenv",
      "sys"
    ],
    "preview": "import os\nimport subprocess\n\nimport openai\nfrom dotenv import load_dotenv\n\n# Load environment variables from .env (make sure your OPENAI_API_KEY is stored here)\nload_dotenv()\nopenai.api_key = os.getenv(\"OPENAI_API_KEY\")\n\n# Directory paths\nVIDEO_DIR = \"/Users/steven/Movies/poject2025/Media\"  # Directory containing MP4 files\nTRANSCRIPT_DIR = (\n    \"/Users/steven/Movies/poject2025/Media/Mp4/transcript\"  # Directory to save transcripts\n)\nANALYSIS_DIR = (\n    \"/Users/steven/Movies/poject2025/Media/Mp4/analysis\"  # Directory to save the analysis files\n)\n\n# Create output directories if they don't exist",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1378",
    "name": "mp4-transcript.py",
    "path": "github_repo/scripts/01_core_ai_analysis/transcription/mp4-transcript.py",
    "category": "01_core_ai_analysis",
    "type": "video_processing",
    "lines": 134,
    "size": 4974,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis",
      "transcription"
    ],
    "functions": [
      "split_video_to_segments",
      "transcribe_video_segment",
      "format_timestamp",
      "analyze_text_for_section",
      "process_video_by_section"
    ],
    "classes": [],
    "imports": [
      "openai",
      "os",
      "subprocess",
      "dotenv",
      "sys"
    ],
    "preview": "from openai import OpenAI\n\nclient = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\nimport os\nimport subprocess\n\nfrom dotenv import load_dotenv\n\n# Load environment variables from .env (make sure your OPENAI_API_KEY is stored here)\n# Load environment variables from .env (make sure your OPENAI_API_KEY is stored here)\nenv_path = \"/Users/steven/.env\"\nload_dotenv(dotenv_path=env_path)\n\n# Directory paths\nVIDEO_DIR = \"/Users/steven/Movies/2025/mp4\"  # Directory containing MP4 files\nTRANSCRIPT_DIR = \"/Users/steven/Movies/2025/mp4/transcribe\"  # Directory to save transcripts\nANALYSIS_DIR = \"/Users/steven/Movies/2025/mp4/analysis\"  # Directory to save the analysis files\n\n# Create output directories if they don't exist\nos.makedirs(TRANSCRIPT_DIR, exist_ok=True)",
    "last_modified": "2025-09-13T05:53:55.342653"
  },
  {
    "id": "1379",
    "name": "FFMpegRoBot.py",
    "path": "github_repo/scripts/01_core_ai_analysis/transcription/FFMpegRoBot.py",
    "category": "01_core_ai_analysis",
    "type": "video_processing",
    "lines": 258,
    "size": 9667,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "logging",
      "os",
      "time",
      "pyrogram",
      "translation",
      "hachoir.metadata",
      "hachoir.parser",
      "helper_funcs.chat_base",
      "helper_funcs.display_progress",
      "helper_funcs.help_Nekmo_ffmpeg"
    ],
    "preview": "import logging\n\nlogging.basicConfig(\n    level=logging.DEBUG, format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\n)\nlogger = logging.getLogger(__name__)\n\nimport os\nimport time\n\n# the secret configuration specific things\nif bool(os.environ.get(\"WEBHOOK\", False)):\n    from sample_config import Config\nelse:\n    from config import Config\n\n# the Strings used for this \"thing\"\nimport pyrogram\nfrom translation import Translation\n",
    "last_modified": "2025-09-13T05:53:43.583647"
  },
  {
    "id": "1380",
    "name": "prompt_analyzer.py",
    "path": "github_repo/scripts/01_core_ai_analysis/transcription/prompt_analyzer.py",
    "category": "01_core_ai_analysis",
    "type": "analysis",
    "lines": 103,
    "size": 5483,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis",
      "transcription"
    ],
    "functions": [
      "transcribe_audio",
      "format_timestamp",
      "analyze_text_for_section",
      "process_audio_directory"
    ],
    "classes": [],
    "imports": [
      "openai",
      "os",
      "subprocess",
      "shared.config"
    ],
    "preview": "from openai import OpenAI\n\nclient = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\nimport os\nimport subprocess\n\nfrom shared.config import *\n\n# Load environment variables from .env (make sure your OPENAI_API_KEY is stored here)\nenv_path = \"/Users/steven/.env\"\nload_dotenv(dotenv_path=env_path)\n\n# Directory paths\nAUDIO_DIR = (\n    \"/Users/steven/Music/NocTurnE-meLoDieS/Discography/Audio\"  # Directory containing MP3 files\n)\nTRANSCRIPT_DIR = \"/Users/steven/Music/NocTurnE-meLoDieS/Discography/Audio/transcript\"  # Directory to save transcripts\nANALYSIS_DIR = \"/Users/steven/Music/NocTurnE-meLoDieS/Discography/Audio/analysis\"  # Directory to save the analysis files\n\n# Create output directories if they don't exist",
    "last_modified": "2025-10-09T05:27:15.575895"
  },
  {
    "id": "1381",
    "name": "batch_image_seo_pipeline_20250530221037.py",
    "path": "github_repo/scripts/01_core_ai_analysis/transcription/batch_image_seo_pipeline_20250530221037.py",
    "category": "01_core_ai_analysis",
    "type": "analysis",
    "lines": 428,
    "size": 17474,
    "docstring": "batch_image_seo_pipeline.py\n\n1) Discovers all image files under a given folder.\n2) Extracts technical metadata via Pillow.\n3) Calls GPT-4o Vision (multimodal) using a \u201csystem\u201d + \u201cuser\u201d prompt.\n4) Parses the JSON response into structured fields:\n   main_subject, style, color_palette, tags, orientation, suggested_products,\n   SEO_title, SEO_description, emotion, safety_rating, dominant_keyword.\n5) Builds a \u201csource\u201d tag: folder-subfolder-YYYYMMDD.\n6) Applies a \u201cTop 5% SEO Analytics\u201d framework to generate SEO columns:\n   \u2013 SEO Keywords, Traffic Source, CRO Tactic, Backlink Source, Engagement Rate.\n7) Adds niche-specific design prompts (Geeky, Dark Humor, Anime).\n8) Writes everything into a final CSV for bulk upload.\n\nDependencies:\n    pip install openai python-dotenv pillow tqdm backoff",
    "keywords": [
      "openai"
    ],
    "functions": [
      "load_openai_client",
      "retry_on_api_error",
      "call_gpt4o",
      "get_image_tech_meta",
      "build_source_tag",
      "discover_images",
      "build_gpt_messages",
      "parse_args",
      "main"
    ],
    "classes": [],
    "imports": [
      "argparse",
      "csv",
      "json",
      "logging",
      "os",
      "sys",
      "datetime",
      "pathlib",
      "typing",
      "backoff"
    ],
    "preview": "#!/usr/bin/env python3\n\"\"\"\nbatch_image_seo_pipeline.py\n\n1) Discovers all image files under a given folder.\n2) Extracts technical metadata via Pillow.\n3) Calls GPT-4o Vision (multimodal) using a \u201csystem\u201d + \u201cuser\u201d prompt.\n4) Parses the JSON response into structured fields:\n   main_subject, style, color_palette, tags, orientation, suggested_products,\n   SEO_title, SEO_description, emotion, safety_rating, dominant_keyword.\n5) Builds a \u201csource\u201d tag: folder-subfolder-YYYYMMDD.\n6) Applies a \u201cTop 5% SEO Analytics\u201d framework to generate SEO columns:\n   \u2013 SEO Keywords, Traffic Source, CRO Tactic, Backlink Source, Engagement Rate.\n7) Adds niche-specific design prompts (Geeky, Dark Humor, Anime).\n8) Writes everything into a final CSV for bulk upload.\n\nDependencies:\n    pip install openai python-dotenv pillow tqdm backoff\n\"\"\"\n",
    "last_modified": "2025-09-13T05:53:25.697217"
  },
  {
    "id": "1382",
    "name": "analyze 1 1.py",
    "path": "github_repo/scripts/01_core_ai_analysis/transcription/analyze 1 1.py",
    "category": "01_core_ai_analysis",
    "type": "analysis",
    "lines": 60,
    "size": 3440,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis"
    ],
    "functions": [
      "analyze_text"
    ],
    "classes": [],
    "imports": [
      "dotenv",
      "openai",
      "os"
    ],
    "preview": "from dotenv import load_dotenv\nfrom openai import OpenAI\n\nclient = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\nimport os\n\n# Load environment variables from .env\nenv_path = os.path.expanduser(\"~/.env\")\n# Update the path if needed\nload_dotenv(dotenv_path=env_path)\n\n# Error checking for openai API key\nif not openai.api_key:\n    raise EnvironmentError(\"openai API key not found. Please check your .env file.\")\n\n\ndef analyze_text(text):\n    response = client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        messages=[",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1383",
    "name": "batch_process.py",
    "path": "github_repo/scripts/01_core_ai_analysis/transcription/batch_process.py",
    "category": "01_core_ai_analysis",
    "type": "transcription",
    "lines": 78,
    "size": 2242,
    "docstring": "Batch processing script for multiple audio/video files",
    "keywords": [],
    "functions": [
      "main"
    ],
    "classes": [],
    "imports": [
      "os",
      "sys",
      "glob",
      "pathlib",
      "transcription_analyzer",
      "dotenv",
      "os"
    ],
    "preview": "#!/usr/bin/env python3\n\"\"\"\nBatch processing script for multiple audio/video files\n\"\"\"\n\nimport os\nimport sys\nimport glob\nfrom pathlib import Path\nfrom transcription_analyzer import TranscriptionAnalyzer\nfrom dotenv import load_dotenv\nimport os\n\n# Load environment variables from ~/.env\nload_dotenv(os.path.expanduser(\"~/.env\"))\n\ndef main():\n    \"\"\"Process multiple files in a directory.\"\"\"\n    # Check for OpenAI API key\n    api_key = os.getenv('OPENAI_API_KEY')",
    "last_modified": "2025-10-09T05:14:19.003548"
  },
  {
    "id": "1384",
    "name": "test_setup.py",
    "path": "github_repo/scripts/01_core_ai_analysis/transcription/test_setup.py",
    "category": "01_core_ai_analysis",
    "type": "analysis",
    "lines": 141,
    "size": 4254,
    "docstring": "Test script to verify the transcription analyzer setup",
    "keywords": [
      "openai",
      "testing"
    ],
    "functions": [
      "test_imports",
      "test_environment",
      "test_ffmpeg",
      "test_whisper_model",
      "main"
    ],
    "classes": [],
    "imports": [
      "os",
      "sys",
      "pathlib",
      "dotenv",
      "subprocess",
      "whisper",
      "moviepy",
      "openai",
      "dotenv",
      "whisper"
    ],
    "preview": "#!/usr/bin/env python3\n\"\"\"\nTest script to verify the transcription analyzer setup\n\"\"\"\n\nimport os\nimport sys\nfrom pathlib import Path\n\ndef test_imports():\n    \"\"\"Test if all required modules can be imported.\"\"\"\n    print(\"Testing imports...\")\n    \n    try:\n        import whisper\n        print(\"\u2705 whisper imported successfully\")\n    except ImportError as e:\n        print(f\"\u274c whisper import failed: {e}\")\n        return False\n    ",
    "last_modified": "2025-10-09T05:14:25.392980"
  },
  {
    "id": "1385",
    "name": "analyze-mp4s copy.py",
    "path": "github_repo/scripts/01_core_ai_analysis/transcription/analyze-mp4s copy.py",
    "category": "01_core_ai_analysis",
    "type": "analysis",
    "lines": 114,
    "size": 5693,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis",
      "transcription"
    ],
    "functions": [
      "format_timestamp",
      "transcribe_audio",
      "analyze_text_for_section",
      "process_audio_directory"
    ],
    "classes": [],
    "imports": [
      "logging",
      "os",
      "openai",
      "dotenv",
      "termcolor",
      "tqdm"
    ],
    "preview": "import logging\nimport os\n\nimport openai\nfrom dotenv import load_dotenv\nfrom termcolor import colored\nfrom tqdm import tqdm\n\n# Load environment variables from .env\nenv_path = os.path.expanduser(\"~/.env\")\nload_dotenv(dotenv_path=env_path)\n\n# Set OpenAI API key\nopenai.api_key = os.getenv(\"OPENAI_API_KEY\")\nif not openai.api_key:\n    raise EnvironmentError(\"OpenAI API key not found. Please check your .env file.\")\n\n\ndef format_timestamp(seconds):\n    minutes = int(seconds // 60)",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1386",
    "name": "CreateVideo.py",
    "path": "github_repo/scripts/01_core_ai_analysis/transcription/CreateVideo.py",
    "category": "01_core_ai_analysis",
    "type": "youtube",
    "lines": 55,
    "size": 1864,
    "docstring": "",
    "keywords": [],
    "functions": [
      "add_captions"
    ],
    "classes": [],
    "imports": [
      "os",
      "random",
      "moviepy.editor",
      "moviepy.audio.AudioClip",
      "moviepy.audio.io.AudioFileClip"
    ],
    "preview": "import os\nimport random\n\nimport moviepy.editor as mp\nfrom moviepy.audio.AudioClip import CompositeAudioClip, concatenate_audioclips\nfrom moviepy.audio.io.AudioFileClip import AudioFileClip\n\n# texts = [\"FACT 1 ABOUT PSICOLOGY\", \"FACT 2 ABOUT PSICOLOGY\", \"FACT 3 ABOUT PSICOLOGY\"]\ntexts = [\n    \"1. Psicology is the science of the mind and behavior, exploring how our experiences shape our thoughts, feelings, and behaviors.\",\n    \"2. It looks at how our biology, environment, and culture shape our mental and emotional states.\",\n    \"3. Psicology can help us understand ourselves and others\",\n]\n\n\ndef add_captions(texts):\n    video = mp.VideoFileClip(\"./background/test.mp4\")\n    height = int(video.w * 16 / 9)\n    top_crop = int((video.h - height) / 2)\n    bottom_crop = video.h - height - top_crop",
    "last_modified": "2025-05-04T22:47:11"
  },
  {
    "id": "1387",
    "name": "cover.py",
    "path": "github_repo/scripts/01_core_ai_analysis/transcription/cover.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 51,
    "size": 1566,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis",
      "web_tools"
    ],
    "functions": [
      "generate_cover_image_with_dalle",
      "convert_mp3_to_mp4",
      "process_directory"
    ],
    "classes": [],
    "imports": [
      "openai",
      "glob",
      "os",
      "io",
      "requests",
      "moviepy.editor",
      "PIL",
      "sys"
    ],
    "preview": "from openai import OpenAI\n\nclient = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\nimport glob\nimport os\nfrom io import BytesIO\n\nimport requests\nfrom moviepy.editor import AudioFileClip, ImageClip\nfrom PIL import Image\n\n\ndef generate_cover_image_with_dalle(file_name, output_path):\n    prompt = f\"lets create a series of typography cover image for '{file_name}' in the Font and style and contexts to tell the story'\"\n    response = client.images.generate(prompt=prompt, n=1, size=\"1024x1024\")\n    image_url = response.data[0].url\n    response = requests.get(image_url)\n    img = Image.open(BytesIO(response.content))\n    img.save(output_path)\n",
    "last_modified": "2025-05-04T22:47:12.619976"
  },
  {
    "id": "1388",
    "name": "speak.py",
    "path": "github_repo/scripts/01_core_ai_analysis/transcription/speak.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 46,
    "size": 1599,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis"
    ],
    "functions": [
      "generate_speech",
      "main"
    ],
    "classes": [],
    "imports": [
      "csv",
      "openai"
    ],
    "preview": "import csv\n\nimport openai\n\n# Initialize the OpenAI client\nclient = openai()\n\n\ndef generate_speech(text, pause_duration=\"5s\", voice=\"shimmer\", output_path=\"speech.mp3\"):\n    # Adding a pause using the SSML <break> tag\n    # Assuming that 'text' contains something like \"Option 3: <your text>\"\n    # and you want to insert a pause right after this before continuing with the answer\n    modified_text = text.replace(\"Option 3:\", f'Option 3:\"/>')\n\n    response = OpenAI.Audio.create(\n        model=\"tts-1-hd\",  # Ensure this model supports the audio creation\n        input=modified_text,\n        voice=voice,\n        format=\"mp3\",\n        ssml=True,  # Indicate that the input text contains SSML",
    "last_modified": "2025-09-13T05:53:51.117240"
  },
  {
    "id": "1389",
    "name": "GenerateTexts.py",
    "path": "github_repo/scripts/01_core_ai_analysis/transcription/GenerateTexts.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 21,
    "size": 548,
    "docstring": "",
    "keywords": [
      "openai"
    ],
    "functions": [
      "get_facts"
    ],
    "classes": [],
    "imports": [
      "openai"
    ],
    "preview": "from openai import OpenAI\n\nclient = OpenAI(api_key=\"[[Insert openai api key]]\")\n\n\ndef get_facts(topic):\n\n    response = client.completions.create(\n        model=\"text-davinci-003\",\n        prompt=\"Create three tik-tok like plain text for curious facts about \"\n        + topic\n        + \" separated by enters without text before or after\",\n        temperature=0.5,\n        max_tokens=150,\n        top_p=1.0,\n        frequency_penalty=0.0,\n        presence_penalty=0.0,\n    )\n    facts = response.choices[0].text.strip().split(\"\\n\")\n    return facts",
    "last_modified": "2025-05-04T22:47:11"
  },
  {
    "id": "1390",
    "name": "speek.py",
    "path": "github_repo/scripts/01_core_ai_analysis/transcription/speek.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 15,
    "size": 320,
    "docstring": "",
    "keywords": [
      "openai"
    ],
    "functions": [],
    "classes": [],
    "imports": [
      "pathlib",
      "openai"
    ],
    "preview": "from pathlib import Path\n\nfrom openai import OpenAI\n\nclient = OpenAI()\n\nspeech_file_path = Path(__file__).parent / \"speech.mp3\"\nresponse = client.audio.speech.create(\n    model=\"tts-1\",\n    voice=\"alloy\",\n    input=\"Today is a wonderful day to build something people love!\",\n)\n\nresponse.stream_to_file(speech_file_path)\n",
    "last_modified": "2025-05-04T22:47:13.052330"
  },
  {
    "id": "1391",
    "name": "transcription_analyzer.py",
    "path": "github_repo/scripts/01_core_ai_analysis/transcription/transcription_analyzer.py",
    "category": "01_core_ai_analysis",
    "type": "transcription",
    "lines": 391,
    "size": 17249,
    "docstring": "Audio/Video Transcription and Analysis Tool\nConverts MP4 to MP3, transcribes with timestamps, and provides GPT-4o analysis",
    "keywords": [
      "openai",
      "analysis",
      "transcription"
    ],
    "functions": [
      "main",
      "__init__",
      "convert_mp4_to_mp3",
      "transcribe_audio",
      "analyze_transcript",
      "_format_timestamp",
      "create_output_structure",
      "process_file",
      "_process_short_file",
      "_process_long_file"
    ],
    "classes": [
      "TranscriptionAnalyzer"
    ],
    "imports": [
      "os",
      "sys",
      "json",
      "logging",
      "pathlib",
      "datetime",
      "typing",
      "whisper",
      "moviepy.editor",
      "openai"
    ],
    "preview": "#!/usr/bin/env python3\n\"\"\"\nAudio/Video Transcription and Analysis Tool\nConverts MP4 to MP3, transcribes with timestamps, and provides GPT-4o analysis\n\"\"\"\n\nimport os\nimport sys\nimport json\nimport logging\nfrom pathlib import Path\nfrom datetime import datetime\nfrom typing import Optional, Dict, Any\n\nimport whisper\nfrom moviepy.editor import VideoFileClip\nfrom openai import OpenAI\nfrom dotenv import load_dotenv\nfrom audio_chunker import AudioChunker\nimport config",
    "last_modified": "2025-10-09T05:17:27.383917"
  },
  {
    "id": "1392",
    "name": "Quiz22sec 2.py",
    "path": "github_repo/scripts/01_core_ai_analysis/transcription/Quiz22sec 2.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 100,
    "size": 3342,
    "docstring": "",
    "keywords": [
      "analysis",
      "web_tools"
    ],
    "functions": [
      "generate_speech",
      "calculate_text_duration",
      "process_csv_and_generate_speech"
    ],
    "classes": [],
    "imports": [
      "csv",
      "os",
      "requests",
      "dotenv",
      "pydub"
    ],
    "preview": "import csv\nimport os\n\nimport requests  # Install using `pip install requests`\nfrom dotenv import load_dotenv\nfrom pydub import AudioSegment  # Install using `pip install pydub`\n\n\ndef generate_speech(input_text, output_path, api_key):\n    \"\"\"\n    Generates speech from text using OpenAI's text-to-speech API.\n    \"\"\"\n    url = \"https://api.openai.com/v1/audio/speech\"\n    headers = {\n        \"Authorization\": f\"Bearer {api_key}\",\n        \"Content-Type\": \"application/json\",\n    }\n    data = {\n        \"model\": \"tts-1\",\n        \"input\": input_text,",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1393",
    "name": "analyze_4.py",
    "path": "github_repo/scripts/01_core_ai_analysis/transcription/analyze_4.py",
    "category": "01_core_ai_analysis",
    "type": "analysis",
    "lines": 110,
    "size": 5586,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis",
      "transcription"
    ],
    "functions": [
      "transcribe_audio",
      "format_timestamp",
      "analyze_text_for_section",
      "process_media_directory"
    ],
    "classes": [],
    "imports": [
      "os",
      "logging",
      "dotenv",
      "openai"
    ],
    "preview": "import os\nimport logging\nfrom dotenv import load_dotenv\nfrom openai import OpenAI, Audio  # Import the correct module for audio processing\n\n# Initialize OpenAI client\nclient = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\naudio_client = Audio(client)  # Initialize the audio client\n\n# Load environment variables\nenv_path = \"~/.env\"\nload_dotenv(dotenv_path=env_path)\n\n# Prompt for base directory\nbase_dir_input = input(\"Enter base directory for media files (leave blank for current): \").strip()\nMEDIA_DIR = base_dir_input or os.getcwd()\n\n# Output directories\nTRANSCRIPT_DIR = os.path.join(MEDIA_DIR, \"transcripts\")\nANALYSIS_DIR = os.path.join(MEDIA_DIR, \"analysis\")",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1394",
    "name": "curl.py",
    "path": "github_repo/scripts/01_core_ai_analysis/transcription/curl.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 44,
    "size": 1382,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "generate_speech_with_curl",
      "main"
    ],
    "classes": [],
    "imports": [
      "csv",
      "os",
      "subprocess"
    ],
    "preview": "import csv\nimport os\nimport subprocess\n\n\ndef generate_speech_with_curl(question_text, question_number, api_key):\n    curl_command = [\n        \"curl\",\n        \"https://api.openai.com/v1/audio/speech\",\n        \"-H\",\n        f\"Authorization: Bearer {api_key}\",\n        \"-H\",\n        \"Content-Type: application/json\",\n        \"-d\",\n        f\"\"\"{{\n            \"model\": \"tts-1\",\n            \"input\": \"{question_text}\",\n            \"voice\": \"alloy\"\n        }}\"\"\",\n        \"--output\",",
    "last_modified": "2025-05-04T22:47:13.042596"
  },
  {
    "id": "1395",
    "name": "analyze (1).py",
    "path": "github_repo/scripts/01_core_ai_analysis/transcription/analyze (1).py",
    "category": "01_core_ai_analysis",
    "type": "analysis",
    "lines": 57,
    "size": 1994,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis",
      "transcription"
    ],
    "functions": [
      "transcribe_audio",
      "analyze_text"
    ],
    "classes": [],
    "imports": [
      "openai",
      "os",
      "dotenv",
      "sys"
    ],
    "preview": "from openai import OpenAI\n\nclient = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\nimport os\n\nfrom dotenv import load_dotenv\n\n# Load environment variables from .env\nload_dotenv()\n\n\n# Function to transcribe audio using OpenAI Whisper\ndef transcribe_audio(file_path):\n    with open(file_path, \"rb\") as audio_file:\n        transcript = client.audio.transcribe(\"whisper-1\", audio_file)\n        return transcript.text\n\n\n# Function to analyze the transcript using GPT\ndef analyze_text(text):",
    "last_modified": "2025-05-04T22:47:13.341006"
  },
  {
    "id": "1396",
    "name": "Media-Analysis-Pipeline-gpt-claude.py",
    "path": "github_repo/scripts/01_core_ai_analysis/transcription/Media-Analysis-Pipeline-gpt-claude.py",
    "category": "01_core_ai_analysis",
    "type": "analysis",
    "lines": 473,
    "size": 17763,
    "docstring": "ULTIMATE Media Analysis Pipeline - Simplified Edition\nUses only GPT-4o and Claude for reliability",
    "keywords": [
      "analysis",
      "openai",
      "transcription",
      "organization"
    ],
    "functions": [
      "main",
      "__init__",
      "extract_audio_from_video",
      "cleanup_temp_files",
      "__init__",
      "setup_directories",
      "setup_clients",
      "get_file_hash",
      "check_cache",
      "save_to_cache"
    ],
    "classes": [
      "LargeFileHandler",
      "UltimateMediaAnalyzer"
    ],
    "imports": [
      "os",
      "logging",
      "time",
      "json",
      "subprocess",
      "hashlib",
      "pathlib",
      "typing",
      "dotenv",
      "openai"
    ],
    "preview": "#!/usr/bin/env python3\n\"\"\"\nULTIMATE Media Analysis Pipeline - Simplified Edition\nUses only GPT-4o and Claude for reliability\n\"\"\"\n\nimport os\nimport logging\nimport time\nimport json\nimport subprocess\nimport hashlib\nfrom pathlib import Path\nfrom typing import Optional, Dict, Any, List\nfrom dotenv import load_dotenv\n\n# Load environment variables first\nload_dotenv()\n\n# Now import other packages",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1397",
    "name": "meida-trans-analyze.py",
    "path": "github_repo/scripts/01_core_ai_analysis/transcription/meida-trans-analyze.py",
    "category": "01_core_ai_analysis",
    "type": "analysis",
    "lines": 124,
    "size": 5779,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis",
      "transcription"
    ],
    "functions": [
      "format_timestamp",
      "transcribe_audio",
      "analyze_text_for_section",
      "process_media_directory"
    ],
    "classes": [],
    "imports": [
      "logging",
      "os",
      "sys",
      "pathlib",
      "openai",
      "dotenv"
    ],
    "preview": "import logging\nimport os\nimport sys\nfrom pathlib import Path\nimport openai\nfrom dotenv import load_dotenv\n\n# Setup basic logging for information tracking\nlogging.basicConfig(level=logging.INFO, format=\"%(asctime)s [%(levelname)s] %(message)s\")\n\n# Load environment variables from the .env file (ensure your OPENAI_API_KEY is stored here)\nenv_path = Path(\"/Users/steven/.env\")\nload_dotenv(dotenv_path=env_path)\n\n# Set the OpenAI API key from environment variables\nopenai.api_key = os.getenv(\"OPENAI_API_KEY\")\n\n# Initialize configuration. This can be expanded as needed.\nconfig = {\"base_dir\": \"\"}  # Leave blank to prompt for base directory\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1398",
    "name": "mp3-transcript.py",
    "path": "github_repo/scripts/01_core_ai_analysis/transcription/mp3-transcript.py",
    "category": "01_core_ai_analysis",
    "type": "audio_processing",
    "lines": 51,
    "size": 1454,
    "docstring": "",
    "keywords": [
      "analysis",
      "transcription"
    ],
    "functions": [
      "transcribe_audio",
      "save_transcription",
      "process_directory",
      "main"
    ],
    "classes": [],
    "imports": [
      "os",
      "whisper"
    ],
    "preview": "import os\n\nimport whisper\n\n\ndef transcribe_audio(file_path):\n    # Load the Whisper model\n    model = whisper.load_model(\"base\")\n\n    # Transcribe the audio file\n    result = model.transcribe(file_path)\n\n    return result[\"segments\"]\n\n\ndef save_transcription(segments, output_file):\n    with open(output_file, \"w\") as f:\n        for segment in segments:\n            start = segment[\"start\"]\n            end = segment[\"end\"]",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1399",
    "name": "mp3-4-transcribe.py",
    "path": "github_repo/scripts/01_core_ai_analysis/transcription/mp3-4-transcribe.py",
    "category": "01_core_ai_analysis",
    "type": "transcription",
    "lines": 178,
    "size": 6449,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis",
      "transcription"
    ],
    "functions": [
      "format_timestamp",
      "convert_mp4_to_mp3",
      "split_audio",
      "transcribe_file",
      "analyze_text_for_section",
      "process_media_directory"
    ],
    "classes": [],
    "imports": [
      "os",
      "subprocess",
      "openai",
      "dotenv"
    ],
    "preview": "import os\nimport subprocess\n\nfrom openai import OpenAI\n\nclient = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\nfrom dotenv import load_dotenv\n\n# Load environment variables from .env\nenv_path = os.path.expanduser(\"~/.env\")\nload_dotenv(dotenv_path=env_path)\n\n# Initialize OpenAI API key\n\nif not openai.api_key:\n    raise EnvironmentError(\"OpenAI API key not found. Please check your .env file.\")\n\n\n# Helper to format timestamps\ndef format_timestamp(seconds):",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1400",
    "name": "shorts.py",
    "path": "github_repo/scripts/01_core_ai_analysis/transcription/shorts.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 105,
    "size": 2815,
    "docstring": "",
    "keywords": [
      "openai"
    ],
    "functions": [],
    "classes": [],
    "imports": [
      "os",
      "random",
      "dotenv",
      "openai",
      "moviepy.video.fx.crop",
      "gtts",
      "moviepy.editor"
    ],
    "preview": "# Import everything\nimport os\nimport random\n\nfrom dotenv import load_dotenv\nfrom openai import OpenAI\n\nclient = OpenAI(api_key=os.environ[\"OPENAI_API\"])\nimport moviepy.video.fx.crop as crop_vid\nfrom gtts import gTTS\nfrom moviepy.editor import *\n\nload_dotenv()\n\n# Ask for video info\ntitle = input(\"\\nEnter the name of the video >  \")\noption = input(\"Do you want AI to generate content? (yes/no) >  \")\n\nif option == \"yes\":\n    # Generate content using OpenAI API",
    "last_modified": "2025-05-04T22:47:12"
  },
  {
    "id": "1401",
    "name": "categorygpt.py",
    "path": "github_repo/scripts/01_core_ai_analysis/transcription/categorygpt.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 61,
    "size": 1969,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis"
    ],
    "functions": [
      "get_openai_category",
      "create_directory",
      "save_to_category",
      "categorize_files"
    ],
    "classes": [],
    "imports": [
      "os",
      "openai"
    ],
    "preview": "import os\n\nfrom openai import OpenAI\n\nclient = OpenAI(api_key=api_key)\n\n# Load the API key from the environment variable\napi_key = os.getenv(\"OPENAI_API_KEY\")\nif not api_key:\n    raise ValueError(\"API key not found. Please ensure it is set in your environment variables.\")\n\n\ndef get_openai_category(script_content):\n    try:\n        response = client.chat.completions.create(\n            model=\"gpt-3.5-turbo\",\n            messages=[\n                {\"role\": \"system\", \"content\": \"You are an expert Python programmer.\"},\n                {\n                    \"role\": \"user\",",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1402",
    "name": "batch_image_seo_pipeline_20250530222550.py",
    "path": "github_repo/scripts/01_core_ai_analysis/transcription/batch_image_seo_pipeline_20250530222550.py",
    "category": "01_core_ai_analysis",
    "type": "image_processing",
    "lines": 105,
    "size": 4746,
    "docstring": "",
    "keywords": [
      "openai"
    ],
    "functions": [
      "call_gpt4o_analysis_and_prompts",
      "batch_flexible_analysis"
    ],
    "classes": [],
    "imports": [
      "csv",
      "json",
      "logging",
      "os",
      "pathlib",
      "dotenv",
      "openai",
      "tqdm"
    ],
    "preview": "import csv\nimport json\nimport logging\nimport os\nfrom pathlib import Path\n\nfrom dotenv import load_dotenv\nfrom openai import OpenAI\nfrom tqdm import tqdm\n\n# CONFIG\nload_dotenv(os.path.expanduser(\"~/.env\"))\nOPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\nif not OPENAI_API_KEY:\n    raise EnvironmentError(\"OPENAI_API_KEY not found in ~/.env\")\nclient = OpenAI(api_key=OPENAI_API_KEY)\n\nLOG_FILE = \"avatararts_flexible_analysis.log\"\nlogging.basicConfig(\n    level=logging.INFO,",
    "last_modified": "2025-09-06T12:24:07.935983"
  },
  {
    "id": "1403",
    "name": "quiz-talk.py",
    "path": "github_repo/scripts/01_core_ai_analysis/transcription/quiz-talk.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 108,
    "size": 3424,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis",
      "web_tools"
    ],
    "functions": [
      "generate_base_prompt",
      "refine_prompt",
      "create_image",
      "upscale_image",
      "text_to_speech"
    ],
    "classes": [],
    "imports": [
      "csv",
      "io",
      "pathlib",
      "requests",
      "openai",
      "PIL"
    ],
    "preview": "import csv\nfrom io import BytesIO\nfrom pathlib import Path\n\nimport requests\nfrom openai import OpenAI\nfrom PIL import Image, ImageResampling\n\n# Initialize the OpenAI client\n\n\ndef generate_base_prompt(question, options):\n    # Combine question and options to generate a base prompt\n    return f\"Question: {question} Options: {', '.join(options)}\"\n\n\ndef refine_prompt(prompt):\n    # Refine the prompt using ChatGPT for more creativity\n    response = openai.Completion.create(\n        model=\"gpt-4\",",
    "last_modified": "2025-09-13T05:53:50.920880"
  },
  {
    "id": "1404",
    "name": "enhanced_pipeline (1).py",
    "path": "github_repo/scripts/01_core_ai_analysis/transcription/enhanced_pipeline (1).py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 461,
    "size": 15867,
    "docstring": "",
    "keywords": [
      "opencv",
      "transcription"
    ],
    "functions": [
      "__init__",
      "__init__",
      "transcribe",
      "_enhanced_hook_score",
      "_analyze_subjects",
      "_sentence_windows",
      "select_topk",
      "_decide_style",
      "_get_platform_specs",
      "export_clip"
    ],
    "classes": [
      "ClipCandidate",
      "PipelineOptions",
      "OpusClonePipeline"
    ],
    "imports": [
      "__future__",
      "json",
      "os",
      "re",
      "subprocess",
      "dataclasses",
      "typing",
      "cv2",
      "numpy",
      "whisper"
    ],
    "preview": "from __future__ import annotations\n\nimport json\nimport os\nimport re\nimport subprocess\nfrom dataclasses import dataclass\nfrom typing import Any, Dict, List, Optional, Tuple\n\nimport cv2\nimport numpy as np\n\nimport whisper\n\nfrom .brand import BrandTemplate\nfrom .captions import burn_subtitles, write_ass_from_words, write_srt\nfrom .overlays import OverlayEngine\nfrom .reframe import SubjectTracker, generate_smooth_path\nfrom .segments import temporal_nms\nfrom .style_engine import decide_style",
    "last_modified": "2025-09-13T05:55:10.022964"
  },
  {
    "id": "1405",
    "name": "transcript_analyzer.py",
    "path": "github_repo/scripts/01_core_ai_analysis/transcription/transcript_analyzer.py",
    "category": "01_core_ai_analysis",
    "type": "analysis",
    "lines": 278,
    "size": 10598,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis",
      "transcription"
    ],
    "functions": [
      "slugify",
      "retry_with_backoff",
      "format_timestamp",
      "parse_transcript",
      "transcribe_audio",
      "analyze_text_for_section",
      "link_timestamps_to_analysis",
      "process_audio_file",
      "check_conda_env",
      "load_progress_cache"
    ],
    "classes": [],
    "imports": [
      "argparse",
      "json",
      "logging",
      "logging.handlers",
      "os",
      "random",
      "re",
      "sys",
      "threading",
      "time"
    ],
    "preview": "#!/usr/bin/env python3\nimport argparse\nimport json\nimport logging\nimport logging.handlers\nimport os\nimport random\nimport re\nimport sys\nimport threading\nimport time\nfrom concurrent.futures import ThreadPoolExecutor\nfrom pathlib import Path\n\nfrom shared.config import *\nfrom openai import OpenAI\nfrom termcolor import colored\nfrom tqdm import tqdm\n\n# ---------- CONFIG & UTILITIES ----------",
    "last_modified": "2025-10-09T05:27:15.572659"
  },
  {
    "id": "1406",
    "name": "pythoncat.py",
    "path": "github_repo/scripts/01_core_ai_analysis/transcription/pythoncat.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 86,
    "size": 3021,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis"
    ],
    "functions": [
      "get_openai_category",
      "sanitize_category",
      "create_directory",
      "save_to_category",
      "categorize_files"
    ],
    "classes": [],
    "imports": [
      "os",
      "openai",
      "hashlib",
      "re",
      "shutil"
    ],
    "preview": "import os\n\nfrom openai import OpenAI\n\nclient = OpenAI(api_key=api_key)\nimport hashlib\nimport re\nimport shutil\n\n# Load the API key from the environment variable\napi_key = os.getenv(\"OPENAI_API_KEY\")\nif not api_key:\n    raise ValueError(\"API key not found. Please ensure it is set in your environment variables.\")\n\n\nSOURCE_DIR = \"/Users/steven/Documents/Python\"\nDEST_DIR = \"/Users/steven/Documents/Categorized\"\n\n\ndef get_openai_category(script_content):",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1407",
    "name": "conda_env_analyzer.py",
    "path": "github_repo/scripts/01_core_ai_analysis/transcription/conda_env_analyzer.py",
    "category": "01_core_ai_analysis",
    "type": "analysis",
    "lines": 247,
    "size": 8029,
    "docstring": "Conda Environment Analyzer\nAnalyzes multiple conda environments to provide insights on packages, sizes, and dependencies.",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "get_dir_size",
      "format_size",
      "get_python_version",
      "get_installed_packages",
      "analyze_environment",
      "find_duplicate_packages",
      "print_summary",
      "save_detailed_report",
      "main"
    ],
    "classes": [],
    "imports": [
      "os",
      "json",
      "subprocess",
      "pathlib",
      "collections",
      "sys"
    ],
    "preview": "#!/usr/bin/env python3\n\"\"\"\nConda Environment Analyzer\nAnalyzes multiple conda environments to provide insights on packages, sizes, and dependencies.\n\"\"\"\n\nimport os\nimport json\nimport subprocess\nfrom pathlib import Path\nfrom collections import defaultdict\nimport sys\n\n# Environment paths to analyze\nENV_PATHS = [\n    \"/Users/steven/miniforge3/envs/analytics\",\n    \"/Users/steven/miniforge3/envs/analyze\",\n    \"/Users/steven/miniforge3/envs/audio-tools\",\n    \"/Users/steven/miniforge3/envs/comic8\",\n    \"/Users/steven/miniforge3/envs/gemini\",",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1408",
    "name": "Quiz-txt2speech.py",
    "path": "github_repo/scripts/01_core_ai_analysis/transcription/Quiz-txt2speech.py",
    "category": "01_core_ai_analysis",
    "type": "transcription",
    "lines": 85,
    "size": 2501,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis",
      "web_tools"
    ],
    "functions": [
      "analyze_image_with_gpt4_vision",
      "upscale_image",
      "text_to_speech"
    ],
    "classes": [],
    "imports": [
      "csv",
      "io",
      "openai",
      "requests",
      "PIL"
    ],
    "preview": "import csv\nfrom io import BytesIO\n\nimport openai\nimport requests\nfrom PIL import Image\n\n# Initialize the OpenAI client\nclient = openai(api_key=\"sk-acw35nWnLFMd23JUzcQoQ7QBhg4y5wLxoQJpR64ITBWHqBT7\")\n\n\ndef analyze_image_with_gpt4_vision(image_url):\n    # Analyze the image using GPT-4 with Vision\n    response = client.chat.completions.create(\n        model=\"gpt-4-vision-preview\",\n        messages=[\n            {\n                \"role\": \"user\",\n                \"content\": [\n                    {\"type\": \"text\", \"text\": \"What\u2019s in this image?\"},",
    "last_modified": "2025-09-13T05:53:50.768630"
  },
  {
    "id": "1409",
    "name": "pythoncatorize.py",
    "path": "github_repo/scripts/01_core_ai_analysis/transcription/pythoncatorize.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 60,
    "size": 1966,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis"
    ],
    "functions": [
      "get_openai_category",
      "create_directory",
      "save_to_category",
      "categorize_files"
    ],
    "classes": [],
    "imports": [
      "os",
      "openai"
    ],
    "preview": "import os\n\nfrom openai import OpenAI\n\nclient = OpenAI(api_key=api_key)\n\n# Load the API key from the environment variable\napi_key = os.getenv(\"OPENAI_API_KEY\")\nif not api_key:\n    raise ValueError(\"API key not found. Please ensure it is set in your environment variables.\")\n\n\ndef get_openai_category(script_content):\n    try:\n        response = client.chat.completions.create(\n            model=\"gpt-3.5-turbo\",\n            messages=[\n                {\"role\": \"system\", \"content\": \"You are an expert Python programmer.\"},\n                {\n                    \"role\": \"user\",",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1410",
    "name": "analyze-prompt.py",
    "path": "github_repo/scripts/01_core_ai_analysis/transcription/analyze-prompt.py",
    "category": "01_core_ai_analysis",
    "type": "analysis",
    "lines": 133,
    "size": 4640,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis",
      "transcription"
    ],
    "functions": [
      "split_video_to_segments",
      "transcribe_video_segment",
      "format_timestamp",
      "analyze_text_for_section",
      "process_video_by_section"
    ],
    "classes": [],
    "imports": [
      "os",
      "subprocess",
      "openai",
      "dotenv",
      "sys"
    ],
    "preview": "import os\nimport subprocess\n\nimport openai\nfrom dotenv import load_dotenv\n\n# Load environment variables from .env (make sure your OPENAI_API_KEY is stored here)\nload_dotenv()\nopenai.api_key = os.getenv(\"OPENAI_API_KEY\")\n\n# Directory paths\nVIDEO_DIR = \"/Users/steven/Movies/2025/mp4\"  # Directory containing MP4 files\nTRANSCRIPT_DIR = \"/Users/steven/Movies/2025/mp4/transcribe\"  # Directory to save transcripts\nANALYSIS_DIR = \"/Users/steven/Movies/2025/mp4/analysis\"  # Directory to save the analysis files\n\n# Create output directories if they don't exist\nos.makedirs(TRANSCRIPT_DIR, exist_ok=True)\nos.makedirs(ANALYSIS_DIR, exist_ok=True)\n\n",
    "last_modified": "2025-09-13T05:55:06.402437"
  },
  {
    "id": "1411",
    "name": "trek 3.py",
    "path": "github_repo/scripts/01_core_ai_analysis/transcription/trek 3.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 70,
    "size": 2498,
    "docstring": "",
    "keywords": [
      "analysis",
      "web_tools"
    ],
    "functions": [
      "generate_speech",
      "process_csv_and_generate_speech"
    ],
    "classes": [],
    "imports": [
      "csv",
      "os",
      "requests"
    ],
    "preview": "import csv\nimport os\n\nimport requests  # Make sure to install this package\n\n\ndef generate_speech(input_text, output_path, api_key):\n    \"\"\"\n    Generates speech from text using an API (e.g., OpenAI's text-to-speech API).\n    \"\"\"\n    url = \"https://api.openai.com/v1/audio/speech\"\n    headers = {\n        \"Authorization\": f\"Bearer {api_key}\",\n        \"Content-Type\": \"application/json\",\n    }\n    data = {\n        \"model\": \"tts-1\",  # Update this according to the specific model you're using\n        \"input\": input_text,\n        \"voice\": \"shimmer\",  # Update the voice parameter as needed\n    }",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1412",
    "name": "clips.py",
    "path": "github_repo/scripts/01_core_ai_analysis/transcription/clips.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 136,
    "size": 4039,
    "docstring": "",
    "keywords": [
      "organization",
      "analysis",
      "transcription"
    ],
    "functions": [
      "gen_comment_image",
      "process_content",
      "add_newlines",
      "change_char",
      "gen_title_message_image",
      "gen_title_message_clip",
      "create_comment_clip",
      "randomStringDigits",
      "gen_audio_clip",
      "audio_concatenate"
    ],
    "classes": [],
    "imports": [
      "random",
      "string",
      "textwrap",
      "io",
      "pathlib",
      "numpy",
      "soundfile",
      "gtts",
      "moviepy.audio.fx.audio_fadein",
      "moviepy.audio.fx.audio_fadeout"
    ],
    "preview": "import random\nimport string\nimport textwrap\nfrom io import BytesIO\nfrom pathlib import Path\n\nimport numpy as np\nimport soundfile as sf\nfrom gtts import gTTS\nfrom moviepy.audio.fx.audio_fadein import audio_fadein\nfrom moviepy.audio.fx.audio_fadeout import audio_fadeout\nfrom moviepy.audio.fx.audio_left_right import audio_left_right\nfrom moviepy.audio.fx.audio_loop import audio_loop\nfrom moviepy.audio.fx.audio_normalize import audio_normalize\nfrom moviepy.audio.fx.volumex import volumex\nfrom moviepy.editor import *\nfrom PIL import Image, ImageDraw, ImageFont\nfrom pydub import AudioSegment\nfrom pydub.playback import play\n",
    "last_modified": "2025-05-04T23:28:22.812944"
  },
  {
    "id": "1413",
    "name": "upcreatmod (1).py",
    "path": "github_repo/scripts/01_core_ai_analysis/transcription/upcreatmod (1).py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 119,
    "size": 3808,
    "docstring": "",
    "keywords": [
      "data_processing",
      "openai",
      "analysis",
      "web_tools"
    ],
    "functions": [
      "generate_clickable_title",
      "generate_description",
      "generate_tags"
    ],
    "classes": [],
    "imports": [
      "os",
      "openai",
      "io",
      "pandas",
      "requests",
      "PIL",
      "tqdm"
    ],
    "preview": "import os\n\nfrom openai import OpenAI\n\nclient = OpenAI(api_key=\"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\")\nimport io\n\nimport pandas as pd\nimport requests\nfrom PIL import Image\nfrom tqdm import tqdm\n\n# Set OpenAI and Stability API keys\nstability_ai_key = \"sk-2O6mutk6X4HI9olxMeGKYv0MpXcNRzs6fVRPJMdEvgEzRRII\"\n\n# Function to generate a clickable title\n\n\ndef generate_clickable_title(detail):\n    prompt = f\"Generate a catchy and clickable title for a 3D Breakthrough Grinch Round Christmas ORnament with the theme: '{detail}'. Maximum 30 characters. At the end of each title write Cork Back Coaster\"",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1414",
    "name": "batch_image_seo_pipeline_20250530220434.py",
    "path": "github_repo/scripts/01_core_ai_analysis/transcription/batch_image_seo_pipeline_20250530220434.py",
    "category": "01_core_ai_analysis",
    "type": "analysis",
    "lines": 429,
    "size": 17578,
    "docstring": "batch_image_seo_pipeline.py\n\n1) Discovers all image files under a given folder.\n2) Extracts technical metadata via Pillow.\n3) Calls GPT-4o Vision (multimodal) using a \u201csystem\u201d + \u201cuser\u201d prompt.\n4) Parses the JSON response into structured fields:\n   main_subject, style, color_palette, tags, orientation, suggested_products,\n   SEO_title, SEO_description, emotion, safety_rating, dominant_keyword.\n5) Builds a \u201csource\u201d tag: folder-subfolder-YYYYMMDD.\n6) Applies a \u201cTop 5% SEO Analytics\u201d framework to generate SEO columns:\n   \u2013 SEO Keywords, Traffic Source, CRO Tactic, Backlink Source, Engagement Rate.\n7) Adds niche-specific design prompts (Geeky, Dark Humor, Anime).\n8) Writes everything into a final CSV for bulk upload.\n\nDependencies:\n    pip install openai python-dotenv pillow tqdm backoff",
    "keywords": [
      "openai"
    ],
    "functions": [
      "load_openai_client",
      "retry_on_api_error",
      "call_gpt4o",
      "get_image_tech_meta",
      "build_source_tag",
      "discover_images",
      "build_gpt_messages",
      "parse_args",
      "main"
    ],
    "classes": [],
    "imports": [
      "argparse",
      "csv",
      "json",
      "logging",
      "os",
      "sys",
      "time",
      "datetime",
      "pathlib",
      "typing"
    ],
    "preview": "#!/usr/bin/env python3\n\"\"\"\nbatch_image_seo_pipeline.py\n\n1) Discovers all image files under a given folder.\n2) Extracts technical metadata via Pillow.\n3) Calls GPT-4o Vision (multimodal) using a \u201csystem\u201d + \u201cuser\u201d prompt.\n4) Parses the JSON response into structured fields:\n   main_subject, style, color_palette, tags, orientation, suggested_products,\n   SEO_title, SEO_description, emotion, safety_rating, dominant_keyword.\n5) Builds a \u201csource\u201d tag: folder-subfolder-YYYYMMDD.\n6) Applies a \u201cTop 5% SEO Analytics\u201d framework to generate SEO columns:\n   \u2013 SEO Keywords, Traffic Source, CRO Tactic, Backlink Source, Engagement Rate.\n7) Adds niche-specific design prompts (Geeky, Dark Humor, Anime).\n8) Writes everything into a final CSV for bulk upload.\n\nDependencies:\n    pip install openai python-dotenv pillow tqdm backoff\n\"\"\"\n",
    "last_modified": "2025-09-13T05:53:26.523803"
  },
  {
    "id": "1415",
    "name": "fancier.py",
    "path": "github_repo/scripts/01_core_ai_analysis/transcription/fancier.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 239,
    "size": 8122,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis"
    ],
    "functions": [
      "sanitize_filename",
      "resize_image",
      "process_batch",
      "process_images_and_generate_csv",
      "write_csv",
      "main"
    ],
    "classes": [],
    "imports": [
      "csv",
      "os",
      "time",
      "datetime",
      "dotenv",
      "openai",
      "PIL"
    ],
    "preview": "import csv\nimport os\nimport time\nfrom datetime import datetime\n\nfrom dotenv import load_dotenv\nfrom openai import OpenAI\nfrom PIL import Image, UnidentifiedImageError\n\n# Load environment variables\nenv_path = \"/Users/steven/.env\"\nload_dotenv(dotenv_path=env_path)\n\n# Initialize OpenAI API key\napi_key = os.getenv(\"OPENAI_API_KEY\")\n\n# Error checking for OpenAI API key\nif not api_key:\n    raise EnvironmentError(\"OpenAI API key not found. Please check your .env file.\")\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1416",
    "name": "split_audio.py",
    "path": "github_repo/scripts/01_core_ai_analysis/transcription/split_audio.py",
    "category": "01_core_ai_analysis",
    "type": "transcription",
    "lines": 96,
    "size": 3975,
    "docstring": "Audio Splitter for Transcription (convenience version)",
    "keywords": [],
    "functions": [
      "human_bytes",
      "compute_chunk_ms_from_target_size",
      "split_file",
      "main"
    ],
    "classes": [],
    "imports": [
      "argparse",
      "csv",
      "pathlib",
      "typing",
      "pydub",
      "json"
    ],
    "preview": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"Audio Splitter for Transcription (convenience version)\"\"\"\nimport argparse, csv\nfrom pathlib import Path\nfrom typing import List, Optional\nfrom pydub import AudioSegment\n\ndef human_bytes(n: int) -> str:\n    units = ['B','KB','MB','GB','TB']\n    i = 0; x = float(n)\n    while x >= 1024 and i < len(units)-1:\n        x /= 1024.0; i += 1\n    return f\"{x:.2f} {units[i]}\"\n\ndef compute_chunk_ms_from_target_size(file_path: Path, audio: AudioSegment, target_size_mb: float,\n                                      min_minutes: int = 1, max_minutes: int = 15) -> int:\n    total_bytes = file_path.stat().st_size\n    total_ms = len(audio)\n    if total_ms == 0:",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1417",
    "name": "mp4-transcript copy.py",
    "path": "github_repo/scripts/01_core_ai_analysis/transcription/mp4-transcript copy.py",
    "category": "01_core_ai_analysis",
    "type": "video_processing",
    "lines": 155,
    "size": 6892,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis",
      "transcription"
    ],
    "functions": [
      "split_video_to_segments",
      "transcribe_video_segment",
      "format_timestamp",
      "analyze_text_for_section",
      "process_video_by_section"
    ],
    "classes": [],
    "imports": [
      "openai",
      "os",
      "subprocess",
      "dotenv",
      "sys"
    ],
    "preview": "from openai import OpenAI\n\nclient = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\nimport os\nimport subprocess\n\nfrom dotenv import load_dotenv\n\n# Load environment variables from .env (make sure your OPENAI_API_KEY is stored here)\n# Load environment variables from .env (make sure your OPENAI_API_KEY is stored here)\nenv_path = \"/Users/steven/.env\"\nload_dotenv(dotenv_path=env_path)\n\n# Directory paths\nVIDEO_DIR = \"/Users/steven/AvaTarArTs/canva/Video\"  # Directory containing MP4 files\nTRANSCRIPT_DIR = \"/Users/steven/AvaTarArTs/canva/Video/transcribe\"  # Directory to save transcripts\nANALYSIS_DIR = (\n    \"/Users/steven/AvaTarArTs/canva/Video/analysis\"  # Directory to save the analysis files\n)\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1418",
    "name": "mp4-trans.py",
    "path": "github_repo/scripts/01_core_ai_analysis/transcription/mp4-trans.py",
    "category": "01_core_ai_analysis",
    "type": "video_processing",
    "lines": 139,
    "size": 5119,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis",
      "transcription"
    ],
    "functions": [
      "split_video_to_segments",
      "transcribe_video_segment",
      "format_timestamp",
      "analyze_text_for_section",
      "process_video_by_section"
    ],
    "classes": [],
    "imports": [
      "openai",
      "os",
      "subprocess",
      "dotenv",
      "sys"
    ],
    "preview": "from openai import OpenAI\n\nclient = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\nimport os\nimport subprocess\n\nfrom dotenv import load_dotenv\n\n# Load environment variables from .env\nload_dotenv()\n\n# Directory paths\nVIDEO_DIR = \"/Users/steven/Movies/Kath/Katheria_and_Salome_The_Daughters_of_Destinay-30m_compressed_segments\"  # Directory containing MP4 files\nTRANSCRIPT_DIR = \"/Users/steven/Movies/Kath/Katheria_and_Salome_The_Daughters_of_Destinay-30m_compressed_segments/transcribe\"  # Directory to save transcripts\nANALYSIS_DIR = \"/Users/steven/Movies/Kath/analysis\"  # Directory to save the analysis files\n\n# Create output directories if they don't exist\nos.makedirs(TRANSCRIPT_DIR, exist_ok=True)\nos.makedirs(ANALYSIS_DIR, exist_ok=True)\n",
    "last_modified": "2025-09-13T05:55:06.473366"
  },
  {
    "id": "1419",
    "name": "trek.py",
    "path": "github_repo/scripts/01_core_ai_analysis/transcription/trek.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 69,
    "size": 2503,
    "docstring": "",
    "keywords": [
      "analysis",
      "web_tools"
    ],
    "functions": [
      "generate_speech",
      "process_csv_and_generate_speech"
    ],
    "classes": [],
    "imports": [
      "csv",
      "os",
      "requests"
    ],
    "preview": "import csv\nimport os\n\nimport requests  # Make sure to install this package\n\n\ndef generate_speech(input_text, output_path, api_key):\n    \"\"\"\n    Generates speech from text using an API (e.g., OpenAI's text-to-speech API).\n    \"\"\"\n    url = \"https://api.openai.com/v1/audio/speech\"\n    headers = {\n        \"Authorization\": f\"Bearer {api_key}\",\n        \"Content-Type\": \"application/json\",\n    }\n    data = {\n        \"model\": \"tts-1\",  # Update this according to the specific model you're using\n        \"input\": input_text,\n        \"voice\": \"shimmer\",  # Update the voice parameter as needed\n    }",
    "last_modified": "2025-09-13T05:53:51.076590"
  },
  {
    "id": "1420",
    "name": "mp3-csv 1.py",
    "path": "github_repo/scripts/01_core_ai_analysis/transcription/mp3-csv 1.py",
    "category": "01_core_ai_analysis",
    "type": "audio_processing",
    "lines": 107,
    "size": 4019,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis",
      "transcription"
    ],
    "functions": [
      "format_timestamp",
      "transcribe_audio",
      "analyze_text_for_section",
      "process_audio_file",
      "process_audio_directory"
    ],
    "classes": [],
    "imports": [
      "os",
      "openai",
      "dotenv"
    ],
    "preview": "import os\n\nimport openai\nfrom dotenv import load_dotenv\n\n# Load environment variables\nload_dotenv()\nopenai.api_key = os.getenv(\"OPENAI_API_KEY\")\nif not openai.api_key:\n    raise EnvironmentError(\"OpenAI API key not found. Please check your .env file.\")\n\n\ndef format_timestamp(seconds):\n    \"\"\"Format seconds into MM:SS timestamp.\"\"\"\n    minutes = int(seconds // 60)\n    seconds = seconds % 60\n    return f\"{minutes:02d}:{int(seconds):02d}\"\n\n\ndef transcribe_audio(file_path):",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1421",
    "name": "thinketh_tts.py",
    "path": "github_repo/scripts/01_core_ai_analysis/transcription/thinketh_tts.py",
    "category": "01_core_ai_analysis",
    "type": "transcription",
    "lines": 155,
    "size": 4665,
    "docstring": "thinketh_tts.py\nText-to-Speech generator for 'As A Man Thinketh' (James Allen)\n\nFeatures:\n - Reads DOCX\n - Splits into chapters automatically\n - Handles token limits by chunking text\n - Merges all chunks into one MP3 per chapter\n - Skips already completed files",
    "keywords": [
      "openai"
    ],
    "functions": [
      "load_env",
      "read_docx_text",
      "normalize_text",
      "slugify",
      "extract_chapters",
      "synthesize_to_mp3",
      "main"
    ],
    "classes": [],
    "imports": [
      "os",
      "re",
      "pathlib",
      "docx",
      "dotenv",
      "openai",
      "pydub"
    ],
    "preview": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\nthinketh_tts.py\nText-to-Speech generator for 'As A Man Thinketh' (James Allen)\n\nFeatures:\n - Reads DOCX\n - Splits into chapters automatically\n - Handles token limits by chunking text\n - Merges all chunks into one MP3 per chapter\n - Skips already completed files\n\"\"\"\n\nimport os\nimport re\nfrom pathlib import Path\nfrom docx import Document\nfrom dotenv import load_dotenv\nfrom openai import OpenAI",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1422",
    "name": "main 3.py",
    "path": "github_repo/scripts/01_core_ai_analysis/transcription/main 3.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 21,
    "size": 613,
    "docstring": "",
    "keywords": [],
    "functions": [
      "main"
    ],
    "classes": [],
    "imports": [
      "csv",
      "generate_speech"
    ],
    "preview": "import csv\n\nfrom generate_speech import generate_speech\n\n\ndef main():\n    csv_path = \"/Users/steven/Music/quiz-talk/Gtrivia - Sheet1.csv\"\n\n    with open(csv_path, newline=\"\", encoding=\"utf-8\") as csvfile:\n        reader = csv.DictReader(csvfile)\n        for i, row in enumerate(reader):\n            # Assuming 'Question' is the column name\n            question_text = row[\"Question\"]\n            output_path = f\"/Users/steven/Music/quiz-talk/speech/question_{\n                i + 1}.mp3\"\n            generate_speech(question_text, voice=\"shimmer\", output_path=output_path)\n\n\nif __name__ == \"__main__\":\n    main()",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1423",
    "name": "upcreatmod.py",
    "path": "github_repo/scripts/01_core_ai_analysis/transcription/upcreatmod.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 113,
    "size": 3796,
    "docstring": "",
    "keywords": [
      "data_processing",
      "openai",
      "analysis",
      "web_tools"
    ],
    "functions": [
      "generate_clickable_title",
      "generate_description",
      "generate_tags"
    ],
    "classes": [],
    "imports": [
      "os",
      "openai",
      "io",
      "pandas",
      "requests",
      "PIL",
      "tqdm"
    ],
    "preview": "import os\n\nfrom openai import OpenAI\n\nclient = OpenAI(api_key=\"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\")\nimport io\n\nimport pandas as pd\nimport requests\nfrom PIL import Image\nfrom tqdm import tqdm\n\n# Set OpenAI and Stability API keys\nstability_ai_key = \"sk-2O6mutk6X4HI9olxMeGKYv0MpXcNRzs6fVRPJMdEvgEzRRII\"\n\n\n# Function to generate a clickable title\ndef generate_clickable_title(detail):\n    prompt = f\"Generate a catchy and clickable title for a 3D Breakthrough Grinch Round Christmas ORnament with the theme: '{detail}'. Maximum 30 characters. At the end of each title write Cork Back Coaster\"\n    response = client.chat.completions.create(",
    "last_modified": "2025-05-04T22:47:11.602837"
  },
  {
    "id": "1424",
    "name": "segments.py",
    "path": "github_repo/scripts/01_core_ai_analysis/transcription/segments.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 75,
    "size": 2011,
    "docstring": "",
    "keywords": [],
    "functions": [
      "detect_scenes",
      "detect_silence_segments",
      "ffprobe_duration",
      "merge_overlaps"
    ],
    "classes": [],
    "imports": [
      "__future__",
      "json",
      "math",
      "os",
      "re",
      "subprocess",
      "typing",
      "pydub",
      "scenedetect",
      "scenedetect.detectors"
    ],
    "preview": "from __future__ import annotations\n\nimport json\nimport math\nimport os\nimport re\nimport subprocess\nfrom typing import Any, Dict, List, Tuple\n\nfrom pydub import AudioSegment, silence\nfrom scenedetect import SceneManager, VideoManager\nfrom scenedetect.detectors import ContentDetector\n\n\ndef detect_scenes(video_path: str, threshold: float = 27.0) -> List[Tuple[float, float]]:\n    \"\"\"Return list of (start_sec, end_sec) scene ranges using PySceneDetect.\"\"\"\n    vm = VideoManager([video_path])\n    vm.set_downscale_factor()\n    sm = SceneManager()\n    sm.add_detector(ContentDetector(threshold=threshold))",
    "last_modified": "2025-09-13T05:55:09.639520"
  },
  {
    "id": "1425",
    "name": "generate_speech 2 (1).py",
    "path": "github_repo/scripts/01_core_ai_analysis/transcription/generate_speech 2 (1).py",
    "category": "01_core_ai_analysis",
    "type": "transcription",
    "lines": 44,
    "size": 1447,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis"
    ],
    "functions": [
      "generate_speech",
      "main"
    ],
    "classes": [],
    "imports": [
      "csv",
      "openai"
    ],
    "preview": "import csv\n\nfrom openai import OpenAI\n\nclient = OpenAI(api_key='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')\n\n# Replace 'your_api_key_here' with your actual OpenAI API key\n\n\ndef generate_speech(text, voice='shimmer', output_path='speech.mp3'):\n    response = client.audio.create(model=\"tts-1-hd\",  # Ensure this model supports the audio creation\n                                   input=text,\n                                   voice=voice,\n                                   format=\"mp3\")\n    with open(output_path, \"wb\") as file:\n        file.write(response.content)\n\n\ndef main():\n    # Update this path to where your CSV is located",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1426",
    "name": "analyze-prompts.py",
    "path": "github_repo/scripts/01_core_ai_analysis/transcription/analyze-prompts.py",
    "category": "01_core_ai_analysis",
    "type": "analysis",
    "lines": 99,
    "size": 4036,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis",
      "transcription"
    ],
    "functions": [
      "transcribe_audio",
      "format_timestamp",
      "analyze_text_for_section",
      "process_audio_directory"
    ],
    "classes": [],
    "imports": [
      "openai",
      "os",
      "subprocess",
      "dotenv"
    ],
    "preview": "from openai import OpenAI\n\nclient = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\nimport os\nimport subprocess\n\nfrom dotenv import load_dotenv\n\n# Load environment variables from .env (make sure your OPENAI_API_KEY is stored here)\nenv_path = \"/Users/steven/.env\"\nload_dotenv(dotenv_path=env_path)\n\n# Directory paths\nAUDIO_DIR = (\n    \"/Users/steven/Music/NocTurnE-meLoDieS/Discography/Audio\"  # Directory containing MP3 files\n)\nTRANSCRIPT_DIR = \"/Users/steven/Music/NocTurnE-meLoDieS/Discography/Audio/transcript\"  # Directory to save transcripts\nANALYSIS_DIR = \"/Users/steven/Music/NocTurnE-meLoDieS/Discography/Audio/analysis\"  # Directory to save the analysis files\n\n# Create output directories if they don't exist",
    "last_modified": "2025-09-13T05:53:55.019781"
  },
  {
    "id": "1427",
    "name": "curld.py",
    "path": "github_repo/scripts/01_core_ai_analysis/transcription/curld.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 50,
    "size": 1786,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "generate_speech_curl",
      "main"
    ],
    "classes": [],
    "imports": [
      "csv",
      "json",
      "os",
      "subprocess"
    ],
    "preview": "import csv\nimport json\nimport os  # Import the os module\nimport subprocess\n\n\ndef generate_speech_curl(input_text, output_path, api_key):\n    data = {\"model\": \"tts-1\", \"input\": input_text, \"voice\": \"shimmer\"}\n    command = [\n        \"curl\",\n        \"https://api.openai.com/v1/audio/speech\",\n        \"-H\",\n        f\"Authorization: Bearer {api_key}\",\n        \"-H\",\n        \"Content-Type: application/json\",\n        \"-d\",\n        json.dumps(data),\n        \"--output\",\n        output_path,\n    ]",
    "last_modified": "2025-09-13T05:53:50.849222"
  },
  {
    "id": "1428",
    "name": "python_repo_analyzer.py",
    "path": "github_repo/scripts/01_core_ai_analysis/transcription/python_repo_analyzer.py",
    "category": "01_core_ai_analysis",
    "type": "analysis",
    "lines": 679,
    "size": 25234,
    "docstring": "Python Repository Analyzer & Portfolio Generator\nAnalyzes Python scripts, categorizes them with GPT-4, and generates portfolio-ready outputs",
    "keywords": [
      "data_processing",
      "openai",
      "analysis"
    ],
    "functions": [
      "__init__",
      "load_environment",
      "is_excluded",
      "extract_imports",
      "analyze_script_content",
      "categorize_script",
      "analyze_with_gpt",
      "scan_repositories",
      "generate_csv_report",
      "generate_html_portfolio"
    ],
    "classes": [
      "PythonRepoAnalyzer"
    ],
    "imports": [
      "csv",
      "hashlib",
      "json",
      "os",
      "re",
      "subprocess",
      "datetime",
      "pathlib",
      "typing",
      "pandas"
    ],
    "preview": "#!/usr/bin/env python3\n\"\"\"\nPython Repository Analyzer & Portfolio Generator\nAnalyzes Python scripts, categorizes them with GPT-4, and generates portfolio-ready outputs\n\"\"\"\n\nimport csv\nimport hashlib\nimport json\nimport os\nimport re\nimport subprocess\nfrom datetime import datetime\nfrom pathlib import Path\nfrom typing import Any, Dict, List, Optional\n\ntry:\n    import pandas as pd\n    from dotenv import load_dotenv\n    from openai import OpenAI",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1429",
    "name": "mp3_batch.py",
    "path": "github_repo/scripts/01_core_ai_analysis/transcription/mp3_batch.py",
    "category": "01_core_ai_analysis",
    "type": "audio_processing",
    "lines": 46,
    "size": 1417,
    "docstring": "",
    "keywords": [],
    "functions": [
      "convert_to_mp3"
    ],
    "classes": [],
    "imports": [
      "os",
      "subprocess"
    ],
    "preview": "import os\nimport subprocess\n\n# Folder containing your source videos\nSOURCE_DIR = \"/Users/steven/Movies/PROJECt2025-DoMinIon\"\n# Bitrate and sample rate for output MP3\nBITRATE = \"32k\"  # change to \"64k\" if you want slightly better quality\nSAMPLERATE = \"16000\"  # Whisper-friendly\nCHANNELS = \"1\"  # mono to reduce size\n\n\ndef convert_to_mp3(input_path, output_path):\n    cmd = [\n        \"ffmpeg\",\n        \"-y\",  # overwrite without asking\n        \"-i\",\n        input_path,  # input file\n        \"-vn\",  # strip video\n        \"-ac\",\n        CHANNELS,",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1430",
    "name": "quantumforgelabs.py",
    "path": "github_repo/scripts/01_core_ai_analysis/transcription/quantumforgelabs.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 349,
    "size": 15121,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "os",
      "zipfile",
      "shutil",
      "textwrap",
      "json",
      "pathlib",
      "PIL"
    ],
    "preview": "# Create a ready-to-deploy static site scaffold for QuantumForgeLabs\n# Files:\n# - index.html\n# - assets/style.css\n# - assets/qfl-logo.svg (simple torus + QFL monogram)\n# - assets/og-image.png (copy from previously generated image if available)\n# - README.md\n# - LICENSE (MIT)\n# Then zip everything for download.\n\nimport os, zipfile, shutil, textwrap, json, pathlib\n\nroot = \"/mnt/data/qfl_site\"\nassets = os.path.join(root, \"assets\")\nos.makedirs(assets, exist_ok=True)\n\n# Minimal SVG logo (monospace QFL with torus-ish ring)\nqfl_svg = \"\"\"<svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 640 640\" role=\"img\" aria-label=\"QuantumForgeLabs logo\">\n  <defs>\n    <linearGradient id=\"g\" x1=\"0\" x2=\"1\" y1=\"0\" y2=\"1\">",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1431",
    "name": "transcribe 1.py",
    "path": "github_repo/scripts/01_core_ai_analysis/transcription/transcribe 1.py",
    "category": "01_core_ai_analysis",
    "type": "transcription",
    "lines": 76,
    "size": 2537,
    "docstring": "",
    "keywords": [
      "analysis",
      "transcription"
    ],
    "functions": [
      "convert_video_to_audio",
      "transcribe_audio",
      "save_transcription",
      "process_directory",
      "main"
    ],
    "classes": [],
    "imports": [
      "os",
      "sys",
      "ffmpeg",
      "whisper"
    ],
    "preview": "import os\nimport sys\n\nimport ffmpeg\n\nimport whisper\n\n\ndef convert_video_to_audio(video_file, output_dir):\n    try:\n        base_name = os.path.basename(video_file)\n        output_file = os.path.join(output_dir, f\"{os.path.splitext(base_name)[0]}.mp3\")\n\n        ffmpeg.input(video_file).output(output_file).run(overwrite_output=True)\n        print(f\"Converted {video_file} to {output_file}\")\n        return output_file\n    except Exception as e:\n        print(f\"An error occurred while converting {video_file}: {e}\")\n        return None\n",
    "last_modified": "2025-09-11T13:27:02.807489"
  },
  {
    "id": "1432",
    "name": "mp3-csv 5.py",
    "path": "github_repo/scripts/01_core_ai_analysis/transcription/mp3-csv 5.py",
    "category": "01_core_ai_analysis",
    "type": "audio_processing",
    "lines": 101,
    "size": 4057,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis",
      "transcription"
    ],
    "functions": [
      "format_timestamp",
      "transcribe_audio",
      "analyze_text_for_section",
      "process_audio_file",
      "process_audio_directory"
    ],
    "classes": [],
    "imports": [
      "os",
      "openai",
      "dotenv"
    ],
    "preview": "import os\n\nimport openai\nfrom dotenv import load_dotenv\n\n# Load environment variables from ~/.env\nload_dotenv()\nopenai.api_key = os.getenv(\"OPENAI_API_KEY\")\nif not openai.api_key:\n    raise EnvironmentError(\"OpenAI API key not found. Please check your .env file.\")\n\n\ndef format_timestamp(seconds):\n    \"\"\"Format seconds into MM:SS timestamp.\"\"\"\n    minutes, seconds = divmod(int(seconds), 60)  # Convert seconds to int\n    return f\"{minutes:02d}:{seconds:02d}\"\n\n\ndef transcribe_audio(file_path):\n    \"\"\"Transcribe audio file using OpenAI's Whisper API.\"\"\"",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1433",
    "name": "deep_analyze_split.py",
    "path": "github_repo/scripts/01_core_ai_analysis/transcription/deep_analyze_split.py",
    "category": "01_core_ai_analysis",
    "type": "analysis",
    "lines": 133,
    "size": 6216,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis",
      "transcription"
    ],
    "functions": [
      "format_timestamp",
      "get_video_duration",
      "transcribe_audio",
      "analyze_text",
      "process_media_directory"
    ],
    "classes": [],
    "imports": [
      "logging",
      "os",
      "sys",
      "pathlib",
      "openai",
      "dotenv",
      "moviepy.editor"
    ],
    "preview": "import logging\nimport os\nimport sys\nfrom pathlib import Path\nimport openai\nfrom dotenv import load_dotenv\nfrom moviepy.editor import VideoFileClip\n\n# Setup logging\nlogging.basicConfig(level=logging.INFO, format=\"%(asctime)s [%(levelname)s] %(message)s\")\n\n# Load environment variables\nenv_path = Path(\"~/.env\")\nload_dotenv(dotenv_path=env_path)\nopenai.api_key = os.getenv(\"OPENAI_API_KEY\")\n\n# Base config\nconfig = {\"base_dir\": \"\"}\nif not config.get(\"base_dir\"):\n    base_dir_input = input(\"Enter base directory (leave blank for current): \").strip()",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1434",
    "name": "analyze-shorts-info (1).py",
    "path": "github_repo/scripts/01_core_ai_analysis/transcription/analyze-shorts-info (1).py",
    "category": "01_core_ai_analysis",
    "type": "analysis",
    "lines": 122,
    "size": 4761,
    "docstring": "",
    "keywords": [
      "analysis",
      "transcription"
    ],
    "functions": [
      "transcribe_audio",
      "format_timestamp",
      "analyze_text_for_section",
      "process_audio_directory"
    ],
    "classes": [],
    "imports": [
      "logging",
      "os",
      "sys",
      "time",
      "concurrent.futures",
      "dotenv",
      "termcolor",
      "tqdm"
    ],
    "preview": "cimport openai\n\nimport logging\nimport os\nimport sys\nimport time\nfrom concurrent.futures import ThreadPoolExecutor\n\nfrom dotenv import load_dotenv\nfrom termcolor import colored\nfrom tqdm import tqdm\n\n# Load environment variables from .env (make sure your OPENAI_API_KEY is stored here)\nenv_path = '/Users/steven/.env'  # Update this path if necessary\nload_dotenv(dotenv_path=env_path)\n\n# Initialize OpenAI API key\nopenai.api_key = os.getenv('OPENAI_API_KEY')\n\n# Error checking for OpenAI API key",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1435",
    "name": "speech_recog_transformers.py",
    "path": "github_repo/scripts/01_core_ai_analysis/transcription/speech_recog_transformers.py",
    "category": "01_core_ai_analysis",
    "type": "transcription",
    "lines": 36,
    "size": 1118,
    "docstring": "author: abhilasha_lodha",
    "keywords": [],
    "functions": [
      "speech2TextTransformer"
    ],
    "classes": [],
    "imports": [
      "os",
      "librosa",
      "numpy",
      "soundfile",
      "torch",
      "IPython.display",
      "scipy.io",
      "transformers"
    ],
    "preview": "\"\"\"\nauthor: abhilasha_lodha\n\"\"\"\n\nimport os\n\nimport librosa\nimport numpy as np\nimport soundfile as sf\nimport torch\nfrom IPython.display import Audio\nfrom scipy.io import wavfile\nfrom transformers import Wav2Vec2ForCTC, Wav2Vec2Tokenizer\n\ntokenizer = Wav2Vec2Tokenizer.from_pretrained(\"facebook/wav2vec2-base-960h\")\nmodel = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-base-960h\")\n\n\ndef speech2TextTransformer(folder_path):\n    for files in os.listdir(folder_path):",
    "last_modified": "2025-03-28T18:37:11"
  },
  {
    "id": "1436",
    "name": "story-key-trans.py",
    "path": "github_repo/scripts/01_core_ai_analysis/transcription/story-key-trans.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 137,
    "size": 6492,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis",
      "transcription"
    ],
    "functions": [
      "format_timestamp",
      "transcribe_file",
      "analyze_text_for_section",
      "process_media_directory"
    ],
    "classes": [],
    "imports": [
      "os",
      "subprocess",
      "openai",
      "dotenv"
    ],
    "preview": "import os\nimport subprocess\n\nimport openai\nfrom dotenv import load_dotenv\n\n# Load OpenAI API Key\nload_dotenv()\nopenai.api_key = os.getenv(\"OPENAI_API_KEY\")\n\n# Error checking for OpenAI API key\nif not openai.api_key:\n    raise EnvironmentError(\"OpenAI API key not found. Please check your .env file.\")\n\n\n# Helper to format timestamps\ndef format_timestamp(seconds):\n    \"\"\"Convert seconds into the format MM:SS.\"\"\"\n    minutes = int(seconds // 60)\n    seconds = seconds % 60",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1437",
    "name": "batch-info-python.py",
    "path": "github_repo/scripts/01_core_ai_analysis/transcription/batch-info-python.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 193,
    "size": 7135,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis"
    ],
    "functions": [
      "categorize_script",
      "generate_pydocgen",
      "get_openai_batch_titles",
      "suggest_script_titles_batch",
      "process_directory_with_batching"
    ],
    "classes": [],
    "imports": [
      "os",
      "openai",
      "csv",
      "re",
      "subprocess",
      "collections",
      "dotenv"
    ],
    "preview": "import os\n\nfrom openai import OpenAI\n\nclient = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\nimport csv\nimport re\nimport subprocess\nfrom collections import Counter\n\nfrom dotenv import load_dotenv\n\n# Load environment variables from the specified .env file\nload_dotenv(\"/Users/steven/Documents/python/.env\")\n\n# Set OpenAI API key from the environment variable\n\n\n# Function to categorize scripts based on content\ndef categorize_script(content, file_name):",
    "last_modified": "2025-09-13T05:53:47.775497"
  },
  {
    "id": "1438",
    "name": "migrate_projects.py",
    "path": "github_repo/scripts/01_core_ai_analysis/transcription/migrate_projects.py",
    "category": "01_core_ai_analysis",
    "type": "organization",
    "lines": 497,
    "size": 18870,
    "docstring": "Automated Python Projects Reorganization Script\nSafely migrates your projects to the new organized structure",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "__init__",
      "log_action",
      "create_backup",
      "create_new_structure",
      "create_shared_libraries",
      "migrate_analysis_scripts",
      "migrate_youtube_projects",
      "migrate_ai_creative_tools",
      "migrate_web_scraping_tools",
      "migrate_audio_video_tools"
    ],
    "classes": [
      "ProjectMigrator"
    ],
    "imports": [
      "os",
      "shutil",
      "json",
      "pathlib",
      "datetime"
    ],
    "preview": "#!/usr/bin/env python3\n\"\"\"\nAutomated Python Projects Reorganization Script\nSafely migrates your projects to the new organized structure\n\"\"\"\n\nimport os\nimport shutil\nimport json\nfrom pathlib import Path\nfrom datetime import datetime\n\nclass ProjectMigrator:\n    def __init__(self, base_path=\"/Users/steven/Documents/python\"):\n        self.base_path = Path(base_path)\n        self.migration_log = []\n        self.backup_path = self.base_path / \"MIGRATION_BACKUP\"\n        \n    def log_action(self, action, source, destination=None, status=\"SUCCESS\"):\n        \"\"\"Log migration actions for rollback capability.\"\"\"",
    "last_modified": "2025-10-09T05:25:40.903623"
  },
  {
    "id": "1439",
    "name": "speech_to_text.py",
    "path": "github_repo/scripts/01_core_ai_analysis/transcription/speech_to_text.py",
    "category": "01_core_ai_analysis",
    "type": "transcription",
    "lines": 97,
    "size": 3323,
    "docstring": "",
    "keywords": [],
    "functions": [
      "mp3_to_wav_Conversion",
      "split_files_with_timestamp",
      "writeInFile_key_value"
    ],
    "classes": [],
    "imports": [
      "os",
      "subprocess",
      "time",
      "speech_recognition",
      "pydub",
      "pydub.utils"
    ],
    "preview": "import os\nimport subprocess\nimport time\n\nimport speech_recognition as sr\nfrom pydub import AudioSegment\nfrom pydub.utils import make_chunks\n\n\ndef mp3_to_wav_Conversion(mp3_src, wav_dst):\n    subprocess.call([\"ffmpeg\", \"-i\", mp3_src, wav_dst])\n    # test audio of the dst file\n    test_audio = AudioSegment.from_file(wav_dst, \"wav\")\n    return test_audio\n\n\ndef split_files_with_timestamp(test_audio):\n    chunk_length_ms = 10000\n    chunks = make_chunks(test_audio, chunk_length_ms)\n    return chunks",
    "last_modified": "2025-09-13T05:53:45.851465"
  },
  {
    "id": "1440",
    "name": "config 2.py",
    "path": "github_repo/scripts/01_core_ai_analysis/transcription/config 2.py",
    "category": "01_core_ai_analysis",
    "type": "setup",
    "lines": 22,
    "size": 762,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [],
    "preview": "# MODEL SETTINGS\nMODEL = \"gpt-4-1106-preview\"  # Updated model name\nAPI_PARAM = {\n    \"engine\": MODEL,\n    \"max_tokens\": 512,\n    \"temperature\": 0.77,\n    \"top_p\": 1,\n    \"frequency_penalty\": 0.28,\n    \"presence_penalty\": 0.13,\n}\n# VIDEO SETTINGS\nCHANNEL_NAME = \"iChoake\"\nDURATION = 8\nSIZE = (1080, 1920)\nFPS = 30\n# FOLDERS\nVIDEO = \"video\"\nMUSIC = \"music\"\n\nVID_TO_GENRATE = 12  # How many videos generate for each request",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1441",
    "name": "batch_image_seo_pipeline_20250530220659.py",
    "path": "github_repo/scripts/01_core_ai_analysis/transcription/batch_image_seo_pipeline_20250530220659.py",
    "category": "01_core_ai_analysis",
    "type": "image_processing",
    "lines": 409,
    "size": 16734,
    "docstring": "",
    "keywords": [
      "openai"
    ],
    "functions": [
      "load_openai_client",
      "retry_on_api_error",
      "call_gpt4o",
      "get_image_tech_meta",
      "build_source_tag",
      "discover_images",
      "build_gpt_messages",
      "parse_args",
      "main"
    ],
    "classes": [],
    "imports": [
      "argparse",
      "csv",
      "json",
      "logging",
      "os",
      "sys",
      "time",
      "datetime",
      "pathlib",
      "typing"
    ],
    "preview": "import argparse\nimport csv\nimport json\nimport logging\nimport os\nimport sys\nimport time\nfrom datetime import datetime\nfrom pathlib import Path\nfrom typing import Any, Dict, List, Optional\n\nimport backoff  # pip install backoff\nfrom dotenv import load_dotenv\nfrom openai import OpenAI, OpenAIAPIError\nfrom PIL import Image, UnidentifiedImageError\nfrom tqdm import tqdm\n\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n# CONFIGURATION\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500",
    "last_modified": "2025-09-13T05:53:25.488247"
  },
  {
    "id": "1442",
    "name": "mp3-csv 4.py",
    "path": "github_repo/scripts/01_core_ai_analysis/transcription/mp3-csv 4.py",
    "category": "01_core_ai_analysis",
    "type": "audio_processing",
    "lines": 101,
    "size": 4057,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis",
      "transcription"
    ],
    "functions": [
      "format_timestamp",
      "transcribe_audio",
      "analyze_text_for_section",
      "process_audio_file",
      "process_audio_directory"
    ],
    "classes": [],
    "imports": [
      "os",
      "openai",
      "dotenv"
    ],
    "preview": "import os\n\nimport openai\nfrom dotenv import load_dotenv\n\n# Load environment variables from ~/.env\nload_dotenv()\nopenai.api_key = os.getenv(\"OPENAI_API_KEY\")\nif not openai.api_key:\n    raise EnvironmentError(\"OpenAI API key not found. Please check your .env file.\")\n\n\ndef format_timestamp(seconds):\n    \"\"\"Format seconds into MM:SS timestamp.\"\"\"\n    minutes, seconds = divmod(int(seconds), 60)  # Convert seconds to int\n    return f\"{minutes:02d}:{seconds:02d}\"\n\n\ndef transcribe_audio(file_path):\n    \"\"\"Transcribe audio file using OpenAI's Whisper API.\"\"\"",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1443",
    "name": "story-key-trans copy.py",
    "path": "github_repo/scripts/01_core_ai_analysis/transcription/story-key-trans copy.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 142,
    "size": 6647,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis",
      "transcription"
    ],
    "functions": [
      "format_timestamp",
      "transcribe_file",
      "analyze_text_for_section",
      "process_media_directory"
    ],
    "classes": [],
    "imports": [
      "os",
      "subprocess",
      "openai",
      "dotenv"
    ],
    "preview": "import os\nimport subprocess\n\nfrom openai import OpenAI\n\nclient = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\nfrom dotenv import load_dotenv\n\n# Load environment variables from .env\nenv_path = os.path.expanduser(\"~/.env\")\n# Update the path if needed\nload_dotenv(dotenv_path=env_path)\n\n# Initialize OpenAI API key\n\n# Error checking for OpenAI API key\nif not openai.api_key:\n    raise EnvironmentError(\"OpenAI API key not found. Please check your .env file.\")\n\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1444",
    "name": "analyze-1.py",
    "path": "github_repo/scripts/01_core_ai_analysis/transcription/analyze-1.py",
    "category": "01_core_ai_analysis",
    "type": "analysis",
    "lines": 60,
    "size": 3440,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis"
    ],
    "functions": [
      "analyze_text"
    ],
    "classes": [],
    "imports": [
      "dotenv",
      "openai",
      "os"
    ],
    "preview": "from dotenv import load_dotenv\nfrom openai import OpenAI\n\nclient = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\nimport os\n\n# Load environment variables from .env\nenv_path = os.path.expanduser(\"~/.env\")\n# Update the path if needed\nload_dotenv(dotenv_path=env_path)\n\n# Error checking for openai API key\nif not openai.api_key:\n    raise EnvironmentError(\"openai API key not found. Please check your .env file.\")\n\n\ndef analyze_text(text):\n    response = client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        messages=[",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1445",
    "name": "deep_content_analyzer.py",
    "path": "github_repo/scripts/01_core_ai_analysis/transcription/deep_content_analyzer.py",
    "category": "01_core_ai_analysis",
    "type": "analysis",
    "lines": 590,
    "size": 25441,
    "docstring": "Deep Content Analysis and Reorganization Tool\nAnalyzes file content to determine actual functionality and reorganize accordingly",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "main",
      "__init__",
      "analyze_python_file",
      "extract_imports",
      "extract_functions",
      "extract_classes",
      "detect_apis",
      "extract_keywords",
      "determine_purpose",
      "identify_main_functionality"
    ],
    "classes": [
      "DeepContentAnalyzer"
    ],
    "imports": [
      "os",
      "re",
      "ast",
      "json",
      "pathlib",
      "collections",
      "typing"
    ],
    "preview": "#!/usr/bin/env python3\n\"\"\"\nDeep Content Analysis and Reorganization Tool\nAnalyzes file content to determine actual functionality and reorganize accordingly\n\"\"\"\n\nimport os\nimport re\nimport ast\nimport json\nfrom pathlib import Path\nfrom collections import defaultdict, Counter\nfrom typing import Dict, List, Set, Tuple\n\nclass DeepContentAnalyzer:\n    def __init__(self, base_path=\"/Users/steven/Documents/python\"):\n        self.base_path = Path(base_path)\n        self.analysis_results = {\n            \"file_analysis\": {},\n            \"function_categories\": defaultdict(list),",
    "last_modified": "2025-10-09T05:35:18.315372"
  },
  {
    "id": "1446",
    "name": "avatar-download.py",
    "path": "github_repo/scripts/01_core_ai_analysis/transcription/avatar-download.py",
    "category": "01_core_ai_analysis",
    "type": "youtube",
    "lines": 108,
    "size": 2360,
    "docstring": "",
    "keywords": [
      "youtube"
    ],
    "functions": [
      "list_remote_files",
      "list_local_files",
      "download_files"
    ],
    "classes": [],
    "imports": [
      "os",
      "paramiko"
    ],
    "preview": "import os\n\nimport paramiko\n\n# SSH Config\nhostname = \"access981577610.webspace-data.io\"\nusername = \"u114071855\"\npassword = \"A^p1yT@AHn*akbhs\"\nlocal_dir = \"/Users/steven/AvaTarArTs\"\nrremote_dirs = [\n    \"/2025\",\n    \"/Users\",\n    \"/all\",\n    \"/audio-texts\",\n    \"/blog\",\n    \"/build\",\n    \"/card\",\n    \"/city\",\n    \"/clickandbuilds\",\n    \"/convo\",",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1447",
    "name": "Whisper-Quiz-Voice.py",
    "path": "github_repo/scripts/01_core_ai_analysis/transcription/Whisper-Quiz-Voice.py",
    "category": "01_core_ai_analysis",
    "type": "transcription",
    "lines": 90,
    "size": 3032,
    "docstring": "",
    "keywords": [
      "image_processing",
      "openai",
      "analysis",
      "web_tools"
    ],
    "functions": [
      "generate_base_prompt",
      "refine_prompt",
      "create_image",
      "upscale_image",
      "text_to_speech"
    ],
    "classes": [],
    "imports": [
      "csv",
      "io",
      "pathlib",
      "requests",
      "openai",
      "PIL"
    ],
    "preview": "import csv\nfrom io import BytesIO\nfrom pathlib import Path\n\nimport requests\nfrom openai import OpenAI\nfrom PIL import Image\n\n# Initialize the OpenAI client\nclient = OpenAI()\n\ndef generate_base_prompt(question, options):\n    # Combine question and options to generate a base prompt\n    return f\"Question: {question} Options: {', '.join(options)}\"\n\ndef refine_prompt(prompt):\n    # Refine the prompt using ChatGPT for more creativity\n    response = client.chat.completions.create(\n        model=\"gpt-4\",\n        messages=[{\"role\": \"system\", \"content\": \"You are a creative writer.\"},",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1448",
    "name": "info 1.py",
    "path": "github_repo/scripts/01_core_ai_analysis/transcription/info 1.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 159,
    "size": 7063,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis",
      "transcription"
    ],
    "functions": [
      "split_media_to_segments",
      "transcribe_media_segment",
      "format_timestamp",
      "analyze_text_for_section",
      "process_media_by_section"
    ],
    "classes": [],
    "imports": [
      "os",
      "subprocess",
      "openai",
      "dotenv",
      "sys"
    ],
    "preview": "import os\nimport subprocess\n\nimport openai\nfrom dotenv import load_dotenv\n\n# Load environment variables from .env (make sure your OPENAI_API_KEY is stored here)\nload_dotenv(\"/Users/steven/.env\")\nopenai.api_key = os.getenv(\"OPENAI_API_KEY\")\n\n# Prompt for directory paths\nproject_root = input(\"Please enter the project root directory path: \").strip()\n\n# Directory paths based on user input\nMEDIA_DIR = os.path.join(project_root, \"Media\")  # Directory containing media files\nTRANSCRIPT_DIR = os.path.join(MEDIA_DIR, \"transcript\")  # Directory to save transcripts\nANALYSIS_DIR = os.path.join(MEDIA_DIR, \"analysis\")  # Directory to save the analysis files\n\n# Create output directories if they don't exist\nos.makedirs(TRANSCRIPT_DIR, exist_ok=True)",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1449",
    "name": "analyze_merged.py",
    "path": "github_repo/scripts/01_core_ai_analysis/transcription/analyze_merged.py",
    "category": "01_core_ai_analysis",
    "type": "analysis",
    "lines": 293,
    "size": 14578,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis",
      "transcription"
    ],
    "functions": [
      "format_timestamp",
      "parse_transcript",
      "transcribe_audio",
      "analyze_text_for_section",
      "link_timestamps_to_analysis",
      "process_audio_file",
      "process_audio_directory",
      "get_openai_api_key",
      "get_directory_path",
      "get_pydocgen_paths"
    ],
    "classes": [
      "Analyze(base.Command)"
    ],
    "imports": [
      "logging",
      "os",
      "sys",
      "time",
      "concurrent.futures",
      "dotenv",
      "openai",
      "termcolor",
      "tqdm",
      "subprocess"
    ],
    "preview": "import logging\nimport os\nimport sys\nimport time\nfrom concurrent.futures import ThreadPoolExecutor\n\nfrom dotenv import load_dotenv\nfrom openai import OpenAI\nfrom termcolor import colored\nfrom tqdm import tqdm\nclient = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n# Load environment variables from .env (make sure your OPENAI_API_KEY is stored here)\nenv_path = os.path.expanduser(\"~/.env\")\nload_dotenv(dotenv_path=env_path)\n# Configure logging for error reporting\nlogging.basicConfig(\n    filename=\"transcription_analysis_errors.log\",\n    level=logging.ERROR,\n    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n)",
    "last_modified": "2025-10-08T06:39:59"
  },
  {
    "id": "1450",
    "name": "all.py",
    "path": "github_repo/scripts/01_core_ai_analysis/transcription/all.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 229,
    "size": 7174,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis"
    ],
    "functions": [
      "get_creation_date",
      "format_file_size",
      "format_duration",
      "write_csv",
      "get_unique_file_path",
      "process_audio_file",
      "process_image_file",
      "process_video_file",
      "process_files",
      "categorize_script"
    ],
    "classes": [],
    "imports": [
      "csv",
      "logging",
      "os",
      "re",
      "datetime",
      "typing",
      "dotenv",
      "mutagen.easyid3",
      "mutagen.mp3",
      "mutagen.mp4"
    ],
    "preview": "import csv\nimport logging\nimport os\nimport re\nfrom datetime import datetime\nfrom typing import Optional, Tuple\n\nfrom dotenv import load_dotenv\nfrom mutagen.easyid3 import EasyID3\nfrom mutagen.mp3 import MP3\nfrom mutagen.mp4 import MP4\nfrom openai import OpenAI\nfrom PIL import Image\n\n# Initialize OpenAI\nload_dotenv(\"/Users/steven/.env\")\nopenai = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n\n# Set up logging\nlogging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")",
    "last_modified": "2025-09-13T05:53:47.534883"
  },
  {
    "id": "1451",
    "name": "WhisperTranscriber.py",
    "path": "github_repo/scripts/01_core_ai_analysis/transcription/WhisperTranscriber.py",
    "category": "01_core_ai_analysis",
    "type": "transcription",
    "lines": 202,
    "size": 7564,
    "docstring": "",
    "keywords": [
      "transcription"
    ],
    "functions": [
      "load_config",
      "get_media_files",
      "transcribe_audio",
      "write_srt_with_word_timestamps",
      "write_srt_with_default_line_breaks",
      "format_timestamp"
    ],
    "classes": [],
    "imports": [
      "json",
      "pathlib",
      "the",
      "yaml",
      "whisper"
    ],
    "preview": "# WhisperTranscriber\n# src: https://github.com/VimWei/WhisperTranscriber\n# Implement Whisper's basic parameter configuration\n# Implement parameter control of srt output, so that verbatim srt can be achieved\n# max_line_width\uff0cmax_line_count\uff0cmax_words_per_line\n# Realize free switching of srt line breaking control: manual or automatic\n\nimport json\nfrom pathlib import Path\n\nimport the\nimport yaml\n\nimport whisper\n\n\ndef load_config(config_file=\"config.yaml\"):\n    \"\"\"Load configuration file\"\"\"\n    with open(config_file, \"r\", encoding=\"utf-8\") as f:\n        return yaml.safe_load(f)",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1452",
    "name": "docgen.py",
    "path": "github_repo/scripts/01_core_ai_analysis/transcription/docgen.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 204,
    "size": 7635,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis"
    ],
    "functions": [
      "categorize_script",
      "generate_pydocgen",
      "get_openai_batch_titles",
      "suggest_script_titles_batch",
      "process_directory_with_batching"
    ],
    "classes": [],
    "imports": [
      "os",
      "openai",
      "csv",
      "subprocess",
      "dotenv"
    ],
    "preview": "import os\n\nfrom openai import OpenAI\n\nclient = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\nimport csv\nimport subprocess\n\nfrom dotenv import load_dotenv\n\n# Load environment variables from the specified .env file\nload_dotenv(\"/Users/steven/Documents/python/.env\")\n\n# Set OpenAI API key from the environment variable\n\n\n# Function to categorize scripts based on content\ndef categorize_script(content, file_name):\n    if \"image\" in content.lower() or \"convert\" in file_name.lower():\n        return \"Image Conversion and Upscaling Script\"",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1453",
    "name": "test.py",
    "path": "github_repo/scripts/01_core_ai_analysis/transcription/test.py",
    "category": "01_core_ai_analysis",
    "type": "testing",
    "lines": 57,
    "size": 1994,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis",
      "transcription"
    ],
    "functions": [
      "transcribe_audio",
      "analyze_text"
    ],
    "classes": [],
    "imports": [
      "openai",
      "os",
      "dotenv",
      "sys"
    ],
    "preview": "from openai import OpenAI\n\nclient = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\nimport os\n\nfrom dotenv import load_dotenv\n\n# Load environment variables from .env\nload_dotenv()\n\n\n# Function to transcribe audio using OpenAI Whisper\ndef transcribe_audio(file_path):\n    with open(file_path, \"rb\") as audio_file:\n        transcript = client.audio.transcribe(\"whisper-1\", audio_file)\n        return transcript.text\n\n\n# Function to analyze the transcript using GPT\ndef analyze_text(text):",
    "last_modified": "2025-05-04T22:47:12.998801"
  },
  {
    "id": "1454",
    "name": "transcribe_mp3.py",
    "path": "github_repo/scripts/01_core_ai_analysis/transcription/transcribe_mp3.py",
    "category": "01_core_ai_analysis",
    "type": "transcription",
    "lines": 51,
    "size": 1454,
    "docstring": "",
    "keywords": [
      "analysis",
      "transcription"
    ],
    "functions": [
      "transcribe_audio",
      "save_transcription",
      "process_directory",
      "main"
    ],
    "classes": [],
    "imports": [
      "os",
      "whisper"
    ],
    "preview": "import os\n\nimport whisper\n\n\ndef transcribe_audio(file_path):\n    # Load the Whisper model\n    model = whisper.load_model(\"base\")\n\n    # Transcribe the audio file\n    result = model.transcribe(file_path)\n\n    return result[\"segments\"]\n\n\ndef save_transcription(segments, output_file):\n    with open(output_file, \"w\") as f:\n        for segment in segments:\n            start = segment[\"start\"]\n            end = segment[\"end\"]",
    "last_modified": "2025-09-13T05:55:25.704656"
  },
  {
    "id": "1455",
    "name": "Quiz22s.py",
    "path": "github_repo/scripts/01_core_ai_analysis/transcription/Quiz22s.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 118,
    "size": 4341,
    "docstring": "",
    "keywords": [
      "analysis",
      "web_tools"
    ],
    "functions": [
      "generate_speech",
      "calculate_text_duration",
      "process_csv_and_generate_speech"
    ],
    "classes": [],
    "imports": [
      "csv",
      "os",
      "requests",
      "dotenv",
      "pydub"
    ],
    "preview": "import csv\nimport os\n\nimport requests  # Make sure to install this package\nfrom dotenv import load_dotenv\nfrom pydub import AudioSegment  # Make sure to install pydub package\n\n\ndef generate_speech(input_text, output_path, api_key):\n    \"\"\"\n    Generates speech from text using an API (e.g., OpenAI's text-to-speech API).\n    \"\"\"\n    url = \"https://api.openai.com/v1/audio/speech\"\n    headers = {\n        \"Authorization\": f\"Bearer {api_key}\",\n        \"Content-Type\": \"application/json\",\n    }\n    data = {\n        \"model\": \"tts-1\",  # Update this according to the specific model you're using\n        \"input\": input_text,",
    "last_modified": "2025-09-13T05:53:54.737514"
  },
  {
    "id": "1456",
    "name": "example_usage.py",
    "path": "github_repo/scripts/01_core_ai_analysis/transcription/example_usage.py",
    "category": "01_core_ai_analysis",
    "type": "analysis",
    "lines": 98,
    "size": 3164,
    "docstring": "Example usage of the Transcription Analyzer",
    "keywords": [],
    "functions": [
      "example_single_file",
      "example_batch_processing",
      "show_output_structure",
      "main"
    ],
    "classes": [],
    "imports": [
      "os",
      "transcription_analyzer",
      "dotenv"
    ],
    "preview": "#!/usr/bin/env python3\n\"\"\"\nExample usage of the Transcription Analyzer\n\"\"\"\n\nimport os\nfrom transcription_analyzer import TranscriptionAnalyzer\nfrom dotenv import load_dotenv\n\n# Load environment variables from ~/.env\nload_dotenv(os.path.expanduser(\"~/.env\"))\n\ndef example_single_file():\n    \"\"\"Example of processing a single file.\"\"\"\n    print(\"Example: Processing a single file\")\n    print(\"-\" * 40)\n    \n    # Check for API key\n    api_key = os.getenv('OPENAI_API_KEY')\n    if not api_key:",
    "last_modified": "2025-10-09T05:14:50.275937"
  },
  {
    "id": "1457",
    "name": "mp4-mp3-analyze2 copy.py",
    "path": "github_repo/scripts/01_core_ai_analysis/transcription/mp4-mp3-analyze2 copy.py",
    "category": "01_core_ai_analysis",
    "type": "analysis",
    "lines": 189,
    "size": 6198,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis",
      "transcription"
    ],
    "functions": [
      "format_timestamp",
      "run_ffmpeg",
      "extract_small_audio",
      "transcribe_audio",
      "analyze_text_for_section",
      "process_media_directory",
      "attempt"
    ],
    "classes": [],
    "imports": [
      "logging",
      "os",
      "subprocess",
      "sys",
      "openai",
      "dotenv"
    ],
    "preview": "import logging\nimport os\nimport subprocess\nimport sys\n\nimport openai\nfrom dotenv import load_dotenv\n\nlogging.basicConfig(level=logging.INFO, format=\"%(asctime)s [%(levelname)s] %(message)s\")\n\nenv_path = \"/Users/steven/.env\"\nload_dotenv(dotenv_path=env_path)\nopenai.api_key = os.getenv(\"OPENAI_API_KEY\")\n\nMAX_SIZE = 25 * 1024 * 1024  # 25 MB hard limit\n\n\n# ----- helpers -----\ndef format_timestamp(seconds: float) -> str:\n    m = int(seconds // 60)",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1458",
    "name": "quiz-text-speech.py",
    "path": "github_repo/scripts/01_core_ai_analysis/transcription/quiz-text-speech.py",
    "category": "01_core_ai_analysis",
    "type": "transcription",
    "lines": 88,
    "size": 2698,
    "docstring": "",
    "keywords": [
      "analysis",
      "web_tools"
    ],
    "functions": [
      "generate_description",
      "upscale_image",
      "text_to_speech"
    ],
    "classes": [],
    "imports": [
      "csv",
      "os",
      "io",
      "pathlib",
      "openAI",
      "requests",
      "PIL"
    ],
    "preview": "import csv\nimport os\nfrom io import BytesIO\nfrom pathlib import Path\n\nimport openAI\nimport requests\nfrom PIL import Image\n\n# Initialize the OpenAI client\nclient = openAI(api_key=\"your-api-key\")\n\n\ndef generate_description(image_url):\n    # Generate a description for the image using GPT-4 with Vision\n    response = client.chat.completions.create(\n        model=\"gpt-4-vision-preview\",\n        messages=[\n            {\n                \"role\": \"user\",",
    "last_modified": "2025-09-13T05:53:50.964576"
  },
  {
    "id": "1459",
    "name": "dalle.py",
    "path": "github_repo/scripts/01_core_ai_analysis/transcription/dalle.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 86,
    "size": 2510,
    "docstring": "",
    "keywords": [
      "data_processing",
      "openai",
      "analysis",
      "web_tools"
    ],
    "functions": [
      "generate_content"
    ],
    "classes": [],
    "imports": [
      "os",
      "openai",
      "io",
      "pandas",
      "requests",
      "PIL",
      "tqdm"
    ],
    "preview": "import os\n\nfrom openai import OpenAI\n\nclient = OpenAI(api_key=\"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\")\nimport io\n\nimport pandas as pd\nimport requests\nfrom PIL import Image\nfrom tqdm import tqdm\n\n# Set OpenAI and Stability API keys\nstability_ai_key = \"sk-2O6mutk6X4HI9olxMeGKYv0MpXcNRzs6fVRPJMdEvgEzRRII\"\n\n\n# Function to generate content based on a prompt\ndef generate_content(prompt):\n    response = client.chat.completions.create(\n        model=\"gpt-4\",",
    "last_modified": "2025-09-13T05:53:45.949415"
  },
  {
    "id": "1460",
    "name": "main 2.py",
    "path": "github_repo/scripts/01_core_ai_analysis/transcription/main 2.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 21,
    "size": 603,
    "docstring": "",
    "keywords": [],
    "functions": [
      "main"
    ],
    "classes": [],
    "imports": [
      "csv",
      "generate_speech"
    ],
    "preview": "import csv\n\nfrom generate_speech import generate_speech\n\n\ndef main():\n    csv_path = \"quiz329.csv\"\n\n    with open(csv_path, newline=\"\", encoding=\"utf-8\") as csvfile:\n        reader = csv.DictReader(csvfile)\n        for i, row in enumerate(reader):\n            # Assuming 'Question' is the column name\n            question_text = row[\"Question\"]\n            output_path = f\"/ Users / steven / Documents / quiz - talk / quiz329 / question / question_{\n                i + 1}.mp3\"\n            generate_speech(question_text, voice=\"shimmer\", output_path=output_path)\n\n\nif __name__ == \"__main__\":\n    main()",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1461",
    "name": "analyze 2.py",
    "path": "github_repo/scripts/01_core_ai_analysis/transcription/analyze 2.py",
    "category": "01_core_ai_analysis",
    "type": "analysis",
    "lines": 120,
    "size": 4133,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis"
    ],
    "functions": [
      "get_openai_api_key",
      "get_directory_path",
      "get_pydocgen_paths",
      "generate_docs",
      "enhance_docs",
      "run_flake8"
    ],
    "classes": [],
    "imports": [
      "os",
      "subprocess",
      "openai"
    ],
    "preview": "import os\nimport subprocess\n\nimport openai\n\n\n# Prompt the user to input the OpenAI API key if it's not found in environment variables\ndef get_openai_api_key():\n    api_key = os.getenv(\"OPENAI_API_KEY\")\n    if not api_key:\n        api_key = input(\"Enter your OpenAI API key: \").strip()\n    return api_key\n\n\n# Prompt the user for directory paths if not provided\ndef get_directory_path(prompt_message, default_path):\n    directory = input(f\"{prompt_message} (default: {default_path}): \").strip()\n    return directory if directory else default_path\n\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1462",
    "name": "trek3.py",
    "path": "github_repo/scripts/01_core_ai_analysis/transcription/trek3.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 73,
    "size": 2547,
    "docstring": "",
    "keywords": [
      "analysis",
      "web_tools"
    ],
    "functions": [
      "generate_speech",
      "process_csv_and_generate_speech"
    ],
    "classes": [],
    "imports": [
      "csv",
      "os",
      "requests"
    ],
    "preview": "import csv\nimport os\n\nimport requests  # Make sure to install this package\n\n\ndef generate_speech(input_text, output_path, api_key):\n    \"\"\"\n    Generates speech from text using an API (e.g., OpenAI's text-to-speech API).\n    \"\"\"\n    url = \"https://api.openai.com/v1/audio/speech\"\n    headers = {\n        \"Authorization\": f\"Bearer {api_key}\",\n        \"Content-Type\": \"application/json\",\n    }\n    data = {\n        \"model\": \"tts-1\",  # Update this according to the specific model you're using\n        \"input\": input_text,\n        \"voice\": \"shimmer\",  # Update the voice parameter as needed\n    }",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1463",
    "name": "analyze-shorts-info.py",
    "path": "github_repo/scripts/01_core_ai_analysis/transcription/analyze-shorts-info.py",
    "category": "01_core_ai_analysis",
    "type": "analysis",
    "lines": 121,
    "size": 4716,
    "docstring": "",
    "keywords": [
      "analysis",
      "transcription"
    ],
    "functions": [
      "transcribe_audio",
      "format_timestamp",
      "analyze_text_for_section",
      "process_audio_directory"
    ],
    "classes": [],
    "imports": [
      "logging",
      "os",
      "sys",
      "time",
      "concurrent.futures",
      "dotenv",
      "termcolor",
      "tqdm"
    ],
    "preview": "cimport openai\n\nimport logging\nimport os\nimport sys\nimport time\nfrom concurrent.futures import ThreadPoolExecutor\n\nfrom dotenv import load_dotenv\nfrom termcolor import colored\nfrom tqdm import tqdm\n\n# Load environment variables from .env (make sure your OPENAI_API_KEY is stored here)\nenv_path = '/Users/steven/.env'  # Update this path if necessary\nload_dotenv(dotenv_path=env_path)\n\n# Initialize OpenAI API key\n\n# Error checking for OpenAI API key\nif not openai.api_key:",
    "last_modified": "2025-05-04T22:47:13.342876"
  },
  {
    "id": "1464",
    "name": "analyze-promptr.py",
    "path": "github_repo/scripts/01_core_ai_analysis/transcription/analyze-promptr.py",
    "category": "01_core_ai_analysis",
    "type": "analysis",
    "lines": 167,
    "size": 6796,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis",
      "transcription"
    ],
    "functions": [
      "split_video_to_segments",
      "transcribe_video_segment",
      "format_timestamp",
      "analyze_text_for_section",
      "process_video_by_section"
    ],
    "classes": [],
    "imports": [
      "os",
      "subprocess",
      "openai",
      "dotenv",
      "sys"
    ],
    "preview": "import os\nimport subprocess\n\nimport openai\nfrom dotenv import load_dotenv\n\n# Load environment variables from .env (make sure your OPENAI_API_KEY is stored here)\nload_dotenv()\nopenai.api_key = os.getenv(\"OPENAI_API_KEY\")\n\n# Directory paths\nVIDEO_DIR = \"/Users/steven/Music/nocTurneMeLoDieS/mp4\"  # Directory containing MP4 files\nTRANSCRIPT_DIR = (\n    \"/Users/steven/Music/nocTurneMeLoDieS/mp4/transcript\"  # Directory to save transcripts\n)\nANALYSIS_DIR = (\n    \"/Users/steven/Music/nocTurneMeLoDieS/mp4/analysis\"  # Directory to save the analysis files\n)\n\n# Create output directories if they don't exist",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1465",
    "name": "main 2 (1).py",
    "path": "github_repo/scripts/01_core_ai_analysis/transcription/main 2 (1).py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 21,
    "size": 588,
    "docstring": "",
    "keywords": [],
    "functions": [
      "main"
    ],
    "classes": [],
    "imports": [
      "csv",
      "generate_speech"
    ],
    "preview": "import csv\n\nfrom generate_speech import generate_speech\n\n\ndef main():\n    csv_path = \"quiz329.csv\"\n\n    with open(csv_path, newline=\"\", encoding=\"utf-8\") as csvfile:\n        reader = csv.DictReader(csvfile)\n        for i, row in enumerate(reader):\n            # Assuming 'Question' is the column name\n            question_text = row[\"Question\"]\n            output_path = f\"/Users/steven/Documents/quiz-talk/quiz329/question/question_{\n                i + 1}.mp3\"\n            generate_speech(question_text, voice=\"shimmer\", output_path=output_path)\n\n\nif __name__ == \"__main__\":\n    main()",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1466",
    "name": "upscalecreateimages.py",
    "path": "github_repo/scripts/01_core_ai_analysis/transcription/upscalecreateimages.py",
    "category": "01_core_ai_analysis",
    "type": "image_processing",
    "lines": 130,
    "size": 4449,
    "docstring": "",
    "keywords": [
      "data_processing",
      "openai",
      "analysis",
      "web_tools"
    ],
    "functions": [
      "generate_clickable_title",
      "generate_description",
      "generate_tags"
    ],
    "classes": [],
    "imports": [
      "os",
      "openai",
      "io",
      "pandas",
      "requests",
      "PIL",
      "tqdm"
    ],
    "preview": "import os\n\nfrom openai import OpenAI\n\nclient = OpenAI(api_key=\"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\")\nimport io\n\nimport pandas as pd\nimport requests\nfrom PIL import Image\nfrom tqdm import tqdm\n\n# Set OpenAI and Stability.ai API keys\nstability_ai_key = \"sk-2O6mutk6X4HI9olxMeGKYv0MpXcNRzs6fVRPJMdEvgEzRRII\"\n\n# Set Stability API key for image upscaling\napi_key = \"sk-2O6mutk6X4HI9olxMeGKYv0MpXcNRzs6fVRPJMdEvgEzRRII\"\nif api_key is None:\n    raise Exception(\"Missing Stability API key.\")\napi_host = os.getenv(\"API_HOST\", \"https://api.stability.ai\")",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1467",
    "name": "batch_image_seo_pipeline_20250530222518.py",
    "path": "github_repo/scripts/01_core_ai_analysis/transcription/batch_image_seo_pipeline_20250530222518.py",
    "category": "01_core_ai_analysis",
    "type": "image_processing",
    "lines": 122,
    "size": 4833,
    "docstring": "",
    "keywords": [
      "openai"
    ],
    "functions": [
      "call_gpt4o_analysis_and_prompts",
      "batch_flexible_analysis"
    ],
    "classes": [],
    "imports": [
      "csv",
      "json",
      "logging",
      "os",
      "pathlib",
      "dotenv",
      "openai",
      "tqdm"
    ],
    "preview": "import csv\nimport json\nimport logging\nimport os\nfrom pathlib import Path\n\nfrom dotenv import load_dotenv\nfrom openai import OpenAI\nfrom tqdm import tqdm\n\n# CONFIG\nload_dotenv(os.path.expanduser(\"~/.env\"))\nOPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\nif not OPENAI_API_KEY:\n    raise EnvironmentError(\"OPENAI_API_KEY not found in ~/.env\")\nclient = OpenAI(api_key=OPENAI_API_KEY)\n\nLOG_FILE = \"avatararts_flexible_analysis.log\"\nlogging.basicConfig(\n    level=logging.INFO,",
    "last_modified": "2025-09-13T05:53:26.113479"
  },
  {
    "id": "1468",
    "name": "mp4-mp3-analyze2.py",
    "path": "github_repo/scripts/01_core_ai_analysis/transcription/mp4-mp3-analyze2.py",
    "category": "01_core_ai_analysis",
    "type": "analysis",
    "lines": 159,
    "size": 6244,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis",
      "transcription"
    ],
    "functions": [
      "format_timestamp",
      "transcribe_audio",
      "analyze_text_for_section",
      "process_media_directory"
    ],
    "classes": [],
    "imports": [
      "logging",
      "os",
      "subprocess",
      "sys",
      "openai",
      "dotenv"
    ],
    "preview": "import logging\nimport os\nimport subprocess\nimport sys\n\nimport openai\nfrom dotenv import load_dotenv\n\n# Setup basic logging\nlogging.basicConfig(level=logging.INFO, format=\"%(asctime)s [%(levelname)s] %(message)s\")\n\n# Load environment variables from .env (ensure your OPENAI_API_KEY is stored here)\nenv_path = \"/Users/steven/.env\"\nload_dotenv(dotenv_path=env_path)\nopenai.api_key = os.getenv(\"OPENAI_API_KEY\")\n\n# Maximum allowed file size (25 MB)\nMAX_SIZE = 25 * 1024 * 1024  # 25 MB in bytes\n\n# Determine base directory from command-line argument or prompt",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1469",
    "name": "main 5.py",
    "path": "github_repo/scripts/01_core_ai_analysis/transcription/main 5.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 21,
    "size": 626,
    "docstring": "",
    "keywords": [],
    "functions": [
      "main"
    ],
    "classes": [],
    "imports": [
      "csv",
      "generate_speech"
    ],
    "preview": "import csv\n\nfrom generate_speech import generate_speech\n\n\ndef main():\n    csv_path = \"/Users/steven/Music/quiz-talk/Gtrivia - Sheet1.csv\"\n\n    with open(csv_path, newline=\"\", encoding=\"utf-8\") as csvfile:\n        reader = csv.DictReader(csvfile)\n        for i, row in enumerate(reader):\n            # Assuming 'Question' is the column name\n            question_text = row[\"Question\"]\n            output_path = f\"/ Users / steven / Music / quiz - talk / speech / question_{\n                i + 1}.mp3\"\n            generate_speech(question_text, voice=\"shimmer\", output_path=output_path)\n\n\nif __name__ == \"__main__\":\n    main()",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1470",
    "name": "story-section-gpt.py",
    "path": "github_repo/scripts/01_core_ai_analysis/transcription/story-section-gpt.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 129,
    "size": 5945,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis",
      "transcription"
    ],
    "functions": [
      "format_timestamp",
      "transcribe_audio",
      "analyze_text_for_section",
      "process_media_directory"
    ],
    "classes": [],
    "imports": [
      "logging",
      "os",
      "sys",
      "dotenv",
      "openai"
    ],
    "preview": "import logging\nimport os\nimport sys\n\nfrom dotenv import load_dotenv\nfrom openai import OpenAI\n\n# Setup basic logging\nlogging.basicConfig(level=logging.INFO, format=\"%(asctime)s [%(levelname)s] %(message)s\")\n\n# Load environment variables from .env (ensure your OPENAI_API_KEY is stored here)\nenv_path = \"/Users/steven/.env\"\nload_dotenv(dotenv_path=env_path)\n\nclient = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n\n# Define a simple config dictionary. You can expand this as needed.\nconfig = {\"base_dir\": \"\"}  # Leave blank to prompt for base directory\n\n# Prompt for base directory if not set in config; default to current directory if input is empty",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1471",
    "name": "mp4-transcript2.py",
    "path": "github_repo/scripts/01_core_ai_analysis/transcription/mp4-transcript2.py",
    "category": "01_core_ai_analysis",
    "type": "video_processing",
    "lines": 166,
    "size": 7158,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis",
      "transcription"
    ],
    "functions": [
      "split_video_to_segments",
      "transcribe_video_segment",
      "format_timestamp",
      "analyze_text_for_section",
      "process_video_by_section",
      "process_segment"
    ],
    "classes": [],
    "imports": [
      "logging",
      "os",
      "subprocess",
      "concurrent.futures",
      "dotenv",
      "openai",
      "sys"
    ],
    "preview": "import logging\nimport os\nimport subprocess\nfrom concurrent.futures import ThreadPoolExecutor\n\nfrom dotenv import load_dotenv\nfrom openai import OpenAI\n\n# Configure logging\nlogging.basicConfig(\n    filename=\"video_processing.log\",\n    level=logging.INFO,\n    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n)\n\n# Load environment variables\nload_dotenv(dotenv_path=\"/Users/steven/.env\")\n\n# Directory paths\nVIDEO_DIR = \"/Users/steven/Movies/project2025/Media/\"",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1472",
    "name": "polly.py",
    "path": "github_repo/scripts/01_core_ai_analysis/transcription/polly.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 81,
    "size": 2928,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "__init__",
      "synthesize_speech",
      "generate_transcript"
    ],
    "classes": [
      "AudioProcessor"
    ],
    "imports": [
      "json",
      "logging",
      "re",
      "boto3",
      "utilities.const"
    ],
    "preview": "import json\nimport logging\nimport re\n\nimport boto3\nfrom utilities.const import (\n    AWS_ACCESS_KEY,\n    AWS_SEC_KEY,\n    LOG_PATH,\n    OUTPUT_AUDIO,\n    OUTPUT_TRANSCRIPT,\n    get_current_date,\n)\n\nlogging.basicConfig(level=logging.INFO, format=\"[%(levelname)s] %(message)s\", filename=LOG_PATH)\n\n\nclass AudioProcessor:\n    def __init__(self, _title, _description):\n        logging.info(f\"AudioProcessor class  {_title} , {_description}\")",
    "last_modified": "2025-09-13T05:53:28.818906"
  },
  {
    "id": "1473",
    "name": "shorts_analyzer.py",
    "path": "github_repo/scripts/01_core_ai_analysis/transcription/shorts_analyzer.py",
    "category": "01_core_ai_analysis",
    "type": "analysis",
    "lines": 105,
    "size": 4771,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis",
      "transcription"
    ],
    "functions": [
      "format_timestamp",
      "transcribe_audio",
      "analyze_text_for_section",
      "process_audio_directory"
    ],
    "classes": [],
    "imports": [
      "logging",
      "os",
      "openai",
      "shared.config",
      "termcolor",
      "tqdm",
      "shared.openai_client"
    ],
    "preview": "import logging\nimport os\n\nimport openai\nfrom shared.config import *\nfrom termcolor import colored\nfrom tqdm import tqdm\n\n# Load environment variables from .env\nenv_path = os.path.expanduser(\"~/.env\")\nload_dotenv(dotenv_path=env_path)\n\n# Set OpenAI API key\nfrom shared.openai_client import get_openai_client\nif not openai.api_key:\n    raise EnvironmentError(\"OpenAI API key not found. Please check your .env file.\")\n\n\ndef format_timestamp(seconds):\n    minutes = int(seconds // 60)",
    "last_modified": "2025-10-09T05:27:15.574791"
  },
  {
    "id": "1474",
    "name": "analyze-mp3-transcript-prompts (1).py",
    "path": "github_repo/scripts/01_core_ai_analysis/transcription/analyze-mp3-transcript-prompts (1).py",
    "category": "01_core_ai_analysis",
    "type": "analysis",
    "lines": 109,
    "size": 4632,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis",
      "transcription"
    ],
    "functions": [
      "transcribe_audio",
      "format_timestamp",
      "analyze_text_for_section",
      "process_audio_directory"
    ],
    "classes": [],
    "imports": [
      "os",
      "openai",
      "dotenv"
    ],
    "preview": "import os\n\nimport openai\nfrom dotenv import load_dotenv\n\n# Load environment variables from .env (make sure your OPENAI_API_KEY is stored here)\nenv_path = \"/Users/steven/.env\"\nload_dotenv(dotenv_path=env_path)\nopenai.api_key = os.getenv(\"OPENAI_API_KEY\")\n\n# Directory pathsp\nAUDIO_DIR = \"/Users/steven/Music/NocTurnE-meLoDieS/mp3\"  # Directory containing MP3 files\nTRANSCRIPT_DIR = \"/Users/steven/Music/NocTurnE-meLoDieS/song-video/analysis-transcript\"  # Directory to save transcripts\nANALYSIS_DIR = \"/Users/steven/Music/NocTurnE-meLoDieS/song-video/analysis-transcript\"  # Directory to save the analysis files\n\n# Create output directories if they don't exist\nos.makedirs(TRANSCRIPT_DIR, exist_ok=True)\nos.makedirs(ANALYSIS_DIR, exist_ok=True)\n\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1475",
    "name": "info 2.py",
    "path": "github_repo/scripts/01_core_ai_analysis/transcription/info 2.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 159,
    "size": 7063,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis",
      "transcription"
    ],
    "functions": [
      "split_media_to_segments",
      "transcribe_media_segment",
      "format_timestamp",
      "analyze_text_for_section",
      "process_media_by_section"
    ],
    "classes": [],
    "imports": [
      "os",
      "subprocess",
      "openai",
      "dotenv",
      "sys"
    ],
    "preview": "import os\nimport subprocess\n\nimport openai\nfrom dotenv import load_dotenv\n\n# Load environment variables from .env (make sure your OPENAI_API_KEY is stored here)\nload_dotenv(\"/Users/steven/.env\")\nopenai.api_key = os.getenv(\"OPENAI_API_KEY\")\n\n# Prompt for directory paths\nproject_root = input(\"Please enter the project root directory path: \").strip()\n\n# Directory paths based on user input\nMEDIA_DIR = os.path.join(project_root, \"Media\")  # Directory containing media files\nTRANSCRIPT_DIR = os.path.join(MEDIA_DIR, \"transcript\")  # Directory to save transcripts\nANALYSIS_DIR = os.path.join(MEDIA_DIR, \"analysis\")  # Directory to save the analysis files\n\n# Create output directories if they don't exist\nos.makedirs(TRANSCRIPT_DIR, exist_ok=True)",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1476",
    "name": "fancyimg (1).py",
    "path": "github_repo/scripts/01_core_ai_analysis/transcription/fancyimg (1).py",
    "category": "01_core_ai_analysis",
    "type": "image_processing",
    "lines": 84,
    "size": 3000,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis"
    ],
    "functions": [
      "generate_filename_with_gpt",
      "pair_and_rename_images"
    ],
    "classes": [],
    "imports": [
      "os",
      "openai",
      "csv",
      "dotenv"
    ],
    "preview": "import os\n\nfrom openai import OpenAI\n\nclient = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\nimport csv\n\nfrom dotenv import load_dotenv\n\n# Load API key from .env file\nload_dotenv(\"/Users/steven/.env\")\n\n\n# Function to generate a filename using GPT based on the prompt\ndef generate_filename_with_gpt(prompt):\n    response = client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        messages=[\n            {\n                \"role\": \"system\",",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1477",
    "name": "gpt-all.py",
    "path": "github_repo/scripts/01_core_ai_analysis/transcription/gpt-all.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 88,
    "size": 2698,
    "docstring": "",
    "keywords": [
      "analysis",
      "web_tools"
    ],
    "functions": [
      "generate_description",
      "upscale_image",
      "text_to_speech"
    ],
    "classes": [],
    "imports": [
      "csv",
      "os",
      "io",
      "pathlib",
      "openAI",
      "requests",
      "PIL"
    ],
    "preview": "import csv\nimport os\nfrom io import BytesIO\nfrom pathlib import Path\n\nimport openAI\nimport requests\nfrom PIL import Image\n\n# Initialize the OpenAI client\nclient = openAI(api_key=\"your-api-key\")\n\n\ndef generate_description(image_url):\n    # Generate a description for the image using GPT-4 with Vision\n    response = client.chat.completions.create(\n        model=\"gpt-4-vision-preview\",\n        messages=[\n            {\n                \"role\": \"user\",",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1478",
    "name": "media-trans-analyze..py",
    "path": "github_repo/scripts/01_core_ai_analysis/transcription/media-trans-analyze..py",
    "category": "01_core_ai_analysis",
    "type": "analysis",
    "lines": 130,
    "size": 5973,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis",
      "transcription"
    ],
    "functions": [
      "format_timestamp",
      "transcribe_audio",
      "analyze_text_for_section",
      "process_media_directory"
    ],
    "classes": [],
    "imports": [
      "logging",
      "os",
      "sys",
      "openai",
      "dotenv"
    ],
    "preview": "import logging\nimport os\nimport sys\nimport openai\nfrom dotenv import load_dotenv\n\n# Setup basic logging\nlogging.basicConfig(level=logging.INFO, format=\"%(asctime)s [%(levelname)s] %(message)s\")\n\n# Load environment variables from .env (ensure your OPENAI_API_KEY is stored here)\nenv_path = \"/Users/steven/.env\"\nload_dotenv(dotenv_path=env_path)\n\n# Set the OpenAI API key\nopenai.api_key = os.getenv(\"OPENAI_API_KEY\")\n\n# Define a simple config dictionary. You can expand this as needed.\nconfig = {\"base_dir\": \"\"}  # Leave blank to prompt for base directory\n\n# Prompt for base directory if not set in config; default to current directory if input is empty",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1479",
    "name": "media_processor2.py",
    "path": "github_repo/scripts/01_core_ai_analysis/transcription/media_processor2.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 172,
    "size": 7477,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis",
      "transcription"
    ],
    "functions": [
      "format_timestamp",
      "transcribe_audio",
      "analyze_text_for_section",
      "process_media_directory"
    ],
    "classes": [],
    "imports": [
      "logging",
      "os",
      "subprocess",
      "sys",
      "openai",
      "dotenv"
    ],
    "preview": "import logging\nimport os\nimport subprocess  # Needed for running ffmpeg\nimport sys\n\nimport openai\nfrom dotenv import load_dotenv\n\n# Setup basic logging\nlogging.basicConfig(level=logging.INFO, format=\"%(asctime)s [%(levelname)s] %(message)s\")\n\n# Load environment variables from .env (ensure your OPENAI_API_KEY is stored here)\nenv_path = \"/Users/steven/.env\"\nload_dotenv(dotenv_path=env_path)\n\n# Set the OpenAI API key\nopenai.api_key = os.getenv(\"OPENAI_API_KEY\")\n\n# Define a simple config dictionary. You can expand this as needed.\nconfig = {\"base_dir\": \"\"}  # Leave blank to prompt for base directory",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1480",
    "name": "mp3-csv 3.py",
    "path": "github_repo/scripts/01_core_ai_analysis/transcription/mp3-csv 3.py",
    "category": "01_core_ai_analysis",
    "type": "audio_processing",
    "lines": 107,
    "size": 4019,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis",
      "transcription"
    ],
    "functions": [
      "format_timestamp",
      "transcribe_audio",
      "analyze_text_for_section",
      "process_audio_file",
      "process_audio_directory"
    ],
    "classes": [],
    "imports": [
      "os",
      "openai",
      "dotenv"
    ],
    "preview": "import os\n\nimport openai\nfrom dotenv import load_dotenv\n\n# Load environment variables\nload_dotenv()\nopenai.api_key = os.getenv(\"OPENAI_API_KEY\")\nif not openai.api_key:\n    raise EnvironmentError(\"OpenAI API key not found. Please check your .env file.\")\n\n\ndef format_timestamp(seconds):\n    \"\"\"Format seconds into MM:SS timestamp.\"\"\"\n    minutes = int(seconds // 60)\n    seconds = seconds % 60\n    return f\"{minutes:02d}:{int(seconds):02d}\"\n\n\ndef transcribe_audio(file_path):",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1481",
    "name": "2.0.py",
    "path": "github_repo/scripts/01_core_ai_analysis/transcription/2.0.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 59,
    "size": 1713,
    "docstring": "",
    "keywords": [
      "data_processing",
      "openai",
      "analysis",
      "web_tools"
    ],
    "functions": [
      "generate_content_based_on_prompt"
    ],
    "classes": [],
    "imports": [
      "os",
      "openai",
      "io",
      "pandas",
      "requests",
      "PIL",
      "tqdm"
    ],
    "preview": "import os\n\nfrom openai import OpenAI\n\nclient = OpenAI(api_key=\"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\")\nimport io\n\nimport pandas as pd\nimport requests\nfrom PIL import Image\nfrom tqdm import tqdm\n\n# Set OpenAI and Stability API keys\nstability_ai_key = \"sk-2O6mutk6X4HI9olxMeGKYv0MpXcNRzs6fVRPJMdEvgEzRRII\"\n\n\n# Function to generate content based on the prompt\ndef generate_content_based_on_prompt(prompt):\n    response = client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",",
    "last_modified": "2025-05-04T22:47:11.582301"
  },
  {
    "id": "1482",
    "name": "adown.py",
    "path": "github_repo/scripts/01_core_ai_analysis/transcription/adown.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 110,
    "size": 2335,
    "docstring": "",
    "keywords": [
      "youtube"
    ],
    "functions": [
      "list_remote_files",
      "list_local_files",
      "download_missing_files"
    ],
    "classes": [],
    "imports": [
      "os",
      "paramiko"
    ],
    "preview": "import os\n\nimport paramiko\n\n# SSH Configuration\nhostname = \"access981577610.webspace-data.io\"\nusername = \"u114071855\"\npassword = \"A^p1yT@AHn*akbhs\"\nlocal_dir = \"/Users/steven/AvaTarArTs\"\nremote_dirs = [\n    \"/2025\",\n    \"/Users\",\n    \"/all\",\n    \"/audio-texts\",\n    \"/blog\",\n    \"/build\",\n    \"/card\",\n    \"/city\",\n    \"/clickandbuilds\",\n    \"/convo\",",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1483",
    "name": "analyze 1.py",
    "path": "github_repo/scripts/01_core_ai_analysis/transcription/analyze 1.py",
    "category": "01_core_ai_analysis",
    "type": "analysis",
    "lines": 223,
    "size": 9464,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis",
      "transcription"
    ],
    "functions": [
      "format_timestamp",
      "parse_transcript",
      "transcribe_audio",
      "analyze_text_for_section",
      "link_timestamps_to_analysis",
      "process_audio_file",
      "process_audio_directory"
    ],
    "classes": [],
    "imports": [
      "logging",
      "os",
      "sys",
      "time",
      "concurrent.futures",
      "dotenv",
      "openai",
      "termcolor",
      "tqdm"
    ],
    "preview": "import logging\nimport os\nimport sys\nimport time\nfrom concurrent.futures import ThreadPoolExecutor\n\nfrom dotenv import load_dotenv\nfrom openai import OpenAI\nfrom termcolor import colored\nfrom tqdm import tqdm\n\nclient = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n\n\n# Load environment variables from .env (make sure your OPENAI_API_KEY is stored here)\nenv_path = os.path.expanduser(\"~/.env\")\nload_dotenv(dotenv_path=env_path)\n\n# Configure logging for error reporting\nlogging.basicConfig(",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1484",
    "name": "migrate_remaining.py",
    "path": "github_repo/scripts/01_core_ai_analysis/transcription/migrate_remaining.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 268,
    "size": 13298,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "create_additional_directories",
      "migrate_remaining_files",
      "migrate_remaining_directories",
      "main"
    ],
    "classes": [],
    "imports": [
      "os",
      "shutil",
      "pathlib"
    ],
    "preview": "#!/usr/bin/env python3\n\"\"\"\nSecond Migration Pass - Organize Remaining Python Files\n\"\"\"\n\nimport os\nimport shutil\nfrom pathlib import Path\n\ndef create_additional_directories():\n    \"\"\"Create additional directories for remaining files.\"\"\"\n    base_path = Path(\"/Users/steven/Documents/python\")\n    \n    additional_dirs = [\n        \"01_core_tools/text_processors\",\n        \"05_audio_video/image_processors\", \n        \"05_audio_video/audio_processors/quiz_tts\",\n        \"06_utilities/converters\",\n        \"06_utilities/data_processors\",\n        \"07_experimental/web_tools\",",
    "last_modified": "2025-10-09T05:31:11.505816"
  },
  {
    "id": "1485",
    "name": "analyze_all_images_20250530220437.py",
    "path": "github_repo/scripts/01_core_ai_analysis/transcription/analyze_all_images_20250530220437.py",
    "category": "01_core_ai_analysis",
    "type": "analysis",
    "lines": 429,
    "size": 17578,
    "docstring": "batch_image_seo_pipeline.py\n\n1) Discovers all image files under a given folder.\n2) Extracts technical metadata via Pillow.\n3) Calls GPT-4o Vision (multimodal) using a \u201csystem\u201d + \u201cuser\u201d prompt.\n4) Parses the JSON response into structured fields:\n   main_subject, style, color_palette, tags, orientation, suggested_products,\n   SEO_title, SEO_description, emotion, safety_rating, dominant_keyword.\n5) Builds a \u201csource\u201d tag: folder-subfolder-YYYYMMDD.\n6) Applies a \u201cTop 5% SEO Analytics\u201d framework to generate SEO columns:\n   \u2013 SEO Keywords, Traffic Source, CRO Tactic, Backlink Source, Engagement Rate.\n7) Adds niche-specific design prompts (Geeky, Dark Humor, Anime).\n8) Writes everything into a final CSV for bulk upload.\n\nDependencies:\n    pip install openai python-dotenv pillow tqdm backoff",
    "keywords": [
      "openai"
    ],
    "functions": [
      "load_openai_client",
      "retry_on_api_error",
      "call_gpt4o",
      "get_image_tech_meta",
      "build_source_tag",
      "discover_images",
      "build_gpt_messages",
      "parse_args",
      "main"
    ],
    "classes": [],
    "imports": [
      "argparse",
      "csv",
      "json",
      "logging",
      "os",
      "sys",
      "time",
      "datetime",
      "pathlib",
      "typing"
    ],
    "preview": "#!/usr/bin/env python3\n\"\"\"\nbatch_image_seo_pipeline.py\n\n1) Discovers all image files under a given folder.\n2) Extracts technical metadata via Pillow.\n3) Calls GPT-4o Vision (multimodal) using a \u201csystem\u201d + \u201cuser\u201d prompt.\n4) Parses the JSON response into structured fields:\n   main_subject, style, color_palette, tags, orientation, suggested_products,\n   SEO_title, SEO_description, emotion, safety_rating, dominant_keyword.\n5) Builds a \u201csource\u201d tag: folder-subfolder-YYYYMMDD.\n6) Applies a \u201cTop 5% SEO Analytics\u201d framework to generate SEO columns:\n   \u2013 SEO Keywords, Traffic Source, CRO Tactic, Backlink Source, Engagement Rate.\n7) Adds niche-specific design prompts (Geeky, Dark Humor, Anime).\n8) Writes everything into a final CSV for bulk upload.\n\nDependencies:\n    pip install openai python-dotenv pillow tqdm backoff\n\"\"\"\n",
    "last_modified": "2025-09-13T05:53:25.324048"
  },
  {
    "id": "1486",
    "name": "thumbs.py",
    "path": "github_repo/scripts/01_core_ai_analysis/transcription/thumbs.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 19,
    "size": 343,
    "docstring": "",
    "keywords": [],
    "functions": [
      "export_thumbnail"
    ],
    "classes": [],
    "imports": [
      "__future__",
      "subprocess"
    ],
    "preview": "from __future__ import annotations\n\nimport subprocess\n\n\ndef export_thumbnail(video_path: str, out_png: str, ss: float = 0.25):\n    cmd = [\n        \"ffmpeg\",\n        \"-y\",\n        \"-i\",\n        video_path,\n        \"-vf\",\n        \"thumbnail,scale=1080:-2\",\n        \"-frames:v\",\n        \"1\",\n        out_png,\n    ]\n    subprocess.check_call(cmd)\n",
    "last_modified": "2025-09-11T13:26:59.332635"
  },
  {
    "id": "1487",
    "name": "info 3.py",
    "path": "github_repo/scripts/01_core_ai_analysis/transcription/info 3.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 159,
    "size": 7063,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis",
      "transcription"
    ],
    "functions": [
      "split_media_to_segments",
      "transcribe_media_segment",
      "format_timestamp",
      "analyze_text_for_section",
      "process_media_by_section"
    ],
    "classes": [],
    "imports": [
      "os",
      "subprocess",
      "openai",
      "dotenv",
      "sys"
    ],
    "preview": "import os\nimport subprocess\n\nimport openai\nfrom dotenv import load_dotenv\n\n# Load environment variables from .env (make sure your OPENAI_API_KEY is stored here)\nload_dotenv(\"/Users/steven/.env\")\nopenai.api_key = os.getenv(\"OPENAI_API_KEY\")\n\n# Prompt for directory paths\nproject_root = input(\"Please enter the project root directory path: \").strip()\n\n# Directory paths based on user input\nMEDIA_DIR = os.path.join(project_root, \"Media\")  # Directory containing media files\nTRANSCRIPT_DIR = os.path.join(MEDIA_DIR, \"transcript\")  # Directory to save transcripts\nANALYSIS_DIR = os.path.join(MEDIA_DIR, \"analysis\")  # Directory to save the analysis files\n\n# Create output directories if they don't exist\nos.makedirs(TRANSCRIPT_DIR, exist_ok=True)",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1488",
    "name": "test copy.py",
    "path": "github_repo/scripts/01_core_ai_analysis/transcription/test copy.py",
    "category": "01_core_ai_analysis",
    "type": "testing",
    "lines": 90,
    "size": 3032,
    "docstring": "",
    "keywords": [
      "image_processing",
      "openai",
      "analysis",
      "web_tools"
    ],
    "functions": [
      "generate_base_prompt",
      "refine_prompt",
      "create_image",
      "upscale_image",
      "text_to_speech"
    ],
    "classes": [],
    "imports": [
      "csv",
      "io",
      "pathlib",
      "requests",
      "openai",
      "PIL"
    ],
    "preview": "import csv\nfrom io import BytesIO\nfrom pathlib import Path\n\nimport requests\nfrom openai import OpenAI\nfrom PIL import Image\n\n# Initialize the OpenAI client\nclient = OpenAI()\n\ndef generate_base_prompt(question, options):\n    # Combine question and options to generate a base prompt\n    return f\"Question: {question} Options: {', '.join(options)}\"\n\ndef refine_prompt(prompt):\n    # Refine the prompt using ChatGPT for more creativity\n    response = client.chat.completions.create(\n        model=\"gpt-4\",\n        messages=[{\"role\": \"system\", \"content\": \"You are a creative writer.\"},",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1489",
    "name": "pipeline.py",
    "path": "github_repo/scripts/01_core_ai_analysis/transcription/pipeline.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 197,
    "size": 6563,
    "docstring": "",
    "keywords": [
      "transcription"
    ],
    "functions": [
      "__init__",
      "transcribe",
      "_hook_score",
      "candidates_from_transcript",
      "select_topk",
      "export_clip",
      "run"
    ],
    "classes": [
      "ClipCandidate",
      "OpusClonePipeline"
    ],
    "imports": [
      "__future__",
      "json",
      "math",
      "os",
      "re",
      "shutil",
      "subprocess",
      "tempfile",
      "dataclasses",
      "typing"
    ],
    "preview": "from __future__ import annotations\n\nimport json\nimport math\nimport os\nimport re\nimport shutil\nimport subprocess\nimport tempfile\nfrom dataclasses import dataclass\nfrom typing import Any, Dict, List, Tuple\n\nimport numpy as np\nfrom moviepy.editor import VideoFileClip\nfrom tqdm import tqdm\n\nimport whisper\n\nfrom .brand import BrandTemplate\nfrom .captions import burn_captions_ffmpeg, write_srt",
    "last_modified": "2025-09-13T05:55:09.593682"
  },
  {
    "id": "1490",
    "name": "deep_analyze.py",
    "path": "github_repo/scripts/01_core_ai_analysis/transcription/deep_analyze.py",
    "category": "01_core_ai_analysis",
    "type": "analysis",
    "lines": 107,
    "size": 5108,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis",
      "transcription"
    ],
    "functions": [
      "format_timestamp",
      "transcribe_audio",
      "analyze_text",
      "process_media_directory"
    ],
    "classes": [],
    "imports": [
      "logging",
      "os",
      "sys",
      "pathlib",
      "openai",
      "dotenv"
    ],
    "preview": "import logging\nimport os\nimport sys\nfrom pathlib import Path\nimport openai\nfrom dotenv import load_dotenv\n\n# Setup logging\nlogging.basicConfig(level=logging.INFO, format=\"%(asctime)s [%(levelname)s] %(message)s\")\n\n# Load environment variables\nenv_path = Path(\"/Users/steven/.env\")\nload_dotenv(dotenv_path=env_path)\nopenai.api_key = os.getenv(\"OPENAI_API_KEY\")\n\n# Base config\nconfig = {\"base_dir\": \"\"}\nif not config.get(\"base_dir\"):\n    base_dir_input = input(\"Enter base directory (leave blank for current): \").strip()\n    config[\"base_dir\"] = base_dir_input or os.getcwd()",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1491",
    "name": "analyze-mp4s.py",
    "path": "github_repo/scripts/01_core_ai_analysis/transcription/analyze-mp4s.py",
    "category": "01_core_ai_analysis",
    "type": "analysis",
    "lines": 114,
    "size": 5693,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis",
      "transcription"
    ],
    "functions": [
      "format_timestamp",
      "transcribe_audio",
      "analyze_text_for_section",
      "process_audio_directory"
    ],
    "classes": [],
    "imports": [
      "logging",
      "os",
      "openai",
      "dotenv",
      "termcolor",
      "tqdm"
    ],
    "preview": "import logging\nimport os\n\nimport openai\nfrom dotenv import load_dotenv\nfrom termcolor import colored\nfrom tqdm import tqdm\n\n# Load environment variables from .env\nenv_path = os.path.expanduser(\"~/.env\")\nload_dotenv(dotenv_path=env_path)\n\n# Set OpenAI API key\nopenai.api_key = os.getenv(\"OPENAI_API_KEY\")\nif not openai.api_key:\n    raise EnvironmentError(\"OpenAI API key not found. Please check your .env file.\")\n\n\ndef format_timestamp(seconds):\n    minutes = int(seconds // 60)",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1492",
    "name": "generate_speech.py",
    "path": "github_repo/scripts/01_core_ai_analysis/transcription/generate_speech.py",
    "category": "01_core_ai_analysis",
    "type": "transcription",
    "lines": 36,
    "size": 1234,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis"
    ],
    "functions": [
      "generate_speech",
      "main"
    ],
    "classes": [],
    "imports": [
      "csv",
      "openai"
    ],
    "preview": "import csv\n\nfrom openai import OpenAI\n\nclient = OpenAI(api_key=\"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\")\n\n# Replace 'your_api_key_here' with your actual OpenAI API key\n\n\ndef generate_speech(text, voice=\"shimmer\", output_path=\"speech.mp3\"):\n    response = client.audio.create(\n        model=\"tts-1-hd\",  # Ensure this model supports the audio creation\n        input=text,\n        voice=voice,\n        format=\"mp3\",\n    )\n    with open(output_path, \"wb\") as file:\n        file.write(response.content)\n\n",
    "last_modified": "2025-09-13T05:53:51.036050"
  },
  {
    "id": "1493",
    "name": "Quiz22sec (1).py",
    "path": "github_repo/scripts/01_core_ai_analysis/transcription/Quiz22sec (1).py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 108,
    "size": 4050,
    "docstring": "",
    "keywords": [
      "analysis",
      "web_tools"
    ],
    "functions": [
      "generate_speech",
      "calculate_text_duration",
      "process_csv_and_generate_speech"
    ],
    "classes": [],
    "imports": [
      "csv",
      "os",
      "requests",
      "pydub"
    ],
    "preview": "import csv\nimport os\n\nimport requests  # Make sure to install this package\nfrom pydub import AudioSegment  # Make sure to install pydub package\n\n\ndef generate_speech(input_text, output_path, api_key):\n    \"\"\"\n    Generates speech from text using an API (e.g., OpenAI's text-to-speech API).\n    \"\"\"\n    url = \"https://api.openai.com/v1/audio/speech\"\n    headers = {\n        \"Authorization\": f\"Bearer {api_key}\",\n        \"Content-Type\": \"application/json\",\n    }\n    data = {\n        \"model\": \"tts-1\",  # Update this according to the specific model you're using\n        \"input\": input_text,\n        \"voice\": \"shimmer\",  # Update the voice parameter as needed",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1494",
    "name": "mp3-csv 2.py",
    "path": "github_repo/scripts/01_core_ai_analysis/transcription/mp3-csv 2.py",
    "category": "01_core_ai_analysis",
    "type": "audio_processing",
    "lines": 107,
    "size": 4019,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis",
      "transcription"
    ],
    "functions": [
      "format_timestamp",
      "transcribe_audio",
      "analyze_text_for_section",
      "process_audio_file",
      "process_audio_directory"
    ],
    "classes": [],
    "imports": [
      "os",
      "openai",
      "dotenv"
    ],
    "preview": "import os\n\nimport openai\nfrom dotenv import load_dotenv\n\n# Load environment variables\nload_dotenv()\nopenai.api_key = os.getenv(\"OPENAI_API_KEY\")\nif not openai.api_key:\n    raise EnvironmentError(\"OpenAI API key not found. Please check your .env file.\")\n\n\ndef format_timestamp(seconds):\n    \"\"\"Format seconds into MM:SS timestamp.\"\"\"\n    minutes = int(seconds // 60)\n    seconds = seconds % 60\n    return f\"{minutes:02d}:{int(seconds):02d}\"\n\n\ndef transcribe_audio(file_path):",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1495",
    "name": "mp4-mp3-analyze_20250531041622.py",
    "path": "github_repo/scripts/01_core_ai_analysis/transcription/mp4-mp3-analyze_20250531041622.py",
    "category": "01_core_ai_analysis",
    "type": "analysis",
    "lines": 174,
    "size": 6778,
    "docstring": "",
    "keywords": [
      "video_processing",
      "openai",
      "analysis",
      "transcription"
    ],
    "functions": [
      "split_video_to_segments",
      "transcribe_video_segment",
      "format_timestamp",
      "analyze_text_for_section",
      "process_video_by_section"
    ],
    "classes": [],
    "imports": [
      "os",
      "subprocess",
      "openai",
      "dotenv",
      "sys"
    ],
    "preview": "import os\nimport subprocess\n\nimport openai\nfrom dotenv import load_dotenv\n\n# Load environment variables from .env (make sure your OPENAI_API_KEY is stored here)\nload_dotenv()\nopenai.api_key = os.getenv(\"OPENAI_API_KEY\")\n\n# Directory paths\nVIDEO_DIR = (\n    \"/Users/steven/AvaTarArTs/canva/Video\"  # Directory containing MP4 files\n)\nTRANSCRIPT_DIR = \"/Users/steven/AvaTarArTs/canva/Video/transcript\"  # Directory to save transcripts\nANALYSIS_DIR = \"/Users/steven/AvaTarArTs/canva/Video/analysis\"  # Directory to save the analysis files\n\n# Create output directories if they don't exist\nos.makedirs(TRANSCRIPT_DIR, exist_ok=True)\nos.makedirs(ANALYSIS_DIR, exist_ok=True)",
    "last_modified": "2025-05-31T07:03:53.918581"
  },
  {
    "id": "1496",
    "name": "createimages.py",
    "path": "github_repo/scripts/01_core_ai_analysis/transcription/createimages.py",
    "category": "01_core_ai_analysis",
    "type": "image_processing",
    "lines": 150,
    "size": 5687,
    "docstring": "",
    "keywords": [
      "data_processing",
      "openai",
      "analysis",
      "web_tools"
    ],
    "functions": [
      "generate_clickable_title",
      "generate_description",
      "generate_image_prompt",
      "generate_tags"
    ],
    "classes": [],
    "imports": [
      "os",
      "openai",
      "io",
      "pandas",
      "requests",
      "PIL",
      "tqdm"
    ],
    "preview": "import os\n\nfrom openai import OpenAI\n\nclient = OpenAI(api_key=\"YOUR_OPEN_AI_KEY\")\nimport io\n\nimport pandas as pd\nimport requests\nfrom PIL import Image, ImageDraw\nfrom tqdm import tqdm\n\n# Set OpenAI and Stability.ai API keys https://platform.openai.com/account/api-keys AND https://beta.dreamstudio.ai/account\nstability_ai_key = \"YOUR_STABILITY_AI_KEY\"\n\n# Function to generate a clickable title using GPT-3.5-turbo\n\n# Change this prompt if you are changing the product type, right now it's Acrylic Wall Art Panels\n\n",
    "last_modified": "2025-05-04T22:47:11.599251"
  },
  {
    "id": "1497",
    "name": "transcribe.py",
    "path": "github_repo/scripts/01_core_ai_analysis/transcription/transcribe.py",
    "category": "01_core_ai_analysis",
    "type": "transcription",
    "lines": 26,
    "size": 594,
    "docstring": "",
    "keywords": [
      "openai",
      "transcription"
    ],
    "functions": [
      "transcribe_audio"
    ],
    "classes": [],
    "imports": [
      "openai",
      "os",
      "dotenv",
      "sys"
    ],
    "preview": "from openai import OpenAI\n\nclient = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\nimport os\n\nfrom dotenv import load_dotenv\n\n# Load environment variables from .env\nload_dotenv()\n\n\ndef transcribe_audio(file_path):\n    with open(file_path, \"rb\") as audio_file:\n        transcript = client.audio.transcribe(\"whisper-1\", audio_file)\n        return transcript.text\n\n\nif __name__ == \"__main__\":\n    import sys\n",
    "last_modified": "2025-05-04T22:47:11"
  },
  {
    "id": "1498",
    "name": "help_Nekmo_ffmpeg.py",
    "path": "github_repo/scripts/01_core_ai_analysis/transcription/help_Nekmo_ffmpeg.py",
    "category": "01_core_ai_analysis",
    "type": "video_processing",
    "lines": 159,
    "size": 5150,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "logging",
      "asyncio",
      "os",
      "time",
      "hachoir.metadata",
      "hachoir.parser"
    ],
    "preview": "import logging\n\nlogging.basicConfig(\n    level=logging.DEBUG, format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\n)\nlogger = logging.getLogger(__name__)\n\n\nimport asyncio\nimport os\nimport time\n\nfrom hachoir.metadata import extractMetadata\nfrom hachoir.parser import createParser\n\n\nasync def place_water_mark(input_file, output_file, water_mark_file):\n    watermarked_file = output_file + \".watermark.png\"\n    metadata = extractMetadata(createParser(input_file))\n    width = metadata.get(\"width\")",
    "last_modified": "2025-05-06T04:35:15.017373"
  },
  {
    "id": "1499",
    "name": "sub.py",
    "path": "github_repo/scripts/01_core_ai_analysis/transcription/sub.py",
    "category": "01_core_ai_analysis",
    "type": "transcription",
    "lines": 92,
    "size": 2610,
    "docstring": "This script is used to generate a transcript from an audio file using AssemblyAI api.",
    "keywords": [
      "transcription",
      "youtube",
      "web_tools"
    ],
    "functions": [
      "read_file",
      "uploadFile",
      "transcribe",
      "poll",
      "get_transcription_result_url",
      "save_transcript"
    ],
    "classes": [],
    "imports": [
      "os",
      "time",
      "requests",
      "config"
    ],
    "preview": "\"\"\"\nThis script is used to generate a transcript from an audio file using AssemblyAI api.\n\"\"\"\n\nimport os\nimport time\n\nimport requests\n\nimport config\n\nupload_url = \"https://api.assemblyai.com/v2/upload\"\ntranscribe_url = \"https://api.assemblyai.com/v2/transcript\"\nsrt_endpoint = \"https://api.assemblyai.com/v2/transcript/\"  # YOUR-TRANSCRIPT-ID-HERE/srt\nheaders = {\"authorization\": config.assemblyai}\n\n\n# Read the audio file to verify\ndef read_file(filename, chunk_size=5242880):\n    with open(filename, \"rb\") as _file:",
    "last_modified": "2025-09-13T05:53:29.691898"
  },
  {
    "id": "1500",
    "name": "trans.py",
    "path": "github_repo/scripts/01_core_ai_analysis/transcription/trans.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 44,
    "size": 1393,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "os",
      "pydub",
      "whisper"
    ],
    "preview": "import os\n\nfrom pydub import AudioSegment\n\nimport whisper\n\n# Initialize Whisper model\nmodel = whisper.load_model(\"base\")\n\n# Directory containing the MP3 files\naudio_dir = os.path.expanduser(\"/Users/steven/Movies/2025-Vid/done\")\n\n# Loop over each MP3 file in the directory\nfor audio_file in os.listdir(audio_dir):\n    if audio_file.endswith(\".mp3\"):\n        # Full path to the audio file\n        audio_path = os.path.join(audio_dir, audio_file)\n\n        # Load audio file\n        audio = AudioSegment.from_mp3(audio_path)",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1501",
    "name": "analyze-shorts-1.py",
    "path": "github_repo/scripts/01_core_ai_analysis/transcription/analyze-shorts-1.py",
    "category": "01_core_ai_analysis",
    "type": "analysis",
    "lines": 122,
    "size": 5310,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis",
      "transcription"
    ],
    "functions": [
      "transcribe_audio",
      "format_timestamp",
      "analyze_text_for_section",
      "process_audio_directory"
    ],
    "classes": [],
    "imports": [
      "logging",
      "os",
      "sys",
      "time",
      "concurrent.futures",
      "openai",
      "dotenv",
      "termcolor",
      "tqdm"
    ],
    "preview": "import logging\nimport os\nimport sys\nimport time\nfrom concurrent.futures import ThreadPoolExecutor\n\nimport openai\nfrom dotenv import load_dotenv\nfrom termcolor import colored\nfrom tqdm import tqdm\n\n# Load environment variables from .env (make sure your OPENAI_API_KEY is stored here)\nenv_path = os.path.expanduser(\"~/.env\")\n # Update this path if necessary\nload_dotenv(dotenv_path=env_path)\n\n# Initialize OpenAI API key\nopenai.api_key = os.getenv('OPENAI_API_KEY')\n\n# Error checking for OpenAI API key",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1502",
    "name": "python-analyze-html.py",
    "path": "github_repo/scripts/01_core_ai_analysis/transcription/python-analyze-html.py",
    "category": "01_core_ai_analysis",
    "type": "analysis",
    "lines": 160,
    "size": 5191,
    "docstring": "",
    "keywords": [
      "data_processing",
      "analysis"
    ],
    "functions": [
      "analyze_ast",
      "categorize_heuristic",
      "esc"
    ],
    "classes": [],
    "imports": [
      "ast",
      "csv",
      "html",
      "json",
      "os",
      "re",
      "dataclasses",
      "pathlib",
      "typing",
      "pandas"
    ],
    "preview": "# Analyze the Python files you've uploaded here (a subset of your /Users/steven/Documents/python)\n# and generate a compact report (CSV + simple HTML) along with an on-screen table.\n\nimport ast\nimport csv\nimport html\nimport json\nimport os\nimport re\nfrom dataclasses import dataclass, field\nfrom pathlib import Path\nfrom typing import List, Tuple\n\nimport pandas as pd\nfrom caas_jupyter_tools import display_dataframe_to_user\n\n# Files visible in this workspace (mirror of some of your local files)\nroot = Path(\"/Users/steven/Documents/python\")\npy_files = sorted([p for p in root.iterdir() if p.suffix == \".py\"])\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1503",
    "name": "quiz-20.py",
    "path": "github_repo/scripts/01_core_ai_analysis/transcription/quiz-20.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 103,
    "size": 3633,
    "docstring": "",
    "keywords": [
      "analysis",
      "web_tools"
    ],
    "functions": [
      "generate_speech",
      "calculate_text_duration",
      "process_csv_and_generate_speech"
    ],
    "classes": [],
    "imports": [
      "csv",
      "os",
      "requests",
      "dotenv",
      "pydub"
    ],
    "preview": "import csv\nimport os\n\nimport requests  # Install using `pip install requests`\nfrom dotenv import load_dotenv\nfrom pydub import AudioSegment  # Install using `pip install pydub`\n\n\ndef generate_speech(input_text, output_path, api_key):\n    \"\"\"\n    Generates speech from text using OpenAI's text-to-speech API.\n    \"\"\"\n    url = \"https://api.openai.com/v1/audio/speech\"\n    headers = {\n        \"Authorization\": f\"Bearer {api_key}\",\n        \"Content-Type\": \"application/json\",\n    }\n    data = {\n        \"model\": \"tts-1\",\n        \"input\": input_text,",
    "last_modified": "2025-09-13T05:53:55.412139"
  },
  {
    "id": "1504",
    "name": "conda_consolidator.py",
    "path": "github_repo/scripts/01_core_ai_analysis/transcription/conda_consolidator.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 499,
    "size": 18738,
    "docstring": "Conda Environment Consolidator\nInteractive tool to safely remove and combine conda environments.",
    "keywords": [
      "analysis",
      "organization"
    ],
    "functions": [
      "print_colored",
      "load_report",
      "export_environment",
      "get_package_list",
      "compare_environments",
      "create_merged_environment",
      "remove_environment",
      "interactive_removal",
      "interactive_merge",
      "clean_conda_cache"
    ],
    "classes": [
      "Colors"
    ],
    "imports": [
      "json",
      "subprocess",
      "sys",
      "pathlib",
      "datetime",
      "shutil"
    ],
    "preview": "#!/usr/bin/env python3\n\"\"\"\nConda Environment Consolidator\nInteractive tool to safely remove and combine conda environments.\n\"\"\"\n\nimport json\nimport subprocess\nimport sys\nfrom pathlib import Path\nfrom datetime import datetime\nimport shutil\n\nclass Colors:\n    \"\"\"ANSI color codes for terminal output.\"\"\"\n    HEADER = '\\033[95m'\n    BLUE = '\\033[94m'\n    CYAN = '\\033[96m'\n    GREEN = '\\033[92m'\n    YELLOW = '\\033[93m'",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1505",
    "name": "scan.py",
    "path": "github_repo/scripts/01_core_ai_analysis/transcription/scan.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 112,
    "size": 3032,
    "docstring": "",
    "keywords": [],
    "functions": [
      "is_excluded",
      "scan_directory",
      "save_csv",
      "main"
    ],
    "classes": [],
    "imports": [
      "csv",
      "os",
      "re",
      "datetime"
    ],
    "preview": "import csv\nimport os\nimport re\nfrom datetime import datetime\n\n# Define base directories to scan\nBASE_DIRS = [\n    \"/Users/steven/Documents/Python_backup\",\n    \"/Users/steven/Documents/Python\",\n    \"/Users/steven/Music/nocTurneMeLoDieS/lyrics-keys-indo\",\n    \"/Users/steven/Music/nocTurneMeLoDieS/mp3-analyze-transcribe\",\n]\n\n# Regex patterns for exclusions\nEXCLUDED_PATTERNS = [\n    r\"^\\..*\",\n    r\".*/venv/.*\",\n    r\".*/\\.venv/.*\",\n    r\".*/lib/.*\",\n    r\".*/\\.lib/.*\",",
    "last_modified": "2025-09-06T12:24:11.720416"
  },
  {
    "id": "1506",
    "name": "analyze11.py",
    "path": "github_repo/scripts/01_core_ai_analysis/transcription/analyze11.py",
    "category": "01_core_ai_analysis",
    "type": "analysis",
    "lines": 223,
    "size": 9464,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis",
      "transcription"
    ],
    "functions": [
      "format_timestamp",
      "parse_transcript",
      "transcribe_audio",
      "analyze_text_for_section",
      "link_timestamps_to_analysis",
      "process_audio_file",
      "process_audio_directory"
    ],
    "classes": [],
    "imports": [
      "logging",
      "os",
      "sys",
      "time",
      "concurrent.futures",
      "dotenv",
      "openai",
      "termcolor",
      "tqdm"
    ],
    "preview": "import logging\nimport os\nimport sys\nimport time\nfrom concurrent.futures import ThreadPoolExecutor\n\nfrom dotenv import load_dotenv\nfrom openai import OpenAI\nfrom termcolor import colored\nfrom tqdm import tqdm\n\nclient = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n\n\n# Load environment variables from .env (make sure your OPENAI_API_KEY is stored here)\nenv_path = os.path.expanduser(\"~/.env\")\nload_dotenv(dotenv_path=env_path)\n\n# Configure logging for error reporting\nlogging.basicConfig(",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1507",
    "name": "cover2.py",
    "path": "github_repo/scripts/01_core_ai_analysis/transcription/cover2.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 51,
    "size": 1566,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis",
      "web_tools"
    ],
    "functions": [
      "generate_cover_image_with_dalle",
      "convert_mp3_to_mp4",
      "process_directory"
    ],
    "classes": [],
    "imports": [
      "openai",
      "glob",
      "os",
      "io",
      "requests",
      "moviepy.editor",
      "PIL",
      "sys"
    ],
    "preview": "from openai import OpenAI\n\nclient = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\nimport glob\nimport os\nfrom io import BytesIO\n\nimport requests\nfrom moviepy.editor import AudioFileClip, ImageClip\nfrom PIL import Image\n\n\ndef generate_cover_image_with_dalle(file_name, output_path):\n    prompt = f\"lets create a series of typography cover image for '{file_name}' in the Font and style and contexts to tell the story'\"\n    response = client.images.generate(prompt=prompt, n=1, size=\"1024x1024\")\n    image_url = response.data[0].url\n    response = requests.get(image_url)\n    img = Image.open(BytesIO(response.content))\n    img.save(output_path)\n",
    "last_modified": "2025-05-04T22:47:12.620496"
  },
  {
    "id": "1508",
    "name": "generate_speech 2 2.py",
    "path": "github_repo/scripts/01_core_ai_analysis/transcription/generate_speech 2 2.py",
    "category": "01_core_ai_analysis",
    "type": "transcription",
    "lines": 44,
    "size": 1447,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis"
    ],
    "functions": [
      "generate_speech",
      "main"
    ],
    "classes": [],
    "imports": [
      "csv",
      "openai"
    ],
    "preview": "import csv\n\nfrom openai import OpenAI\n\nclient = OpenAI(api_key='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')\n\n# Replace 'your_api_key_here' with your actual OpenAI API key\n\n\ndef generate_speech(text, voice='shimmer', output_path='speech.mp3'):\n    response = client.audio.create(model=\"tts-1-hd\",  # Ensure this model supports the audio creation\n                                   input=text,\n                                   voice=voice,\n                                   format=\"mp3\")\n    with open(output_path, \"wb\") as file:\n        file.write(response.content)\n\n\ndef main():\n    # Update this path to where your CSV is located",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1509",
    "name": "audio_chunker.py",
    "path": "github_repo/scripts/01_core_ai_analysis/transcription/audio_chunker.py",
    "category": "01_core_ai_analysis",
    "type": "transcription",
    "lines": 174,
    "size": 7538,
    "docstring": "Audio chunking utilities for handling long files",
    "keywords": [
      "organization"
    ],
    "functions": [
      "__init__",
      "get_audio_duration",
      "should_split_file",
      "calculate_chunks",
      "split_audio_file",
      "merge_transcripts",
      "_format_timestamp",
      "cleanup_chunks"
    ],
    "classes": [
      "AudioChunker"
    ],
    "imports": [
      "os",
      "logging",
      "pathlib",
      "typing",
      "moviepy.editor",
      "config"
    ],
    "preview": "\"\"\"\nAudio chunking utilities for handling long files\n\"\"\"\n\nimport os\nimport logging\nfrom pathlib import Path\nfrom typing import List, Tuple, Optional\nfrom moviepy.editor import AudioFileClip\nimport config\n\nlogger = logging.getLogger(__name__)\n\nclass AudioChunker:\n    def __init__(self):\n        self.max_chunk_duration = config.MAX_CHUNK_DURATION_MINUTES * 60  # Convert to seconds\n        self.chunk_overlap = config.CHUNK_OVERLAP_SECONDS\n        self.min_chunk_duration = config.MIN_CHUNK_DURATION_MINUTES * 60  # Convert to seconds\n    \n    def get_audio_duration(self, audio_path: str) -> float:",
    "last_modified": "2025-10-09T05:16:30.799674"
  },
  {
    "id": "1510",
    "name": "pydoc-analyze.py",
    "path": "github_repo/scripts/01_core_ai_analysis/transcription/pydoc-analyze.py",
    "category": "01_core_ai_analysis",
    "type": "analysis",
    "lines": 121,
    "size": 4155,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis"
    ],
    "functions": [
      "get_openai_api_key",
      "get_directory_path",
      "get_pydocgen_paths",
      "generate_docs",
      "enhance_docs",
      "run_flake8"
    ],
    "classes": [],
    "imports": [
      "os",
      "openai",
      "subprocess"
    ],
    "preview": "import os\n\nfrom openai import OpenAI\n\nclient = OpenAI(api_key=get_openai_api_key())\nimport subprocess\n\n\n# Prompt the user to input the OpenAI API key if it's not found in environment variables\ndef get_openai_api_key():\n    api_key = os.getenv(\"OPENAI_API_KEY\")\n    if not api_key:\n        api_key = input(\"Enter your OpenAI API key: \").strip()\n    return api_key\n\n\n# Prompt the user for directory paths if not provided\ndef get_directory_path(prompt_message, default_path):\n    directory = input(f\"{prompt_message} (default: {default_path}): \").strip()\n    return directory if directory else default_path",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1511",
    "name": "start-with-openai.py",
    "path": "github_repo/scripts/01_core_ai_analysis/transcription/start-with-openai.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 11,
    "size": 265,
    "docstring": "",
    "keywords": [
      "openai"
    ],
    "functions": [],
    "classes": [],
    "imports": [
      "openai"
    ],
    "preview": "import openai\n\n# Change YOUR_API_KEY with your actual API key\nopenai.api_key = \"YOUR_API_KEY\"\n\n# Request is saved in a variable\nresponse = openai.Completion.create(engine=\"davinci\", prompt=\"Hello, world!\")\n\n# Prints out the Response\nprint(response.choices[0].text)\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1512",
    "name": "captions.py",
    "path": "github_repo/scripts/01_core_ai_analysis/transcription/captions.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 40,
    "size": 1343,
    "docstring": "",
    "keywords": [],
    "functions": [
      "write_srt",
      "burn_captions_ffmpeg",
      "fmt_time"
    ],
    "classes": [],
    "imports": [
      "__future__",
      "json",
      "math",
      "os",
      "subprocess",
      "tempfile",
      "typing"
    ],
    "preview": "from __future__ import annotations\n\nimport json\nimport math\nimport os\nimport subprocess\nimport tempfile\nfrom typing import Any, Dict, List\n\n\ndef write_srt(segments: List[Dict[str, Any]], srt_path: str):\n    def fmt_time(t):\n        ms = int((t - int(t)) * 1000)\n        h = int(t // 3600)\n        m = int((t % 3600) // 60)\n        s = int(t % 60)\n        return f\"{h:02d}:{m:02d}:{s:02d},{ms:03d}\"\n\n    lines = []\n    for i, seg in enumerate(segments, 1):",
    "last_modified": "2025-09-13T05:55:09.460705"
  },
  {
    "id": "1513",
    "name": "mp3-trans-storytime.py",
    "path": "github_repo/scripts/01_core_ai_analysis/transcription/mp3-trans-storytime.py",
    "category": "01_core_ai_analysis",
    "type": "audio_processing",
    "lines": 136,
    "size": 5114,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis",
      "transcription"
    ],
    "functions": [
      "format_timestamp",
      "transcribe_file",
      "analyze_text_for_section",
      "process_media_directory"
    ],
    "classes": [],
    "imports": [
      "os",
      "subprocess",
      "openai",
      "dotenv"
    ],
    "preview": "import os\nimport subprocess\n\nfrom openai import OpenAI\n\nclient = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\nfrom dotenv import load_dotenv\n\n# Load environment variables from .env\nenv_path = \"/Users/steven/.env\"  # Update the path if needed\nload_dotenv(dotenv_path=env_path)\n\n# Initialize OpenAI API key\n\n# Error checking for OpenAI API key\nif not openai.api_key:\n    raise EnvironmentError(\"OpenAI API key not found. Please check your .env file.\")\n\n\n# Helper to format timestamps",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1514",
    "name": "elevenlabs.py",
    "path": "github_repo/scripts/01_core_ai_analysis/transcription/elevenlabs.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 43,
    "size": 1099,
    "docstring": "",
    "keywords": [],
    "functions": [
      "__init__",
      "run",
      "randomvoice"
    ],
    "classes": [
      "elevenlabs"
    ],
    "imports": [
      "random",
      "elevenlabs",
      "utils"
    ],
    "preview": "import random\n\nfrom elevenlabs import generate, save\n\nfrom utils import settings\n\nvoices = [\n    \"Adam\",\n    \"Antoni\",\n    \"Arnold\",\n    \"Bella\",\n    \"Domi\",\n    \"Elli\",\n    \"Josh\",\n    \"Rachel\",\n    \"Sam\",\n]\n\n\nclass elevenlabs:",
    "last_modified": "2025-09-13T05:53:59.626342"
  },
  {
    "id": "1515",
    "name": "batch_image_seo_pipeline_20250530222615.py",
    "path": "github_repo/scripts/01_core_ai_analysis/transcription/batch_image_seo_pipeline_20250530222615.py",
    "category": "01_core_ai_analysis",
    "type": "image_processing",
    "lines": 122,
    "size": 4843,
    "docstring": "",
    "keywords": [
      "openai"
    ],
    "functions": [
      "call_gpt4o_analysis_and_prompts",
      "batch_flexible_analysis"
    ],
    "classes": [],
    "imports": [
      "csv",
      "json",
      "logging",
      "os",
      "pathlib",
      "dotenv",
      "openai",
      "tqdm"
    ],
    "preview": "import csv\nimport json\nimport logging\nimport os\nfrom pathlib import Path\n\nfrom dotenv import load_dotenv\nfrom openai import OpenAI\nfrom tqdm import tqdm\n\n# CONFIG\nload_dotenv(os.path.expanduser(\"~/.env\"))\nOPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\nif not OPENAI_API_KEY:\n    raise EnvironmentError(\"OPENAI_API_KEY not found in ~/.env\")\nclient = OpenAI(api_key=OPENAI_API_KEY)\n\nLOG_FILE = \"avatararts_flexible_analysis.log\"\nlogging.basicConfig(\n    level=logging.INFO,",
    "last_modified": "2025-09-13T05:53:26.316918"
  },
  {
    "id": "1516",
    "name": "evaluate.py",
    "path": "github_repo/scripts/01_core_ai_analysis/transcription/evaluate.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 251,
    "size": 9083,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis"
    ],
    "functions": [
      "generate_answers",
      "evaluate_answer",
      "evaluate_batch",
      "evaluate_answers",
      "check_if_huggingface_model_exists",
      "check_if_huggingface_dataset_exists",
      "format"
    ],
    "classes": [],
    "imports": [
      "concurrent.futures",
      "gc",
      "json",
      "os",
      "datasets",
      "huggingface_hub",
      "huggingface_hub.utils",
      "openai",
      "tqdm.auto",
      "vllm"
    ],
    "preview": "import concurrent.futures\nimport gc\nimport json\nimport os\n\nfrom datasets import Dataset, load_dataset\nfrom huggingface_hub import HfApi\nfrom huggingface_hub.utils import RepositoryNotFoundError\nfrom openai import OpenAI\nfrom tqdm.auto import tqdm\nfrom vllm import LLM, SamplingParams\n\nOPENAI_API_KEY = os.environ[\"OPENAI_API_KEY\"]\nDATASET_HUGGINGFACE_WORKSPACE = os.environ[\"DATASET_HUGGINGFACE_WORKSPACE\"]\nMODEL_HUGGINGFACE_WORKSPACE = os.environ[\"MODEL_HUGGINGFACE_WORKSPACE\"]\nIS_DUMMY = os.environ.get(\"IS_DUMMY\", False)\n\nprint(\"====== EVAL PARAMETERS ======\")  # noqa\nprint(f\"{DATASET_HUGGINGFACE_WORKSPACE=}\")  # noqa\nprint(f\"{MODEL_HUGGINGFACE_WORKSPACE=}\")  # noqa",
    "last_modified": "2025-09-13T05:53:42.236007"
  },
  {
    "id": "1517",
    "name": "transcribe (1).py",
    "path": "github_repo/scripts/01_core_ai_analysis/transcription/transcribe (1).py",
    "category": "01_core_ai_analysis",
    "type": "transcription",
    "lines": 57,
    "size": 1994,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis",
      "transcription"
    ],
    "functions": [
      "transcribe_audio",
      "analyze_text"
    ],
    "classes": [],
    "imports": [
      "openai",
      "os",
      "dotenv",
      "sys"
    ],
    "preview": "from openai import OpenAI\n\nclient = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\nimport os\n\nfrom dotenv import load_dotenv\n\n# Load environment variables from .env\nload_dotenv()\n\n\n# Function to transcribe audio using OpenAI Whisper\ndef transcribe_audio(file_path):\n    with open(file_path, \"rb\") as audio_file:\n        transcript = client.audio.transcribe(\"whisper-1\", audio_file)\n        return transcript.text\n\n\n# Function to analyze the transcript using GPT\ndef analyze_text(text):",
    "last_modified": "2025-05-04T22:47:13.352714"
  },
  {
    "id": "1518",
    "name": "transcribe-mp3.py",
    "path": "github_repo/scripts/01_core_ai_analysis/transcription/transcribe-mp3.py",
    "category": "01_core_ai_analysis",
    "type": "transcription",
    "lines": 48,
    "size": 1347,
    "docstring": "",
    "keywords": [
      "analysis",
      "transcription"
    ],
    "functions": [
      "transcribe_audio",
      "save_transcription",
      "process_directory",
      "main"
    ],
    "classes": [],
    "imports": [
      "os",
      "whisper"
    ],
    "preview": "import os\n\nimport whisper\n\n\ndef transcribe_audio(file_path):\n    # Load the Whisper model\n    model = whisper.load_model(\"base\")\n\n    # Transcribe the audio file\n    result = model.transcribe(file_path)\n\n    return result[\"segments\"]\n\n\ndef save_transcription(segments, output_file):\n    with open(output_file, \"w\") as f:\n        for segment in segments:\n            start = segment[\"start\"]\n            end = segment[\"end\"]",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1519",
    "name": "batch_image_seo_pipeline_20250530221306.py",
    "path": "github_repo/scripts/01_core_ai_analysis/transcription/batch_image_seo_pipeline_20250530221306.py",
    "category": "01_core_ai_analysis",
    "type": "analysis",
    "lines": 422,
    "size": 17257,
    "docstring": "batch_image_seo_pipeline.py\n\n1) Discovers all image files under a given folder.\n2) Extracts technical metadata via Pillow.\n3) Calls GPT-4o Vision (multimodal) using a \u201csystem\u201d + \u201cuser\u201d prompt.\n4) Parses the JSON response into structured fields:\n   main_subject, style, color_palette, tags, orientation, suggested_products,\n   SEO_title, SEO_description, emotion, safety_rating, dominant_keyword.\n5) Builds a \u201csource\u201d tag: folder-subfolder-YYYYMMDD.\n6) Applies a \u201cTop 5% SEO Analytics\u201d framework to generate SEO columns:\n   \u2013 SEO Keywords, Traffic Source, CRO Tactic, Backlink Source, Engagement Rate.\n7) Adds niche-specific design prompts (Geeky, Dark Humor, Anime).\n8) Writes everything into a final CSV for bulk upload.\n\nDependencies:\n    pip install openai python-dotenv pillow tqdm backoff",
    "keywords": [
      "openai"
    ],
    "functions": [
      "load_openai_client",
      "retry_on_exception",
      "call_gpt4o",
      "get_image_tech_meta",
      "build_source_tag",
      "discover_images",
      "build_gpt_messages",
      "parse_args",
      "main"
    ],
    "classes": [],
    "imports": [
      "argparse",
      "csv",
      "json",
      "logging",
      "os",
      "sys",
      "datetime",
      "pathlib",
      "typing",
      "backoff"
    ],
    "preview": "#!/usr/bin/env python3\n\"\"\"\nbatch_image_seo_pipeline.py\n\n1) Discovers all image files under a given folder.\n2) Extracts technical metadata via Pillow.\n3) Calls GPT-4o Vision (multimodal) using a \u201csystem\u201d + \u201cuser\u201d prompt.\n4) Parses the JSON response into structured fields:\n   main_subject, style, color_palette, tags, orientation, suggested_products,\n   SEO_title, SEO_description, emotion, safety_rating, dominant_keyword.\n5) Builds a \u201csource\u201d tag: folder-subfolder-YYYYMMDD.\n6) Applies a \u201cTop 5% SEO Analytics\u201d framework to generate SEO columns:\n   \u2013 SEO Keywords, Traffic Source, CRO Tactic, Backlink Source, Engagement Rate.\n7) Adds niche-specific design prompts (Geeky, Dark Humor, Anime).\n8) Writes everything into a final CSV for bulk upload.\n\nDependencies:\n    pip install openai python-dotenv pillow tqdm backoff\n\"\"\"\n",
    "last_modified": "2025-09-13T05:53:26.033209"
  },
  {
    "id": "1520",
    "name": "SpeechReco.py",
    "path": "github_repo/scripts/01_core_ai_analysis/transcription/SpeechReco.py",
    "category": "01_core_ai_analysis",
    "type": "transcription",
    "lines": 87,
    "size": 3672,
    "docstring": "",
    "keywords": [],
    "functions": [
      "mp3_to_wav_Conversion",
      "split_files_with_timestamp",
      "writeInFile_key_value"
    ],
    "classes": [],
    "imports": [
      "os",
      "subprocess",
      "time",
      "speech_recognition",
      "pydub",
      "pydub.utils"
    ],
    "preview": "# PLEASE READ THE COMMENTS and PROVIDE REVIEW COMMENTS IN THE CODE\n# This file take mp3 file as an input and produces <key,value> pair of <timestamp,transcribed text> in a single file using Google's Speech recognition library.\n# This pair can later be pre-processed for deleting the articles, prepositions, conjections etc.. and can be added to the DB\n# The granularity is currently set as 10 seconds. we can reduce or increase it depending.\n\n\n# Trial Number#1: with 532 recording on Oct12.(1:15 hrs) <-- the transcribed text does not match with the actual words due to an unique accent.\n# Trial Number#2: with 661 recording by Brian Levine(20 mins) <-- the transcribed output is perfect.\n\n# TBD::: what is the time granularity? which timestamp style do we use? Need american accent dataset, to get proper result.\n# Next step of code addition:::\n#       Loop through multiple videos\n#       Timestamp correction\n#       add the results in the DB\n#       eliminate unimportant words\n\nimport os\nimport subprocess\nimport time\n",
    "last_modified": "2025-03-28T18:37:11"
  },
  {
    "id": "1521",
    "name": "cover2 2.py",
    "path": "github_repo/scripts/01_core_ai_analysis/transcription/cover2 2.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 51,
    "size": 1566,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis",
      "web_tools"
    ],
    "functions": [
      "generate_cover_image_with_dalle",
      "convert_mp3_to_mp4",
      "process_directory"
    ],
    "classes": [],
    "imports": [
      "glob",
      "os",
      "io",
      "requests",
      "moviepy.editor",
      "openai",
      "PIL",
      "sys"
    ],
    "preview": "import glob\nimport os\nfrom io import BytesIO\n\nimport requests\nfrom moviepy.editor import AudioFileClip, ImageClip\nfrom openai import OpenAI\nfrom PIL import Image\n\nclient = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n\n\ndef generate_cover_image_with_dalle(file_name, output_path):\n    prompt = f\"lets create a series of typography cover image for '{file_name}' in the Font and style and contexts to tell the story'\"\n    response = client.images.generate(prompt=prompt, n=1, size=\"1024x1024\")\n    image_url = response.data[0].url\n    response = requests.get(image_url)\n    img = Image.open(BytesIO(response.content))\n    img.save(output_path)\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1522",
    "name": "convert-webm-mp3.py",
    "path": "github_repo/scripts/01_core_ai_analysis/transcription/convert-webm-mp3.py",
    "category": "01_core_ai_analysis",
    "type": "audio_processing",
    "lines": 43,
    "size": 1273,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "os",
      "subprocess"
    ],
    "preview": "import os\nimport subprocess\n\n# Path to the TXT file containing .webm file paths\ntxt_file_path = \"/Users/steven/webm.txt\"\n\n# Read the file paths from the TXT file\nwith open(txt_file_path, \"r\") as file:\n    webm_paths = file.readlines()\n\n# Loop through each path and convert to .mp3\nfor webm_path in webm_paths:\n    webm_path = webm_path.strip()  # Remove newline and extra spaces\n    if webm_path.endswith(\".webm\"):\n        # Define the output .mp3 path\n        mp3_path = webm_path.replace(\".webm\", \".mp3\")\n\n        # Ensure the output directory exists\n        os.makedirs(os.path.dirname(mp3_path), exist_ok=True)\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1523",
    "name": "fancyimg (1) 1.py",
    "path": "github_repo/scripts/01_core_ai_analysis/transcription/fancyimg (1) 1.py",
    "category": "01_core_ai_analysis",
    "type": "image_processing",
    "lines": 84,
    "size": 3000,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis"
    ],
    "functions": [
      "generate_filename_with_gpt",
      "pair_and_rename_images"
    ],
    "classes": [],
    "imports": [
      "os",
      "openai",
      "csv",
      "dotenv"
    ],
    "preview": "import os\n\nfrom openai import OpenAI\n\nclient = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\nimport csv\n\nfrom dotenv import load_dotenv\n\n# Load API key from .env file\nload_dotenv(\"/Users/steven/.env\")\n\n\n# Function to generate a filename using GPT based on the prompt\ndef generate_filename_with_gpt(prompt):\n    response = client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        messages=[\n            {\n                \"role\": \"system\",",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1524",
    "name": "trek 3 2.py",
    "path": "github_repo/scripts/01_core_ai_analysis/transcription/trek 3 2.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 70,
    "size": 2498,
    "docstring": "",
    "keywords": [
      "analysis",
      "web_tools"
    ],
    "functions": [
      "generate_speech",
      "process_csv_and_generate_speech"
    ],
    "classes": [],
    "imports": [
      "csv",
      "os",
      "requests"
    ],
    "preview": "import csv\nimport os\n\nimport requests  # Make sure to install this package\n\n\ndef generate_speech(input_text, output_path, api_key):\n    \"\"\"\n    Generates speech from text using an API (e.g., OpenAI's text-to-speech API).\n    \"\"\"\n    url = \"https://api.openai.com/v1/audio/speech\"\n    headers = {\n        \"Authorization\": f\"Bearer {api_key}\",\n        \"Content-Type\": \"application/json\",\n    }\n    data = {\n        \"model\": \"tts-1\",  # Update this according to the specific model you're using\n        \"input\": input_text,\n        \"voice\": \"shimmer\",  # Update the voice parameter as needed\n    }",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1525",
    "name": "media_processor.py",
    "path": "github_repo/scripts/01_core_ai_analysis/transcription/media_processor.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 131,
    "size": 5982,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis",
      "transcription"
    ],
    "functions": [
      "format_timestamp",
      "transcribe_audio",
      "analyze_text_for_section",
      "process_media_directory"
    ],
    "classes": [],
    "imports": [
      "logging",
      "os",
      "sys",
      "openai",
      "dotenv"
    ],
    "preview": "import logging\nimport os\nimport sys\n\nimport openai\nfrom dotenv import load_dotenv\n\n# Setup basic logging\nlogging.basicConfig(level=logging.INFO, format=\"%(asctime)s [%(levelname)s] %(message)s\")\n\n# Load environment variables from .env (ensure your OPENAI_API_KEY is stored here)\nenv_path = \"/Users/steven/.env\"\nload_dotenv(dotenv_path=env_path)\n\n# Set the OpenAI API key\nopenai.api_key = os.getenv(\"OPENAI_API_KEY\")\n\n# Define a simple config dictionary. You can expand this as needed.\nconfig = {\"base_dir\": \"\"}  # Leave blank to prompt for base directory\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1526",
    "name": "ana-trans.py",
    "path": "github_repo/scripts/01_core_ai_analysis/transcription/ana-trans.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 158,
    "size": 7062,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis",
      "transcription"
    ],
    "functions": [
      "split_media_to_segments",
      "transcribe_media_segment",
      "format_timestamp",
      "analyze_text_for_section",
      "process_media_by_section"
    ],
    "classes": [],
    "imports": [
      "openai",
      "os",
      "subprocess",
      "dotenv",
      "sys"
    ],
    "preview": "import openai\nimport os\nimport subprocess\nfrom dotenv import load_dotenv\n\n# Load environment variables from .env (make sure your OPENAI_API_KEY is stored here)\nload_dotenv(\"/Users/steven/.env\")\nopenai.api_key = os.getenv(\"OPENAI_API_KEY\")\n\n# Prompt for directory paths\nproject_root = input(\"Please enter the project root directory path: \").strip()\n\n# Directory paths based on user input\nMEDIA_DIR = os.path.join(project_root, \"Media\")  # Directory containing media files\nTRANSCRIPT_DIR = os.path.join(MEDIA_DIR, \"transcript\")  # Directory to save transcripts\nANALYSIS_DIR = os.path.join(MEDIA_DIR, \"analysis\")  # Directory to save the analysis files\n\n# Create output directories if they don't exist\nos.makedirs(TRANSCRIPT_DIR, exist_ok=True)\nos.makedirs(ANALYSIS_DIR, exist_ok=True)",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1527",
    "name": "song-process-remix.py",
    "path": "github_repo/scripts/01_core_ai_analysis/transcription/song-process-remix.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 241,
    "size": 8710,
    "docstring": "",
    "keywords": [
      "analysis",
      "transcription"
    ],
    "functions": [
      "extract_duration_from_transcript",
      "get_all_song_data",
      "read_file_content",
      "write_to_csv",
      "format_timestamp",
      "transcribe_audio",
      "analyze_text_for_section",
      "process_audio_file",
      "process_audio_directory"
    ],
    "classes": [],
    "imports": [
      "csv",
      "os",
      "sys",
      "dotenv"
    ],
    "preview": "import csv\nimport os\nimport sys\n\nfrom dotenv import load_dotenv\n\n# BASE_DIR = \"/Users/steven/Music/NocTurnE-meLoDieS/mp3\"\n# CSV_OUTPUT = os.path.join(BASE_DIR, \"final_song_data.csv\")\n\n\ndef extract_duration_from_transcript(transcript_text):\n    lines = transcript_text.strip().split(\"\\n\")\n    last_line = lines[-1] if lines else \"\"\n    if \"--\" in last_line:\n        time_range = last_line.split(\"--\")[-1].split(\":\")\n        minutes = int(time_range[0].strip())\n        seconds = int(time_range[1].strip().split()[0])\n        return f\"{minutes}:{str(seconds).zfill(2)}\"\n    return \"0:00\"\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1528",
    "name": "audiototext.py",
    "path": "github_repo/scripts/01_core_ai_analysis/transcription/audiototext.py",
    "category": "01_core_ai_analysis",
    "type": "transcription",
    "lines": 753,
    "size": 28563,
    "docstring": "AudioToText.ipynb",
    "keywords": [
      "openai",
      "transcription"
    ],
    "functions": [
      "get_audio",
      "add_chunk",
      "raw_split",
      "write_result",
      "write_result"
    ],
    "classes": [
      "WriteText(WriteTXT)"
    ],
    "imports": [
      "subprocess",
      "sys",
      "platform",
      "io",
      "base64",
      "os.path",
      "ffmpeg",
      "numpy",
      "google.colab.output",
      "IPython.display"
    ],
    "preview": "# -*- coding: utf-8 -*-\n\"\"\"AudioToText.ipynb\n\nAutomatically generated by Colab.\n\nOriginal file is located at\n    https://colab.research.google.com/github/Carleslc/AudioToText/blob/master/AudioToText.ipynb\n\n# \ud83d\udde3\ufe0f [**AudioToText**](https://github.com/Carleslc/AudioToText)\n\n[![Donate](https://www.ko-fi.com/img/githubbutton_sm.svg)](https://ko-fi.com/carleslc)\n\n### \ud83d\udee0 [Whisper by OpenAI](https://github.com/openai/whisper)\n\n## [Step 1] \u2699\ufe0f Install the required libraries\n\nClick \u25b6\ufe0f button below to install the dependencies for this notebook.\n\"\"\"\n\n#@title { display-mode: \"form\" }",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1529",
    "name": "Quiz22sec.py",
    "path": "github_repo/scripts/01_core_ai_analysis/transcription/Quiz22sec.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 98,
    "size": 3336,
    "docstring": "",
    "keywords": [
      "analysis",
      "web_tools"
    ],
    "functions": [
      "generate_speech",
      "calculate_text_duration",
      "process_csv_and_generate_speech"
    ],
    "classes": [],
    "imports": [
      "csv",
      "os",
      "requests",
      "dotenv",
      "pydub"
    ],
    "preview": "import csv\nimport os\n\nimport requests  # Install using `pip install requests`\nfrom dotenv import load_dotenv\nfrom pydub import AudioSegment  # Install using `pip install pydub`\n\n\ndef generate_speech(input_text, output_path, api_key):\n    \"\"\"\n    Generates speech from text using OpenAI's text-to-speech API.\n    \"\"\"\n    url = \"https://api.openai.com/v1/audio/speech\"\n    headers = {\n        \"Authorization\": f\"Bearer {api_key}\",\n        \"Content-Type\": \"application/json\",\n    }\n    data = {\n        \"model\": \"tts-1\",\n        \"input\": input_text,",
    "last_modified": "2025-09-13T05:53:54.781162"
  },
  {
    "id": "1530",
    "name": "speech.py",
    "path": "github_repo/scripts/01_core_ai_analysis/transcription/speech.py",
    "category": "01_core_ai_analysis",
    "type": "transcription",
    "lines": 35,
    "size": 1144,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis"
    ],
    "functions": [
      "generate_speech"
    ],
    "classes": [],
    "imports": [
      "csv",
      "pathlib",
      "openai"
    ],
    "preview": "import csv\nfrom pathlib import Path\n\nfrom openai import OpenAI\n\n# Initialize the OpenAI client\nclient = OpenAI()\n\n# Define the path to the CSV file\ncsv_path = \"quiz329.csv\"\n\n# Define the directory to save the audio files\noutput_dir = Path(__file__).parent / \"speech\"\noutput_dir.mkdir(exist_ok=True)  # Create the directory if it doesn't exist\n\n\n# Function to generate speech with pauses\ndef generate_speech(text, file_path):\n    response = client.audio.speech.create(\n        model=\"tts-1\", voice=\"shimmer\", input=text, ssml=True  # Enable SSML processing",
    "last_modified": "2025-05-04T22:47:13.048193"
  },
  {
    "id": "1531",
    "name": "analyze_remaining.py",
    "path": "github_repo/scripts/01_core_ai_analysis/transcription/analyze_remaining.py",
    "category": "01_core_ai_analysis",
    "type": "analysis",
    "lines": 506,
    "size": 23831,
    "docstring": "Analyze remaining Python files and directories for second migration pass",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "analyze_remaining_files",
      "show_remaining_analysis",
      "create_second_migration_script",
      "main"
    ],
    "classes": [],
    "imports": [
      "os",
      "re",
      "pathlib",
      "collections"
    ],
    "preview": "#!/usr/bin/env python3\n\"\"\"\nAnalyze remaining Python files and directories for second migration pass\n\"\"\"\n\nimport os\nimport re\nfrom pathlib import Path\nfrom collections import defaultdict\n\ndef analyze_remaining_files():\n    \"\"\"Analyze all remaining Python files and directories.\"\"\"\n    base_path = Path(\"/Users/steven/Documents/python\")\n    \n    # Categories for remaining files\n    categories = {\n        \"image_processing\": [],\n        \"quiz_tts\": [],\n        \"conversion_tools\": [],\n        \"gallery_html\": [],",
    "last_modified": "2025-10-09T05:30:43.977227"
  },
  {
    "id": "1532",
    "name": "Multi-Modal.py",
    "path": "github_repo/scripts/01_core_ai_analysis/transcription/Multi-Modal.py",
    "category": "01_core_ai_analysis",
    "type": "analysis",
    "lines": 649,
    "size": 24805,
    "docstring": "ULTIMATE Media Analysis Pipeline - Multi-API Edition\nEnhanced with large file support (2GB+ MP4s)\nCompatible with Pydantic 2.x",
    "keywords": [
      "analysis",
      "openai",
      "transcription",
      "organization"
    ],
    "functions": [
      "main",
      "__init__",
      "extract_audio_from_large_video",
      "cleanup_temp_files",
      "__init__",
      "setup_directories",
      "setup_clients",
      "get_file_hash",
      "check_cache",
      "save_to_cache"
    ],
    "classes": [
      "LargeFileHandler",
      "UltimateMediaAnalyzer"
    ],
    "imports": [
      "os",
      "logging",
      "time",
      "json",
      "subprocess",
      "hashlib",
      "pathlib",
      "typing",
      "dotenv",
      "openai"
    ],
    "preview": "#!/usr/bin/env python3\n\"\"\"\nULTIMATE Media Analysis Pipeline - Multi-API Edition\nEnhanced with large file support (2GB+ MP4s)\nCompatible with Pydantic 2.x\n\"\"\"\n\nimport os\nimport logging\nimport time\nimport json\nimport subprocess\nimport hashlib\nfrom pathlib import Path\nfrom typing import Optional, Dict, Any, List\nfrom dotenv import load_dotenv\n\n# Load environment variables first\nload_dotenv()\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1533",
    "name": "engine_wrapper.py",
    "path": "github_repo/scripts/01_core_ai_analysis/transcription/engine_wrapper.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 183,
    "size": 7785,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "process_text",
      "__init__",
      "add_periods",
      "run",
      "split_post",
      "call_tts",
      "create_silence_mp3"
    ],
    "classes": [
      "TTSEngine"
    ],
    "imports": [
      "os",
      "re",
      "pathlib",
      "typing",
      "numpy",
      "translators",
      "moviepy.audio.AudioClip",
      "moviepy.audio.fx.volumex",
      "moviepy.editor",
      "rich.progress"
    ],
    "preview": "import os\nimport re\nfrom pathlib import Path\nfrom typing import Tuple\n\nimport numpy as np\nimport translators\nfrom moviepy.audio.AudioClip import AudioClip\nfrom moviepy.audio.fx.volumex import volumex\nfrom moviepy.editor import AudioFileClip\nfrom rich.progress import track\n\nfrom utils import settings\nfrom utils.console import print_step, print_substep\nfrom utils.voice import sanitize_text\n\nDEFAULT_MAX_LENGTH: int = (\n    50  # Video length variable, edit this on your own risk. It should work, but it's not supported\n)\n",
    "last_modified": "2025-09-13T05:53:59.726247"
  },
  {
    "id": "1534",
    "name": "mp3-csv.py",
    "path": "github_repo/scripts/01_core_ai_analysis/transcription/mp3-csv.py",
    "category": "01_core_ai_analysis",
    "type": "audio_processing",
    "lines": 107,
    "size": 4019,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis",
      "transcription"
    ],
    "functions": [
      "format_timestamp",
      "transcribe_audio",
      "analyze_text_for_section",
      "process_audio_file",
      "process_audio_directory"
    ],
    "classes": [],
    "imports": [
      "os",
      "openai",
      "dotenv"
    ],
    "preview": "import os\n\nimport openai\nfrom dotenv import load_dotenv\n\n# Load environment variables\nload_dotenv()\nopenai.api_key = os.getenv(\"OPENAI_API_KEY\")\nif not openai.api_key:\n    raise EnvironmentError(\"OpenAI API key not found. Please check your .env file.\")\n\n\ndef format_timestamp(seconds):\n    \"\"\"Format seconds into MM:SS timestamp.\"\"\"\n    minutes = int(seconds // 60)\n    seconds = seconds % 60\n    return f\"{minutes:02d}:{int(seconds):02d}\"\n\n\ndef transcribe_audio(file_path):",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1535",
    "name": "import os.py",
    "path": "github_repo/scripts/01_core_ai_analysis/transcription/import os.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 61,
    "size": 1969,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis"
    ],
    "functions": [
      "get_openai_category",
      "create_directory",
      "save_to_category",
      "categorize_files"
    ],
    "classes": [],
    "imports": [
      "os",
      "openai"
    ],
    "preview": "import os\n\nfrom openai import OpenAI\n\nclient = OpenAI(api_key=api_key)\n\n# Load the API key from the environment variable\napi_key = os.getenv(\"OPENAI_API_KEY\")\nif not api_key:\n    raise ValueError(\"API key not found. Please ensure it is set in your environment variables.\")\n\n\ndef get_openai_category(script_content):\n    try:\n        response = client.chat.completions.create(\n            model=\"gpt-3.5-turbo\",\n            messages=[\n                {\"role\": \"system\", \"content\": \"You are an expert Python programmer.\"},\n                {\n                    \"role\": \"user\",",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1536",
    "name": "pyrepo_doc_organizer.py",
    "path": "github_repo/scripts/01_core_ai_analysis/transcription/pyrepo_doc_organizer.py",
    "category": "01_core_ai_analysis",
    "type": "analysis",
    "lines": 478,
    "size": 14913,
    "docstring": "pyrepo_doc_organizer.py\nA comprehensive CLI tool to:\n  1) Audit a Python repository using AST to summarize modules, functions, classes, and imports.\n  2) Generate Markdown summaries and a CSV index for each module.\n  3) Categorize scripts using heuristics and/or OpenAI (optional).\n  4) (Optional) Export simple HTML index (blog-like list) from CSV.\n\nSafe defaults:\n- Works offline (std. library only). OpenAI integration is optional.\n- Does NOT modify your repo (read-only); all outputs go to an output folder.\n- Cross-platform (macOS/Linux/Windows) for Python 3.10+.\n\nUsage examples:\n  # Basic: analyze the current directory and write outputs to ./_py_audit\n  python pyrepo_doc_organizer.py .\n\n  # Include virtualenv directories\n  python pyrepo_doc_organizer.py . --include-venv\n\n  # Ask OpenAI to help with titles/descriptions/categories (requires OPENAI_API_KEY)\n  python pyrepo_doc_organizer.py . --ai-describe --ai-categorize --model gpt-4o-mini\n\n  # Also emit a minimal HTML index (from the CSV) for quick browsing\n  python pyrepo_doc_organizer.py . --html\n\nAuthor: ChatGPT for Steven (TechnoMancer)\nLicense: MIT",
    "keywords": [
      "openai",
      "analysis"
    ],
    "functions": [
      "iter_py_files",
      "read_text",
      "analyze_ast",
      "categorize_heuristic",
      "sanitize_filename",
      "write_markdown",
      "write_csv",
      "write_html_from_csv",
      "ensure_openai_client",
      "ai_describe_and_categorize"
    ],
    "classes": [
      "ModuleInfo"
    ],
    "imports": [
      "__future__",
      "argparse",
      "ast",
      "csv",
      "html",
      "json",
      "os",
      "re",
      "sys",
      "dataclasses"
    ],
    "preview": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\npyrepo_doc_organizer.py\nA comprehensive CLI tool to:\n  1) Audit a Python repository using AST to summarize modules, functions, classes, and imports.\n  2) Generate Markdown summaries and a CSV index for each module.\n  3) Categorize scripts using heuristics and/or OpenAI (optional).\n  4) (Optional) Export simple HTML index (blog-like list) from CSV.\n\nSafe defaults:\n- Works offline (std. library only). OpenAI integration is optional.\n- Does NOT modify your repo (read-only); all outputs go to an output folder.\n- Cross-platform (macOS/Linux/Windows) for Python 3.10+.\n\nUsage examples:\n  # Basic: analyze the current directory and write outputs to ./_py_audit\n  python pyrepo_doc_organizer.py .\n\n  # Include virtualenv directories",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1537",
    "name": "reddit.py",
    "path": "github_repo/scripts/01_core_ai_analysis/transcription/reddit.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 57,
    "size": 1627,
    "docstring": "",
    "keywords": [
      "data_processing"
    ],
    "functions": [
      "comment_html",
      "gen_comment_image"
    ],
    "classes": [],
    "imports": [
      "datetime",
      "pandas",
      "praw",
      "moviepy.editor"
    ],
    "preview": "import datetime as dt\n\nimport pandas as pd\nimport praw\nfrom moviepy.editor import *\n\n\ndef comment_html(username, content):\n    str = (\n        \"<!DOCTYPE html><html><head>\"\n        \"<style>body {background-color: rgb(26, 26, 27);color: white;font-family: BentonSans, sans-serif;}.username {color: rgb(79, 188, 255);}.content {padding: 5px}.header {padding: 0 0 0 5px}</style>\"\n        \"</head>\"\n        \"<body><div><div class = 'header'><span class=username>\"\n        + username\n        + \"</span></div><div class = 'content'>\"\n        + content\n        + \"</div></div></body></html>\"\n    )\n    return str\n",
    "last_modified": "2025-05-04T23:28:22.832649"
  },
  {
    "id": "1538",
    "name": "enhanced_image_pipeline.py",
    "path": "github_repo/scripts/01_core_ai_analysis/transcription/enhanced_image_pipeline.py",
    "category": "01_core_ai_analysis",
    "type": "analysis",
    "lines": 1299,
    "size": 51574,
    "docstring": "Hybrid Image Analysis Pipeline\n==============================\n\nA cost-optimized pipeline combining Google Cloud Vision API for technical analysis\nwith OpenAI GPT-4 Vision for complex semantic understanding. This approach reduces\nAPI costs by 60-80% while maintaining high-quality results.\n\nArchitecture:\n- Google Cloud Vision: Object detection, label classification, color analysis, OCR\n- OpenAI GPT-4 Vision: Emotional context, style descriptions, marketing copy, SEO optimization\n\nFeatures:\n- Intelligent API selection based on analysis type\n- Cost tracking and optimization\n- Fallback mechanisms for API failures\n- Batch processing with rate limiting\n- Comprehensive error handling and retry logic\n\nAuthor: Enhanced by Claude\nVersion: 2.1 (Hybrid)",
    "keywords": [
      "openai",
      "analysis"
    ],
    "functions": [
      "setup_logging",
      "parse_arguments",
      "validate_environment",
      "main",
      "__init__",
      "load_config",
      "_deep_merge",
      "save_config",
      "get",
      "__init__"
    ],
    "classes": [
      "ImageMetadata",
      "GoogleVisionResults",
      "OpenAIResults",
      "CombinedAnalysis",
      "ProcessingResult",
      "Config",
      "CostTracker",
      "GoogleVisionClient",
      "OpenAIClient",
      "HybridImageProcessor",
      "CSVWriter",
      "HybridImagePipeline"
    ],
    "imports": [
      "argparse",
      "base64",
      "csv",
      "json",
      "logging",
      "os",
      "sys",
      "time",
      "concurrent.futures",
      "datetime"
    ],
    "preview": "#!/usr/bin/env python3\n\"\"\"\nHybrid Image Analysis Pipeline\n==============================\n\nA cost-optimized pipeline combining Google Cloud Vision API for technical analysis\nwith OpenAI GPT-4 Vision for complex semantic understanding. This approach reduces\nAPI costs by 60-80% while maintaining high-quality results.\n\nArchitecture:\n- Google Cloud Vision: Object detection, label classification, color analysis, OCR\n- OpenAI GPT-4 Vision: Emotional context, style descriptions, marketing copy, SEO optimization\n\nFeatures:\n- Intelligent API selection based on analysis type\n- Cost tracking and optimization\n- Fallback mechanisms for API failures\n- Batch processing with rate limiting\n- Comprehensive error handling and retry logic\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1539",
    "name": "organize_avatararts.py",
    "path": "github_repo/scripts/01_core_ai_analysis/transcription/organize_avatararts.py",
    "category": "01_core_ai_analysis",
    "type": "organization",
    "lines": 95,
    "size": 2281,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "os",
      "shutil",
      "pathlib"
    ],
    "preview": "import os\nimport shutil\nfrom pathlib import Path\n\nbase_path = Path(\"/Users/steven/AvaTarArTs\")\n\nstructure_map = {\n    \"site\": [\n        \"index.html\",\n        \"playlist.html\",\n        \"globe.html\",\n        \"glitch.html\",\n        \"404.md\",\n        \"style.css\",\n        \"styles.css\",\n        \"styles\",\n        \"script.js\",\n        \"js\",\n        \"img\",\n        \"site.html\",",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1540",
    "name": "video_analyzer.py",
    "path": "github_repo/scripts/01_core_ai_analysis/transcription/video_analyzer.py",
    "category": "01_core_ai_analysis",
    "type": "analysis",
    "lines": 114,
    "size": 5696,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis",
      "transcription"
    ],
    "functions": [
      "format_timestamp",
      "transcribe_audio",
      "analyze_text_for_section",
      "process_audio_directory"
    ],
    "classes": [],
    "imports": [
      "logging",
      "os",
      "openai",
      "shared.config",
      "termcolor",
      "tqdm",
      "shared.openai_client"
    ],
    "preview": "import logging\nimport os\n\nimport openai\nfrom shared.config import *\nfrom termcolor import colored\nfrom tqdm import tqdm\n\n# Load environment variables from .env\nenv_path = os.path.expanduser(\"~/.env\")\nload_dotenv(dotenv_path=env_path)\n\n# Set OpenAI API key\nfrom shared.openai_client import get_openai_client\nif not openai.api_key:\n    raise EnvironmentError(\"OpenAI API key not found. Please check your .env file.\")\n\n\ndef format_timestamp(seconds):\n    minutes = int(seconds // 60)",
    "last_modified": "2025-10-09T05:27:15.573723"
  },
  {
    "id": "1541",
    "name": "mp4-mp3-analyze.py",
    "path": "github_repo/scripts/01_core_ai_analysis/transcription/mp4-mp3-analyze.py",
    "category": "01_core_ai_analysis",
    "type": "analysis",
    "lines": 174,
    "size": 6748,
    "docstring": "",
    "keywords": [
      "video_processing",
      "openai",
      "analysis",
      "transcription"
    ],
    "functions": [
      "split_video_to_segments",
      "transcribe_video_segment",
      "format_timestamp",
      "analyze_text_for_section",
      "process_video_by_section"
    ],
    "classes": [],
    "imports": [
      "os",
      "subprocess",
      "openai",
      "dotenv",
      "sys"
    ],
    "preview": "import os\nimport subprocess\n\nimport openai\nfrom dotenv import load_dotenv\n\n# Load environment variables from .env (make sure your OPENAI_API_KEY is stored here)\nload_dotenv()\nopenai.api_key = os.getenv(\"OPENAI_API_KEY\")\n\n# Directory paths\nVIDEO_DIR = (\n    \"/Users/steven/Movies/j2025\"  # Directory containing MP4 files\n)\nTRANSCRIPT_DIR = \"/Users/steven/Movies/j2025/transcript\"  # Directory to save transcripts\nANALYSIS_DIR = \"/Users/steven/Movies/j2025/analysis\"  # Directory to save the analysis files\n\n# Create output directories if they don't exist\nos.makedirs(TRANSCRIPT_DIR, exist_ok=True)\nos.makedirs(ANALYSIS_DIR, exist_ok=True)",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1542",
    "name": "analyze-shorts.py",
    "path": "github_repo/scripts/01_core_ai_analysis/transcription/analyze-shorts.py",
    "category": "01_core_ai_analysis",
    "type": "analysis",
    "lines": 123,
    "size": 5264,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis",
      "transcription"
    ],
    "functions": [
      "transcribe_audio",
      "format_timestamp",
      "analyze_text_for_section",
      "process_audio_directory"
    ],
    "classes": [],
    "imports": [
      "openai",
      "logging",
      "os",
      "sys",
      "time",
      "concurrent.futures",
      "dotenv",
      "termcolor",
      "tqdm"
    ],
    "preview": "from openai import OpenAI\n\nclient = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))\nimport logging\nimport os\nimport sys\nimport time\nfrom concurrent.futures import ThreadPoolExecutor\n\nfrom dotenv import load_dotenv\nfrom termcolor import colored\nfrom tqdm import tqdm\n\n# Load environment variables from .env (make sure your OPENAI_API_KEY is stored here)\nenv_path = os.path.expanduser(\"~/.env\")\n # Update this path if necessary\nload_dotenv(dotenv_path=env_path)\n\n# Initialize OpenAI API key\n",
    "last_modified": "2025-05-04T22:47:13.343475"
  },
  {
    "id": "1543",
    "name": "info.py",
    "path": "github_repo/scripts/01_core_ai_analysis/transcription/info.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 159,
    "size": 7063,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis",
      "transcription"
    ],
    "functions": [
      "split_media_to_segments",
      "transcribe_media_segment",
      "format_timestamp",
      "analyze_text_for_section",
      "process_media_by_section"
    ],
    "classes": [],
    "imports": [
      "os",
      "subprocess",
      "openai",
      "dotenv",
      "sys"
    ],
    "preview": "import os\nimport subprocess\n\nimport openai\nfrom dotenv import load_dotenv\n\n# Load environment variables from .env (make sure your OPENAI_API_KEY is stored here)\nload_dotenv(\"/Users/steven/.env\")\nopenai.api_key = os.getenv(\"OPENAI_API_KEY\")\n\n# Prompt for directory paths\nproject_root = input(\"Please enter the project root directory path: \").strip()\n\n# Directory paths based on user input\nMEDIA_DIR = os.path.join(project_root, \"Media\")  # Directory containing media files\nTRANSCRIPT_DIR = os.path.join(MEDIA_DIR, \"transcript\")  # Directory to save transcripts\nANALYSIS_DIR = os.path.join(MEDIA_DIR, \"analysis\")  # Directory to save the analysis files\n\n# Create output directories if they don't exist\nos.makedirs(TRANSCRIPT_DIR, exist_ok=True)",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1544",
    "name": "batch_image_seo_pipeline_20250530220526.py",
    "path": "github_repo/scripts/01_core_ai_analysis/transcription/batch_image_seo_pipeline_20250530220526.py",
    "category": "01_core_ai_analysis",
    "type": "image_processing",
    "lines": 409,
    "size": 16734,
    "docstring": "",
    "keywords": [
      "openai"
    ],
    "functions": [
      "load_openai_client",
      "retry_on_api_error",
      "call_gpt4o",
      "get_image_tech_meta",
      "build_source_tag",
      "discover_images",
      "build_gpt_messages",
      "parse_args",
      "main"
    ],
    "classes": [],
    "imports": [
      "argparse",
      "csv",
      "json",
      "logging",
      "os",
      "sys",
      "time",
      "datetime",
      "pathlib",
      "typing"
    ],
    "preview": "import argparse\nimport csv\nimport json\nimport logging\nimport os\nimport sys\nimport time\nfrom datetime import datetime\nfrom pathlib import Path\nfrom typing import Any, Dict, List, Optional\n\nimport backoff  # pip install backoff\nfrom dotenv import load_dotenv\nfrom openai import OpenAI, OpenAIAPIError\nfrom PIL import Image, UnidentifiedImageError\nfrom tqdm import tqdm\n\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n# CONFIGURATION\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500",
    "last_modified": "2025-09-13T05:53:26.971810"
  },
  {
    "id": "1545",
    "name": "main (1).py",
    "path": "github_repo/scripts/01_core_ai_analysis/transcription/main (1).py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 21,
    "size": 626,
    "docstring": "",
    "keywords": [],
    "functions": [
      "main"
    ],
    "classes": [],
    "imports": [
      "csv",
      "generate_speech"
    ],
    "preview": "import csv\n\nfrom generate_speech import generate_speech\n\n\ndef main():\n    csv_path = \"/Users/steven/Music/quiz-talk/Gtrivia - Sheet1.csv\"\n\n    with open(csv_path, newline=\"\", encoding=\"utf-8\") as csvfile:\n        reader = csv.DictReader(csvfile)\n        for i, row in enumerate(reader):\n            # Assuming 'Question' is the column name\n            question_text = row[\"Question\"]\n            output_path = f\"/ Users / steven / Music / quiz - talk / speech / question_{\n                i + 1}.mp3\"\n            generate_speech(question_text, voice=\"shimmer\", output_path=output_path)\n\n\nif __name__ == \"__main__\":\n    main()",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1546",
    "name": "mp4-mp3-analyze_20250531070353.py",
    "path": "github_repo/scripts/01_core_ai_analysis/transcription/mp4-mp3-analyze_20250531070353.py",
    "category": "01_core_ai_analysis",
    "type": "analysis",
    "lines": 174,
    "size": 6748,
    "docstring": "",
    "keywords": [
      "video_processing",
      "openai",
      "analysis",
      "transcription"
    ],
    "functions": [
      "split_video_to_segments",
      "transcribe_video_segment",
      "format_timestamp",
      "analyze_text_for_section",
      "process_video_by_section"
    ],
    "classes": [],
    "imports": [
      "os",
      "subprocess",
      "openai",
      "dotenv",
      "sys"
    ],
    "preview": "import os\nimport subprocess\n\nimport openai\nfrom dotenv import load_dotenv\n\n# Load environment variables from .env (make sure your OPENAI_API_KEY is stored here)\nload_dotenv()\nopenai.api_key = os.getenv(\"OPENAI_API_KEY\")\n\n# Directory paths\nVIDEO_DIR = (\n    \"/Users/steven/Movies/j2025\"  # Directory containing MP4 files\n)\nTRANSCRIPT_DIR = \"/Users/steven/Movies/j2025/transcript\"  # Directory to save transcripts\nANALYSIS_DIR = \"/Users/steven/Movies/j2025/analysis\"  # Directory to save the analysis files\n\n# Create output directories if they don't exist\nos.makedirs(TRANSCRIPT_DIR, exist_ok=True)\nos.makedirs(ANALYSIS_DIR, exist_ok=True)",
    "last_modified": "2025-05-31T07:03:53.971234"
  },
  {
    "id": "1547",
    "name": "quiz-time.py",
    "path": "github_repo/scripts/01_core_ai_analysis/transcription/quiz-time.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 121,
    "size": 4449,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis",
      "web_tools"
    ],
    "functions": [
      "generate_speech",
      "calculate_text_duration",
      "process_csv_and_generate_speech"
    ],
    "classes": [],
    "imports": [
      "csv",
      "os",
      "requests",
      "openai",
      "pydub"
    ],
    "preview": "import csv\nimport os\n\nimport requests\nfrom openai import OpenAI\nfrom pydub import AudioSegment\n\n# Initialize the OpenAI client\n\n\ndef generate_speech(input_text, output_path, api_key):\n    \"\"\"\n    Generates speech from text using an API (e.g., OpenAI's text-to-speech API).\n    \"\"\"\n    url = \"https://api.openai.com/v1/audio/speech\"\n    headers = {\n        \"Authorization\": f\"Bearer {api_key}\",\n        \"Content-Type\": \"application/json\",\n    }\n    data = {",
    "last_modified": "2025-09-13T05:53:51.009657"
  },
  {
    "id": "1548",
    "name": "named.py",
    "path": "github_repo/scripts/01_core_ai_analysis/transcription/named.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 198,
    "size": 6669,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis"
    ],
    "functions": [
      "sanitize_filename",
      "get_closest_aspect_ratio",
      "resize_image",
      "process_batch",
      "process_images_and_generate_csv",
      "write_csv",
      "main"
    ],
    "classes": [],
    "imports": [
      "csv",
      "os",
      "time",
      "datetime",
      "dotenv",
      "openai",
      "PIL"
    ],
    "preview": "import csv\nimport os\nimport time\nfrom datetime import datetime\n\nfrom dotenv import load_dotenv\nfrom openai import OpenAI\nfrom PIL import Image, UnidentifiedImageError\n\n# Load environment variables\nenv_path = \"/Users/steven/.env\"\nload_dotenv(dotenv_path=env_path)\n\n# Initialize OpenAI API key\napi_key = os.getenv(\"OPENAI_API_KEY\")\nif not api_key:\n    raise EnvironmentError(\"OpenAI API key not found. Please check your .env file.\")\n\n# Constants\nMAX_WIDTH, MAX_HEIGHT = 4500, 5400",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1549",
    "name": "mp4tomp3.py",
    "path": "github_repo/scripts/01_core_ai_analysis/transcription/mp4tomp3.py",
    "category": "01_core_ai_analysis",
    "type": "video_processing",
    "lines": 59,
    "size": 1759,
    "docstring": "",
    "keywords": [
      "analysis",
      "transcription"
    ],
    "functions": [
      "convert_mp4_to_mp3",
      "transcribe_audio",
      "save_transcription",
      "process_directory",
      "main"
    ],
    "classes": [],
    "imports": [
      "os",
      "subprocess",
      "whisper"
    ],
    "preview": "import os\nimport subprocess\n\nimport whisper\n\n\ndef convert_mp4_to_mp3(mp4_file, mp3_file):\n    # Use ffmpeg to convert mp4 to mp3\n    subprocess.run([\"ffmpeg\", \"-i\", mp4_file, \"-q:a\", \"0\", \"-map\", \"a\", mp3_file])\n\n\ndef transcribe_audio(file_path):\n    # Load the Whisper model\n    model = whisper.load_model(\"base\")\n\n    # Transcribe the audio file\n    result = model.transcribe(file_path)\n\n    return result[\"segments\"]\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1550",
    "name": "media-trans-analyze.py",
    "path": "github_repo/scripts/01_core_ai_analysis/transcription/media-trans-analyze.py",
    "category": "01_core_ai_analysis",
    "type": "analysis",
    "lines": 159,
    "size": 7063,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis",
      "transcription"
    ],
    "functions": [
      "split_media_to_segments",
      "transcribe_media_segment",
      "format_timestamp",
      "analyze_text_for_section",
      "process_media_by_section"
    ],
    "classes": [],
    "imports": [
      "os",
      "subprocess",
      "openai",
      "dotenv",
      "sys"
    ],
    "preview": "import os\nimport subprocess\n\nimport openai\nfrom dotenv import load_dotenv\n\n# Load environment variables from .env (make sure your OPENAI_API_KEY is stored here)\nload_dotenv(\"/Users/steven/.env\")\nopenai.api_key = os.getenv(\"OPENAI_API_KEY\")\n\n# Prompt for directory paths\nproject_root = input(\"Please enter the project root directory path: \").strip()\n\n# Directory paths based on user input\nMEDIA_DIR = os.path.join(project_root, \"Media\")  # Directory containing media files\nTRANSCRIPT_DIR = os.path.join(MEDIA_DIR, \"transcript\")  # Directory to save transcripts\nANALYSIS_DIR = os.path.join(MEDIA_DIR, \"analysis\")  # Directory to save the analysis files\n\n# Create output directories if they don't exist\nos.makedirs(TRANSCRIPT_DIR, exist_ok=True)",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1551",
    "name": "untitled.py",
    "path": "github_repo/scripts/01_core_ai_analysis/transcription/untitled.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 142,
    "size": 6647,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis",
      "transcription"
    ],
    "functions": [
      "format_timestamp",
      "transcribe_file",
      "analyze_text_for_section",
      "process_media_directory"
    ],
    "classes": [],
    "imports": [
      "os",
      "subprocess",
      "openai",
      "dotenv"
    ],
    "preview": "import os\nimport subprocess\n\nfrom openai import OpenAI\n\nclient = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\nfrom dotenv import load_dotenv\n\n# Load environment variables from .env\nenv_path = os.path.expanduser(\"~/.env\")\n# Update the path if needed\nload_dotenv(dotenv_path=env_path)\n\n# Initialize OpenAI API key\n\n# Error checking for OpenAI API key\nif not openai.api_key:\n    raise EnvironmentError(\"OpenAI API key not found. Please check your .env file.\")\n\n",
    "last_modified": "2025-09-13T05:53:55.629302"
  },
  {
    "id": "1552",
    "name": "migrate_remaining_fixed.py",
    "path": "github_repo/scripts/01_core_ai_analysis/transcription/migrate_remaining_fixed.py",
    "category": "01_core_ai_analysis",
    "type": "organization",
    "lines": 301,
    "size": 14502,
    "docstring": "Second Migration Pass - Organize Remaining Python Files (Fixed Version)",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "create_additional_directories",
      "migrate_remaining_files",
      "migrate_remaining_directories",
      "archive_remaining_duplicates",
      "main"
    ],
    "classes": [],
    "imports": [
      "os",
      "shutil",
      "pathlib"
    ],
    "preview": "#!/usr/bin/env python3\n\"\"\"\nSecond Migration Pass - Organize Remaining Python Files (Fixed Version)\n\"\"\"\n\nimport os\nimport shutil\nfrom pathlib import Path\n\ndef create_additional_directories():\n    \"\"\"Create additional directories for remaining files.\"\"\"\n    base_path = Path(\"/Users/steven/Documents/python\")\n    \n    additional_dirs = [\n        \"01_core_tools/text_processors\",\n        \"05_audio_video/image_processors\", \n        \"05_audio_video/audio_processors/quiz_tts\",\n        \"06_utilities/converters\",\n        \"06_utilities/data_processors\",\n        \"07_experimental/web_tools\",",
    "last_modified": "2025-10-09T05:32:18.982367"
  },
  {
    "id": "1553",
    "name": "batch_image_seo_pipeline.py",
    "path": "github_repo/scripts/01_core_ai_analysis/transcription/batch_image_seo_pipeline.py",
    "category": "01_core_ai_analysis",
    "type": "image_processing",
    "lines": 421,
    "size": 16834,
    "docstring": "",
    "keywords": [
      "openai"
    ],
    "functions": [
      "load_openai_client",
      "retry_on_api_error",
      "call_gpt4o",
      "get_image_tech_meta",
      "build_source_tag",
      "discover_images",
      "build_gpt_messages",
      "parse_args",
      "main"
    ],
    "classes": [],
    "imports": [
      "argparse",
      "csv",
      "json",
      "logging",
      "os",
      "sys",
      "time",
      "datetime",
      "pathlib",
      "typing"
    ],
    "preview": "import argparse\nimport csv\nimport json\nimport logging\nimport os\nimport sys\nimport time\nfrom datetime import datetime\nfrom pathlib import Path\nfrom typing import Any, Dict, List, Optional\n\nimport backoff  # pip install backoff\nfrom dotenv import load_dotenv\nfrom openai import OpenAI, OpenAIAPIError\nfrom PIL import Image, UnidentifiedImageError\nfrom tqdm import tqdm\n\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n# CONFIGURATION\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1554",
    "name": "analyzer-prompt.py",
    "path": "github_repo/scripts/01_core_ai_analysis/transcription/analyzer-prompt.py",
    "category": "01_core_ai_analysis",
    "type": "analysis",
    "lines": 156,
    "size": 7044,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis",
      "transcription"
    ],
    "functions": [
      "split_video_to_segments",
      "transcribe_video_segment",
      "format_timestamp",
      "analyze_text_for_section",
      "process_video_by_section"
    ],
    "classes": [],
    "imports": [
      "os",
      "subprocess",
      "openai",
      "dotenv",
      "sys"
    ],
    "preview": "import os\nimport subprocess\n\nimport openai\nfrom dotenv import load_dotenv\n\n# Load environment variables from .env (make sure your OPENAI_API_KEY is stored here)\nload_dotenv()\nopenai.api_key = os.getenv(\"OPENAI_API_KEY\")\n\n# Prompt user for the video directory path\nVIDEO_DIR = input(\"\ud83c\udfa5 Please enter the directory path containing your MP4 files: \")\n\n# Directory paths\nTRANSCRIPT_DIR = os.path.join(VIDEO_DIR, \"transcript\")  # Directory to save transcripts\nANALYSIS_DIR = os.path.join(VIDEO_DIR, \"analysis\")  # Directory to save the analysis files\n\n# Create output directories if they don't exist\nos.makedirs(TRANSCRIPT_DIR, exist_ok=True)\nos.makedirs(ANALYSIS_DIR, exist_ok=True)",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1555",
    "name": "batch_image_seo_pipeline_20250530222614.py",
    "path": "github_repo/scripts/01_core_ai_analysis/transcription/batch_image_seo_pipeline_20250530222614.py",
    "category": "01_core_ai_analysis",
    "type": "image_processing",
    "lines": 122,
    "size": 4843,
    "docstring": "",
    "keywords": [
      "openai"
    ],
    "functions": [
      "call_gpt4o_analysis_and_prompts",
      "batch_flexible_analysis"
    ],
    "classes": [],
    "imports": [
      "csv",
      "json",
      "logging",
      "os",
      "pathlib",
      "dotenv",
      "openai",
      "tqdm"
    ],
    "preview": "import csv\nimport json\nimport logging\nimport os\nfrom pathlib import Path\n\nfrom dotenv import load_dotenv\nfrom openai import OpenAI\nfrom tqdm import tqdm\n\n# CONFIG\nload_dotenv(os.path.expanduser(\"~/.env\"))\nOPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\nif not OPENAI_API_KEY:\n    raise EnvironmentError(\"OPENAI_API_KEY not found in ~/.env\")\nclient = OpenAI(api_key=OPENAI_API_KEY)\n\nLOG_FILE = \"avatararts_flexible_analysis.log\"\nlogging.basicConfig(\n    level=logging.INFO,",
    "last_modified": "2025-09-13T05:53:26.215232"
  },
  {
    "id": "1556",
    "name": "gpt4.py",
    "path": "github_repo/scripts/01_core_ai_analysis/transcription/gpt4.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 85,
    "size": 2501,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis",
      "web_tools"
    ],
    "functions": [
      "analyze_image_with_gpt4_vision",
      "upscale_image",
      "text_to_speech"
    ],
    "classes": [],
    "imports": [
      "csv",
      "io",
      "openai",
      "requests",
      "PIL"
    ],
    "preview": "import csv\nfrom io import BytesIO\n\nimport openai\nimport requests\nfrom PIL import Image\n\n# Initialize the OpenAI client\nclient = openai(api_key=\"sk-acw35nWnLFMd23JUzcQoQ7QBhg4y5wLxoQJpR64ITBWHqBT7\")\n\n\ndef analyze_image_with_gpt4_vision(image_url):\n    # Analyze the image using GPT-4 with Vision\n    response = client.chat.completions.create(\n        model=\"gpt-4-vision-preview\",\n        messages=[\n            {\n                \"role\": \"user\",\n                \"content\": [\n                    {\"type\": \"text\", \"text\": \"What\u2019s in this image?\"},",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1557",
    "name": "sora.py",
    "path": "github_repo/scripts/01_core_ai_analysis/transcription/sora.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 12,
    "size": 202,
    "docstring": "",
    "keywords": [
      "openai"
    ],
    "functions": [],
    "classes": [],
    "imports": [
      "os",
      "openai"
    ],
    "preview": "import os\n\nimport openai\n\nresponse = openai.Video.create(\n    model=\"sora\",\n    prompt=\"A futuristic cityscape with flying cars\",\n    duration=60,  # Duration in seconds\n)\n\nprint(response[\"video_url\"])\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1558",
    "name": "speak (1).py",
    "path": "github_repo/scripts/01_core_ai_analysis/transcription/speak (1).py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 55,
    "size": 1741,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis"
    ],
    "functions": [
      "generate_speech",
      "main"
    ],
    "classes": [],
    "imports": [
      "csv",
      "openai"
    ],
    "preview": "import csv\n\nimport openai\n\n# Initialize the OpenAI client\nclient = openai()\n\n\ndef generate_speech(text, pause_duration=\"5s\", voice=\"shimmer\", output_path=\"speech.mp3\"):\n    # Adding a pause using the SSML <break> tag\n    # Assuming that 'text' contains something like \"Option 3: <your text>\"\n    # and you want to insert a pause right after this before continuing with\n    # the answer\n    modified_text = text.replace(\"Option 3:\", f'Option 3:\"/>')\n\n    response = OpenAI.Audio.create(\n        model=\"tts-1-hd\",  # Ensure this model supports the audio creation\n        input=modified_text,\n        voice=voice,\n        format=\"mp3\",",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1559",
    "name": "yt-dalle-prompt.py",
    "path": "github_repo/scripts/01_core_ai_analysis/transcription/yt-dalle-prompt.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 77,
    "size": 2509,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis",
      "web_tools"
    ],
    "functions": [
      "upscale_image",
      "generate_youtube_content",
      "analyze_and_generate"
    ],
    "classes": [],
    "imports": [
      "csv",
      "io",
      "requests",
      "openai",
      "PIL"
    ],
    "preview": "import csv\nfrom io import BytesIO\n\nimport requests\nfrom openai import OpenAI\nfrom PIL import Image\n\nclient = OpenAI(api_key=\"sk-acw35nWnLFMd23JUzcQoQ7QBhg4y5wLxoQJpR64ITBWHqBT7\")\n\n# Set your OpenAI API key here\n\n\ndef upscale_image(image_url):\n    # Fetch the image\n    response = requests.get(image_url)\n    image = Image.open(BytesIO(response.content))\n\n    # Calculate the new size, doubling the width and height\n    new_size = (image.width * 2, image.height * 2)\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1560",
    "name": "QuizPrompts.py",
    "path": "github_repo/scripts/01_core_ai_analysis/transcription/QuizPrompts.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 100,
    "size": 3394,
    "docstring": "",
    "keywords": [
      "analysis",
      "web_tools"
    ],
    "functions": [
      "generate_speech",
      "calculate_text_duration",
      "process_csv_and_generate_speech"
    ],
    "classes": [],
    "imports": [
      "csv",
      "os",
      "requests",
      "dotenv",
      "pydub"
    ],
    "preview": "import csv\nimport os\n\nimport requests  # Install using `pip install requests`\nfrom dotenv import load_dotenv\nfrom pydub import AudioSegment  # Install using `pip install pydub`\n\n\ndef generate_speech(input_text, output_path, api_key):\n    \"\"\"\n    Generates speech from text using OpenAI's text-to-speech API.\n    \"\"\"\n    url = \"https://api.openai.com/v1/audio/speech\"\n    headers = {\n        \"Authorization\": f\"Bearer {api_key}\",\n        \"Content-Type\": \"application/json\",\n    }\n    data = {\n        \"model\": \"tts-1\",\n        \"input\": input_text,",
    "last_modified": "2025-09-13T05:53:54.821397"
  },
  {
    "id": "1561",
    "name": "batch_image_seo_pipeline_20250530221257.py",
    "path": "github_repo/scripts/01_core_ai_analysis/transcription/batch_image_seo_pipeline_20250530221257.py",
    "category": "01_core_ai_analysis",
    "type": "analysis",
    "lines": 428,
    "size": 17474,
    "docstring": "batch_image_seo_pipeline.py\n\n1) Discovers all image files under a given folder.\n2) Extracts technical metadata via Pillow.\n3) Calls GPT-4o Vision (multimodal) using a \u201csystem\u201d + \u201cuser\u201d prompt.\n4) Parses the JSON response into structured fields:\n   main_subject, style, color_palette, tags, orientation, suggested_products,\n   SEO_title, SEO_description, emotion, safety_rating, dominant_keyword.\n5) Builds a \u201csource\u201d tag: folder-subfolder-YYYYMMDD.\n6) Applies a \u201cTop 5% SEO Analytics\u201d framework to generate SEO columns:\n   \u2013 SEO Keywords, Traffic Source, CRO Tactic, Backlink Source, Engagement Rate.\n7) Adds niche-specific design prompts (Geeky, Dark Humor, Anime).\n8) Writes everything into a final CSV for bulk upload.\n\nDependencies:\n    pip install openai python-dotenv pillow tqdm backoff",
    "keywords": [
      "openai"
    ],
    "functions": [
      "load_openai_client",
      "retry_on_api_error",
      "call_gpt4o",
      "get_image_tech_meta",
      "build_source_tag",
      "discover_images",
      "build_gpt_messages",
      "parse_args",
      "main"
    ],
    "classes": [],
    "imports": [
      "argparse",
      "csv",
      "json",
      "logging",
      "os",
      "sys",
      "datetime",
      "pathlib",
      "typing",
      "backoff"
    ],
    "preview": "#!/usr/bin/env python3\n\"\"\"\nbatch_image_seo_pipeline.py\n\n1) Discovers all image files under a given folder.\n2) Extracts technical metadata via Pillow.\n3) Calls GPT-4o Vision (multimodal) using a \u201csystem\u201d + \u201cuser\u201d prompt.\n4) Parses the JSON response into structured fields:\n   main_subject, style, color_palette, tags, orientation, suggested_products,\n   SEO_title, SEO_description, emotion, safety_rating, dominant_keyword.\n5) Builds a \u201csource\u201d tag: folder-subfolder-YYYYMMDD.\n6) Applies a \u201cTop 5% SEO Analytics\u201d framework to generate SEO columns:\n   \u2013 SEO Keywords, Traffic Source, CRO Tactic, Backlink Source, Engagement Rate.\n7) Adds niche-specific design prompts (Geeky, Dark Humor, Anime).\n8) Writes everything into a final CSV for bulk upload.\n\nDependencies:\n    pip install openai python-dotenv pillow tqdm backoff\n\"\"\"\n",
    "last_modified": "2025-09-13T05:53:25.864120"
  },
  {
    "id": "1562",
    "name": "main 2 2.py",
    "path": "github_repo/scripts/01_core_ai_analysis/transcription/main 2 2.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 21,
    "size": 588,
    "docstring": "",
    "keywords": [],
    "functions": [
      "main"
    ],
    "classes": [],
    "imports": [
      "csv",
      "generate_speech"
    ],
    "preview": "import csv\n\nfrom generate_speech import generate_speech\n\n\ndef main():\n    csv_path = \"quiz329.csv\"\n\n    with open(csv_path, newline=\"\", encoding=\"utf-8\") as csvfile:\n        reader = csv.DictReader(csvfile)\n        for i, row in enumerate(reader):\n            # Assuming 'Question' is the column name\n            question_text = row[\"Question\"]\n            output_path = f\"/Users/steven/Documents/quiz-talk/quiz329/question/question_{\n                i + 1}.mp3\"\n            generate_speech(question_text, voice=\"shimmer\", output_path=output_path)\n\n\nif __name__ == \"__main__\":\n    main()",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1563",
    "name": "analyze-prompt1.py",
    "path": "github_repo/scripts/01_core_ai_analysis/transcription/analyze-prompt1.py",
    "category": "01_core_ai_analysis",
    "type": "analysis",
    "lines": 64,
    "size": 3523,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "analyze_text"
    ],
    "classes": [],
    "imports": [
      "os",
      "dotenv",
      "sys"
    ],
    "preview": "vfrom openai import OpenAI\n\nclient = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\nimport os\n\nfrom dotenv import load_dotenv\n\n# Load environment variables from .env\nenv_path = os.path.expanduser(\"~/.env\")\n # Update the path if needed\nload_dotenv(dotenv_path=env_path)\n\n# Initialize openai API key\n\n# Error checking for openai API key\nif not openai.api_key:\n    raise EnvironmentError(\"openai API key not found. Please check your .env file.\")\n\ndef analyze_text(text):\n    response = client.chat.completions.create(",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1564",
    "name": "analyze-prompt (1).py",
    "path": "github_repo/scripts/01_core_ai_analysis/transcription/analyze-prompt (1).py",
    "category": "01_core_ai_analysis",
    "type": "analysis",
    "lines": 99,
    "size": 4036,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis",
      "transcription"
    ],
    "functions": [
      "transcribe_audio",
      "format_timestamp",
      "analyze_text_for_section",
      "process_audio_directory"
    ],
    "classes": [],
    "imports": [
      "openai",
      "os",
      "subprocess",
      "dotenv"
    ],
    "preview": "from openai import OpenAI\n\nclient = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\nimport os\nimport subprocess\n\nfrom dotenv import load_dotenv\n\n# Load environment variables from .env (make sure your OPENAI_API_KEY is stored here)\nenv_path = \"/Users/steven/.env\"\nload_dotenv(dotenv_path=env_path)\n\n# Directory paths\nAUDIO_DIR = (\n    \"/Users/steven/Music/NocTurnE-meLoDieS/Discography/Audio\"  # Directory containing MP3 files\n)\nTRANSCRIPT_DIR = \"/Users/steven/Music/NocTurnE-meLoDieS/Discography/Audio/transcript\"  # Directory to save transcripts\nANALYSIS_DIR = \"/Users/steven/Music/NocTurnE-meLoDieS/Discography/Audio/analysis\"  # Directory to save the analysis files\n\n# Create output directories if they don't exist",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1565",
    "name": "image-gpt.py",
    "path": "github_repo/scripts/01_core_ai_analysis/content_analysis/image-gpt.py",
    "category": "01_core_ai_analysis",
    "type": "image_processing",
    "lines": 59,
    "size": 1831,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis"
    ],
    "functions": [
      "generate_description"
    ],
    "classes": [],
    "imports": [
      "csv",
      "openai"
    ],
    "preview": "import csv\n\nfrom openai import OpenAI\n\n# Initialize the OpenAI client\nclient = OpenAI()\n\n\ndef generate_description(image_url):\n    response = client.chat.completions.create(\n        model=\"gpt-4-vision-preview\",\n        messages=[\n            {\n                \"role\": \"user\",\n                \"content\": [\n                    {\n                        \"type\": \"text\",\n                        \"text\": \"Can you create a detailed and descriptive image prompt for the image as if you were to recreate it?\",\n                    },\n                    {",
    "last_modified": "2025-09-13T05:53:55.205434"
  },
  {
    "id": "1566",
    "name": "testing2.py",
    "path": "github_repo/scripts/01_core_ai_analysis/content_analysis/testing2.py",
    "category": "01_core_ai_analysis",
    "type": "testing",
    "lines": 119,
    "size": 5316,
    "docstring": "",
    "keywords": [
      "analysis",
      "openai",
      "youtube"
    ],
    "functions": [
      "upload_file",
      "wait_for_run_completion",
      "get_internal_links",
      "process_blog_post",
      "process_content_plan"
    ],
    "classes": [],
    "imports": [
      "csv",
      "os",
      "time",
      "openai",
      "tqdm"
    ],
    "preview": "import csv\nimport os\nimport time\n\nimport openai\nfrom tqdm import tqdm\n\n# Set your OpenAI API key\nOPENAI_API_TOKEN = \"put_your_secret_key_here\"\nos.environ[\"OPENAI_API_KEY\"] = OPENAI_API_TOKEN\n\n# Initialize the OpenAI client\nclient = openai.OpenAI()\n\n\n# Function to upload a file to OpenAI\ndef upload_file(file_path, purpose):\n    with open(file_path, \"rb\") as file:\n        response = client.files.create(file=file, purpose=purpose)\n    return response.id",
    "last_modified": "2025-09-13T05:54:17.514006"
  },
  {
    "id": "1567",
    "name": "pydescribe.py",
    "path": "github_repo/scripts/01_core_ai_analysis/content_analysis/pydescribe.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 96,
    "size": 3258,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis"
    ],
    "functions": [
      "extract_functions_and_classes",
      "save_to_text_files",
      "get_description_from_file",
      "analyze_scripts"
    ],
    "classes": [],
    "imports": [
      "openai",
      "os",
      "re"
    ],
    "preview": "from openai import OpenAI\n\nclient = OpenAI(api_key=api_key)\nimport os\nimport re\n\n# Load the API key from the environment variable\napi_key = os.getenv(\"OPENAI_API_KEY\")\nif not api_key:\n    raise ValueError(\"API key not found. Please ensure it is set in your environment variables.\")\n\n\ndef extract_functions_and_classes(content):\n    \"\"\"\n    Extracts top-level functions and classes from the script content.\n    \"\"\"\n    pattern = re.compile(r\"^\\s*(def|class)\\s+\\w+\\s*\\(.*?\\):\", re.MULTILINE)\n    matches = pattern.split(content)\n    headers = pattern.findall(content)\n    functions_and_classes = []",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1568",
    "name": "batch copy.py",
    "path": "github_repo/scripts/01_core_ai_analysis/content_analysis/batch copy.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 85,
    "size": 2693,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis"
    ],
    "functions": [
      "get_openai_batch_titles",
      "suggest_script_titles_batch",
      "process_directory_with_batching"
    ],
    "classes": [],
    "imports": [
      "os",
      "openai"
    ],
    "preview": "import os\n\nfrom openai import OpenAI\n\nclient = OpenAI()\n\n\ndef get_openai_batch_titles(script_contents):\n    # Prepare messages for the batch request\n    messages = [\n        {\n            \"role\": \"system\",\n            \"content\": \"You are an expert Python programmer. Suggest appropriate titles for the following scripts.\",\n        },\n        {\n            \"role\": \"user\",\n            \"content\": \"\\n\\n\".join(\n                f\"Script {i+1}:\\n{content[:1000]}\" for i, content in enumerate(script_contents)\n            ),\n        },",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1569",
    "name": "song--analyze-keys.py",
    "path": "github_repo/scripts/01_core_ai_analysis/content_analysis/song--analyze-keys.py",
    "category": "01_core_ai_analysis",
    "type": "analysis",
    "lines": 42,
    "size": 1100,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis"
    ],
    "functions": [
      "analyze_text"
    ],
    "classes": [],
    "imports": [
      "openai",
      "os",
      "dotenv",
      "sys"
    ],
    "preview": "from openai import OpenAI\n\nclient = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\nimport os\n\nfrom dotenv import load_dotenv\n\n# Load environment variables from .env\nload_dotenv()\n\n\ndef analyze_text(text):\n    response = client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",  # Ensure you're using a chat model like gpt-3.5-turbo\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": \"You are a helpful assistant that analyzes song lyrics.\",\n            },\n            {",
    "last_modified": "2025-05-04T22:47:13.351254"
  },
  {
    "id": "1570",
    "name": "batch_image_seo_pipeline_20250530221254.py",
    "path": "github_repo/scripts/01_core_ai_analysis/content_analysis/batch_image_seo_pipeline_20250530221254.py",
    "category": "01_core_ai_analysis",
    "type": "image_processing",
    "lines": 6,
    "size": 406,
    "docstring": "",
    "keywords": [
      "openai"
    ],
    "functions": [],
    "classes": [],
    "imports": [
      "openai.error"
    ],
    "preview": " python /Users/steven/Documents/python/batch/batch_image_seo_pipeline.py                                           \u2039git:main \u2718\u203a 22:11.04 Fri May 30 2025 >>> \nTraceback (most recent call last):\n  File \"/Users/steven/Documents/python/batch/batch_image_seo_pipeline.py\", line 35, in <module>\n    from openai.error import OpenAIError\nModuleNotFoundError: No module named 'openai.error'\n(media_tools) <<< ",
    "last_modified": "2025-05-30T22:12:54.349281"
  },
  {
    "id": "1571",
    "name": "whispertext-combine.py",
    "path": "github_repo/scripts/01_core_ai_analysis/content_analysis/whispertext-combine.py",
    "category": "01_core_ai_analysis",
    "type": "transcription",
    "lines": 72,
    "size": 1917,
    "docstring": "",
    "keywords": [
      "data_processing"
    ],
    "functions": [],
    "classes": [],
    "imports": [
      "json",
      "os",
      "glob",
      "pandas",
      "natsort"
    ],
    "preview": "import json\nimport os\nfrom glob import glob\n\nimport pandas as pd\nfrom natsort import natsorted\n\nROOT_DIR = \"/Users/steven/Documents/Whisper-Text/ALL\"\n\njson_files = glob(os.path.join(ROOT_DIR, \"**/*.json\"), recursive=True)\njson_files = natsorted(json_files)\n\nHEADERS = [\n    \"Title\",\n    \"Summary\",\n    \"Quotes\",\n    \"Chapters\",\n    \"Show Notes\",\n    \"Newsletter\",\n    \"Blog post\",",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1572",
    "name": "testing3.py",
    "path": "github_repo/scripts/01_core_ai_analysis/content_analysis/testing3.py",
    "category": "01_core_ai_analysis",
    "type": "testing",
    "lines": 117,
    "size": 6790,
    "docstring": "",
    "keywords": [
      "analysis",
      "openai",
      "youtube"
    ],
    "functions": [
      "upload_file",
      "wait_for_run_completion",
      "get_internal_links",
      "process_blog_post",
      "process_content_plan"
    ],
    "classes": [],
    "imports": [
      "csv",
      "os",
      "time",
      "openai",
      "tqdm"
    ],
    "preview": "import csv\nimport os\nimport time\n\nimport openai\nfrom tqdm import tqdm\n\n# Set your OpenAI API key\nOPENAI_API_TOKEN = \"JUST_ADD_API_KEY_HERE\"\nos.environ[\"OPENAI_API_KEY\"] = OPENAI_API_TOKEN\n\n# Initialize the OpenAI client\nclient = openai.OpenAI()\n\n\n# Function to upload a file to OpenAI\ndef upload_file(file_path, purpose):\n    with open(file_path, \"rb\") as file:\n        response = client.files.create(file=file, purpose=purpose)\n    return response.id",
    "last_modified": "2025-09-13T05:54:17.587676"
  },
  {
    "id": "1573",
    "name": "breakdown.py",
    "path": "github_repo/scripts/01_core_ai_analysis/content_analysis/breakdown.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 51,
    "size": 1669,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis"
    ],
    "functions": [
      "get_script_description",
      "analyze_scripts"
    ],
    "classes": [],
    "imports": [
      "openai",
      "os"
    ],
    "preview": "from openai import OpenAI\n\nclient = OpenAI(api_key=api_key)\nimport os\n\n# Load the API key from the environment variable\napi_key = os.getenv(\"OPENAI_API_KEY\")\nif not api_key:\n    raise ValueError(\"API key not found. Please ensure it is set in your environment variables.\")\n\n\ndef get_script_description(file_path):\n    with open(file_path, \"r\") as file:\n        script_content = file.read()\n\n    # OpenAI API call to get the script description\n    response = client.chat.completions.create(\n        model=\"gpt-4\",\n        messages=[\n            {\"role\": \"system\", \"content\": \"You are an expert Python programmer.\"},",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1574",
    "name": "pyorganize.py",
    "path": "github_repo/scripts/01_core_ai_analysis/content_analysis/pyorganize.py",
    "category": "01_core_ai_analysis",
    "type": "organization",
    "lines": 73,
    "size": 2358,
    "docstring": "",
    "keywords": [
      "openai"
    ],
    "functions": [
      "extract_functions_and_classes",
      "get_script_description",
      "categorize_scripts"
    ],
    "classes": [],
    "imports": [
      "openai",
      "os",
      "re"
    ],
    "preview": "from openai import OpenAI\n\nclient = OpenAI(api_key=api_key)\nimport os\nimport re\n\n# Load the API key from the environment variable\napi_key = os.getenv(\"OPENAI_API_KEY\")\nif not api_key:\n    raise ValueError(\"API key not found. Please ensure it is set in your environment variables.\")\n\n\ndef extract_functions_and_classes(content):\n    \"\"\"\n    Extracts top-level functions and classes from the script content.\n    \"\"\"\n    pattern = re.compile(r\"^\\s*(def|class)\\s+\\w+\\s*\\(.*?\\):\", re.MULTILINE)\n    matches = pattern.findall(content)\n    headers = pattern.findall(content)\n    functions_and_classes = []",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1575",
    "name": "fancy 1.py",
    "path": "github_repo/scripts/01_core_ai_analysis/content_analysis/fancy 1.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 113,
    "size": 3902,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis"
    ],
    "functions": [
      "generate_creative_filename",
      "pair_and_rename_images",
      "main"
    ],
    "classes": [],
    "imports": [
      "csv",
      "os",
      "datetime",
      "dotenv",
      "openai"
    ],
    "preview": "import csv\nimport os\nfrom datetime import datetime\n\nfrom dotenv import load_dotenv\nfrom openai import OpenAI\n\n# Load environment variables from the specified .env file\nenv_path = \"/Users/steven/.env\"\nload_dotenv(dotenv_path=env_path)\n\n# Initialize OpenAI API key\napi_key = os.getenv(\"OPENAI_API_KEY\")\n\n# Error checking for OpenAI API key\nif not api_key:\n    raise EnvironmentError(\"OpenAI API key not found. Please check your .env file.\")\n\n# Initialize OpenAI client\nclient = OpenAI(api_key=api_key)",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1576",
    "name": "fancyname.py",
    "path": "github_repo/scripts/01_core_ai_analysis/content_analysis/fancyname.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 101,
    "size": 3334,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis"
    ],
    "functions": [
      "generate_creative_filename",
      "process_images",
      "main"
    ],
    "classes": [],
    "imports": [
      "csv",
      "os",
      "datetime",
      "dotenv",
      "openai"
    ],
    "preview": "import csv\nimport os\nfrom datetime import datetime\n\nfrom dotenv import load_dotenv\nfrom openai import OpenAI\n\n# Load environment variables from the specified .env file\nenv_path = \"/Users/steven/.env\"\nload_dotenv(dotenv_path=env_path)\n\n# Initialize OpenAI API key\napi_key = os.getenv(\"OPENAI_API_KEY\")\n\n# Error checking for OpenAI API key\nif not api_key:\n    raise EnvironmentError(\"OpenAI API key not found. Please check your .env file.\")\n\n# Initialize OpenAI client\nclient = OpenAI(api_key=api_key)",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1577",
    "name": "emport_prompts.py",
    "path": "github_repo/scripts/01_core_ai_analysis/content_analysis/emport_prompts.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 108,
    "size": 4158,
    "docstring": "",
    "keywords": [
      "openai"
    ],
    "functions": [
      "call_gpt4o_analysis_and_prompts",
      "batch_flexible_analysis"
    ],
    "classes": [],
    "imports": [
      "csv",
      "json",
      "logging",
      "os",
      "pathlib",
      "dotenv",
      "openai",
      "tqdm"
    ],
    "preview": "# image_flexible_analysis.py\nimport csv\nimport json\nimport logging\nimport os\nfrom pathlib import Path\n\nfrom dotenv import load_dotenv\nfrom openai import OpenAI\nfrom tqdm import tqdm\n\nload_dotenv(os.path.expanduser(\"~/.env\"))\nOPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\nif not OPENAI_API_KEY:\n    raise EnvironmentError(\"OPENAI_API_KEY not found in ~/.env\")\nclient = OpenAI(api_key=OPENAI_API_KEY)\n\nSYSTEM_PROMPT = (\n    \"You are an expert AI creative director, SEO specialist, and product designer for print-on-demand. \"\n    \"For each image, analyze and output a JSON object with: main_subject, style, color_palette, tags (list), \"",
    "last_modified": "2025-09-13T05:53:48.121869"
  },
  {
    "id": "1578",
    "name": "batch-info.py",
    "path": "github_repo/scripts/01_core_ai_analysis/content_analysis/batch-info.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 147,
    "size": 4910,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis"
    ],
    "functions": [
      "get_openai_batch_titles",
      "suggest_script_titles_batch",
      "process_directory_with_batching"
    ],
    "classes": [],
    "imports": [
      "os",
      "openai",
      "dotenv"
    ],
    "preview": "import os\n\nfrom openai import OpenAI\n\nclient = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\nfrom dotenv import load_dotenv\n\n# Load environment variables from the specified .env file\nload_dotenv(\"/Users/steven/Documents/python/.env\")\n\n# Set OpenAI API key from the environment variable\n\n\ndef get_openai_batch_titles(script_contents):\n    \"\"\"\n    Prepare a batch request to OpenAI for suggesting appropriate titles for scripts.\n\n    Args:\n        script_contents (list): List of script contents as strings.\n",
    "last_modified": "2025-09-13T05:53:55.716688"
  },
  {
    "id": "1579",
    "name": "vision.py",
    "path": "github_repo/scripts/01_core_ai_analysis/content_analysis/vision.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 60,
    "size": 1845,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis"
    ],
    "functions": [
      "generate_description"
    ],
    "classes": [],
    "imports": [
      "csv",
      "openai"
    ],
    "preview": "import csv\n\nfrom openai import OpenAI\n\n# Initialize the OpenAI client\nclient = OpenAI()\n\n\ndef generate_description(image_url):\n    response = client.chat.completions.create(\n        model=\"gpt-4-vision-preview\",\n        messages=[\n            {\n                \"role\": \"user\",\n                \"content\": [\n                    {\n                        \"type\": \"text\",\n                        \"text\": \"Can you create a detailed and descriptive image prompt for the image as if you were to recreate it?\",\n                    },\n                    {",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1580",
    "name": "rename.py",
    "path": "github_repo/scripts/01_core_ai_analysis/content_analysis/rename.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 104,
    "size": 3887,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis"
    ],
    "functions": [
      "generate_filename_from_image",
      "rename_and_copy_files"
    ],
    "classes": [],
    "imports": [
      "csv",
      "os",
      "shutil",
      "openai",
      "dotenv"
    ],
    "preview": "import csv\nimport os\nimport shutil\n\nimport openai\nfrom dotenv import load_dotenv\n\n# Load API Key from .env\nload_dotenv()\nopenai.api_key = os.getenv(\"OPENAI_API_KEY\")\n\n\n# Function to generate a filename from image content\ndef generate_filename_from_image(filepath):\n    \"\"\"\n    Uses OpenAI to describe the image content and generate a filename.\n    \"\"\"\n    try:\n        # Describe the image content (simplified example using keywords)\n        response = openai.ChatCompletion.create(",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1581",
    "name": "better.py",
    "path": "github_repo/scripts/01_core_ai_analysis/content_analysis/better.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 17,
    "size": 553,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [],
    "preview": "\nmodel=\"gpt-3.5-turbo\",\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": (\n                    \"You are a witty and imaginative assistant. Generate playful, creative, and descriptive filenames \"\n                    \"for digital products based on provided prompts.\"\n                ),\n            },\n            {\n                \"role\": \"user\",\n                \"content\": f\"Create a unique filename for this design prompt: '{prompt}'\",\n            },\n        ],\n        max_tokens=20,\n        temperature=0.7",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1582",
    "name": "finaltry.py",
    "path": "github_repo/scripts/01_core_ai_analysis/content_analysis/finaltry.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 81,
    "size": 2761,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis",
      "web_tools"
    ],
    "functions": [
      "generate_image_prompt",
      "generate_youtube_content",
      "create_image_with_dalle",
      "process_csv_and_generate_content"
    ],
    "classes": [],
    "imports": [
      "csv",
      "requests",
      "openai"
    ],
    "preview": "import csv\n\nimport requests\nfrom openai import OpenAI\n\n# Initialize the OpenAI client\n\n# Function to generate an image prompt based on the input data\n\n\ndef generate_image_prompt(question, description, keywords):\n    prompt = f\"Create a detailed and descriptive image based on the following: {description}. Related to the question: '{question}'. Keywords: {keywords}\"\n    return prompt\n\n\n# Function to generate YouTube titles and descriptions with emojis\n\n\ndef generate_youtube_content(prompt):\n    response = openai.Completion.create(",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1583",
    "name": "openai_local.py",
    "path": "github_repo/scripts/01_core_ai_analysis/content_analysis/openai_local.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 27,
    "size": 4524,
    "docstring": "",
    "keywords": [
      "openai"
    ],
    "functions": [],
    "classes": [],
    "imports": [
      "openai"
    ],
    "preview": "from openai import OpenAI\n\nclient = OpenAI()\n\nresponse = client.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=[\n        {\n            \"role\": \"system\",\n            \"content\": '\\n1. Image Generation Guidelines:\\n   - Utilize DALL-E 3 for image creation.\\n   - Ensure each prompt is creatively detailed.\\n   - Strictly adhere to the Prompt Guidelines.\\n\\n2. Image Batching Protocol:\\n   - Generate images in separate batches: one image per sentence, matching the total number of sentences.\\n\\n3. Conceptualization for New Ideas:\\n   - After image generation, suggest four new, simple, yet related concepts based on the original theme.\\n\\nDefault Settings (Unless Specified Otherwise):\\n1. Aspect Ratio: Adopt a 9:16 ratio (1080x1920px, Vertical).\\n2. Image Quantity: Always produce four separate images.\\n3. Background Setting: Isolate graphics on a solid color background.\\n4. Post-Creation Requirements:\\n   a) Title: Create titles (50-60 characters) suitable for SEO, enhanced for YouTube Shorts, reflecting content, theme, and story.\\n   b) Keywords: List a minimum of 20 relevant SEO keywords, emphasizing the strongest. Exclude artistic style, camera type, and setup.\\n   c) Description: Write a compelling description (max 256 characters, no longer than one paragraph), incorporating relevant keywords and narrative style.\\n\\nNote: To initiate the next set of image generations, respond with \"DO\" for creating images based on the four newly suggested ideas.\\n',\n        },\n        {\n            \"role\": \"user\",\n            \"content\": 'Solarflare (Real Name: Ethan Clarke)\\nNormal Life: Ethan was an astrophysicist, fascinated by the mysteries of the universe.\\nTransformation: Gains solar energy powers and becomes a hero, later succumbing to the lure of power and turning into a villain.\\nAqua-Mistress (Real Name: Mia Waters)\\nNormal Life: Mia was a dedicated marine biologist, advocating for ocean conservation.\\nTransformation: Discovers mystical powers over water, initially uses them for good, but later exploits them for personal gain.\\nGaia (Real Name: Ivy Green)\\nNormal Life: Ivy, an environmental activist, fought tirelessly to protect natural habitats.\\nTransformation: Gains earth-related powers, starts as a guardian of nature but becomes more extreme in her methods.\\nWind Warden (Real Name: Leo Storm)\\nNormal Life: Leo was a charismatic and adventurous pilot.\\nTransformation: After gaining air control powers, he starts as a hero but gets carried away by the thrill of his abilities.\\nQuantum (Real Name: Dr. Max Quantum)\\nNormal Life: A brilliant physicist researching quantum mechanics.\\nTransformation: His powers of teleportation and time manipulation lead him down a path from hero to rogue scientist.\\nSpectra (Real Name: Natasha \"Tasha\" Blake)\\nNormal Life: Tasha was a skilled intelligence agent known for her stealth.\\nTransformation: Gains invisibility and force fields, initially fights crime, but later uses her powers for espionage and personal agendas.\\nThunderstrike (Real Name: Chris Bolt)\\nNormal Life: A popular professional athlete known for his electrifying personality.\\nTransformation: Chris becomes a superhuman with electrical powers, but his desire for fame corrupts his heroism.\\n',\n        },\n        {\n            \"role\": \"assistant\",\n            \"content\": 'Image Generation: In Progress\\n\\n1. Solarflare (Ethan Clarke) as an astrophysicist staring into the heart of a solar storm.\\n   - Title: \"Astrophysicist Glimpses Solar Storm Secret\"\\n   - Keywords: solarflare, astrophysicist, solar storm, universe, Ethan Clarke, mysteries, powerful gaze, cosmic event, hero-to-villain, energy powers.\\n   - Description: Astrophysicist Ethan Clarke, soon-to-be Solarflare, peers into the dazzling abyss of a solar storm, moments before a fateful transformation.\\n\\n2. Aqua-Mistress (Mia Waters) at a coral reef conducting research.\\n   - Title: \"Marine Biologist\\'s Encounter at Vibrant Coral Reef\"\\n   - Keywords: Aqua-Mistress, marine biologist, coral reef, ocean conservation, Mia Waters, mystical powers, water hero, underwater research, marine life.\\n   - Description: Mia Waters, destined to become Aqua-Mistress, passionately studies the teeming life within a vibrant coral reef, echoing her future connection to the seas.\\n\\n3. Gaia (Ivy Green) planting trees in a deforested area.\\n   - Title: \"Activist Restores Forests Before Becoming',\n        },\n    ],",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1584",
    "name": "filenamer 1.py",
    "path": "github_repo/scripts/01_core_ai_analysis/content_analysis/filenamer 1.py",
    "category": "01_core_ai_analysis",
    "type": "organization",
    "lines": 100,
    "size": 3150,
    "docstring": "",
    "keywords": [
      "data_processing",
      "openai",
      "analysis"
    ],
    "functions": [
      "analyze_image",
      "process_images"
    ],
    "classes": [],
    "imports": [
      "json",
      "logging",
      "os",
      "pathlib",
      "pandas",
      "dotenv",
      "openai",
      "tenacity"
    ],
    "preview": "import json\nimport logging\nimport os\nfrom pathlib import Path\n\nimport pandas as pd\nfrom dotenv import load_dotenv\nfrom openai import OpenAI\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\n# Configure logging\nlogging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n\n# Load environment variables\nenv_path = Path(\".\") / \".env\"\nload_dotenv(dotenv_path=env_path)\n\n# Initialize OpenAI client\ntry:\n    client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1585",
    "name": "riddle.py",
    "path": "github_repo/scripts/01_core_ai_analysis/content_analysis/riddle.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 96,
    "size": 2754,
    "docstring": "",
    "keywords": [
      "openai"
    ],
    "functions": [
      "fetch_riddles",
      "play_dynamic_game_with_choices",
      "output_to_csv"
    ],
    "classes": [],
    "imports": [
      "openai",
      "csv",
      "os"
    ],
    "preview": "from openai import OpenAI\n\nclient = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\nimport csv\nimport os\n\n# Set up OpenAI API key\n\n\ndef fetch_riddles(number_of_riddles=3):\n    prompt_text = \"Generate a sphinx riddle with a question, an answer, a correct response message, and an incorrect response message.\"\n    messages = [\n        {\n            \"role\": \"system\",\n            \"content\": \"You are about to generate a series of sphinx riddles.\",\n        },\n        {\"role\": \"user\", \"content\": prompt_text * number_of_riddles},\n    ]\n\n    try:",
    "last_modified": "2025-05-04T22:47:11.911164"
  },
  {
    "id": "1586",
    "name": "classify.py",
    "path": "github_repo/scripts/01_core_ai_analysis/content_analysis/classify.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 169,
    "size": 5837,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis"
    ],
    "functions": [
      "generate_category_with_openai",
      "classify_script",
      "scan_directory",
      "save_classification_to_csv"
    ],
    "classes": [],
    "imports": [
      "ast",
      "os",
      "openai",
      "csv",
      "datetime",
      "dotenv"
    ],
    "preview": "import ast\nimport os\n\nfrom openai import OpenAI\n\nclient = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\nimport csv\nfrom datetime import datetime\n\nfrom dotenv import load_dotenv\n\n# Load environment variables from .env\nenv_path = \"/Users/steven/.env\"\nload_dotenv(dotenv_path=env_path)\n\n# Initialize OpenAI API key\n\n# Error checking for OpenAI API key\nif not openai.api_key:\n    raise EnvironmentError(\"OpenAI API key not found. Please check your .env file.\")",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1587",
    "name": "emport_prompts_20250530225121.py",
    "path": "github_repo/scripts/01_core_ai_analysis/content_analysis/emport_prompts_20250530225121.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 108,
    "size": 4158,
    "docstring": "",
    "keywords": [
      "openai"
    ],
    "functions": [
      "call_gpt4o_analysis_and_prompts",
      "batch_flexible_analysis"
    ],
    "classes": [],
    "imports": [
      "csv",
      "json",
      "logging",
      "os",
      "pathlib",
      "dotenv",
      "openai",
      "tqdm"
    ],
    "preview": "# image_flexible_analysis.py\nimport csv\nimport json\nimport logging\nimport os\nfrom pathlib import Path\n\nfrom dotenv import load_dotenv\nfrom openai import OpenAI\nfrom tqdm import tqdm\n\nload_dotenv(os.path.expanduser(\"~/.env\"))\nOPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\nif not OPENAI_API_KEY:\n    raise EnvironmentError(\"OPENAI_API_KEY not found in ~/.env\")\nclient = OpenAI(api_key=OPENAI_API_KEY)\n\nSYSTEM_PROMPT = (\n    \"You are an expert AI creative director, SEO specialist, and product designer for print-on-demand. \"\n    \"For each image, analyze and output a JSON object with: main_subject, style, color_palette, tags (list), \"",
    "last_modified": "2025-09-13T05:53:27.123820"
  },
  {
    "id": "1588",
    "name": "batch.py",
    "path": "github_repo/scripts/01_core_ai_analysis/content_analysis/batch.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 146,
    "size": 4763,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis"
    ],
    "functions": [
      "get_openai_batch_titles",
      "suggest_script_titles_batch",
      "process_directory_with_batching"
    ],
    "classes": [],
    "imports": [
      "os",
      "openai",
      "dotenv"
    ],
    "preview": "import os\n\nfrom openai import OpenAI\n\nclient = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\nfrom dotenv import load_dotenv\n\n# Load environment variables from .env file\nload_dotenv()\n\n# Set OpenAI API key from the environment variable\n\n\ndef get_openai_batch_titles(script_contents):\n    \"\"\"\n    Prepare a batch request to OpenAI for suggesting appropriate titles for scripts.\n\n    Args:\n        script_contents (list): List of script contents as strings.\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1589",
    "name": "pydescriber.py",
    "path": "github_repo/scripts/01_core_ai_analysis/content_analysis/pydescriber.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 96,
    "size": 3258,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis"
    ],
    "functions": [
      "extract_functions_and_classes",
      "save_to_text_files",
      "get_description_from_file",
      "analyze_scripts"
    ],
    "classes": [],
    "imports": [
      "openai",
      "os",
      "re"
    ],
    "preview": "from openai import OpenAI\n\nclient = OpenAI(api_key=api_key)\nimport os\nimport re\n\n# Load the API key from the environment variable\napi_key = os.getenv(\"OPENAI_API_KEY\")\nif not api_key:\n    raise ValueError(\"API key not found. Please ensure it is set in your environment variables.\")\n\n\ndef extract_functions_and_classes(content):\n    \"\"\"\n    Extracts top-level functions and classes from the script content.\n    \"\"\"\n    pattern = re.compile(r\"^\\s*(def|class)\\s+\\w+\\s*\\(.*?\\):\", re.MULTILINE)\n    matches = pattern.split(content)\n    headers = pattern.findall(content)\n    functions_and_classes = []",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1590",
    "name": "__init__.py",
    "path": "github_repo/scripts/01_core_ai_analysis/content_analysis/__init__.py",
    "category": "01_core_ai_analysis",
    "type": "setup",
    "lines": 7,
    "size": 162,
    "docstring": "Shared utilities for Python projects",
    "keywords": [
      "openai"
    ],
    "functions": [],
    "classes": [],
    "imports": [
      "config",
      "openai_client",
      "file_utils"
    ],
    "preview": "\"\"\"\nShared utilities for Python projects\n\"\"\"\nfrom .config import *\nfrom .openai_client import get_openai_client\nfrom .file_utils import ensure_dir, get_file_size\n",
    "last_modified": "2025-10-09T05:27:15.569506"
  },
  {
    "id": "1591",
    "name": "scanpythons.py",
    "path": "github_repo/scripts/01_core_ai_analysis/content_analysis/scanpythons.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 66,
    "size": 2149,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis"
    ],
    "functions": [
      "extract_functions_and_classes",
      "get_script_description",
      "analyze_scripts"
    ],
    "classes": [],
    "imports": [
      "openai",
      "os",
      "re"
    ],
    "preview": "from openai import OpenAI\n\nclient = OpenAI(api_key=api_key)\nimport os\nimport re\n\n# Load the API key from the environment variable\napi_key = os.getenv(\"OPENAI_API_KEY\")\nif not api_key:\n    raise ValueError(\"API key not found. Please ensure it is set in your environment variables.\")\n\n\ndef extract_functions_and_classes(content):\n    \"\"\"\n    Extracts top-level functions and classes from the script content.\n    \"\"\"\n    pattern = re.compile(r\"^\\s*(def|class)\\s+\\w+\\s*\\(.*?\\):\", re.MULTILINE)\n    matches = pattern.findall(content)\n    return matches\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1592",
    "name": "enhance_img_csv.py",
    "path": "github_repo/scripts/01_core_ai_analysis/content_analysis/enhance_img_csv.py",
    "category": "01_core_ai_analysis",
    "type": "image_processing",
    "lines": 297,
    "size": 10725,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis"
    ],
    "functions": [
      "analyze_image_gpt4o",
      "get_image_tech_meta",
      "build_source_tag_from_csv_row",
      "main"
    ],
    "classes": [],
    "imports": [
      "csv",
      "json",
      "os",
      "datetime",
      "pathlib",
      "typing",
      "openai",
      "dotenv",
      "PIL",
      "tqdm"
    ],
    "preview": "#!/usr/bin/env python3\nimport csv\nimport json\nimport os\nfrom datetime import datetime\nfrom pathlib import Path\nfrom typing import Any, Dict, List, Optional\n\nimport openai\nfrom dotenv import load_dotenv\nfrom PIL import Image, UnidentifiedImageError\nfrom tqdm import tqdm\n\n# \u2500\u2500\u2500 CONFIGURATION \u2500\u2500\u2500\n# 1) Path to your existing CSV file (prompted at runtime)\n# 2) Output CSV file name\n# 3) Base directory for building \u201csource\u201d tags (optional; default = parent folder of CSV)\nload_dotenv(Path.home() / \".env\")\nOPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1593",
    "name": "analyze_img_csv 1.py",
    "path": "github_repo/scripts/01_core_ai_analysis/content_analysis/analyze_img_csv 1.py",
    "category": "01_core_ai_analysis",
    "type": "analysis",
    "lines": 246,
    "size": 8267,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis"
    ],
    "functions": [
      "analyze_image_gpt4o",
      "get_image_tech_meta",
      "discover_images",
      "build_source_tag",
      "main"
    ],
    "classes": [],
    "imports": [
      "csv",
      "json",
      "os",
      "datetime",
      "pathlib",
      "typing",
      "openai",
      "dotenv",
      "PIL",
      "tqdm"
    ],
    "preview": "import csv\nimport json\nimport os\nfrom datetime import datetime\nfrom pathlib import Path\nfrom typing import Any, Dict, List, Optional\n\nimport openai\nfrom dotenv import load_dotenv\nfrom PIL import Image, UnidentifiedImageError\nfrom tqdm import tqdm\n\n# --- CONFIGURATION ---\n# Load environment variables from ~/.env\nload_dotenv(Path.home() / \".env\")\nOPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n\nif not OPENAI_API_KEY:\n    raise ValueError(\"No OpenAI API key found! Please set it in ~/.env as OPENAI_API_KEY=...\")\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1594",
    "name": "self_query.py",
    "path": "github_repo/scripts/01_core_ai_analysis/content_analysis/self_query.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 51,
    "size": 1554,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis"
    ],
    "functions": [
      "generate"
    ],
    "classes": [
      "SelfQuery"
    ],
    "imports": [
      "opik",
      "langchain_openai",
      "llm_engineering.application",
      "llm_engineering.domain.documents",
      "llm_engineering.domain.queries",
      "llm_engineering.settings",
      "loguru",
      "base",
      "prompt_templates"
    ],
    "preview": "import opik\nfrom langchain_openai import ChatOpenAI\nfrom llm_engineering.application import utils\nfrom llm_engineering.domain.documents import UserDocument\nfrom llm_engineering.domain.queries import Query\nfrom llm_engineering.settings import settings\nfrom loguru import logger\n\nfrom .base import RAGStep\nfrom .prompt_templates import SelfQueryTemplate\n\n\nclass SelfQuery(RAGStep):\n    @opik.track(name=\"SelfQuery.generate\")\n    def generate(self, query: Query) -> Query:\n        if self._mock:\n            return query\n\n        prompt = SelfQueryTemplate().create_template()\n        model = ChatOpenAI(",
    "last_modified": "2025-05-06T04:35:14.983667"
  },
  {
    "id": "1595",
    "name": "autofill.py",
    "path": "github_repo/scripts/01_core_ai_analysis/content_analysis/autofill.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 96,
    "size": 3448,
    "docstring": "",
    "keywords": [
      "openai"
    ],
    "functions": [
      "gpt_fill",
      "fill_missing_fields"
    ],
    "classes": [],
    "imports": [
      "csv",
      "json",
      "os",
      "pathlib",
      "dotenv",
      "openai",
      "tqdm"
    ],
    "preview": "import csv\nimport json\nimport os\nfrom pathlib import Path\n\nfrom dotenv import load_dotenv\nfrom openai import OpenAI\nfrom tqdm import tqdm\n\n# CONFIG\nload_dotenv(os.path.expanduser(\"~/.env\"))\nOPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\nclient = OpenAI(api_key=OPENAI_API_KEY)\nCSV_PATH = \"/Users/steven/Documents/python/clean/CSV/prompts_expanded_image_data-05-30-22-21.csv\"\nOUT_CSV = (\n    \"/Users/steven/Documents/python/clean/CSV/filled_prompts_expanded_image_data-05-30-22-21.csv\"\n)\nLOG_PATH = \"/Users/steven/Documents/python/clean/CSV/fill_log.txt\"\n\n# List analytic fields and prompt fields to fill",
    "last_modified": "2025-09-13T05:53:47.685388"
  },
  {
    "id": "1596",
    "name": "autofill_20250530225752.py",
    "path": "github_repo/scripts/01_core_ai_analysis/content_analysis/autofill_20250530225752.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 96,
    "size": 3448,
    "docstring": "",
    "keywords": [
      "openai"
    ],
    "functions": [
      "gpt_fill",
      "fill_missing_fields"
    ],
    "classes": [],
    "imports": [
      "csv",
      "json",
      "os",
      "pathlib",
      "dotenv",
      "openai",
      "tqdm"
    ],
    "preview": "import csv\nimport json\nimport os\nfrom pathlib import Path\n\nfrom dotenv import load_dotenv\nfrom openai import OpenAI\nfrom tqdm import tqdm\n\n# CONFIG\nload_dotenv(os.path.expanduser(\"~/.env\"))\nOPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\nclient = OpenAI(api_key=OPENAI_API_KEY)\nCSV_PATH = \"/Users/steven/Documents/python/clean/CSV/prompts_expanded_image_data-05-30-22-21.csv\"\nOUT_CSV = (\n    \"/Users/steven/Documents/python/clean/CSV/filled_prompts_expanded_image_data-05-30-22-21.csv\"\n)\nLOG_PATH = \"/Users/steven/Documents/python/clean/CSV/fill_log.txt\"\n\n# List analytic fields and prompt fields to fill",
    "last_modified": "2025-09-13T05:53:47.415963"
  },
  {
    "id": "1597",
    "name": "filenamer.py",
    "path": "github_repo/scripts/01_core_ai_analysis/content_analysis/filenamer.py",
    "category": "01_core_ai_analysis",
    "type": "organization",
    "lines": 100,
    "size": 3150,
    "docstring": "",
    "keywords": [
      "data_processing",
      "openai",
      "analysis"
    ],
    "functions": [
      "analyze_image",
      "process_images"
    ],
    "classes": [],
    "imports": [
      "json",
      "logging",
      "os",
      "pathlib",
      "pandas",
      "dotenv",
      "openai",
      "tenacity"
    ],
    "preview": "import json\nimport logging\nimport os\nfrom pathlib import Path\n\nimport pandas as pd\nfrom dotenv import load_dotenv\nfrom openai import OpenAI\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\n# Configure logging\nlogging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n\n# Load environment variables\nenv_path = Path(\".\") / \".env\"\nload_dotenv(dotenv_path=env_path)\n\n# Initialize OpenAI client\ntry:\n    client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1598",
    "name": "fancy.py",
    "path": "github_repo/scripts/01_core_ai_analysis/content_analysis/fancy.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 113,
    "size": 3902,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis"
    ],
    "functions": [
      "generate_creative_filename",
      "pair_and_rename_images",
      "main"
    ],
    "classes": [],
    "imports": [
      "csv",
      "os",
      "datetime",
      "dotenv",
      "openai"
    ],
    "preview": "import csv\nimport os\nfrom datetime import datetime\n\nfrom dotenv import load_dotenv\nfrom openai import OpenAI\n\n# Load environment variables from the specified .env file\nenv_path = \"/Users/steven/.env\"\nload_dotenv(dotenv_path=env_path)\n\n# Initialize OpenAI API key\napi_key = os.getenv(\"OPENAI_API_KEY\")\n\n# Error checking for OpenAI API key\nif not api_key:\n    raise EnvironmentError(\"OpenAI API key not found. Please check your .env file.\")\n\n# Initialize OpenAI client\nclient = OpenAI(api_key=api_key)",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1599",
    "name": "fancyimg copy.py",
    "path": "github_repo/scripts/01_core_ai_analysis/content_analysis/fancyimg copy.py",
    "category": "01_core_ai_analysis",
    "type": "image_processing",
    "lines": 84,
    "size": 2952,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis"
    ],
    "functions": [
      "generate_filename_with_gpt",
      "pair_and_rename_images"
    ],
    "classes": [],
    "imports": [
      "os",
      "openai",
      "csv",
      "dotenv"
    ],
    "preview": "import os\n\nfrom openai import OpenAI\n\nclient = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\nimport csv\n\nfrom dotenv import load_dotenv\n\n# Load API key from .env file\nload_dotenv(\"/Users/steven/.env\")\n\n\n# Function to generate a filename using GPT based on the prompt\ndef generate_filename_with_gpt(prompt):\n    response = client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        messages=[\n            {\n                \"role\": \"system\",",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1600",
    "name": "whisper.py",
    "path": "github_repo/scripts/01_core_ai_analysis/content_analysis/whisper.py",
    "category": "01_core_ai_analysis",
    "type": "transcription",
    "lines": 59,
    "size": 1425,
    "docstring": "",
    "keywords": [
      "data_processing"
    ],
    "functions": [],
    "classes": [],
    "imports": [
      "json",
      "os",
      "glob",
      "pandas"
    ],
    "preview": "import json\nimport os\nfrom glob import glob\n\nimport pandas as pd\n\n# Define the directory path where JSON files are stored\nDIR_PATH = \"/Users/steven/Documents/Whisper-Text/content34\"\n\n# Get all JSON files in the directory\njson_files = sorted(\n    glob(os.path.join(DIR_PATH, \"content*.json\")),\n    key=lambda x: (int(\"\".join(filter(str.isdigit, x))) if any(c.isdigit() for c in x) else 0),\n)\n\ndata_list = []\n\n# Define the updated headers we want in the CSV\nHEADERS = [\n    \"Title\",",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1601",
    "name": "ident.py",
    "path": "github_repo/scripts/01_core_ai_analysis/content_analysis/ident.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 172,
    "size": 5935,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis"
    ],
    "functions": [
      "get_openai_batch_titles",
      "suggest_script_titles_batch",
      "process_directory_with_batching"
    ],
    "classes": [],
    "imports": [
      "os",
      "openai",
      "csv",
      "dotenv"
    ],
    "preview": "import os\n\nfrom openai import OpenAI\n\nclient = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\nimport csv\n\nfrom dotenv import load_dotenv\n\n# Load environment variables from the specified .env file\nload_dotenv(\"/Users/steven/Documents/python/.env\")\n\n# Set OpenAI API key from the environment variable\n\n\ndef get_openai_batch_titles(script_contents):\n    \"\"\"\n    Prepare a batch request to OpenAI for suggesting appropriate titles for scripts.\n\n    Args:",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1602",
    "name": "groq.py",
    "path": "github_repo/scripts/01_core_ai_analysis/content_analysis/groq.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 42,
    "size": 942,
    "docstring": "",
    "keywords": [
      "openai"
    ],
    "functions": [],
    "classes": [],
    "imports": [
      "os",
      "dotenv",
      "openai"
    ],
    "preview": "import os\n\nfrom dotenv import load_dotenv\nfrom openai import OpenAI\n\n# Load environment variables from .env file\nload_dotenv(dotenv_path=os.path.expanduser(\"~/.env\"))\n\nXAI_API_KEY = os.getenv(\"XAI_API_KEY\")\nimage_url = \"https://science.nasa.gov/wp-content/uploads/2023/09/web-first-images-release.png\"\n\nclient = OpenAI(\n    api_key=XAI_API_KEY,\n    base_url=\"https://api.x.ai/v1\",\n)\n\nmessages = [\n    {\n        \"role\": \"user\",\n        \"content\": [",
    "last_modified": "2025-09-13T05:53:54.172126"
  },
  {
    "id": "1603",
    "name": "sagemaker.py",
    "path": "github_repo/scripts/01_core_ai_analysis/content_analysis/sagemaker.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 64,
    "size": 2011,
    "docstring": "",
    "keywords": [],
    "functions": [
      "run_evaluation_on_sagemaker"
    ],
    "classes": [],
    "imports": [
      "pathlib",
      "huggingface_hub",
      "loguru",
      "llm_engineering",
      "sagemaker.huggingface"
    ],
    "preview": "from pathlib import Path\n\nfrom huggingface_hub import HfApi\nfrom loguru import logger\n\ntry:\n    from sagemaker.huggingface import HuggingFaceProcessor\nexcept ModuleNotFoundError:\n    logger.warning(\n        \"Couldn't load SageMaker imports. Run 'poetry install --with aws' to support AWS.\"\n    )\n\nfrom llm_engineering import settings\n\nevaluation_dir = Path(__file__).resolve().parent\nevaluation_requirements_path = evaluation_dir / \"requirements.txt\"\n\n\ndef run_evaluation_on_sagemaker(is_dummy: bool = True) -> None:\n    assert settings.HUGGINGFACE_ACCESS_TOKEN, \"Hugging Face access token is required.\"",
    "last_modified": "2025-09-13T05:53:42.270688"
  },
  {
    "id": "1604",
    "name": "narrative_gen.py",
    "path": "github_repo/scripts/01_core_ai_analysis/content_analysis/narrative_gen.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 182,
    "size": 6374,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis"
    ],
    "functions": [
      "generate_theme_title",
      "construct_image_prompt",
      "generate_batch",
      "generate_continuation_concepts",
      "save_output"
    ],
    "classes": [],
    "imports": [
      "json",
      "logging",
      "os",
      "random",
      "pathlib",
      "typing",
      "dotenv",
      "openai"
    ],
    "preview": "import json\nimport logging\nimport os\nimport random\nfrom pathlib import Path\nfrom typing import Dict, List\n\nfrom dotenv import load_dotenv\nfrom openai import APIError, OpenAI\n\n# Configure logging\nlogging.basicConfig(\n    level=logging.INFO,\n    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n    handlers=[logging.FileHandler(\"narrative_gen.log\"), logging.StreamHandler()],\n)\n\n# Load environment variables\nenv_path = Path(\".\") / \".env\"\nload_dotenv(dotenv_path=env_path)",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1605",
    "name": "my_openai_utils.py",
    "path": "github_repo/scripts/01_core_ai_analysis/content_analysis/my_openai_utils.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 27,
    "size": 4524,
    "docstring": "",
    "keywords": [
      "openai"
    ],
    "functions": [],
    "classes": [],
    "imports": [
      "openai"
    ],
    "preview": "from openai import OpenAI\n\nclient = OpenAI()\n\nresponse = client.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=[\n        {\n            \"role\": \"system\",\n            \"content\": '\\n1. Image Generation Guidelines:\\n   - Utilize DALL-E 3 for image creation.\\n   - Ensure each prompt is creatively detailed.\\n   - Strictly adhere to the Prompt Guidelines.\\n\\n2. Image Batching Protocol:\\n   - Generate images in separate batches: one image per sentence, matching the total number of sentences.\\n\\n3. Conceptualization for New Ideas:\\n   - After image generation, suggest four new, simple, yet related concepts based on the original theme.\\n\\nDefault Settings (Unless Specified Otherwise):\\n1. Aspect Ratio: Adopt a 9:16 ratio (1080x1920px, Vertical).\\n2. Image Quantity: Always produce four separate images.\\n3. Background Setting: Isolate graphics on a solid color background.\\n4. Post-Creation Requirements:\\n   a) Title: Create titles (50-60 characters) suitable for SEO, enhanced for YouTube Shorts, reflecting content, theme, and story.\\n   b) Keywords: List a minimum of 20 relevant SEO keywords, emphasizing the strongest. Exclude artistic style, camera type, and setup.\\n   c) Description: Write a compelling description (max 256 characters, no longer than one paragraph), incorporating relevant keywords and narrative style.\\n\\nNote: To initiate the next set of image generations, respond with \"DO\" for creating images based on the four newly suggested ideas.\\n',\n        },\n        {\n            \"role\": \"user\",\n            \"content\": 'Solarflare (Real Name: Ethan Clarke)\\nNormal Life: Ethan was an astrophysicist, fascinated by the mysteries of the universe.\\nTransformation: Gains solar energy powers and becomes a hero, later succumbing to the lure of power and turning into a villain.\\nAqua-Mistress (Real Name: Mia Waters)\\nNormal Life: Mia was a dedicated marine biologist, advocating for ocean conservation.\\nTransformation: Discovers mystical powers over water, initially uses them for good, but later exploits them for personal gain.\\nGaia (Real Name: Ivy Green)\\nNormal Life: Ivy, an environmental activist, fought tirelessly to protect natural habitats.\\nTransformation: Gains earth-related powers, starts as a guardian of nature but becomes more extreme in her methods.\\nWind Warden (Real Name: Leo Storm)\\nNormal Life: Leo was a charismatic and adventurous pilot.\\nTransformation: After gaining air control powers, he starts as a hero but gets carried away by the thrill of his abilities.\\nQuantum (Real Name: Dr. Max Quantum)\\nNormal Life: A brilliant physicist researching quantum mechanics.\\nTransformation: His powers of teleportation and time manipulation lead him down a path from hero to rogue scientist.\\nSpectra (Real Name: Natasha \"Tasha\" Blake)\\nNormal Life: Tasha was a skilled intelligence agent known for her stealth.\\nTransformation: Gains invisibility and force fields, initially fights crime, but later uses her powers for espionage and personal agendas.\\nThunderstrike (Real Name: Chris Bolt)\\nNormal Life: A popular professional athlete known for his electrifying personality.\\nTransformation: Chris becomes a superhuman with electrical powers, but his desire for fame corrupts his heroism.\\n',\n        },\n        {\n            \"role\": \"assistant\",\n            \"content\": 'Image Generation: In Progress\\n\\n1. Solarflare (Ethan Clarke) as an astrophysicist staring into the heart of a solar storm.\\n   - Title: \"Astrophysicist Glimpses Solar Storm Secret\"\\n   - Keywords: solarflare, astrophysicist, solar storm, universe, Ethan Clarke, mysteries, powerful gaze, cosmic event, hero-to-villain, energy powers.\\n   - Description: Astrophysicist Ethan Clarke, soon-to-be Solarflare, peers into the dazzling abyss of a solar storm, moments before a fateful transformation.\\n\\n2. Aqua-Mistress (Mia Waters) at a coral reef conducting research.\\n   - Title: \"Marine Biologist\\'s Encounter at Vibrant Coral Reef\"\\n   - Keywords: Aqua-Mistress, marine biologist, coral reef, ocean conservation, Mia Waters, mystical powers, water hero, underwater research, marine life.\\n   - Description: Mia Waters, destined to become Aqua-Mistress, passionately studies the teeming life within a vibrant coral reef, echoing her future connection to the seas.\\n\\n3. Gaia (Ivy Green) planting trees in a deforested area.\\n   - Title: \"Activist Restores Forests Before Becoming',\n        },\n    ],",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1606",
    "name": "fancyname 1.py",
    "path": "github_repo/scripts/01_core_ai_analysis/content_analysis/fancyname 1.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 101,
    "size": 3334,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis"
    ],
    "functions": [
      "generate_creative_filename",
      "process_images",
      "main"
    ],
    "classes": [],
    "imports": [
      "csv",
      "os",
      "datetime",
      "dotenv",
      "openai"
    ],
    "preview": "import csv\nimport os\nfrom datetime import datetime\n\nfrom dotenv import load_dotenv\nfrom openai import OpenAI\n\n# Load environment variables from the specified .env file\nenv_path = \"/Users/steven/.env\"\nload_dotenv(dotenv_path=env_path)\n\n# Initialize OpenAI API key\napi_key = os.getenv(\"OPENAI_API_KEY\")\n\n# Error checking for OpenAI API key\nif not api_key:\n    raise EnvironmentError(\"OpenAI API key not found. Please check your .env file.\")\n\n# Initialize OpenAI client\nclient = OpenAI(api_key=api_key)",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1607",
    "name": "whisper2.py",
    "path": "github_repo/scripts/01_core_ai_analysis/content_analysis/whisper2.py",
    "category": "01_core_ai_analysis",
    "type": "transcription",
    "lines": 64,
    "size": 1635,
    "docstring": "",
    "keywords": [
      "data_processing"
    ],
    "functions": [],
    "classes": [],
    "imports": [
      "json",
      "os",
      "glob",
      "pandas"
    ],
    "preview": "import json\nimport os\nfrom glob import glob\n\nimport pandas as pd\n\n# Define the root directory path where JSON files are stored\nROOT_DIR = \"/Users/steven/Library/Application Support/WhisperTranscribe/library\"\n\n# Recursively find all JSON files in subdirectories\njson_files = sorted(\n    glob(os.path.join(ROOT_DIR, \"**/*.json\"), recursive=True),\n    key=lambda x: (int(\"\".join(filter(str.isdigit, x))) if any(c.isdigit() for c in x) else 0),\n)\n\ndata_list = []\n\n# Define the updated headers we want in the CSV\nHEADERS = [\n    \"Title\",",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1608",
    "name": "pyorganizerr.py",
    "path": "github_repo/scripts/01_core_ai_analysis/content_analysis/pyorganizerr.py",
    "category": "01_core_ai_analysis",
    "type": "organization",
    "lines": 73,
    "size": 2358,
    "docstring": "",
    "keywords": [
      "openai"
    ],
    "functions": [
      "extract_functions_and_classes",
      "get_script_description",
      "categorize_scripts"
    ],
    "classes": [],
    "imports": [
      "openai",
      "os",
      "re"
    ],
    "preview": "from openai import OpenAI\n\nclient = OpenAI(api_key=api_key)\nimport os\nimport re\n\n# Load the API key from the environment variable\napi_key = os.getenv(\"OPENAI_API_KEY\")\nif not api_key:\n    raise ValueError(\"API key not found. Please ensure it is set in your environment variables.\")\n\n\ndef extract_functions_and_classes(content):\n    \"\"\"\n    Extracts top-level functions and classes from the script content.\n    \"\"\"\n    pattern = re.compile(r\"^\\s*(def|class)\\s+\\w+\\s*\\(.*?\\):\", re.MULTILINE)\n    matches = pattern.findall(content)\n    headers = pattern.findall(content)\n    functions_and_classes = []",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1609",
    "name": "textgenerator.py",
    "path": "github_repo/scripts/01_core_ai_analysis/content_analysis/textgenerator.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 36,
    "size": 982,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis",
      "organization"
    ],
    "functions": [
      "clean_response",
      "generate_text_list"
    ],
    "classes": [],
    "imports": [
      "os",
      "dotenv",
      "openai",
      "config"
    ],
    "preview": "import os\n\nfrom dotenv import load_dotenv\nfrom openai import OpenAI\n\nfrom config import API_PARAM, PROMPT_TEMPLATE\n\nclient = OpenAI(api_key=os.getenv(\"OPENAI_TOKEN\"))\n\nload_dotenv()  # Load environment variables from .env.\n\n# Define the API key\n\n\ndef clean_response(text: str) -> list[str]:\n    text = text.strip().strip(\"[]\").split(\"\\n\")\n    return [t.strip().strip(\",\").strip('\"') for t in text]\n\n\ndef generate_text_list(date: str) -> str:",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1610",
    "name": "combiner.py",
    "path": "github_repo/scripts/01_core_ai_analysis/content_analysis/combiner.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 72,
    "size": 1914,
    "docstring": "",
    "keywords": [
      "data_processing"
    ],
    "functions": [],
    "classes": [],
    "imports": [
      "json",
      "os",
      "glob",
      "pandas",
      "natsort"
    ],
    "preview": "import json\nimport os\nfrom glob import glob\n\nimport pandas as pd\nfrom natsort import natsorted\n\nROOT_DIR = \"/Users/steven/Documents/Whisper-Text/\"\n\njson_files = glob(os.path.join(ROOT_DIR, \"**/*.json\"), recursive=True)\njson_files = natsorted(json_files)\n\nHEADERS = [\n    \"Title\",\n    \"Summary\",\n    \"Quotes\",\n    \"Chapters\",\n    \"Show Notes\",\n    \"Newsletter\",\n    \"Blog post\",",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1611",
    "name": "overlay_theme_engines (3).py",
    "path": "github_repo/scripts/01_core_ai_analysis/content_analysis/overlay_theme_engines (3).py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 67,
    "size": 2394,
    "docstring": "",
    "keywords": [],
    "functions": [
      "get_overlay_filter",
      "_graffiti_filter",
      "_sticker_filter",
      "_doodle_filter",
      "_ascii_filter",
      "_scanlines_filter"
    ],
    "classes": [
      "OverlayEngine",
      "ThemeEngine"
    ],
    "imports": [
      "__future__",
      "random",
      "typing"
    ],
    "preview": "\"\"\"\nOverlay and Theme engines for enhanced visual effects\n\"\"\"\nfrom __future__ import annotations\n\nimport random\nfrom typing import Any, Dict, List\n\n\nclass OverlayEngine:\n    \"\"\"Generates overlay effects for clips\"\"\"\n    \n    def get_overlay_filter(self, overlay_type: str) -> str:\n        \"\"\"Generate FFmpeg filter for overlay effects\"\"\"\n        overlay_type = overlay_type.lower()\n        \n        if overlay_type == \"graffiti\":\n            # Random graffiti-style elements\n            return self._graffiti_filter()\n        elif overlay_type == \"stickers\":",
    "last_modified": "2025-09-11T13:26:59.346287"
  },
  {
    "id": "1612",
    "name": "verify_connections2.py",
    "path": "github_repo/scripts/01_core_ai_analysis/content_analysis/verify_connections2.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 52,
    "size": 1645,
    "docstring": "",
    "keywords": [],
    "functions": [
      "verify_ssh_connection",
      "list_repo_contents"
    ],
    "classes": [],
    "imports": [
      "os",
      "paramiko",
      "dotenv"
    ],
    "preview": "import os\n\nimport paramiko\nfrom dotenv import load_dotenv\n\n# Load environment variables from .env (ensure your OPENAI_API_KEY is stored here)\nenv_path = \"/Users/steven/.env\"\nload_dotenv(dotenv_path=env_path)\n\nhostname = os.getenv(\"HOSTNAME\")\nusername = os.getenv(\"USERNAME\")\npassword = os.getenv(\"PASSWORD\")\nremote_dir = \"/repo/zip\"  # Adjust this path if needed\n\n\ndef verify_ssh_connection():\n    \"\"\"Run a simple command (like 'who') to verify an SSH connection.\"\"\"\n    client = paramiko.SSHClient()\n    client.set_missing_host_key_policy(paramiko.AutoAddPolicy())\n    try:",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1613",
    "name": "whisper-combiner.py",
    "path": "github_repo/scripts/01_core_ai_analysis/content_analysis/whisper-combiner.py",
    "category": "01_core_ai_analysis",
    "type": "transcription",
    "lines": 72,
    "size": 1944,
    "docstring": "",
    "keywords": [
      "data_processing"
    ],
    "functions": [],
    "classes": [],
    "imports": [
      "json",
      "os",
      "glob",
      "pandas",
      "natsort"
    ],
    "preview": "import json\nimport os\nfrom glob import glob\n\nimport pandas as pd\nfrom natsort import natsorted\n\nROOT_DIR = \"/Users/steven/Library/Application Support/WhisperTranscribe/library\"\n\njson_files = glob(os.path.join(ROOT_DIR, \"**/*.json\"), recursive=True)\njson_files = natsorted(json_files)\n\nHEADERS = [\n    \"Title\",\n    \"Summary\",\n    \"Quotes\",\n    \"Chapters\",\n    \"Show Notes\",\n    \"Newsletter\",\n    \"Blog post\",",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1614",
    "name": "fancyimg 1.py",
    "path": "github_repo/scripts/01_core_ai_analysis/content_analysis/fancyimg 1.py",
    "category": "01_core_ai_analysis",
    "type": "image_processing",
    "lines": 84,
    "size": 3036,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis"
    ],
    "functions": [
      "generate_filename_with_gpt",
      "pair_and_rename_images"
    ],
    "classes": [],
    "imports": [
      "os",
      "openai",
      "csv",
      "dotenv"
    ],
    "preview": "import os\n\nfrom openai import OpenAI\n\nclient = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\nimport csv\n\nfrom dotenv import load_dotenv\n\n# Load API key from .env file\nload_dotenv(\"/Users/steven/.env\")\n\n\n# Function to generate a filename using GPT based on the prompt\ndef generate_filename_with_gpt(prompt):\n    response = client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        messages=[\n            {\n                \"role\": \"system\",",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1615",
    "name": "fancyimg.py",
    "path": "github_repo/scripts/01_core_ai_analysis/content_analysis/fancyimg.py",
    "category": "01_core_ai_analysis",
    "type": "image_processing",
    "lines": 84,
    "size": 2936,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis"
    ],
    "functions": [
      "generate_filename_with_gpt",
      "pair_and_rename_images"
    ],
    "classes": [],
    "imports": [
      "os",
      "openai",
      "csv",
      "dotenv"
    ],
    "preview": "import os\n\nfrom openai import OpenAI\n\nclient = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\nimport csv\n\nfrom dotenv import load_dotenv\n\n# Load API key from .env file\nload_dotenv(\"/Users/steven/.env\")\n\n\n# Function to generate a filename using GPT based on the prompt\ndef generate_filename_with_gpt(prompt):\n    response = client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        messages=[\n            {\n                \"role\": \"system\",",
    "last_modified": "2025-05-04T22:47:13.345543"
  },
  {
    "id": "1616",
    "name": "openai_client.py",
    "path": "github_repo/scripts/01_core_ai_analysis/content_analysis/openai_client.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 12,
    "size": 237,
    "docstring": "Centralized OpenAI client for all projects",
    "keywords": [
      "openai"
    ],
    "functions": [
      "get_openai_client"
    ],
    "classes": [],
    "imports": [
      "openai",
      "config"
    ],
    "preview": "\"\"\"\nCentralized OpenAI client for all projects\n\"\"\"\nfrom openai import OpenAI\nfrom .config import OPENAI_API_KEY\n\nclient = OpenAI(api_key=OPENAI_API_KEY)\n\ndef get_openai_client():\n    \"\"\"Get configured OpenAI client.\"\"\"\n    return client\n",
    "last_modified": "2025-10-09T05:27:15.568968"
  },
  {
    "id": "1617",
    "name": "fancyimg copy 1.py",
    "path": "github_repo/scripts/01_core_ai_analysis/content_analysis/fancyimg copy 1.py",
    "category": "01_core_ai_analysis",
    "type": "image_processing",
    "lines": 84,
    "size": 2952,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis"
    ],
    "functions": [
      "generate_filename_with_gpt",
      "pair_and_rename_images"
    ],
    "classes": [],
    "imports": [
      "os",
      "openai",
      "csv",
      "dotenv"
    ],
    "preview": "import os\n\nfrom openai import OpenAI\n\nclient = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\nimport csv\n\nfrom dotenv import load_dotenv\n\n# Load API key from .env file\nload_dotenv(\"/Users/steven/.env\")\n\n\n# Function to generate a filename using GPT based on the prompt\ndef generate_filename_with_gpt(prompt):\n    response = client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        messages=[\n            {\n                \"role\": \"system\",",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1618",
    "name": "batch_img_seo_pipeline.py",
    "path": "github_repo/scripts/01_core_ai_analysis/content_analysis/batch_img_seo_pipeline.py",
    "category": "01_core_ai_analysis",
    "type": "image_processing",
    "lines": 258,
    "size": 8436,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis"
    ],
    "functions": [
      "analyze_image_gpt4o",
      "get_image_tech_meta",
      "discover_images",
      "build_source_tag",
      "parse_args",
      "process_image",
      "main"
    ],
    "classes": [],
    "imports": [
      "argparse",
      "csv",
      "json",
      "logging",
      "os",
      "concurrent.futures",
      "datetime",
      "pathlib",
      "typing",
      "backoff"
    ],
    "preview": "import argparse\nimport csv\nimport json\nimport logging\nimport os\nfrom concurrent.futures import ThreadPoolExecutor, as_completed\nfrom datetime import datetime\nfrom pathlib import Path\nfrom typing import Any, Dict, List, Optional\n\nimport backoff\nimport openai\nfrom dotenv import load_dotenv\nfrom PIL import Image, UnidentifiedImageError\nfrom tqdm import tqdm\n\n# --- CONFIGURATION ---\nload_dotenv(Path.home() / \".env\")\nOPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1619",
    "name": "query_expanison.py",
    "path": "github_repo/scripts/01_core_ai_analysis/content_analysis/query_expanison.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 50,
    "size": 1619,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis"
    ],
    "functions": [
      "generate"
    ],
    "classes": [
      "QueryExpansion"
    ],
    "imports": [
      "opik",
      "langchain_openai",
      "llm_engineering.domain.queries",
      "llm_engineering.settings",
      "loguru",
      "base",
      "prompt_templates"
    ],
    "preview": "import opik\nfrom langchain_openai import ChatOpenAI\nfrom llm_engineering.domain.queries import Query\nfrom llm_engineering.settings import settings\nfrom loguru import logger\n\nfrom .base import RAGStep\nfrom .prompt_templates import QueryExpansionTemplate\n\n\nclass QueryExpansion(RAGStep):\n    @opik.track(name=\"QueryExpansion.generate\")\n    def generate(self, query: Query, expand_to_n: int) -> list[Query]:\n        assert expand_to_n > 0, f\"'expand_to_n' should be greater than 0. Got {expand_to_n}.\"\n\n        if self._mock:\n            return [query for _ in range(expand_to_n)]\n\n        query_expansion_template = QueryExpansionTemplate()\n        prompt = query_expansion_template.create_template(expand_to_n - 1)",
    "last_modified": "2025-09-13T05:53:41.743127"
  },
  {
    "id": "1620",
    "name": "batch.py.bak.py",
    "path": "github_repo/scripts/01_core_ai_analysis/content_analysis/batch.py.bak.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 146,
    "size": 4763,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis"
    ],
    "functions": [
      "get_openai_batch_titles",
      "suggest_script_titles_batch",
      "process_directory_with_batching"
    ],
    "classes": [],
    "imports": [
      "os",
      "openai",
      "dotenv"
    ],
    "preview": "import os\n\nfrom openai import OpenAI\n\nclient = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\nfrom dotenv import load_dotenv\n\n# Load environment variables from .env file\nload_dotenv()\n\n# Set OpenAI API key from the environment variable\n\n\ndef get_openai_batch_titles(script_contents):\n    \"\"\"\n    Prepare a batch request to OpenAI for suggesting appropriate titles for scripts.\n\n    Args:\n        script_contents (list): List of script contents as strings.\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1621",
    "name": "content-new.py",
    "path": "github_repo/scripts/01_core_ai_analysis/content_analysis/content-new.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 83,
    "size": 2377,
    "docstring": "",
    "keywords": [
      "data_processing"
    ],
    "functions": [],
    "classes": [],
    "imports": [
      "json",
      "os",
      "datetime",
      "glob",
      "pandas"
    ],
    "preview": "import json\nimport os\nfrom datetime import datetime\nfrom glob import glob\n\nimport pandas as pd\n\n# Define the root directory path where JSON files are stored\nROOT_DIR = \"/Users/steven/Library/Application Support/WhisperTranscribe/library\"\n\n# Recursively find all JSON files in subdirectories\njson_files = sorted(\n    glob(os.path.join(ROOT_DIR, \"**/*.json\"), recursive=True),\n    key=lambda x: (int(\"\".join(filter(str.isdigit, x))) if any(c.isdigit() for c in x) else 0),\n)\n\ndata_list = []\n\n# Define the headers we want in the CSV\nHEADERS = [",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1622",
    "name": "analyze_img_csv.py",
    "path": "github_repo/scripts/01_core_ai_analysis/content_analysis/analyze_img_csv.py",
    "category": "01_core_ai_analysis",
    "type": "analysis",
    "lines": 246,
    "size": 8267,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis"
    ],
    "functions": [
      "analyze_image_gpt4o",
      "get_image_tech_meta",
      "discover_images",
      "build_source_tag",
      "main"
    ],
    "classes": [],
    "imports": [
      "csv",
      "json",
      "os",
      "datetime",
      "pathlib",
      "typing",
      "openai",
      "dotenv",
      "PIL",
      "tqdm"
    ],
    "preview": "import csv\nimport json\nimport os\nfrom datetime import datetime\nfrom pathlib import Path\nfrom typing import Any, Dict, List, Optional\n\nimport openai\nfrom dotenv import load_dotenv\nfrom PIL import Image, UnidentifiedImageError\nfrom tqdm import tqdm\n\n# --- CONFIGURATION ---\n# Load environment variables from ~/.env\nload_dotenv(Path.home() / \".env\")\nOPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n\nif not OPENAI_API_KEY:\n    raise ValueError(\"No OpenAI API key found! Please set it in ~/.env as OPENAI_API_KEY=...\")\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1623",
    "name": "openai-Utilize DALL-E 3 for image creation.py",
    "path": "github_repo/scripts/01_core_ai_analysis/content_analysis/openai-Utilize DALL-E 3 for image creation.py",
    "category": "01_core_ai_analysis",
    "type": "image_processing",
    "lines": 27,
    "size": 4524,
    "docstring": "",
    "keywords": [
      "openai"
    ],
    "functions": [],
    "classes": [],
    "imports": [
      "openai"
    ],
    "preview": "from openai import OpenAI\n\nclient = OpenAI()\n\nresponse = client.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=[\n        {\n            \"role\": \"system\",\n            \"content\": '\\n1. Image Generation Guidelines:\\n   - Utilize DALL-E 3 for image creation.\\n   - Ensure each prompt is creatively detailed.\\n   - Strictly adhere to the Prompt Guidelines.\\n\\n2. Image Batching Protocol:\\n   - Generate images in separate batches: one image per sentence, matching the total number of sentences.\\n\\n3. Conceptualization for New Ideas:\\n   - After image generation, suggest four new, simple, yet related concepts based on the original theme.\\n\\nDefault Settings (Unless Specified Otherwise):\\n1. Aspect Ratio: Adopt a 9:16 ratio (1080x1920px, Vertical).\\n2. Image Quantity: Always produce four separate images.\\n3. Background Setting: Isolate graphics on a solid color background.\\n4. Post-Creation Requirements:\\n   a) Title: Create titles (50-60 characters) suitable for SEO, enhanced for YouTube Shorts, reflecting content, theme, and story.\\n   b) Keywords: List a minimum of 20 relevant SEO keywords, emphasizing the strongest. Exclude artistic style, camera type, and setup.\\n   c) Description: Write a compelling description (max 256 characters, no longer than one paragraph), incorporating relevant keywords and narrative style.\\n\\nNote: To initiate the next set of image generations, respond with \"DO\" for creating images based on the four newly suggested ideas.\\n',\n        },\n        {\n            \"role\": \"user\",\n            \"content\": 'Solarflare (Real Name: Ethan Clarke)\\nNormal Life: Ethan was an astrophysicist, fascinated by the mysteries of the universe.\\nTransformation: Gains solar energy powers and becomes a hero, later succumbing to the lure of power and turning into a villain.\\nAqua-Mistress (Real Name: Mia Waters)\\nNormal Life: Mia was a dedicated marine biologist, advocating for ocean conservation.\\nTransformation: Discovers mystical powers over water, initially uses them for good, but later exploits them for personal gain.\\nGaia (Real Name: Ivy Green)\\nNormal Life: Ivy, an environmental activist, fought tirelessly to protect natural habitats.\\nTransformation: Gains earth-related powers, starts as a guardian of nature but becomes more extreme in her methods.\\nWind Warden (Real Name: Leo Storm)\\nNormal Life: Leo was a charismatic and adventurous pilot.\\nTransformation: After gaining air control powers, he starts as a hero but gets carried away by the thrill of his abilities.\\nQuantum (Real Name: Dr. Max Quantum)\\nNormal Life: A brilliant physicist researching quantum mechanics.\\nTransformation: His powers of teleportation and time manipulation lead him down a path from hero to rogue scientist.\\nSpectra (Real Name: Natasha \"Tasha\" Blake)\\nNormal Life: Tasha was a skilled intelligence agent known for her stealth.\\nTransformation: Gains invisibility and force fields, initially fights crime, but later uses her powers for espionage and personal agendas.\\nThunderstrike (Real Name: Chris Bolt)\\nNormal Life: A popular professional athlete known for his electrifying personality.\\nTransformation: Chris becomes a superhuman with electrical powers, but his desire for fame corrupts his heroism.\\n',\n        },\n        {\n            \"role\": \"assistant\",\n            \"content\": 'Image Generation: In Progress\\n\\n1. Solarflare (Ethan Clarke) as an astrophysicist staring into the heart of a solar storm.\\n   - Title: \"Astrophysicist Glimpses Solar Storm Secret\"\\n   - Keywords: solarflare, astrophysicist, solar storm, universe, Ethan Clarke, mysteries, powerful gaze, cosmic event, hero-to-villain, energy powers.\\n   - Description: Astrophysicist Ethan Clarke, soon-to-be Solarflare, peers into the dazzling abyss of a solar storm, moments before a fateful transformation.\\n\\n2. Aqua-Mistress (Mia Waters) at a coral reef conducting research.\\n   - Title: \"Marine Biologist\\'s Encounter at Vibrant Coral Reef\"\\n   - Keywords: Aqua-Mistress, marine biologist, coral reef, ocean conservation, Mia Waters, mystical powers, water hero, underwater research, marine life.\\n   - Description: Mia Waters, destined to become Aqua-Mistress, passionately studies the teeming life within a vibrant coral reef, echoing her future connection to the seas.\\n\\n3. Gaia (Ivy Green) planting trees in a deforested area.\\n   - Title: \"Activist Restores Forests Before Becoming',\n        },\n    ],",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1624",
    "name": "category.py",
    "path": "github_repo/scripts/01_core_ai_analysis/content_analysis/category.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 137,
    "size": 4604,
    "docstring": "",
    "keywords": [
      "openai"
    ],
    "functions": [
      "determine_category",
      "split_content",
      "get_openai_category",
      "sanitize_category",
      "categorize_files"
    ],
    "classes": [],
    "imports": [
      "openai",
      "os",
      "re",
      "shutil"
    ],
    "preview": "from openai import OpenAI\n\nclient = OpenAI(api_key=api_key)\nimport os\nimport re\nimport shutil\n\n# Load the API key from the environment variable\napi_key = os.getenv(\"OPENAI_API_KEY\")\nif not api_key:\n    raise ValueError(\"API key not found. Please ensure it is set in your environment variables.\")\n\n\n# Define initial categories and their corresponding keywords\nCATEGORIES = {\n    \"data_processing\": [\"pandas\", \"numpy\", \"dataframe\"],\n    \"web_scraping\": [\"requests\", \"BeautifulSoup\", \"scrapy\"],\n    \"machine_learning\": [\"sklearn\", \"tensorflow\", \"keras\"],\n    \"utilities\": [\"os\", \"sys\", \"argparse\"],\n    \"networking\": [\"socket\", \"http.server\", \"flask\"],",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1625",
    "name": "rephrase.py",
    "path": "github_repo/scripts/01_core_ai_analysis/content_analysis/rephrase.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 49,
    "size": 1437,
    "docstring": "",
    "keywords": [
      "openai"
    ],
    "functions": [
      "__init__",
      "rephrase_with_gpt",
      "rephrase_sentence"
    ],
    "classes": [
      "Rephrase"
    ],
    "imports": [
      "openai",
      "logging",
      "utilities.const"
    ],
    "preview": "from openai import OpenAI\n\nclient = OpenAI(api_key=YOUR_OPENAI_API_KEY)\nimport logging\n\nfrom utilities.const import GPT_MODEL, YOUR_OPENAI_API_KEY\n\n# Configure OpenAI API\n\n# Configure logger\nlogger = logging.getLogger(__name__)\nlogger.setLevel(logging.INFO)\nformatter = logging.Formatter(\"%(asctime)s - %(levelname)s - %(message)s\")\nfile_handler = logging.FileHandler(\"rephraser.log\")\nfile_handler.setLevel(logging.INFO)\nfile_handler.setFormatter(formatter)\nlogger.addHandler(file_handler)\n\n\nclass Rephrase:",
    "last_modified": "2025-09-13T05:53:28.762167"
  },
  {
    "id": "1626",
    "name": "scanpython.py",
    "path": "github_repo/scripts/01_core_ai_analysis/content_analysis/scanpython.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 52,
    "size": 1681,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis"
    ],
    "functions": [
      "get_script_description",
      "analyze_scripts"
    ],
    "classes": [],
    "imports": [
      "openai",
      "os"
    ],
    "preview": "from openai import OpenAI\n\nclient = OpenAI(api_key=api_key)\nimport os\n\n# Load the API key from the environment variable\napi_key = os.getenv(\"OPENAI_API_KEY\")\nif not api_key:\n    raise ValueError(\"API key not found. Please ensure it is set in your environment variables.\")\n\n\ndef get_script_description(file_path):\n    with open(file_path, \"r\") as file:\n        script_content = file.read()\n\n    # OpenAI API call to get the script description\n    response = client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        messages=[\n            {\"role\": \"system\", \"content\": \"You are an expert Python programmer.\"},",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1627",
    "name": "verify_connections.py",
    "path": "github_repo/scripts/01_core_ai_analysis/content_analysis/verify_connections.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 52,
    "size": 1645,
    "docstring": "",
    "keywords": [],
    "functions": [
      "verify_ssh_connection",
      "list_repo_contents"
    ],
    "classes": [],
    "imports": [
      "os",
      "paramiko",
      "dotenv"
    ],
    "preview": "import os\n\nimport paramiko\nfrom dotenv import load_dotenv\n\n# Load environment variables from .env (ensure your OPENAI_API_KEY is stored here)\nenv_path = \"/Users/steven/.env\"\nload_dotenv(dotenv_path=env_path)\n\nhostname = os.getenv(\"HOSTNAME\")\nusername = os.getenv(\"USERNAME\")\npassword = os.getenv(\"PASSWORD\")\nremote_dir = \"/repo/zip\"  # Adjust this path if needed\n\n\ndef verify_ssh_connection():\n    \"\"\"Run a simple command (like 'who') to verify an SSH connection.\"\"\"\n    client = paramiko.SSHClient()\n    client.set_missing_host_key_policy(paramiko.AutoAddPolicy())\n    try:",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1628",
    "name": "generate_songs_csv.py_02.py",
    "path": "github_repo/scripts/01_core_ai_analysis/generate_songs_csv.py_consolidated/generate_songs_csv.py_02.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 64,
    "size": 2088,
    "docstring": "",
    "keywords": [],
    "functions": [
      "get_matching_files"
    ],
    "classes": [],
    "imports": [
      "csv",
      "os"
    ],
    "preview": "import csv\nimport os\n\n# Define the directories\nmp3_dir = \"/Users/steven/Music/suno/mp3\"\ntxt_dir = \"/Users/steven/Music/suno/txt\"\ncsv_output = \"/Users/steven/Music/suno/music_project/songs_data.csv\"\n\n# Collect the list of MP3 and text files\nmp3_files = [f for f in os.listdir(mp3_dir) if f.endswith(\".mp3\")]\ntxt_files = [f for f in os.listdir(txt_dir) if f.endswith(\".txt\")]\n\n\n# Function to match song with corresponding text files\ndef get_matching_files(song_title, txt_files):\n    analysis_file = None\n    transcript_file = None\n    base_title = song_title.replace(\".mp3\", \"\")\n    for txt in txt_files:\n        if base_title in txt:",
    "last_modified": "2025-09-13T05:53:55.953591"
  },
  {
    "id": "1629",
    "name": "generate_songs_csv.py.py",
    "path": "github_repo/scripts/01_core_ai_analysis/generate_songs_csv.py_consolidated/generate_songs_csv.py.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 45,
    "size": 1665,
    "docstring": "",
    "keywords": [
      "data_processing"
    ],
    "functions": [],
    "classes": [],
    "imports": [
      "pandas",
      "bs4",
      "ace_tools"
    ],
    "preview": "import pandas as pd\nfrom bs4 import BeautifulSoup\n\n# Prompt the user for the directory\ndirectory = input(\"Enter the directory containing the files: \").strip()\n\n# Load the newly uploaded HTML file for processing\nfile_path_new_html = input(\"Enter the HTML Url: \").strip()\nwith open(file_path_new_html, \"r\", encoding=\"utf-8\") as file:\n    html_content_new_html = file.read()\n\n# Parsing the HTML content with BeautifulSoup\nsoup = BeautifulSoup(html_content_new_html, \"html.parser\")\n\n# Extracting relevant details\nsong_details_new_html = []\nfor item in soup.find_all(\"div\", class_=\"css-79jxux\"):  # Adjust the class if necessary\n    time_element = item.find_previous(\"span\", class_=\"font-mono\")\n    title_element = item.find(\"span\", class_=\"text-primary\")\n    song_url_element = item.find(\"a\", href=True)",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1630",
    "name": "autofixer.py",
    "path": "github_repo/scripts/01_core_ai_analysis/data_processing/autofixer.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 109,
    "size": 3630,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "create_backup",
      "apply_autopep8",
      "run_pylint",
      "process_directory"
    ],
    "classes": [],
    "imports": [
      "csv",
      "os",
      "shutil",
      "subprocess",
      "datetime"
    ],
    "preview": "import csv\nimport os\nimport shutil\nimport subprocess\nfrom datetime import datetime\n\n\ndef create_backup(file_path, backup_dir):\n    \"\"\"\n    Create a backup of the given file in the specified backup directory.\n\n    :param file_path: Path to the original file.\n    :param backup_dir: Path to the backup directory.\n    :return: None\n    \"\"\"\n    os.makedirs(backup_dir, exist_ok=True)\n    file_name = os.path.basename(file_path)\n    backup_path = os.path.join(backup_dir, file_name)\n    shutil.copy(file_path, backup_path)\n    print(f\"Backup created for {file_path} at {backup_path}\")",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1631",
    "name": "try.py",
    "path": "github_repo/scripts/01_core_ai_analysis/data_processing/try.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 79,
    "size": 3493,
    "docstring": "",
    "keywords": [
      "data_processing",
      "web_tools"
    ],
    "functions": [],
    "classes": [],
    "imports": [
      "base64",
      "pandas",
      "requests"
    ],
    "preview": "import base64\n\nimport pandas as pd\nimport requests\n\n# Set your API credentials\naccess_token = \"eyJ0eXAiOiJKV1QiLCJhbGciOiJSUzI1NiJ9.eyJhdWQiOiIzN2Q0YmQzMDM1ZmUxMWU5YTgwM2FiN2VlYjNjY2M5NyIsImp0aSI6IjQyMmQ1MWFlODI0YjEyMGY2MmY5N2YwZWExOWQ1YzRjMjhlZGM5OTRjYWQ0MmJmNzViOTZlNTdkZWQ3Mzk3YWUyZmQxZDAxMjJhZWRkMTU2IiwiaWF0IjoxNzAyMzYwNjg5LjA0MDcyMywibmJmIjoxNzAyMzYwNjg5LjA0MDcyNiwiZXhwIjoxNzMzOTgzMDg5LjAzNDUzNiwic3ViIjoiMTEzMjQxNTMiLCJzY29wZXMiOlsic2hvcHMubWFuYWdlIiwic2hvcHMucmVhZCIsImNhdGFsb2cucmVhZCIsIm9yZGVycy5yZWFkIiwib3JkZXJzLndyaXRlIiwicHJvZHVjdHMucmVhZCIsInByb2R1Y3RzLndyaXRlIiwid2ViaG9va3MucmVhZCIsIndlYmhvb2tzLndyaXRlIiwidXBsb2Fkcy5yZWFkIiwidXBsb2Fkcy53cml0ZSIsInByaW50X3Byb3ZpZGVycy5yZWFkIl19.AkWVerYdnoGceJShqCIhFpw6o0m7Nz0mqE6moOvuAdy9F4YS-G4rVePuxPp6u9C-y9VxF2pGDYF3yg6kQEo\"  # Replace with your actual access token\nshop_id = \"6511744\"  # Replace with your actual shop ID\n\n# Set the URL for the API endpoints\nbase_url = \"https://api.printify.com/v1\"\nupload_url = f\"{base_url}/uploads/images.json\"\nproduct_url = f\"{base_url}/shops/{shop_id}/products.json\"\n\n# Load the CSV file\ncsv_path = \"product_information.csv\"  # Update this to your CSV file path\nimage_df = pd.read_csv(csv_path)\n\n# Set headers for requests\nheaders = {",
    "last_modified": "2025-09-13T05:53:45.998793"
  },
  {
    "id": "1632",
    "name": "quiz-tts.py",
    "path": "github_repo/scripts/01_core_ai_analysis/data_processing/quiz-tts.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 38,
    "size": 1165,
    "docstring": "",
    "keywords": [
      "data_processing",
      "analysis"
    ],
    "functions": [
      "generate_trivia_quiz"
    ],
    "classes": [],
    "imports": [
      "pandas"
    ],
    "preview": "import pandas as pd\n\n\n# Function to read the CSV file and generate trivia quiz questions\ndef generate_trivia_quiz(csv_path):\n    # Read the CSV file into a pandas DataFrame\n    df = pd.read_csv(csv_path)\n\n    # Iterate over each row in the DataFrame\n    for index, row in df.iterrows():\n        # Extract question and options from the row\n        question = row[\"Question\"]\n        options = [row[\"Option 1\"], row[\"Option 2\"], row[\"Option 3\"], row]\n\n        # Print the question\n        print(f\"Question {index + 1}: {question}\")\n\n        # Print the options\n        for i, option in enumerate(options):\n            print(f\"{i + 1}. {option}\")",
    "last_modified": "2025-05-04T22:47:13.049111"
  },
  {
    "id": "1633",
    "name": "csv-output.py",
    "path": "github_repo/scripts/01_core_ai_analysis/data_processing/csv-output.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 26,
    "size": 772,
    "docstring": "",
    "keywords": [
      "data_processing"
    ],
    "functions": [],
    "classes": [],
    "imports": [
      "pandas"
    ],
    "preview": "import pandas as pd\n\n# Load the CSV file\nfile_path = \"/Users/steven/Pictures/DALLe/pic.csv\"  # Replace with your file path\ndf = pd.read_csv(file_path)\n\n# Extract URLs (assuming they start with \"http\")\nurls = df[df.iloc[:, 0].str.startswith(\"http\")]\n\n# Extract info (assuming they don't start with \"http\")\ninfo = df[~df.iloc[:, 0].str.startswith(\"http\")]\n\n# Resetting indices\nurls.reset_index(drop=True, inplace=True)\ninfo.reset_index(drop=True, inplace=True)\n\n# Combine into a new dataframe\nresult_df = pd.DataFrame({\"URL\": urls.iloc[:, 0], \"Info\": info.iloc[:, 0]})\n\n# Save to CSV",
    "last_modified": "2025-05-04T22:47:11.552797"
  },
  {
    "id": "1634",
    "name": "vance.py",
    "path": "github_repo/scripts/01_core_ai_analysis/data_processing/vance.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 106,
    "size": 4205,
    "docstring": "",
    "keywords": [
      "data_processing",
      "analysis",
      "web_tools"
    ],
    "functions": [
      "upscale_image",
      "process_image",
      "process_directory"
    ],
    "classes": [],
    "imports": [
      "logging",
      "os",
      "time",
      "concurrent.futures",
      "pandas",
      "requests"
    ],
    "preview": "import logging\nimport os\nimport time\nfrom concurrent.futures import ThreadPoolExecutor, as_completed\n\nimport pandas as pd\nimport requests\n\n# Set up logging\nlogging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n\n\ndef upscale_image(api_token, image_path, scale):\n    try:\n        url = \"https://api-service.vanceai.com/web_api/v1/enlarge3\"\n        headers = {\"Authorization\": f\"Bearer {api_token}\"}\n\n        with open(image_path, \"rb\") as image_file:\n            files = {\"image\": image_file}\n            params = {",
    "last_modified": "2025-09-13T05:55:29.391468"
  },
  {
    "id": "1635",
    "name": "outs.py",
    "path": "github_repo/scripts/01_core_ai_analysis/data_processing/outs.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 28,
    "size": 891,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "csv"
    ],
    "preview": "import csv\n\n# File paths\ninput_file = \"/Users/steven/avatararts/dall-e.txt\"\noutput_file = \"output_image_data.csv\"\n\n# List to store extracted data\ndata = []\n\n# Read the file and process the data\nwith open(input_file, \"r\", encoding=\"utf-8\") as file:\n    content = file.read().strip()\n    entries = content.split(\"\\n\\n\")  # each entry is separated by two newlines\n\n    for entry in entries:\n        lines = entry.split(\"\\n\", 1)\n        if len(lines) == 2:\n            url, description = lines\n            data.append([url.strip(), description.strip()])\n",
    "last_modified": "2025-05-04T22:47:13"
  },
  {
    "id": "1636",
    "name": "bot_support.py",
    "path": "github_repo/scripts/01_core_ai_analysis/data_processing/bot_support.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 105,
    "size": 4805,
    "docstring": "Support instabot's methods.",
    "keywords": [],
    "functions": [
      "check_if_file_exists",
      "read_list_from_file",
      "console_print",
      "extract_urls"
    ],
    "classes": [],
    "imports": [
      "__future__",
      "codecs",
      "os",
      "re",
      "sys",
      "huepy"
    ],
    "preview": "\"\"\"\nSupport instabot's methods.\n\"\"\"\n\nfrom __future__ import unicode_literals\n\nimport codecs\nimport os\nimport re\nimport sys\n\nimport huepy\n\n\ndef check_if_file_exists(file_path, quiet=False):\n    if not os.path.exists(file_path):\n        if not quiet:\n            print(\"Can't find '%s' file.\" % file_path)\n        return False\n    return True",
    "last_modified": "2025-03-28T18:35:47"
  },
  {
    "id": "1637",
    "name": "convert 3.py",
    "path": "github_repo/scripts/01_core_ai_analysis/data_processing/convert 3.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 78,
    "size": 2071,
    "docstring": "",
    "keywords": [],
    "functions": [
      "convert_to_colab"
    ],
    "classes": [],
    "imports": [
      "os",
      "re"
    ],
    "preview": "import os\nimport re\n\n\ndef convert_to_colab(python_code):\n    # Install necessary libraries\n    python_code = re.sub(\n        r\"^\\s*#\\s*Install\\s*required\\s*libraries\\s*\",\n        \"\",\n        python_code,\n        flags=re.MULTILINE,\n    )\n    python_code = re.sub(\n        r\"^\\s*!pip\\s*install\\s*(\\w+)\",\n        r\"!pip install \\1\",\n        python_code,\n        flags=re.MULTILINE,\n    )\n\n    # Adjust file paths to be compatible with Colab",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1638",
    "name": "mask.py",
    "path": "github_repo/scripts/01_core_ai_analysis/data_processing/mask.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 71,
    "size": 2445,
    "docstring": "",
    "keywords": [
      "opencv",
      "analysis"
    ],
    "functions": [
      "apply_circular_mask",
      "process_images_in_directory"
    ],
    "classes": [],
    "imports": [
      "os",
      "cv2",
      "numpy"
    ],
    "preview": "import os\n\nimport cv2\nimport numpy as np\n\n\ndef apply_circular_mask(image_path, output_path):\n    \"\"\"\n    Applies a circular mask to the input image and saves it with transparency.\n\n    Args:\n        image_path (str): Path to the input image file.\n        output_path (str): Path to save the masked image.\n    \"\"\"\n    # Load the image\n    image = cv2.imread(image_path, cv2.IMREAD_UNCHANGED)\n\n    # If the image is not loaded correctly, skip it\n    if image is None:\n        print(f\"Failed to load image: {image_path}\")",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1639",
    "name": "process_csv.py",
    "path": "github_repo/scripts/01_core_ai_analysis/data_processing/process_csv.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 14,
    "size": 405,
    "docstring": "",
    "keywords": [
      "data_processing"
    ],
    "functions": [
      "read_csv"
    ],
    "classes": [],
    "imports": [
      "pandas"
    ],
    "preview": "import pandas as pd\n\n\ndef read_csv():\n    # Prompt the user for the path to the CSV file\n    file_path = input(\"Enter the path to the CSV file: \")\n    # Load the CSV data\n    data = pd.read_csv(file_path, header=None, names=['date', 'url'])\n    return data\n\n# Test the function\ndata = read_csv()\nprint(data.head())  # Print the first 5 rows to check the data\n/Users/steven/Pictures/mid-date/3/march.csv   ",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1640",
    "name": "img-resizer.py",
    "path": "github_repo/scripts/01_core_ai_analysis/data_processing/img-resizer.py",
    "category": "01_core_ai_analysis",
    "type": "image_processing",
    "lines": 121,
    "size": 4392,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "setup_logging",
      "process_image",
      "load_paths_from_csv",
      "main"
    ],
    "classes": [],
    "imports": [
      "argparse",
      "csv",
      "logging",
      "os",
      "concurrent.futures",
      "PIL",
      "tqdm"
    ],
    "preview": "#!/usr/bin/env python3\nimport argparse\nimport csv\nimport logging\nimport os\nfrom concurrent.futures import ThreadPoolExecutor\n\nfrom PIL import Image, UnidentifiedImageError\nfrom tqdm import tqdm\n\n\n# Configure logging\ndef setup_logging(log_file=None, verbose=False):\n    level = logging.DEBUG if verbose else logging.INFO\n    handlers = [logging.StreamHandler()]\n    if log_file:\n        handlers.append(logging.FileHandler(log_file))\n    logging.basicConfig(\n        level=level, format=\"%(asctime)s [%(levelname)s] %(message)s\", handlers=handlers\n    )",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1641",
    "name": "album_sorter.py",
    "path": "github_repo/scripts/01_core_ai_analysis/data_processing/album_sorter.py",
    "category": "01_core_ai_analysis",
    "type": "organization",
    "lines": 93,
    "size": 2825,
    "docstring": "",
    "keywords": [
      "data_processing"
    ],
    "functions": [
      "extract_album"
    ],
    "classes": [],
    "imports": [
      "os",
      "shutil",
      "pandas",
      "collections"
    ],
    "preview": "import os\nimport shutil\n\nimport pandas as pd\n\n# Input file locations (change if needed)\nVIDS_CSV = \"/Users/steven/Movies/project2025/vids-05-31-18:16.csv\"\nDOCS_CSV = \"/Users/steven/Movies/project2025/docs-05-31-18:16.csv\"\nPATHS_TXT = \"/Users/steven/Movies/project2025/paths-2025.txt\"\n\n# Album file types to sort (ignore .png, .jpg, etc.)\nSUFFIXES = [\".mp4\", \".mp3\", \"_analysis.txt\", \"_transcript.txt\"]\nMP4_BASE = \"/Users/steven/Movies/project2025/mp4\"\n\n\ndef extract_album(filename):\n    for suf in SUFFIXES:\n        if filename.endswith(suf):\n            return filename[: -len(suf)]\n    return None",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1642",
    "name": "fixer.py",
    "path": "github_repo/scripts/01_core_ai_analysis/data_processing/fixer.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 50,
    "size": 1449,
    "docstring": "",
    "keywords": [
      "analysis",
      "organization"
    ],
    "functions": [
      "format_with_black",
      "sort_imports_with_isort",
      "analyze_script"
    ],
    "classes": [],
    "imports": [
      "ast",
      "csv",
      "os",
      "subprocess",
      "io",
      "pylint.lint",
      "radon.complexity"
    ],
    "preview": "import ast\nimport csv\nimport os\nimport subprocess\nfrom io import StringIO\n\nfrom pylint.lint import Run\nfrom radon.complexity import cc_visit\n\n\n# Function to auto-format code with black\ndef format_with_black(filepath):\n    try:\n        subprocess.run([\"black\", filepath], check=True)\n        print(f\"Formatted {filepath} with black.\")\n    except subprocess.CalledProcessError as e:\n        print(f\"Error formatting {filepath}: {e}\")\n\n\n# Function to auto-sort imports with isort",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1643",
    "name": "resize-img.py",
    "path": "github_repo/scripts/01_core_ai_analysis/data_processing/resize-img.py",
    "category": "01_core_ai_analysis",
    "type": "image_processing",
    "lines": 201,
    "size": 5873,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "get_closest_aspect_ratio",
      "get_creation_date",
      "optimize_file_size",
      "upscale_image",
      "get_image_metadata",
      "process_image",
      "process_images",
      "save_log",
      "save_last_directory",
      "load_last_directory"
    ],
    "classes": [],
    "imports": [
      "csv",
      "os",
      "re",
      "time",
      "datetime",
      "PIL",
      "tqdm"
    ],
    "preview": "import csv\nimport os\nimport re\nimport time\nfrom datetime import datetime\n\nfrom PIL import Image, UnidentifiedImageError\nfrom tqdm import tqdm\n\n# \ud83c\udf1f Constants\nMAX_WIDTH, MAX_HEIGHT = 4550, 5400\nTARGET_DPI = 300\nUPSCALE_MULTIPLIER = 2\nBATCH_SIZE = 50\nPAUSE_DURATION = 3\nSIZE_THRESHOLD_MB = 9\nTARGET_MAX_FILE_SIZE_MB = 10\nLAST_DIRECTORY_FILE = \"image_data.txt\"\n\n# \ud83d\udcca Common Aspect Ratios",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1644",
    "name": "re-size.py",
    "path": "github_repo/scripts/01_core_ai_analysis/data_processing/re-size.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 146,
    "size": 4974,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "get_closest_aspect_ratio",
      "apply_dpi",
      "resize_image",
      "process_batch",
      "process_images",
      "main"
    ],
    "classes": [],
    "imports": [
      "os",
      "time",
      "datetime",
      "PIL",
      "tqdm"
    ],
    "preview": "import os\nimport time\nfrom datetime import datetime\n\nfrom PIL import Image, UnidentifiedImageError\nfrom tqdm import tqdm\n\n# Constants\nMAX_WIDTH, MAX_HEIGHT = 4500, 5400\nTARGET_DPI = 300\nBATCH_SIZE = 50\nPAUSE_DURATION = 5\nMAX_FILE_SIZE_BYTES = 8 * 1024 * 1024  # 8MB in bytes\n\n# Aspect Ratio Minimums\nASPECT_RATIO_MINIMUMS = {\n    \"16:9\": (720, 1280),  # Landscape\n    \"9:16\": (1080, 1920),  # Portrait\n    \"1:1\": (1024, 1024),  # Square\n    \"3:4\": (768, 1024),  # Portrait typically used for photography",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1645",
    "name": "noted.py",
    "path": "github_repo/scripts/01_core_ai_analysis/data_processing/noted.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 55,
    "size": 1648,
    "docstring": "",
    "keywords": [
      "data_processing"
    ],
    "functions": [
      "parse_markdown_to_table"
    ],
    "classes": [],
    "imports": [
      "re",
      "pandas"
    ],
    "preview": "import re\n\nimport pandas as pd\n\n\ndef parse_markdown_to_table(md_file):\n    \"\"\"\n    Parses a Markdown file to extract trends, designs, and tags into a structured table.\n    \"\"\"\n    with open(md_file, \"r\") as file:\n        content = file.read()\n\n    # Regex patterns to match required components\n    trend_pattern = r\"\\| (\\d+)\\s+\\|\"\n    title_pattern = r\"\\| \\[(.*?)\\]\\((.*?)\\)\"\n    tags_pattern = r\"\\| ([^\\|]+)\\s*\\|$\"\n\n    # Splitting lines to process each row\n    lines = content.splitlines()\n    data = []",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1646",
    "name": "create_csv_from_json.py",
    "path": "github_repo/scripts/01_core_ai_analysis/data_processing/create_csv_from_json.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 75,
    "size": 2218,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "process_json_to_csv",
      "combine_json_to_csv"
    ],
    "classes": [],
    "imports": [
      "csv",
      "gzip",
      "json",
      "os"
    ],
    "preview": "import csv\nimport gzip\nimport json\nimport os\n\n# Configuration\nOUTPUT_DIR = \"/Users/steven/Movies/printtricks/metadata.json\"\nCSV_FILE = os.path.join(OUTPUT_DIR, \"leonardo.csv\")\n\n# Headers for CSV\nHEADERS = [\n    \"id\",\n    \"prompt\",\n    \"negativePrompt\",\n    \"motionStrength\",\n    \"createdAt\",\n    \"image_url\",\n    \"motion_url\",\n    \"local_image_path\",\n    \"local_motion_path\",",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1647",
    "name": "expand_prompts_by_style.py",
    "path": "github_repo/scripts/01_core_ai_analysis/data_processing/expand_prompts_by_style.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 56,
    "size": 1990,
    "docstring": "",
    "keywords": [],
    "functions": [
      "collect_all_styles",
      "expand_prompts"
    ],
    "classes": [],
    "imports": [
      "csv",
      "json",
      "pathlib"
    ],
    "preview": "import csv\nimport json\nfrom pathlib import Path\n\nINPUT_CSV = \"/Users/steven/Documents/python/clean/CSV/flexible_analyzed_image_data-05-30-22-21.csv\"\nOUTPUT_CSV = \"/Users/steven/Documents/python/clean/CSV/prompts_expanded_image_data-05-30-22-21.csv\"\nPROMPTS_FIELD = \"design_prompts_json\"\n\n\ndef collect_all_styles(input_csv):\n    styles = set()\n    with open(input_csv, newline=\"\", encoding=\"utf-8\") as infile:\n        reader = csv.DictReader(infile)\n        for row in reader:\n            prompt_json = row.get(PROMPTS_FIELD, \"\")\n            if prompt_json:\n                try:\n                    prompts = json.loads(prompt_json)\n                    for style in prompts.keys():\n                        styles.add(style)",
    "last_modified": "2025-09-13T05:53:48.150952"
  },
  {
    "id": "1648",
    "name": "process_leonardo_20250102110033.py",
    "path": "github_repo/scripts/01_core_ai_analysis/data_processing/process_leonardo_20250102110033.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 75,
    "size": 2220,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "process_json_to_csv",
      "combine_json_to_csv"
    ],
    "classes": [],
    "imports": [
      "csv",
      "gzip",
      "json",
      "os"
    ],
    "preview": "import csv\nimport gzip\nimport json\nimport os\n\n# Configuration\nOUTPUT_DIR = \"/Users/steven/Downloads/leonardo_images\"\nCSV_FILE = os.path.join(OUTPUT_DIR, \"leonardo_metadata.csv\")\n\n# Headers for CSV\nHEADERS = [\n    \"id\",\n    \"prompt\",\n    \"negativePrompt\",\n    \"motionStrength\",\n    \"createdAt\",\n    \"image_url\",\n    \"motion_url\",\n    \"local_image_path\",\n    \"local_motion_path\",",
    "last_modified": "2025-03-28T18:37:01.031009"
  },
  {
    "id": "1649",
    "name": "quiz.py",
    "path": "github_repo/scripts/01_core_ai_analysis/data_processing/quiz.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 39,
    "size": 1207,
    "docstring": "",
    "keywords": [
      "data_processing",
      "analysis"
    ],
    "functions": [
      "generate_trivia_quiz"
    ],
    "classes": [],
    "imports": [
      "pandas"
    ],
    "preview": "import pandas as pd\n\n# Function to read the CSV file and generate trivia quiz questions\n\n\ndef generate_trivia_quiz(csv_path):\n    # Read the CSV file into a pandas DataFrame\n    df = pd.read_csv(csv_path)\n\n    # Iterate over each row in the DataFrame\n    for index, row in df.iterrows():\n        # Extract question and options from the row\n        question = row[\"Question\"]\n        options = [row[\"Option1\"], row[\"Option2\"], row[\"Option3\"], row[\"Option4\"]]\n\n        # Print the question\n        print(f\"Question {index + 1}: {question}\")\n\n        # Print the options\n        for i, option in enumerate(options):",
    "last_modified": "2025-05-04T22:47:13.044891"
  },
  {
    "id": "1650",
    "name": "yplaylist.py",
    "path": "github_repo/scripts/01_core_ai_analysis/data_processing/yplaylist.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 31,
    "size": 942,
    "docstring": "",
    "keywords": [
      "data_processing"
    ],
    "functions": [],
    "classes": [],
    "imports": [
      "pandas",
      "pytube"
    ],
    "preview": "import pandas as pd\nfrom pytube import Playlist\n\ntry:\n    playlist_url = \"https://www.youtube.com/playlist?list=PLfudK7D_bQIjRgAqVU_jkbnb1N5V_pNiG\"\n    playlist = Playlist(playlist_url)\n\n    videos_info = {\n        \"Title\": [],\n        \"Video URL\": [],\n        \"Length (seconds)\": [],\n        \"Views\": [],\n        \"Thumbnail URL\": [],\n        \"Description\": [],\n    }\n\n    for video in playlist.videos:\n        videos_info[\"Title\"].append(video.title)\n        videos_info[\"Video URL\"].append(video.watch_url)\n        videos_info[\"Length (seconds)\"].append(video.length)",
    "last_modified": "2025-09-13T05:53:55.644704"
  },
  {
    "id": "1651",
    "name": "custom_thumbnail.py",
    "path": "github_repo/scripts/01_core_ai_analysis/data_processing/custom_thumbnail.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 159,
    "size": 5719,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "logging",
      "os",
      "time",
      "numpy",
      "PIL",
      "pyrogram",
      "translation",
      "helper_funcs.chat_base",
      "sample_config",
      "config"
    ],
    "preview": "import logging\n\nlogging.basicConfig(\n    level=logging.DEBUG, format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\n)\nlogger = logging.getLogger(__name__)\n\nimport os\nimport time\n\nimport numpy\nfrom PIL import Image\n\n# the secret configuration specific things\nif bool(os.environ.get(\"WEBHOOK\", False)):\n    from sample_config import Config\nelse:\n    from config import Config\n\n# the Strings used for this \"thing\"",
    "last_modified": "2025-09-13T05:53:43.860320"
  },
  {
    "id": "1652",
    "name": "upscalerr.py",
    "path": "github_repo/scripts/01_core_ai_analysis/data_processing/upscalerr.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 168,
    "size": 5941,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "get_user_choice",
      "apply_dpi",
      "resize_image",
      "upscale_image",
      "process_batch",
      "process_images",
      "write_log",
      "main"
    ],
    "classes": [],
    "imports": [
      "csv",
      "os",
      "time",
      "datetime",
      "PIL",
      "tqdm"
    ],
    "preview": "import csv\nimport os\nimport time\nfrom datetime import datetime\n\nfrom PIL import Image, UnidentifiedImageError\nfrom tqdm import tqdm\n\n# \ud83d\ude80 Constants\nTARGET_DPI = 300\nUPSCALE_MULTIPLIER = 2  # How much to enlarge small images\nBATCH_SIZE = 50\nPAUSE_DURATION = 3  # Just for dramatic effect \ud83c\udfad\nSIZE_THRESHOLD_MB = 9  # The Holy Grail of size rules\n\n# \ud83d\udcdc Log Data\nlog_data = []\n\n\n# \ud83c\udfc6 Ask the user for the processing mode",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1653",
    "name": "generate_song_csv.py",
    "path": "github_repo/scripts/01_core_ai_analysis/data_processing/generate_song_csv.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 89,
    "size": 2892,
    "docstring": "",
    "keywords": [
      "data_processing"
    ],
    "functions": [
      "prompt_for_files",
      "predefined_file_paths",
      "extract_song_details",
      "main"
    ],
    "classes": [],
    "imports": [
      "os",
      "pandas",
      "bs4"
    ],
    "preview": "import os\n\nimport pandas as pd\nfrom bs4 import BeautifulSoup\n\n\n# Option A: Prompt for file paths\ndef prompt_for_files():\n    print(\"Enter the file paths for the HTML files (comma-separated):\")\n    file_paths = input().split(\",\")\n    return [file.strip() for file in file_paths]\n\n\n# Option B: Predefined file paths\ndef predefined_file_paths():\n    return [\"/Users/steven/Music/NocTurnE-meLoDieS/suno/01.html\"]\n\n\n# Main function to process HTML files\ndef extract_song_details(file_paths):",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1654",
    "name": "txt-csv.py",
    "path": "github_repo/scripts/01_core_ai_analysis/data_processing/txt-csv.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 41,
    "size": 1270,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "csv",
      "re"
    ],
    "preview": "import csv\nimport re\n\n# Prompt the user for the input and output file paths\ninput_file_path = input(\"Enter the path to the input .txt file: \").strip()\noutput_file_path = input(\"Enter the path for the output .csv file: \").strip()\n\n# Define the regex pattern to extract URL, Info, and Keywords\npattern = re.compile(r'\\[src=\"(https[^\"]+)\" alt=\"([^\"]+)\"\\]')\n\n# List to store the extracted data\ndata = []\n\n# Read the input file and extract data\ntry:\n    with open(input_file_path, \"r\") as file:\n        for line in file:\n            match = pattern.search(line)\n            if match:\n                url = match.group(1)",
    "last_modified": "2025-05-04T22:47:11.553258"
  },
  {
    "id": "1655",
    "name": "gthumb.py",
    "path": "github_repo/scripts/01_core_ai_analysis/data_processing/gthumb.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 485,
    "size": 46791,
    "docstring": "",
    "keywords": [
      "data_processing"
    ],
    "functions": [],
    "classes": [],
    "imports": [
      "pandas"
    ],
    "preview": "import pandas as pd\n\n# Sample data - replace this with your actual DataFrame loading method\ndata = {\n    \"URL\": [\n        \"https://www.youtube.com/watch?v=2bbQdfVWr_8\",\n        \"https://www.youtube.com/watch?v=KZ4WHCmB51c\",\n        \"https://www.youtube.com/watch?v=n-o0a_DvhA8\",\n        \"https://www.youtube.com/watch?v=6XgEyP4PBxA\",\n        \"https://www.youtube.com/watch?v=ItQuj0NQCxk\",\n        \"https://www.youtube.com/watch?v=JB8bsCJzZaw\",\n        \"https://www.youtube.com/watch?v=6yRFOeb1nZI\",\n        \"https://www.youtube.com/watch?v=92jt7rgPPt0\",\n        \"https://www.youtube.com/watch?v=jBrEhPdY6Mw\",\n        \"https://www.youtube.com/watch?v=Rn5VognknAs\",\n        \"https://www.youtube.com/watch?v=J4kjVVxokKg\",\n        \"https://www.youtube.com/watch?v=8fDbfexgO3E\",\n        \"https://www.youtube.com/watch?v=MyJW1wVxR3s\",\n        \"https://www.youtube.com/watch?v=NdiDxzm4VcI\",\n        \"https://www.youtube.com/watch?v=RL1v9FCaO5s\",",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1656",
    "name": "ai_methods.py",
    "path": "github_repo/scripts/01_core_ai_analysis/data_processing/ai_methods.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 67,
    "size": 2805,
    "docstring": "",
    "keywords": [
      "organization"
    ],
    "functions": [
      "mean_pooling",
      "sort_by_similarity"
    ],
    "classes": [],
    "imports": [
      "numpy",
      "torch",
      "transformers"
    ],
    "preview": "import numpy as np\nimport torch\nfrom transformers import AutoModel, AutoTokenizer\n\n\n# Mean Pooling - Take attention mask into account for correct averaging\ndef mean_pooling(model_output, attention_mask):\n    token_embeddings = model_output[\n        0\n    ]  # First element of model_output contains all token embeddings\n    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(\n        input_mask_expanded.sum(1), min=1e-9\n    )\n\n\n# This function sort the given threads based on their total similarity with the given keywords\ndef sort_by_similarity(thread_objects, keywords):\n    # Initialize tokenizer + model.\n    tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")",
    "last_modified": "2025-09-13T05:54:00.046373"
  },
  {
    "id": "1657",
    "name": "generate-info 1.py",
    "path": "github_repo/scripts/01_core_ai_analysis/data_processing/generate-info 1.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 94,
    "size": 3021,
    "docstring": "",
    "keywords": [
      "data_processing"
    ],
    "functions": [
      "prompt_for_files",
      "predefined_file_paths",
      "extract_song_details",
      "main"
    ],
    "classes": [],
    "imports": [
      "os",
      "pandas",
      "bs4"
    ],
    "preview": "import os\n\nimport pandas as pd\nfrom bs4 import BeautifulSoup\n\n\n# Option A: Prompt for file paths\ndef prompt_for_files():\n    print(\"Enter the file paths for the HTML files (comma-separated):\")\n    file_paths = input().split(\",\")\n    return [file.strip() for file in file_paths]\n\n\n# Option B: Predefined file paths\ndef predefined_file_paths():\n    return [\n        \"/Users/steven/Music/suno/1.html\",\n        \"/Users/steven/Music/suno/2.html\",\n        \"/Users/steven/Music/suno/3.html\",\n        \"/Users/steven/Music/suno/4.html\",",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1658",
    "name": "organize_csv.py",
    "path": "github_repo/scripts/01_core_ai_analysis/data_processing/organize_csv.py",
    "category": "01_core_ai_analysis",
    "type": "organization",
    "lines": 114,
    "size": 4573,
    "docstring": "",
    "keywords": [
      "data_processing",
      "analysis"
    ],
    "functions": [
      "analyze_df",
      "standardize_columns"
    ],
    "classes": [],
    "imports": [
      "pandas"
    ],
    "preview": "import pandas as pd\n\n# --------------------------------------------------------------------------------\n# 1) Read the CSV files\n# --------------------------------------------------------------------------------\n\ndf_vids = pd.read_csv(\"/Users/steven/clean/vids-03-29-17-53.csv\")\ndf_other = pd.read_csv(\"/Users/steven/clean/other-03-29-17-52.csv\")\ndf_img = pd.read_csv(\"/Users/steven/clean/image_data-03-29-17-51.csv\")\ndf_docs = pd.read_csv(\"/Users/steven/clean/docs-03-29-17-49.csv\")\ndf_audio = pd.read_csv(\"/Users/steven/clean/audio-03-29-17-49.csv\")\n\n# --------------------------------------------------------------------------------\n# 2) Basic analysis: shapes, duplicates, and a few lines to see structure\n# --------------------------------------------------------------------------------\n\n\ndef analyze_df(df, df_name):\n    print(f\"--- Analyzing {df_name} ---\")\n    print(\"Shape (rows, columns):\", df.shape)",
    "last_modified": "2025-09-13T05:53:48.775045"
  },
  {
    "id": "1659",
    "name": "suno-song-info.py",
    "path": "github_repo/scripts/01_core_ai_analysis/data_processing/suno-song-info.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 34,
    "size": 891,
    "docstring": "",
    "keywords": [
      "data_processing"
    ],
    "functions": [],
    "classes": [],
    "imports": [
      "re",
      "pandas",
      "ace_tools"
    ],
    "preview": "import re\n\nimport pandas as pd\n\n# Load HTML content from file\nfile_path = \"/Users/steven/Music/suno/1.html\"\nwith open(file_path, \"r\", encoding=\"utf-8\") as file:\n    html_content = file.read()\n\n# Adjusted regex pattern to extract src, title, song_href, style_href, and style\npattern = r'src=\"([^\"]+)\".*?title=\"([^\"]+)\".*?href=\"([^\"]+)\".*?href=\"([^\"]+)\">([^<]+)'\n\n# Extract matches\nmatches = re.findall(pattern, html_content)\n\n# Prepare results for output\nresults = []\nfor match in matches:\n    results.append(\n        {",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1660",
    "name": "YTubeDLthumbs (1).py",
    "path": "github_repo/scripts/01_core_ai_analysis/data_processing/YTubeDLthumbs (1).py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 59,
    "size": 1918,
    "docstring": "",
    "keywords": [
      "data_processing",
      "web_tools"
    ],
    "functions": [
      "fetch_video_details"
    ],
    "classes": [],
    "imports": [
      "os",
      "pandas",
      "requests",
      "googleapiclient.discovery"
    ],
    "preview": "import os\n\nimport pandas as pd  # This line is necessary to use pandas in your script\nimport requests\nfrom googleapiclient.discovery import build\n\n# Initialize YouTube API\napi_key = \"AIzaSyCyfGm1oF2syfzfsPeCRHb10tMKmC0dbCo\"\nyoutube = build(\"youtube\", \"v3\", developerKey=api_key)\n\n# Define paths\ncsv_path = \"/Users/steven/Downloads/Misc/ytube - youtube_videos.csv\"\nthumbnail_dir = \"/Users/steven/Downloads/Misc/Thumbnails/d2\"\nos.makedirs(thumbnail_dir, exist_ok=True)\n\n# Load CSV\ndf = pd.read_csv(csv_path)\n\n\n# Function to fetch video details and download thumbnail",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1661",
    "name": "suno-extract-song.py",
    "path": "github_repo/scripts/01_core_ai_analysis/data_processing/suno-extract-song.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 113,
    "size": 3802,
    "docstring": "",
    "keywords": [
      "data_processing"
    ],
    "functions": [
      "prompt_for_files",
      "predefined_file_paths",
      "extract_song_details",
      "main"
    ],
    "classes": [],
    "imports": [
      "os",
      "re",
      "pandas",
      "bs4"
    ],
    "preview": "import os\nimport re\n\nimport pandas as pd\nfrom bs4 import BeautifulSoup\n\n\n# Option A: Prompt for file paths\ndef prompt_for_files():\n    print(\"Enter the file paths for the HTML files (comma-separated):\")\n    file_paths = input().split(\",\")\n    return [file.strip() for file in file_paths]\n\n\n# Option B: Predefined file paths\ndef predefined_file_paths():\n    return [\"pg1-310.html\"]\n\n\n# Main function to process HTML files",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1662",
    "name": "gen-songs.py",
    "path": "github_repo/scripts/01_core_ai_analysis/data_processing/gen-songs.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 45,
    "size": 1665,
    "docstring": "",
    "keywords": [
      "data_processing"
    ],
    "functions": [],
    "classes": [],
    "imports": [
      "pandas",
      "bs4",
      "ace_tools"
    ],
    "preview": "import pandas as pd\nfrom bs4 import BeautifulSoup\n\n# Prompt the user for the directory\ndirectory = input(\"Enter the directory containing the files: \").strip()\n\n# Load the newly uploaded HTML file for processing\nfile_path_new_html = input(\"Enter the HTML Url: \").strip()\nwith open(file_path_new_html, \"r\", encoding=\"utf-8\") as file:\n    html_content_new_html = file.read()\n\n# Parsing the HTML content with BeautifulSoup\nsoup = BeautifulSoup(html_content_new_html, \"html.parser\")\n\n# Extracting relevant details\nsong_details_new_html = []\nfor item in soup.find_all(\"div\", class_=\"css-79jxux\"):  # Adjust the class if necessary\n    time_element = item.find_previous(\"span\", class_=\"font-mono\")\n    title_element = item.find(\"span\", class_=\"text-primary\")\n    song_url_element = item.find(\"a\", href=True)",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1663",
    "name": "check.py",
    "path": "github_repo/scripts/01_core_ai_analysis/data_processing/check.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 67,
    "size": 2389,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "check_with_pylint",
      "check_with_flake8",
      "process_directory"
    ],
    "classes": [],
    "imports": [
      "os",
      "subprocess"
    ],
    "preview": "import os\nimport subprocess\n\n\ndef check_with_pylint(file_path, log_file):\n    \"\"\"\n    Run pylint on the given Python file to check for errors and style issues.\n\n    :param file_path: Path to the Python file.\n    :param log_file: File object to log the errors.\n    :return: None\n    \"\"\"\n    result = subprocess.run([\"pylint\", file_path], capture_output=True, text=True)\n    if result.returncode != 0:\n        log_file.write(f\"Pylint issues in {file_path}:\\n\")\n        log_file.write(result.stdout + \"\\n\")\n        print(f\"Pylint issues in {file_path}:\\n{result.stdout}\")\n    else:\n        log_file.write(f\"No pylint issues found in {file_path}\\n\")\n        print(f\"No pylint issues found in {file_path}\")",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1664",
    "name": "netlify_uploader.py",
    "path": "github_repo/scripts/01_core_ai_analysis/data_processing/netlify_uploader.py",
    "category": "01_core_ai_analysis",
    "type": "youtube",
    "lines": 245,
    "size": 8349,
    "docstring": "",
    "keywords": [
      "youtube",
      "analysis",
      "web_tools"
    ],
    "functions": [
      "get_netlify_site_id",
      "deploy_to_netlify",
      "create_website_zip",
      "get_params",
      "render_page",
      "process_index",
      "process_token",
      "process_error",
      "log_message",
      "do_GET"
    ],
    "classes": [
      "SimplePhotoGalleryHTTPServer",
      "SimplePhotoGalleryHTTPRequestHandler",
      "NetlifyUploader"
    ],
    "imports": [
      "json",
      "os",
      "tempfile",
      "webbrowser",
      "zipfile",
      "http.server",
      "urllib",
      "jinja2",
      "pkg_resources",
      "requests"
    ],
    "preview": "import json\nimport os\nimport tempfile\nimport webbrowser\nimport zipfile\nfrom http.server import BaseHTTPRequestHandler, HTTPServer\nfrom urllib import parse\n\nimport jinja2\nimport pkg_resources\nimport requests\nimport simplegallery.common as spg_common\nfrom simplegallery.upload.base_uploader import BaseUploader\n\n\nclass SimplePhotoGalleryHTTPServer(HTTPServer):\n    \"\"\"\n    Class deriving from the HTTPServer, defining new properties\n    \"\"\"\n",
    "last_modified": "2025-09-13T05:53:53.351252"
  },
  {
    "id": "1665",
    "name": "csvmerge.py",
    "path": "github_repo/scripts/01_core_ai_analysis/data_processing/csvmerge.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 67,
    "size": 1981,
    "docstring": "",
    "keywords": [
      "data_processing"
    ],
    "functions": [],
    "classes": [],
    "imports": [
      "os",
      "pandas"
    ],
    "preview": "import os\n\nimport pandas as pd\n\n# Directory containing the CSV files\ncsv_directory = \"/Users/steven/Pictures/mydesign\"\n\n# List to hold dataframes\ndataframes = []\n\n# Define the expected headers\nexpected_headers = [\n    \"ID\",\n    \"Default_slot_file_name\",\n    \"Default_slot_image_url\",\n    \"Digital Image_slot_file_name\",\n    \"Digital Image_slot_image_url\",\n    \"Boy_slot_file_name\",\n    \"Boy_slot_image_url\",\n    \"VID_slot_file_name\",",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1666",
    "name": "euckrprober.py",
    "path": "github_repo/scripts/01_core_ai_analysis/data_processing/euckrprober.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 48,
    "size": 1753,
    "docstring": "",
    "keywords": [],
    "functions": [
      "__init__",
      "charset_name",
      "language"
    ],
    "classes": [
      "EUCKRProber"
    ],
    "imports": [
      "chardistribution",
      "codingstatemachine",
      "mbcharsetprober",
      "mbcssm"
    ],
    "preview": "######################## BEGIN LICENSE BLOCK ########################\n# The Original Code is mozilla.org code.\n#\n# The Initial Developer of the Original Code is\n# Netscape Communications Corporation.\n# Portions created by the Initial Developer are Copyright (C) 1998\n# the Initial Developer. All Rights Reserved.\n#\n# Contributor(s):\n#   Mark Pilgrim - port to Python\n#\n# This library is free software; you can redistribute it and/or\n# modify it under the terms of the GNU Lesser General Public\n# License as published by the Free Software Foundation; either\n# version 2.1 of the License, or (at your option) any later version.\n#\n# This library is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU\n# Lesser General Public License for more details.",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1667",
    "name": "upload.py",
    "path": "github_repo/scripts/01_core_ai_analysis/data_processing/upload.py",
    "category": "01_core_ai_analysis",
    "type": "youtube",
    "lines": 82,
    "size": 3510,
    "docstring": "",
    "keywords": [
      "data_processing",
      "web_tools"
    ],
    "functions": [],
    "classes": [],
    "imports": [
      "base64",
      "os",
      "pandas",
      "requests"
    ],
    "preview": "import base64\nimport os\n\nimport pandas as pd\nimport requests\n\n# Set your API credentials\naccess_token = \"eyJ0eXAiOiJKV1QiLCJhbGciOiJSUzI1NiJ9.eyJhdWQiOiIzN2Q0YmQzMDM1ZmUxMWU5YTgwM2FiN2VlYjNjY2M5NyIsImp0aSI6IjQyMmQ1MWFlODI0YjEyMGY2MmY5N2YwZWExOWQ1YzRjMjhlZGM5OTRjYWQ0MmJmNzViOTZlNTdkZWQ3Mzk3YWUyZmQxZDAxMjJhZWRkMTU2IiwiaWF0IjoxNzAyMzYwNjg5LjA0MDcyMywibmJmIjoxNzAyMzYwNjg5LjA0MDcyNiwiZXhwIjoxNzMzOTgzMDg5LjAzNDUzNiwic3ViIjoiMTEzMjQxNTMiLCJzY29wZXMiOlsic2hvcHMubWFuYWdlIiwic2hvcHMucmVhZCIsImNhdGFsb2cucmVhZCIsIm9yZGVycy5yZWFkIiwib3JkZXJzLndyaXRlIiwicHJvZHVjdHMucmVhZCIsInByb2R1Y3RzLndyaXRlIiwid2ViaG9va3MucmVhZCIsIndlYmhvb2tzLndyaXRlIiwidXBsb2Fkcy5yZWFkIiwidXBsb2Fkcy53cml0ZSIsInByaW50X3Byb3ZpZGVycy5yZWFkIl19.AkWVerYdnoGceJShqCIhFpw6o0m7Nz0mqE6moOvuAdy9F4YS-G4rVePuxPp6u9C-y9VxF2pGDYF3yg6kQEo\"\n\n# Find your shop ID by running this: curl -X GET https://api.printify.com/v1/shops.json --header \"Authorization: Bearer YOUR_PRINTIFY_API_KEY\"\n\nshop_id = \"6511744\"\n\n# Set the URL for the API endpoints\nbase_url = \"https://api.printify.com/v1\"\nupload_url = f\"{base_url}/uploads/images.json\"\nproduct_url = f\"{base_url}/shops/{shop_id}/products.json\"\n\n# Load the CSV file\ncsv_path = \"trick.csv\"  # Update this to your CSV file path",
    "last_modified": "2025-09-13T05:53:46.060530"
  },
  {
    "id": "1668",
    "name": "process_leonardo_20250102110137.py",
    "path": "github_repo/scripts/01_core_ai_analysis/data_processing/process_leonardo_20250102110137.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 75,
    "size": 2240,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "process_json_to_csv",
      "combine_json_to_csv"
    ],
    "classes": [],
    "imports": [
      "csv",
      "gzip",
      "json",
      "os"
    ],
    "preview": "import csv\nimport gzip\nimport json\nimport os\n\n# Configuration\nOUTPUT_DIR = \"/Users/steven/Documents/python/leonardo/process_leonardo.py\"\nCSV_FILE = os.path.join(OUTPUT_DIR, \"leonardo_metadata.csv\")\n\n# Headers for CSV\nHEADERS = [\n    \"id\",\n    \"prompt\",\n    \"negativePrompt\",\n    \"motionStrength\",\n    \"createdAt\",\n    \"image_url\",\n    \"motion_url\",\n    \"local_image_path\",\n    \"local_motion_path\",",
    "last_modified": "2025-03-28T18:37:01.032665"
  },
  {
    "id": "1669",
    "name": "div.py",
    "path": "github_repo/scripts/01_core_ai_analysis/data_processing/div.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 35,
    "size": 1243,
    "docstring": "",
    "keywords": [],
    "functions": [
      "extract_product_info_to_csv"
    ],
    "classes": [],
    "imports": [
      "csv",
      "re"
    ],
    "preview": "import csv\nimport re\n\n\ndef extract_product_info_to_csv(input_file_path, output_csv_path):\n    with open(input_file_path, \"r\", encoding=\"utf-8\") as file:\n        content = file.read()\n\n    # Regular expression to match product titles and image URLs\n    pattern = re.compile(\n        r\"(.*?)\\n(https://images-na\\.ssl-images-amazon\\.com/images/W/MEDIAX_792452-T2/images/I/[^\\s]+\\.jpg)\",\n        re.DOTALL,\n    )\n\n    # Write the extracted data to a CSV file\n    with open(output_csv_path, \"w\", newline=\"\", encoding=\"utf-8\") as csvfile:\n        csvwriter = csv.writer(csvfile)\n        for match in pattern.finditer(content):\n            title = match.group(1).strip()\n            urls = re.findall(",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1670",
    "name": "idt.py",
    "path": "github_repo/scripts/01_core_ai_analysis/data_processing/idt.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 34,
    "size": 1067,
    "docstring": "",
    "keywords": [
      "data_processing"
    ],
    "functions": [],
    "classes": [],
    "imports": [
      "os",
      "pandas"
    ],
    "preview": "import os\n\nimport pandas as pd\n\n# Load your YouTube video data\ncsv_path = \"/Users/steven/Downloads/Misc/ytube - youtube_videos.csv\"\ndf = pd.read_csv(csv_path)\n\n# Directory containing the downloaded thumbnails\nthumbnail_dir = \"/Users/steven/Downloads/Misc/Thumbnails/idT\"\n\n# Add a new column for the thumbnail path if it doesn't exist\nif \"Thumbnail Path\" not in df.columns:\n    df[\"Thumbnail Path\"] = \"\"\n\n# Iterate through the thumbnails in the directory\nfor filename in os.listdir(thumbnail_dir):\n    if filename.endswith(\".jpg\") or filename.endswith(\".png\"):\n        # Extract the video ID from the filename\n        video_id = filename.split(\".\")[0]",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1671",
    "name": "dbl.py",
    "path": "github_repo/scripts/01_core_ai_analysis/data_processing/dbl.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 186,
    "size": 6189,
    "docstring": "",
    "keywords": [
      "data_processing",
      "analysis"
    ],
    "functions": [
      "is_excluded",
      "compute_md5",
      "generate_detailed_duplicate_report",
      "prompt_for_csv_files"
    ],
    "classes": [],
    "imports": [
      "csv",
      "hashlib",
      "os",
      "re",
      "collections",
      "pandas"
    ],
    "preview": "import csv\nimport hashlib\nimport os\nimport re\nfrom collections import defaultdict\n\nimport pandas as pd\n\n\ndef is_excluded(path, patterns):\n    \"\"\"\n    Check if a given path matches any of the exclusion patterns.\n\n    Parameters:\n    path (str): The path to check.\n    patterns (list): A list of regex patterns for exclusion.\n\n    Returns:\n    bool: True if path matches any pattern, False otherwise.\n    \"\"\"",
    "last_modified": "2025-09-13T05:54:29.153600"
  },
  {
    "id": "1672",
    "name": "resize-skip-8below.py",
    "path": "github_repo/scripts/01_core_ai_analysis/data_processing/resize-skip-8below.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 134,
    "size": 4444,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "get_closest_aspect_ratio",
      "resize_image",
      "process_batch",
      "process_images",
      "main"
    ],
    "classes": [],
    "imports": [
      "os",
      "time",
      "datetime",
      "PIL"
    ],
    "preview": "import os\nimport time\nfrom datetime import datetime\n\nfrom PIL import Image, UnidentifiedImageError\n\n# Constants\nMAX_WIDTH, MAX_HEIGHT = 4500, 5400\nTARGET_DPI = 300\nBATCH_SIZE = 50\nPAUSE_DURATION = 5\nMIN_FILE_SIZE_BYTES = 8 * 1024 * 1024  # 8MB in bytes\n\n# Aspect Ratio Minimums\nASPECT_RATIO_MINIMUMS = {\n    \"16:9\": (720, 1280),  # Landscape\n    \"9:16\": (1080, 1920),  # Portrait\n    \"1:1\": (1024, 1024),  # Square\n}\n",
    "last_modified": "2025-09-13T05:53:48.995810"
  },
  {
    "id": "1673",
    "name": "deepseek_python_20250530205148.py",
    "path": "github_repo/scripts/01_core_ai_analysis/data_processing/deepseek_python_20250530205148.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 497,
    "size": 17786,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "__init__",
      "get_system_info",
      "analyze",
      "analyze_file",
      "analyze_path_relationships",
      "get_file_metrics",
      "ast_analysis",
      "run_analysis_tools",
      "complexity_analysis",
      "update_summary"
    ],
    "classes": [
      "AdvancedPythonAnalyzer"
    ],
    "imports": [
      "ast",
      "csv",
      "datetime",
      "json",
      "os",
      "platform",
      "subprocess",
      "sys",
      "collections",
      "matplotlib.pyplot"
    ],
    "preview": "import ast\nimport csv\nimport datetime\nimport json\nimport os\nimport platform\nimport subprocess\nimport sys\nfrom collections import defaultdict\n\nimport matplotlib.pyplot as plt\nimport networkx as nx\nimport radon\nfrom pylint import epylint as lint\nfrom radon.complexity import cc_visit\nfrom radon.metrics import mi_visit\n\n\nclass AdvancedPythonAnalyzer:\n    def __init__(self, directory):",
    "last_modified": "2025-09-13T05:53:27.372995"
  },
  {
    "id": "1674",
    "name": "mydesigner.py",
    "path": "github_repo/scripts/01_core_ai_analysis/data_processing/mydesigner.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 97,
    "size": 3180,
    "docstring": "",
    "keywords": [
      "data_processing",
      "analysis",
      "youtube",
      "web_tools"
    ],
    "functions": [
      "ensure_directories",
      "download_image",
      "process_images",
      "write_log_to_csv",
      "main"
    ],
    "classes": [],
    "imports": [
      "os",
      "datetime",
      "pandas",
      "requests"
    ],
    "preview": "import os\nfrom datetime import datetime\n\nimport pandas as pd\nimport requests\n\n\n# Ensure that CSV and output directories exist based on the user prompt\ndef ensure_directories(base_dir):\n    output_dir = os.path.join(base_dir, \"processed_images\")\n    os.makedirs(output_dir, exist_ok=True)\n    return output_dir\n\n\n# Function to download an image\ndef download_image(url, filename):\n    try:\n        response = requests.get(url, timeout=10)\n        if response.status_code == 200:\n            with open(filename, \"wb\") as f:",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1675",
    "name": "etsy-csv-sort.py",
    "path": "github_repo/scripts/01_core_ai_analysis/data_processing/etsy-csv-sort.py",
    "category": "01_core_ai_analysis",
    "type": "organization",
    "lines": 75,
    "size": 2679,
    "docstring": "",
    "keywords": [
      "data_processing",
      "organization"
    ],
    "functions": [
      "organize_etsy_csv"
    ],
    "classes": [],
    "imports": [
      "pandas"
    ],
    "preview": "import pandas as pd\n\n\ndef organize_etsy_csv():\n    # Prompt for file locations\n    input_file_path = input(\"Enter the path to your input CSV file: \")\n    output_file_path = input(\n        \"Enter the path to save the organized output CSV file (with .csv extension): \"\n    )\n\n    # Read the input CSV file\n    try:\n        df = pd.read_csv(input_file_path)\n\n        # Define expected columns and provide options to rename if needed\n        required_columns = {\n            \"iD\": \"Listing URL\",\n            \"Title\": \"Etsy Listing\",\n            \"price\": \"price\",\n            \"est_total_sales\": \"est_total_sales\",",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1676",
    "name": "sjisprober.py",
    "path": "github_repo/scripts/01_core_ai_analysis/data_processing/sjisprober.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 104,
    "size": 3961,
    "docstring": "",
    "keywords": [],
    "functions": [
      "__init__",
      "reset",
      "charset_name",
      "language",
      "feed",
      "get_confidence"
    ],
    "classes": [
      "SJISProber"
    ],
    "imports": [
      "typing",
      "chardistribution",
      "codingstatemachine",
      "enums",
      "jpcntx",
      "mbcharsetprober",
      "mbcssm"
    ],
    "preview": "######################## BEGIN LICENSE BLOCK ########################\n# The Original Code is mozilla.org code.\n#\n# The Initial Developer of the Original Code is\n# Netscape Communications Corporation.\n# Portions created by the Initial Developer are Copyright (C) 1998\n# the Initial Developer. All Rights Reserved.\n#\n# Contributor(s):\n#   Mark Pilgrim - port to Python\n#\n# This library is free software; you can redistribute it and/or\n# modify it under the terms of the GNU Lesser General Public\n# License as published by the Free Software Foundation; either\n# version 2.1 of the License, or (at your option) any later version.\n#\n# This library is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU\n# Lesser General Public License for more details.",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1677",
    "name": "csv-download.py",
    "path": "github_repo/scripts/01_core_ai_analysis/data_processing/csv-download.py",
    "category": "01_core_ai_analysis",
    "type": "youtube",
    "lines": 35,
    "size": 990,
    "docstring": "",
    "keywords": [
      "data_processing",
      "youtube",
      "web_tools"
    ],
    "functions": [
      "read_csv",
      "download_images"
    ],
    "classes": [],
    "imports": [
      "os",
      "pandas",
      "requests"
    ],
    "preview": "import os\n\nimport pandas as pd\nimport requests\n\n\ndef read_csv():\n    # Prompt the user for the path to the CSV file\n    file_path = input(\"Enter the path to the CSV file: \")\n    # Load the CSV data\n    data = pd.read_csv(file_path, header=None, names=[\"date\", \"url\"])\n    return data\n\n\ndef download_images(data):\n    # Prompt the user for the output directory\n    output_directory = input(\"Enter the directory where images should be saved: \")\n    for index, row in data.iterrows():\n        url = row[\"url\"]\n        date = row[\"date\"].replace(\" \", \"_\").replace(\":\", \"-\")",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1678",
    "name": "cp949prober.py",
    "path": "github_repo/scripts/01_core_ai_analysis/data_processing/cp949prober.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 50,
    "size": 1860,
    "docstring": "",
    "keywords": [],
    "functions": [
      "__init__",
      "charset_name",
      "language"
    ],
    "classes": [
      "CP949Prober"
    ],
    "imports": [
      "chardistribution",
      "codingstatemachine",
      "mbcharsetprober",
      "mbcssm"
    ],
    "preview": "######################## BEGIN LICENSE BLOCK ########################\n# The Original Code is mozilla.org code.\n#\n# The Initial Developer of the Original Code is\n# Netscape Communications Corporation.\n# Portions created by the Initial Developer are Copyright (C) 1998\n# the Initial Developer. All Rights Reserved.\n#\n# Contributor(s):\n#   Mark Pilgrim - port to Python\n#\n# This library is free software; you can redistribute it and/or\n# modify it under the terms of the GNU Lesser General Public\n# License as published by the Free Software Foundation; either\n# version 2.1 of the License, or (at your option) any later version.\n#\n# This library is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU\n# Lesser General Public License for more details.",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1679",
    "name": "yt-thumbnail-dl.py",
    "path": "github_repo/scripts/01_core_ai_analysis/data_processing/yt-thumbnail-dl.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 58,
    "size": 1883,
    "docstring": "",
    "keywords": [
      "data_processing",
      "web_tools"
    ],
    "functions": [
      "fetch_video_details"
    ],
    "classes": [],
    "imports": [
      "os",
      "pandas",
      "requests",
      "googleapiclient.discovery"
    ],
    "preview": "import os\n\nimport pandas as pd  # This line is necessary to use pandas in your script\nimport requests\nfrom googleapiclient.discovery import build\n\n# Initialize YouTube API\napi_key = \"AIzaSyCyfGm1oF2syfzfsPeCRHb10tMKmC0dbCo\"\nyoutube = build(\"youtube\", \"v3\", developerKey=api_key)\n\n# Define paths\ncsv_path = \"/Users/steven/etsy-automation/ytube - youtube_videos.csv\"\nthumbnail_dir = \"/Users/steven/Downloads/Misc/Thumbnails\"\nos.makedirs(thumbnail_dir, exist_ok=True)\n\n# Load CSV\ndf = pd.read_csv(csv_path)\n\n# Function to fetch video details and download thumbnail\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1680",
    "name": "ythumb.py",
    "path": "github_repo/scripts/01_core_ai_analysis/data_processing/ythumb.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 46,
    "size": 1506,
    "docstring": "",
    "keywords": [
      "data_processing",
      "youtube",
      "web_tools"
    ],
    "functions": [
      "download_thumbnail"
    ],
    "classes": [],
    "imports": [
      "os",
      "pandas",
      "requests"
    ],
    "preview": "import os\n\nimport pandas as pd\nimport requests\n\n# YTUBE API: AIzaSyCyfGm1oF2syfzfsPeCRHb10tMKmC0dbCo\n# Path to the CSV file\ncsv_path = \"/Users/steven/Downloads/Misc/ytube - youtube_videos.csv\"\n\n# Load the CSV file containing your YouTube video data\ndf = pd.read_csv(csv_path)\n\n# Directory where you want to save the thumbnails\nthumbnail_dir = \"/Users/steven/Downloads/Misc/Thumbnails\"\nos.makedirs(thumbnail_dir, exist_ok=True)\n\n\ndef download_thumbnail(url, video_id):\n    # Construct the URL for the video's max resolution thumbnail\n    thumbnail_url = f\"https://img.youtube.com/vi/{video_id}/maxresdefault.jpg\"",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1681",
    "name": "cleanup2.py",
    "path": "github_repo/scripts/01_core_ai_analysis/data_processing/cleanup2.py",
    "category": "01_core_ai_analysis",
    "type": "organization",
    "lines": 157,
    "size": 5003,
    "docstring": "",
    "keywords": [
      "data_processing"
    ],
    "functions": [
      "is_excluded",
      "prompt_for_csv_file",
      "prompt_for_output_directory",
      "prompt_for_action",
      "remove_duplicates"
    ],
    "classes": [],
    "imports": [
      "csv",
      "os",
      "re",
      "datetime",
      "pandas"
    ],
    "preview": "import csv\nimport os\nimport re\nfrom datetime import datetime\n\nimport pandas as pd\n\n\ndef is_excluded(path, patterns):\n    \"\"\"\n    Check if a given path matches any of the exclusion patterns.\n\n    Parameters:\n    path (str): The path to check.\n    patterns (list): A list of regex patterns for exclusion.\n\n    Returns:\n    bool: True if path matches any pattern, False otherwise.\n    \"\"\"\n    for pattern in patterns:",
    "last_modified": "2025-09-13T05:54:29.070546"
  },
  {
    "id": "1682",
    "name": "expand_prompts_by_style_20250530223233.py",
    "path": "github_repo/scripts/01_core_ai_analysis/data_processing/expand_prompts_by_style_20250530223233.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 56,
    "size": 1990,
    "docstring": "",
    "keywords": [],
    "functions": [
      "collect_all_styles",
      "expand_prompts"
    ],
    "classes": [],
    "imports": [
      "csv",
      "json",
      "pathlib"
    ],
    "preview": "import csv\nimport json\nfrom pathlib import Path\n\nINPUT_CSV = \"/Users/steven/Documents/python/clean/CSV/flexible_analyzed_image_data-05-30-22-21.csv\"\nOUTPUT_CSV = \"/Users/steven/Documents/python/clean/CSV/prompts_expanded_image_data-05-30-22-21.csv\"\nPROMPTS_FIELD = \"design_prompts_json\"\n\n\ndef collect_all_styles(input_csv):\n    styles = set()\n    with open(input_csv, newline=\"\", encoding=\"utf-8\") as infile:\n        reader = csv.DictReader(infile)\n        for row in reader:\n            prompt_json = row.get(PROMPTS_FIELD, \"\")\n            if prompt_json:\n                try:\n                    prompts = json.loads(prompt_json)\n                    for style in prompts.keys():\n                        styles.add(style)",
    "last_modified": "2025-09-13T05:53:27.159445"
  },
  {
    "id": "1683",
    "name": "convert.py",
    "path": "github_repo/scripts/01_core_ai_analysis/data_processing/convert.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 78,
    "size": 2071,
    "docstring": "",
    "keywords": [],
    "functions": [
      "convert_to_colab"
    ],
    "classes": [],
    "imports": [
      "os",
      "re"
    ],
    "preview": "import os\nimport re\n\n\ndef convert_to_colab(python_code):\n    # Install necessary libraries\n    python_code = re.sub(\n        r\"^\\s*#\\s*Install\\s*required\\s*libraries\\s*\",\n        \"\",\n        python_code,\n        flags=re.MULTILINE,\n    )\n    python_code = re.sub(\n        r\"^\\s*!pip\\s*install\\s*(\\w+)\",\n        r\"!pip install \\1\",\n        python_code,\n        flags=re.MULTILINE,\n    )\n\n    # Adjust file paths to be compatible with Colab",
    "last_modified": "2025-09-13T05:53:49.156763"
  },
  {
    "id": "1684",
    "name": "resize-skip-8below2.py",
    "path": "github_repo/scripts/01_core_ai_analysis/data_processing/resize-skip-8below2.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 134,
    "size": 4366,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "get_closest_aspect_ratio",
      "apply_dpi",
      "resize_image",
      "process_batch",
      "process_images",
      "main"
    ],
    "classes": [],
    "imports": [
      "os",
      "time",
      "datetime",
      "PIL",
      "tqdm"
    ],
    "preview": "import os\nimport time\nfrom datetime import datetime\n\nfrom PIL import Image, UnidentifiedImageError\nfrom tqdm import tqdm\n\n# Constants\nMAX_WIDTH, MAX_HEIGHT = 4500, 5400\nTARGET_DPI = 300\nBATCH_SIZE = 50\nPAUSE_DURATION = 5\nMAX_FILE_SIZE_BYTES = 8 * 1024 * 1024  # 8MB in bytes\n\n# Aspect Ratio Minimums\nASPECT_RATIO_MINIMUMS = {\n    \"16:9\": (720, 1280),  # Landscape\n    \"9:16\": (1080, 1920),  # Portrait\n    \"1:1\": (1024, 1024),  # Square\n}",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1685",
    "name": "quantum_media_processor.py",
    "path": "github_repo/scripts/01_core_ai_analysis/data_processing/quantum_media_processor.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 32,
    "size": 1295,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "__init__",
      "_quantum_dct",
      "process_image"
    ],
    "classes": [
      "QuantumMediaProcessor"
    ],
    "imports": [
      "numpy",
      "PIL",
      "scipy.fftpack"
    ],
    "preview": "import numpy as np\nfrom PIL import Image, ImageOps\nfrom scipy.fftpack import dct, idct\n\n\nclass QuantumMediaProcessor:\n    def __init__(self, chaos_factor=0.07):\n        self.chaos_factor = chaos_factor\n\n    def _quantum_dct(self, block):\n        coeffs = dct(dct(block.T, norm=\"ortho\").T, norm=\"ortho\")\n        threshold = np.percentile(np.abs(coeffs), 95) * self.chaos_factor\n        coeffs[np.abs(coeffs) < threshold] *= np.random.choice(\n            [0, 1], p=[0.3, 0.7], size=coeffs.shape\n        )\n        return coeffs\n\n    def process_image(self, image_path, output_path):\n        img = ImageOps.exif_transpose(Image.open(image_path))\n        ycbcr = img.convert(\"YCbCr\")",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1686",
    "name": "resize-skip-8below-og.py",
    "path": "github_repo/scripts/01_core_ai_analysis/data_processing/resize-skip-8below-og.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 134,
    "size": 4444,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "get_closest_aspect_ratio",
      "resize_image",
      "process_batch",
      "process_images",
      "main"
    ],
    "classes": [],
    "imports": [
      "os",
      "time",
      "datetime",
      "PIL"
    ],
    "preview": "import os\nimport time\nfrom datetime import datetime\n\nfrom PIL import Image, UnidentifiedImageError\n\n# Constants\nMAX_WIDTH, MAX_HEIGHT = 4500, 5400\nTARGET_DPI = 300\nBATCH_SIZE = 50\nPAUSE_DURATION = 5\nMIN_FILE_SIZE_BYTES = 8 * 1024 * 1024  # 8MB in bytes\n\n# Aspect Ratio Minimums\nASPECT_RATIO_MINIMUMS = {\n    \"16:9\": (720, 1280),  # Landscape\n    \"9:16\": (1080, 1920),  # Portrait\n    \"1:1\": (1024, 1024),  # Square\n}\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1687",
    "name": "resize-skip-image-fixer.py",
    "path": "github_repo/scripts/01_core_ai_analysis/data_processing/resize-skip-image-fixer.py",
    "category": "01_core_ai_analysis",
    "type": "image_processing",
    "lines": 168,
    "size": 5941,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "get_user_choice",
      "apply_dpi",
      "resize_image",
      "upscale_image",
      "process_batch",
      "process_images",
      "write_log",
      "main"
    ],
    "classes": [],
    "imports": [
      "csv",
      "os",
      "time",
      "datetime",
      "PIL",
      "tqdm"
    ],
    "preview": "import csv\nimport os\nimport time\nfrom datetime import datetime\n\nfrom PIL import Image, UnidentifiedImageError\nfrom tqdm import tqdm\n\n# \ud83d\ude80 Constants\nTARGET_DPI = 300\nUPSCALE_MULTIPLIER = 2  # How much to enlarge small images\nBATCH_SIZE = 50\nPAUSE_DURATION = 3  # Just for dramatic effect \ud83c\udfad\nSIZE_THRESHOLD_MB = 9  # The Holy Grail of size rules\n\n# \ud83d\udcdc Log Data\nlog_data = []\n\n\n# \ud83c\udfc6 Ask the user for the processing mode",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1688",
    "name": "deepseek_python-copy.py",
    "path": "github_repo/scripts/01_core_ai_analysis/data_processing/deepseek_python-copy.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 481,
    "size": 18382,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "__init__",
      "get_system_info",
      "analyze",
      "analyze_file",
      "analyze_path_relationships",
      "get_file_metrics",
      "ast_analysis",
      "run_analysis_tools",
      "complexity_analysis",
      "update_summary"
    ],
    "classes": [
      "AdvancedPythonAnalyzer"
    ],
    "imports": [
      "ast",
      "csv",
      "datetime",
      "json",
      "os",
      "platform",
      "subprocess",
      "sys",
      "collections",
      "matplotlib.pyplot"
    ],
    "preview": "import ast\nimport csv\nimport datetime\nimport json\nimport os\nimport platform\nimport subprocess\nimport sys\nfrom collections import defaultdict\n\nimport matplotlib.pyplot as plt\nimport networkx as nx\nimport radon\nfrom pylint import epylint as lint\nfrom radon.complexity import cc_visit\nfrom radon.metrics import mi_visit\n\n\nclass AdvancedPythonAnalyzer:\n    def __init__(self, directory):",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1689",
    "name": "mydesign-csv-download.py",
    "path": "github_repo/scripts/01_core_ai_analysis/data_processing/mydesign-csv-download.py",
    "category": "01_core_ai_analysis",
    "type": "youtube",
    "lines": 77,
    "size": 2705,
    "docstring": "",
    "keywords": [
      "data_processing",
      "web_tools"
    ],
    "functions": [
      "sanitize_title"
    ],
    "classes": [],
    "imports": [
      "pathlib",
      "pandas",
      "requests"
    ],
    "preview": "from pathlib import Path\n\nimport pandas as pd\nimport requests\n\n\n# Function to sanitize the title to create a valid directory name\ndef sanitize_title(title):\n    return title.replace(\" \", \"_\").replace(\"/\", \"_\").replace(\"|\", \"\").replace(\",\", \"\")\n\n\n# Read the original CSV file\ncsv_file = Path(\"/Users/steven/Documents/python/x-python/mydesigns-export.CSV\")\ndf = pd.read_csv(csv_file)\n\n# Directory where images will be downloaded\nbase_dir = Path(\"/Users/steven/Pictures/etsy/mydesign\")\nbase_dir.mkdir(exist_ok=True)\n\n# Process each row in the DataFrame",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1690",
    "name": "printify_matcher.py",
    "path": "github_repo/scripts/01_core_ai_analysis/data_processing/printify_matcher.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 179,
    "size": 6590,
    "docstring": "",
    "keywords": [
      "opencv",
      "analysis"
    ],
    "functions": [
      "get_dominant_color",
      "detect_text",
      "match_product",
      "process_directory",
      "main"
    ],
    "classes": [],
    "imports": [
      "csv",
      "os",
      "datetime",
      "cv2",
      "numpy",
      "pytesseract",
      "PIL"
    ],
    "preview": "import csv\nimport os\nfrom datetime import datetime\n\nimport cv2\nimport numpy as np\nimport pytesseract\nfrom PIL import Image\n\n# Configure Tesseract for macOS (adjust for your system if needed)\npytesseract.pytesseract.tesseract_cmd = \"/usr/local/bin/tesseract\"\n\n# \ud83c\udfaf Etsy & TikTok Bestsellers\nPLATFORMS = {\n    \"tiktok\": {\n        \"hoodie\": [\"bold colors\", \"dark tones\", \"statement text\"],\n        \"t-shirt\": [\"minimalist\", \"memes\", \"high contrast\"],\n        \"tote bag\": [\"artistic\", \"neutral tones\", \"simple graphics\"],\n        \"phone case\": [\"vibrant\", \"pop culture\", \"sharp details\"],\n        \"sticker\": [\"high contrast\", \"small details\", \"text-heavy\"],",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1691",
    "name": "ascii-python.py",
    "path": "github_repo/scripts/01_core_ai_analysis/data_processing/ascii-python.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 236,
    "size": 8302,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "generate_ascii_art",
      "__init__",
      "get_system_info",
      "analyze",
      "analyze_file",
      "run_pylint",
      "run_analysis_tools",
      "generate_html_report"
    ],
    "classes": [
      "PythonAnalyzer"
    ],
    "imports": [
      "os",
      "ast",
      "sys",
      "json",
      "csv",
      "platform",
      "datetime",
      "subprocess",
      "radon",
      "networkx"
    ],
    "preview": "import os\nimport ast\nimport sys\nimport json\nimport csv\nimport platform\nimport datetime\nimport subprocess\nimport radon\nimport networkx as nx\nimport matplotlib.pyplot as plt\nfrom collections import defaultdict\nfrom radon.complexity import cc_visit\nfrom radon.metrics import mi_visit\n\n# Fixed import - removed problematic epylint import\n# Added ASCII art generation function\ndef generate_ascii_art(text):\n    \"\"\"Generate ASCII art text using simple character patterns\"\"\"\n    art_map = {",
    "last_modified": "2025-05-30T22:44:18.020078"
  },
  {
    "id": "1692",
    "name": "doubles.py",
    "path": "github_repo/scripts/01_core_ai_analysis/data_processing/doubles.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 143,
    "size": 4754,
    "docstring": "",
    "keywords": [
      "data_processing",
      "analysis"
    ],
    "functions": [
      "is_excluded",
      "compute_md5",
      "generate_detailed_duplicate_report",
      "prompt_for_csv_files"
    ],
    "classes": [],
    "imports": [
      "csv",
      "hashlib",
      "os",
      "collections",
      "pandas"
    ],
    "preview": "import csv\nimport hashlib\nimport os\nfrom collections import defaultdict\n\nimport pandas as pd\n\n\ndef is_excluded(path, patterns):\n    \"\"\"\n    Check if a given path matches any of the exclusion patterns.\n\n    Parameters:\n    path (str): The path to check.\n    patterns (list): A list of regex patterns for exclusion.\n\n    Returns:\n    bool: True if path matches any pattern, False otherwise.\n    \"\"\"\n    for pattern in patterns:",
    "last_modified": "2025-09-13T05:54:29.310043"
  },
  {
    "id": "1693",
    "name": "diag_20250530223533.py",
    "path": "github_repo/scripts/01_core_ai_analysis/data_processing/diag_20250530223533.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 55,
    "size": 2012,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "csv",
      "json"
    ],
    "preview": "import csv\nimport json\n\nINPUT_CSV = \"/Users/steven/Documents/python/clean/CSV/flexible_analyzed_image_data-05-30-22-21.csv\"\nOUTPUT_CSV = \"/Users/steven/Documents/python/clean/CSV/prompts_expanded_image_data-05-30-22-21.csv\"\nPROMPTS_FIELD = \"design_prompts_json\"\nLOG_FILE = \"/Users/steven/Documents/python/clean/CSV/prompt_expand_issues.log\"\n\n# First pass: find all unique styles\nall_styles = set()\nrows = []\nwith open(INPUT_CSV, newline=\"\", encoding=\"utf-8\") as infile:\n    reader = csv.DictReader(infile)\n    for row in reader:\n        prompt_json = row.get(PROMPTS_FIELD, \"\")\n        if prompt_json:\n            try:\n                prompts = json.loads(prompt_json)\n                if isinstance(prompts, dict):\n                    all_styles.update(prompts.keys())",
    "last_modified": "2025-09-13T05:53:27.041579"
  },
  {
    "id": "1694",
    "name": "csvsort.py",
    "path": "github_repo/scripts/01_core_ai_analysis/data_processing/csvsort.py",
    "category": "01_core_ai_analysis",
    "type": "organization",
    "lines": 55,
    "size": 1717,
    "docstring": "",
    "keywords": [
      "data_processing",
      "youtube",
      "web_tools"
    ],
    "functions": [
      "safe_filename",
      "download_image"
    ],
    "classes": [],
    "imports": [
      "os",
      "pathlib",
      "urllib.parse",
      "pandas",
      "requests"
    ],
    "preview": "import os\nfrom pathlib import Path\nfrom urllib.parse import urlparse\n\nimport pandas as pd\nimport requests\n\n\ndef safe_filename(title):\n    \"\"\"Create a safe filename from a title.\"\"\"\n    return \"\".join([c if c.isalnum() else \"_\" for c in title])\n\n\ndef download_image(url, path):\n    \"\"\"Download an image from a URL and save it to a path.\"\"\"\n    try:\n        response = requests.get(url, stream=True)\n        if response.status_code == 200:\n            with open(path, \"wb\") as file:\n                for chunk in response.iter_content(1024):",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1695",
    "name": "resize-skip-8below3.py",
    "path": "github_repo/scripts/01_core_ai_analysis/data_processing/resize-skip-8below3.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 177,
    "size": 6353,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "get_closest_aspect_ratio",
      "resize_image_to_target_size",
      "process_image",
      "process_images_in_directory",
      "write_log_to_csv",
      "display_summary",
      "main"
    ],
    "classes": [],
    "imports": [
      "csv",
      "os",
      "datetime",
      "PIL",
      "tqdm"
    ],
    "preview": "import csv\nimport os\nfrom datetime import datetime\n\nfrom PIL import Image, UnidentifiedImageError\nfrom tqdm import tqdm\n\n# Constants\nTARGET_DPI = 300\nTARGET_MIN_FILE_SIZE_MB = 9  # Minimum file size in MB\nTARGET_MAX_FILE_SIZE_MB = 10  # Maximum file size in MB\nASPECT_RATIOS = [(9, 16), (16, 9), (1, 1), (2, 3)]  # Supported aspect ratios\n\n# Initialize log data\nlog_data = []\n\n\n# Function to dynamically calculate the closest aspect ratio\ndef get_closest_aspect_ratio(width, height):\n    current_ratio = width / height",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1696",
    "name": "deepseek_python.py",
    "path": "github_repo/scripts/01_core_ai_analysis/data_processing/deepseek_python.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 489,
    "size": 18399,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "main",
      "__init__",
      "get_system_info",
      "analyze",
      "analyze_file",
      "analyze_path_relationships",
      "get_file_metrics",
      "ast_analysis",
      "run_analysis_tools",
      "complexity_analysis"
    ],
    "classes": [
      "AdvancedPythonAnalyzer"
    ],
    "imports": [
      "ast",
      "csv",
      "datetime",
      "json",
      "os",
      "platform",
      "subprocess",
      "sys",
      "collections",
      "matplotlib.pyplot"
    ],
    "preview": "import ast\nimport csv\nimport datetime\nimport json\nimport os\nimport platform\nimport subprocess\nimport sys\nfrom collections import defaultdict\n\nimport matplotlib.pyplot as plt\nimport networkx as nx\nimport radon\nfrom pylint import epylint as lint\nfrom radon.complexity import cc_visit\nfrom radon.metrics import mi_visit\n\n\nclass AdvancedPythonAnalyzer:\n    def __init__(self, directory):",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1697",
    "name": "tts.py",
    "path": "github_repo/scripts/01_core_ai_analysis/data_processing/tts.py",
    "category": "01_core_ai_analysis",
    "type": "organization",
    "lines": 166,
    "size": 5686,
    "docstring": "Reformat 'As a Man Thinketh' Markdown into clean assets:\n- cleaned .md\n- chapters.jsonl (order, slug, title, text)\n- toc.json\n\nDefaults to: /Users/steven/Documents/AS A MAN THINKETH.md\nLoads secrets from ~/.env (ignored if absent).",
    "keywords": [
      "organization"
    ],
    "functions": [
      "slugify",
      "read_and_normalize",
      "extract_epigraph",
      "find_chapter_spans",
      "clean_for_tts",
      "main"
    ],
    "classes": [],
    "imports": [
      "json",
      "re",
      "sys",
      "pathlib",
      "typing",
      "tqdm",
      "dotenv"
    ],
    "preview": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\nReformat 'As a Man Thinketh' Markdown into clean assets:\n- cleaned .md\n- chapters.jsonl (order, slug, title, text)\n- toc.json\n\nDefaults to: /Users/steven/Documents/AS A MAN THINKETH.md\nLoads secrets from ~/.env (ignored if absent).\n\"\"\"\n\nimport json\nimport re\nimport sys\nfrom pathlib import Path\nfrom typing import List, Tuple\nfrom tqdm import tqdm\n\n# Optional: load ~/.env as you prefer",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1698",
    "name": "mp4-mp4.py",
    "path": "github_repo/scripts/01_core_ai_analysis/data_processing/mp4-mp4.py",
    "category": "01_core_ai_analysis",
    "type": "video_processing",
    "lines": 60,
    "size": 1819,
    "docstring": "",
    "keywords": [
      "opencv",
      "analysis"
    ],
    "functions": [
      "process_frame",
      "process_large_video"
    ],
    "classes": [],
    "imports": [
      "cv2"
    ],
    "preview": "import cv2\n\n\ndef process_frame(frame):\n    \"\"\"\n    Example processing function for large video files.\n    Modify this to apply your custom image processing logic.\n    \"\"\"\n    # Example: Convert frame to grayscale\n    processed_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n    return processed_frame\n\n\ndef process_large_video(input_video_path, output_video_path):\n    \"\"\"\n    Process large MP4 video files efficiently.\n    \"\"\"\n    # Open the video file\n    cap = cv2.VideoCapture(input_video_path)\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1699",
    "name": "suno-csv-card-html-seo1.py",
    "path": "github_repo/scripts/01_core_ai_analysis/data_processing/suno-csv-card-html-seo1.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 121,
    "size": 4202,
    "docstring": "",
    "keywords": [
      "data_processing"
    ],
    "functions": [],
    "classes": [],
    "imports": [
      "os",
      "datetime",
      "pandas"
    ],
    "preview": "import os\nfrom datetime import datetime\n\nimport pandas as pd\n\n# Load and preprocess CSV data\ndf = pd.read_csv(\n    \"/Users/steven/Music/nocTurne MeLoDieS/Song-origins-html/Discography-ALL.csv\"\n).fillna(\"\")\n\n# HTML template\nhtml_template = \"\"\"\n<!DOCTYPE html>\n<html lang=\"en\">\n<head>\n    <meta charset=\"UTF-8\">\n    <title>Avatar Arts Full Discography</title>\n    <style>\n        body {{ font-family: -apple-system, BlinkMacSystemFont, \"Segoe UI\", sans-serif; \n               max-width: 1200px; margin: 0 auto; padding: 2rem; background: #f5f5f5; }}",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1700",
    "name": "analyze 6.py",
    "path": "github_repo/scripts/01_core_ai_analysis/data_processing/analyze 6.py",
    "category": "01_core_ai_analysis",
    "type": "analysis",
    "lines": 65,
    "size": 2019,
    "docstring": "Analyze batches command.",
    "keywords": [],
    "functions": [
      "Args",
      "Run"
    ],
    "classes": [
      "Analyze"
    ],
    "imports": [
      "__future__",
      "googlecloudsdk.api_lib.dataproc",
      "googlecloudsdk.api_lib.dataproc",
      "googlecloudsdk.calliope",
      "googlecloudsdk.command_lib.dataproc"
    ],
    "preview": "# -*- coding: utf-8 -*- #\n# Copyright 2024 Google LLC. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Analyze batches command.\"\"\"\n\nfrom __future__ import absolute_import, division, unicode_literals\n\nfrom googlecloudsdk.api_lib.dataproc import dataproc as dp",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1701",
    "name": "dthumb.py",
    "path": "github_repo/scripts/01_core_ai_analysis/data_processing/dthumb.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 58,
    "size": 1881,
    "docstring": "",
    "keywords": [
      "data_processing",
      "web_tools"
    ],
    "functions": [
      "fetch_video_details"
    ],
    "classes": [],
    "imports": [
      "os",
      "pandas",
      "requests",
      "googleapiclient.discovery"
    ],
    "preview": "import os\n\nimport pandas as pd  # This line is necessary to use pandas in your script\nimport requests\nfrom googleapiclient.discovery import build\n\n# Initialize YouTube API\napi_key = \"AIzaSyCyfGm1oF2syfzfsPeCRHb10tMKmC0dbCo\"\nyoutube = build(\"youtube\", \"v3\", developerKey=api_key)\n\n# Define paths\ncsv_path = \"/Users/steven/etsy-automation/ytube - youtube_videos.csv\"\nthumbnail_dir = \"/Users/steven/Downloads/Misc/Thumbnails\"\nos.makedirs(thumbnail_dir, exist_ok=True)\n\n# Load CSV\ndf = pd.read_csv(csv_path)\n\n# Function to fetch video details and download thumbnail\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1702",
    "name": "extract_topic.py",
    "path": "github_repo/scripts/01_core_ai_analysis/data_processing/extract_topic.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 54,
    "size": 2201,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "__init__",
      "process_data",
      "polly_tts"
    ],
    "classes": [
      "ExtractNews"
    ],
    "imports": [
      "logging",
      "topic.news",
      "tts.polly",
      "utilities.const"
    ],
    "preview": "import logging\n\nfrom topic.news import NEWS\nfrom tts.polly import AudioProcessor\nfrom utilities.const import LOG_PATH, TECH_NEWS\n\nlogging.basicConfig(level=logging.INFO, format=\"[%(levelname)s] %(message)s\", filename=LOG_PATH)\n\n\nclass ExtractNews:\n    def __init__(self, news_url):\n        self.url = news_url\n        self.generated_files = []\n\n    @property\n    def process_data(self):\n        try:\n            news = NEWS(self.url)\n            news_response = news.getnews()\n            logging.info(f\"news_response: {news_response}\")",
    "last_modified": "2025-09-13T05:53:28.738519"
  },
  {
    "id": "1703",
    "name": "sort-tik-etsy-best-seller.py",
    "path": "github_repo/scripts/01_core_ai_analysis/data_processing/sort-tik-etsy-best-seller.py",
    "category": "01_core_ai_analysis",
    "type": "organization",
    "lines": 69,
    "size": 1859,
    "docstring": "",
    "keywords": [
      "data_processing"
    ],
    "functions": [
      "categorize_platform"
    ],
    "classes": [],
    "imports": [
      "os",
      "pandas"
    ],
    "preview": "import os\n\nimport pandas as pd\n\n\n# \ud83d\ude80 Your Custom Sorting Logic \u2013 Because Machines Should Think for You\ndef categorize_platform(filename):\n    \"\"\"Determines if an image belongs on Etsy or TikTok based on filename magic.\"\"\"\n    etsy_keywords = [\n        \"vintage\",\n        \"handmade\",\n        \"rustic\",\n        \"cozy\",\n        \"aesthetic\",\n        \"custom\",\n        \"wedding\",\n        \"gift\",\n        \"doormat\",\n        \"candle\",\n    ]",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1704",
    "name": "cleanup.py",
    "path": "github_repo/scripts/01_core_ai_analysis/data_processing/cleanup.py",
    "category": "01_core_ai_analysis",
    "type": "organization",
    "lines": 123,
    "size": 3922,
    "docstring": "",
    "keywords": [
      "data_processing"
    ],
    "functions": [
      "is_excluded",
      "prompt_for_csv_file",
      "prompt_for_action",
      "remove_duplicates"
    ],
    "classes": [],
    "imports": [
      "csv",
      "os",
      "re",
      "pandas"
    ],
    "preview": "import csv\nimport os\nimport re\n\nimport pandas as pd\n\n\ndef is_excluded(path, patterns):\n    \"\"\"\n    Check if a given path matches any of the exclusion patterns.\n\n    Parameters:\n    path (str): The path to check.\n    patterns (list): A list of regex patterns for exclusion.\n\n    Returns:\n    bool: True if path matches any pattern, False otherwise.\n    \"\"\"\n    for pattern in patterns:\n        if re.search(pattern, path):",
    "last_modified": "2025-09-13T05:54:29.015795"
  },
  {
    "id": "1705",
    "name": "etsycsv.py",
    "path": "github_repo/scripts/01_core_ai_analysis/data_processing/etsycsv.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 36,
    "size": 911,
    "docstring": "",
    "keywords": [
      "data_processing"
    ],
    "functions": [],
    "classes": [],
    "imports": [
      "pandas"
    ],
    "preview": "import pandas as pd\n\n# Assuming 'image_file_names.txt' is the file with image file names, one\n# per line\nfile_path = \"/Users/steven/Documents/etsypng.txt\"\n\n# Read the file and split each line into a list of file names\nwith open(file_path, \"r\") as file:\n    lines = file.readlines()\n    image_lines = [line.strip().split() for line in lines]\n\n# Create a DataFrame to hold the table structure\ncolumns = [\n    \"IMAGE1\",\n    \"IMAGE2\",\n    \"IMAGE3\",\n    \"IMAGE4\",\n    \"IMAGE5\",\n    \"IMAGE6\",\n    \"IMAGE7\",",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1706",
    "name": "docs4.py",
    "path": "github_repo/scripts/01_core_ai_analysis/data_processing/docs4.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 67,
    "size": 2260,
    "docstring": "",
    "keywords": [
      "data_processing"
    ],
    "functions": [
      "combine_csvs"
    ],
    "classes": [],
    "imports": [
      "os",
      "pandas",
      "pandas.errors"
    ],
    "preview": "import os\n\nimport pandas as pd\nfrom pandas.errors import EmptyDataError\n\n\ndef combine_csvs():\n    \"\"\"\n    Reads multiple CSV files from specified paths, concatenates them, and\n    saves the result as 'all_combined.csv' (or whatever you name it).\n    Handles empty or missing files gracefully.\n    \"\"\"\n\n    # List of CSV file paths\n    CSV_PATHS = [\n        \"/Users/steven/clean/clean-csv/combined_csv.csv\",\n        \"/Users/steven/clean/clean-csv/docs-03-28-18-52.csv\",\n        \"/Users/steven/clean/clean-csv/docs-03-28-18-57.csv\",\n        \"/Users/steven/clean/clean-csv/docs-03-29-06-11.csv\",\n        \"/Users/steven/clean/clean-csv/docs-03-29-11-47.csv\",",
    "last_modified": "2025-09-13T05:53:47.806686"
  },
  {
    "id": "1707",
    "name": "bubblespider-amazon-scraper.py",
    "path": "github_repo/scripts/01_core_ai_analysis/data_processing/bubblespider-amazon-scraper.py",
    "category": "01_core_ai_analysis",
    "type": "web_tools",
    "lines": 39,
    "size": 1256,
    "docstring": "",
    "keywords": [],
    "functions": [
      "extract_amazon_urls_to_csv"
    ],
    "classes": [],
    "imports": [
      "csv",
      "os",
      "re"
    ],
    "preview": "import csv\nimport os\nimport re\n\n\ndef extract_amazon_urls_to_csv(input_file_path):\n    # Regular expressions for matching the URLs\n    product_url_pattern = re.compile(r\"https:\\/\\/www\\.amazon\\.com\\/dp\\/[A-Za-z0-9]+\")\n    image_url_pattern = re.compile(\n        r\"https:\\/\\/m\\.media-amazon\\.com\\/images\\/I\\/[A-Za-z0-9_-]+\\.\\w+\"\n    )\n\n    with open(input_file_path, \"r\", encoding=\"utf-8\") as file:\n        content = file.read()\n\n    # Find all matching URLs\n    product_urls = product_url_pattern.findall(content)\n    image_urls = image_url_pattern.findall(content)\n\n    # Prepare the output CSV file name",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1708",
    "name": "gb2312prober.py",
    "path": "github_repo/scripts/01_core_ai_analysis/data_processing/gb2312prober.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 48,
    "size": 1759,
    "docstring": "",
    "keywords": [],
    "functions": [
      "__init__",
      "charset_name",
      "language"
    ],
    "classes": [
      "GB2312Prober"
    ],
    "imports": [
      "chardistribution",
      "codingstatemachine",
      "mbcharsetprober",
      "mbcssm"
    ],
    "preview": "######################## BEGIN LICENSE BLOCK ########################\n# The Original Code is mozilla.org code.\n#\n# The Initial Developer of the Original Code is\n# Netscape Communications Corporation.\n# Portions created by the Initial Developer are Copyright (C) 1998\n# the Initial Developer. All Rights Reserved.\n#\n# Contributor(s):\n#   Mark Pilgrim - port to Python\n#\n# This library is free software; you can redistribute it and/or\n# modify it under the terms of the GNU Lesser General Public\n# License as published by the Free Software Foundation; either\n# version 2.1 of the License, or (at your option) any later version.\n#\n# This library is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU\n# Lesser General Public License for more details.",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1709",
    "name": "process_leonardo_20250102110148.py",
    "path": "github_repo/scripts/01_core_ai_analysis/data_processing/process_leonardo_20250102110148.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 75,
    "size": 2220,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "process_json_to_csv",
      "combine_json_to_csv"
    ],
    "classes": [],
    "imports": [
      "csv",
      "gzip",
      "json",
      "os"
    ],
    "preview": "import csv\nimport gzip\nimport json\nimport os\n\n# Configuration\nOUTPUT_DIR = \"/Users/steven/Downloads/leonardo_images\"\nCSV_FILE = os.path.join(OUTPUT_DIR, \"leonardo_metadata.csv\")\n\n# Headers for CSV\nHEADERS = [\n    \"id\",\n    \"prompt\",\n    \"negativePrompt\",\n    \"motionStrength\",\n    \"createdAt\",\n    \"image_url\",\n    \"motion_url\",\n    \"local_image_path\",\n    \"local_motion_path\",",
    "last_modified": "2025-03-28T18:37:01.035450"
  },
  {
    "id": "1710",
    "name": "printify-organize.py",
    "path": "github_repo/scripts/01_core_ai_analysis/data_processing/printify-organize.py",
    "category": "01_core_ai_analysis",
    "type": "organization",
    "lines": 205,
    "size": 6941,
    "docstring": "",
    "keywords": [
      "opencv",
      "analysis"
    ],
    "functions": [
      "get_dominant_color",
      "detect_text",
      "suggest_product",
      "process_batch",
      "process_images",
      "write_csv",
      "main"
    ],
    "classes": [],
    "imports": [
      "csv",
      "os",
      "time",
      "datetime",
      "cv2",
      "numpy",
      "pytesseract",
      "PIL",
      "tqdm"
    ],
    "preview": "import csv\nimport os\nimport time\nfrom datetime import datetime\n\nimport cv2\nimport numpy as np\nimport pytesseract\nfrom PIL import Image, UnidentifiedImageError\nfrom tqdm import tqdm\n\n# Configure Tesseract for OCR\npytesseract.pytesseract.tesseract_cmd = \"/usr/local/bin/tesseract\"  # Adjust path as needed\n\n# Constants\nBATCH_SIZE = 50\nPAUSE_DURATION = 2  # Pause between batches for readability\nCSV_FIELDS = [\n    \"Platform\",\n    \"File\",",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1711",
    "name": "csvr.py",
    "path": "github_repo/scripts/01_core_ai_analysis/data_processing/csvr.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 52,
    "size": 1731,
    "docstring": "",
    "keywords": [
      "data_processing",
      "web_tools"
    ],
    "functions": [
      "sanitize_title"
    ],
    "classes": [],
    "imports": [
      "pathlib",
      "pandas",
      "requests"
    ],
    "preview": "from pathlib import Path\n\nimport pandas as pd\nimport requests\n\n\ndef sanitize_title(title):\n    # Sanitize the title to create a valid file name\n    return title.replace(\" \", \"_\").replace(\"/\", \"_\").replace(\"|\", \"\").replace(\",\", \"\")\n\n\n# Read the original CSV file\ncsv_file = Path(\"/Users/steven/Downloads/NeAt/Misc/reformatted_mydesigns - Sheet1.csv\")\ndf = pd.read_csv(csv_file)\n\n# Directory where images will be downloaded\nbase_dir = Path(\"/Users/steven/csv2/\")\nbase_dir.mkdir(exist_ok=True)\n\n# Process each row in the DataFrame",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1712",
    "name": "YTubeDLthumbs.py",
    "path": "github_repo/scripts/01_core_ai_analysis/data_processing/YTubeDLthumbs.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 60,
    "size": 1919,
    "docstring": "",
    "keywords": [
      "data_processing",
      "web_tools"
    ],
    "functions": [
      "fetch_video_details"
    ],
    "classes": [],
    "imports": [
      "os",
      "pandas",
      "requests",
      "googleapiclient.discovery"
    ],
    "preview": "import os\n\nimport pandas as pd  # This line is necessary to use pandas in your script\nimport requests\nfrom googleapiclient.discovery import build\n\n# Initialize YouTube API\napi_key = \"AIzaSyCyfGm1oF2syfzfsPeCRHb10tMKmC0dbCo\"\nyoutube = build(\"youtube\", \"v3\", developerKey=api_key)\n\n# Define paths\ncsv_path = \"/Users/steven/Downloads/Misc/ytube - youtube_videos.csv\"\nthumbnail_dir = \"/Users/steven/Downloads/Misc/Thumbnails/d2\"\nos.makedirs(thumbnail_dir, exist_ok=True)\n\n# Load CSV\ndf = pd.read_csv(csv_path)\n\n# Function to fetch video details and download thumbnail\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1713",
    "name": "300dpi.py",
    "path": "github_repo/scripts/01_core_ai_analysis/data_processing/300dpi.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 58,
    "size": 2080,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "set_dpi",
      "process_directory"
    ],
    "classes": [],
    "imports": [
      "os",
      "PIL"
    ],
    "preview": "import os\n\nfrom PIL import Image, ImageResampling\n\n\ndef set_dpi(image_path, output_path, dpi=(300, 300)):\n    \"\"\"\n    Set the DPI of an image and save it to a new file.\n\n    :param image_path: Path to the input image file.\n    :param output_path: Path to save the output image file.\n    :param dpi: Tuple containing the DPI to set for the image.\n    \"\"\"\n    # Open the image file\n    with Image.open(image_path) as img:\n        # Save the image with the new DPI\n        img.save(output_path, dpi=dpi)\n    print(f\"Image saved with {dpi} DPI at {output_path}\")\n\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1714",
    "name": "bak.py",
    "path": "github_repo/scripts/01_core_ai_analysis/data_processing/bak.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 76,
    "size": 3347,
    "docstring": "",
    "keywords": [
      "data_processing",
      "web_tools"
    ],
    "functions": [],
    "classes": [],
    "imports": [
      "base64",
      "pandas",
      "requests"
    ],
    "preview": "import base64\n\nimport pandas as pd\nimport requests\n\n# Set your API credentials\naccess_token = \"eyJ0eXAiOiJKV1QiLCJhbGciOiJSUzI1NiJ9.eyJhdWQiOiIzN2Q0YmQzMDM1ZmUxMWU5YTgwM2FiN2VlYjNjY2M5NyIsImp0aSI6IjQyMmQ1MWFlODI0YjEyMGY2MmY5N2YwZWExOWQ1YzRjMjhlZGM5OTRjYWQ0MmJmNzViOTZlNTdkZWQ3Mzk3YWUyZmQxZDAxMjJhZWRkMTU2IiwiaWF0IjoxNzAyMzYwNjg5LjA0MDcyMywibmJmIjoxNzAyMzYwNjg5LjA0MDcyNiwiZXhwIjoxNzMzOTgzMDg5LjAzNDUzNiwic3ViIjoiMTEzMjQxNTMiLCJzY29wZXMiOlsic2hvcHMubWFuYWdlIiwic2hvcHMucmVhZCIsImNhdGFsb2cucmVhZCIsIm9yZGVycy5yZWFkIiwib3JkZXJzLndyaXRlIiwicHJvZHVjdHMucmVhZCIsInByb2R1Y3RzLndyaXRlIiwid2ViaG9va3MucmVhZCIsIndlYmhvb2tzLndyaXRlIiwidXBsb2Fkcy5yZWFkIiwidXBsb2Fkcy53cml0ZSIsInByaW50X3Byb3ZpZGVycy5yZWFkIl19.AkWVerYdnoGceJShqCIhFpw6o0m7Nz0mqE6moOvuAdy9F4YS-G4rVePuxPp6u9C-y9VxF2pGDYF3yg6kQEo\"\nshop_id = \"6511744\"\n\n# Set the URL for the API endpoints\nbase_url = \"https://api.printify.com/v1\"\nupload_url = f\"{base_url}/uploads/images.json\"\nproduct_url = f\"{base_url}/shops/{shop_id}/products.json\"\n\n# Load the CSV file\ncsv_path = \"product_information.csv\"  # Update this to your CSV file path\nimage_df = pd.read_csv(csv_path)\n\n# Set headers for requests\nheaders = {",
    "last_modified": "2025-09-13T05:53:45.903007"
  },
  {
    "id": "1715",
    "name": "mbcharsetprober.py",
    "path": "github_repo/scripts/01_core_ai_analysis/data_processing/mbcharsetprober.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 96,
    "size": 3715,
    "docstring": "",
    "keywords": [],
    "functions": [
      "__init__",
      "reset",
      "feed",
      "get_confidence"
    ],
    "classes": [
      "MultiByteCharSetProber"
    ],
    "imports": [
      "typing",
      "chardistribution",
      "charsetprober",
      "codingstatemachine",
      "enums"
    ],
    "preview": "######################## BEGIN LICENSE BLOCK ########################\n# The Original Code is Mozilla Universal charset detector code.\n#\n# The Initial Developer of the Original Code is\n# Netscape Communications Corporation.\n# Portions created by the Initial Developer are Copyright (C) 2001\n# the Initial Developer. All Rights Reserved.\n#\n# Contributor(s):\n#   Mark Pilgrim - port to Python\n#   Shy Shalom - original C code\n#   Proofpoint, Inc.\n#\n# This library is free software; you can redistribute it and/or\n# modify it under the terms of the GNU Lesser General Public\n# License as published by the Free Software Foundation; either\n# version 2.1 of the License, or (at your option) any later version.\n#\n# This library is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1716",
    "name": "uploadimages.py",
    "path": "github_repo/scripts/01_core_ai_analysis/data_processing/uploadimages.py",
    "category": "01_core_ai_analysis",
    "type": "youtube",
    "lines": 89,
    "size": 4369,
    "docstring": "",
    "keywords": [
      "data_processing",
      "web_tools"
    ],
    "functions": [],
    "classes": [],
    "imports": [
      "base64",
      "os",
      "pandas",
      "requests"
    ],
    "preview": "import base64\nimport os\n\nimport pandas as pd\nimport requests\n\n# Set your API credentials \naccess_token = \"eyJ0eXAiOiJKV1QiLCJhbGciOiJSUzI1NiJ9.eyJhdWQiOiIzN2Q0YmQzMDM1ZmUxMWU5YTgwM2FiN2VlYjNjY2M5NyIsImp0aSI6IjQyMmQ1MWFlODI0YjEyMGY2MmY5N2YwZWExOWQ1YzRjMjhlZGM5OTRjYWQ0MmJmNzViOTZlNTdkZWQ3Mzk3YWUyZmQxZDAxMjJhZWRkMTU2IiwiaWF0IjoxNzAyMzYwNjg5LjA0MDcyMywibmJmIjoxNzAyMzYwNjg5LjA0MDcyNiwiZXhwIjoxNzMzOTgzMDg5LjAzNDUzNiwic3ViIjoiMTEzMjQxNTMiLCJzY29wZXMiOlsic2hvcHMubWFuYWdlIiwic2hvcHMucmVhZCIsImNhdGFsb2cucmVhZCIsIm9yZGVycy5yZWFkIiwib3JkZXJzLndyaXRlIiwicHJvZHVjdHMucmVhZCIsInByb2R1Y3RzLndyaXRlIiwid2ViaG9va3MucmVhZCIsIndlYmhvb2tzLndyaXRlIiwidXBsb2Fkcy5yZWFkIiwidXBsb2Fkcy53cml0ZSIsInByaW50X3Byb3ZpZGVycy5yZWFkIl19.AkWVerYdnoGceJShqCIhFpw6o0m7Nz0mqE6moOvuAdy9F4YS-G4rVePuxPp6u9C-y9VxF2pGDYF3yg6kQEo\"\n\n# Find your shop ID by running this: curl -X GET https://api.printify.com/v1/shops.json --header \"Authorization: Bearer YOUR_SECRET_KEY\"\n\nshop_id = \"6511744\"\n\n# Set the URL for the API endpoints\nbase_url = \"https://api.printify.com/v1\"\nupload_url = f\"{base_url}/uploads/images.json\"\nproduct_url = f\"{base_url}/shops/{shop_id}/products.json\"\n\n# Load the CSV file\ncsv_path = \"product_information.csv\"  # Update this to your CSV file path",
    "last_modified": "2025-05-04T22:47:11.603355"
  },
  {
    "id": "1717",
    "name": "download_etsy_images.py",
    "path": "github_repo/scripts/01_core_ai_analysis/data_processing/download_etsy_images.py",
    "category": "01_core_ai_analysis",
    "type": "youtube",
    "lines": 92,
    "size": 2953,
    "docstring": "",
    "keywords": [
      "data_processing",
      "analysis",
      "youtube",
      "web_tools"
    ],
    "functions": [
      "download_image",
      "process_csv"
    ],
    "classes": [],
    "imports": [
      "os",
      "pandas",
      "requests"
    ],
    "preview": "import os\n\nimport pandas as pd\nimport requests\n\n# Save all images into this one folder\noutput_dir = \"/Users/steven/Pictures/etsy/MyDesign-IMGs\"\nos.makedirs(output_dir, exist_ok=True)\n\n# Define the CSVs and which image URL columns to use\nfiles_info = {\n    \"cookieshirt\": {\n        \"path\": \"/Users/steven/Pictures/etsy/cookieshirt.CSV\",\n        \"url_columns\": [\n            \"Default_slot_image_url\",\n            \"Digital Image_slot_image_url\",\n            \"Boy_slot_image_url\",\n            \"VID_slot_image_url\",\n            \"boy size_slot_image_url\",\n            \"girl size_slot_image_url\",",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1718",
    "name": "yplaylist 2.py",
    "path": "github_repo/scripts/01_core_ai_analysis/data_processing/yplaylist 2.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 31,
    "size": 942,
    "docstring": "",
    "keywords": [
      "data_processing"
    ],
    "functions": [],
    "classes": [],
    "imports": [
      "pandas",
      "pytube"
    ],
    "preview": "import pandas as pd\nfrom pytube import Playlist\n\ntry:\n    playlist_url = \"https://www.youtube.com/playlist?list=PLfudK7D_bQIjRgAqVU_jkbnb1N5V_pNiG\"\n    playlist = Playlist(playlist_url)\n\n    videos_info = {\n        \"Title\": [],\n        \"Video URL\": [],\n        \"Length (seconds)\": [],\n        \"Views\": [],\n        \"Thumbnail URL\": [],\n        \"Description\": [],\n    }\n\n    for video in playlist.videos:\n        videos_info[\"Title\"].append(video.title)\n        videos_info[\"Video URL\"].append(video.watch_url)\n        videos_info[\"Length (seconds)\"].append(video.length)",
    "last_modified": "2025-09-13T05:54:10.627026"
  },
  {
    "id": "1719",
    "name": "suno-csv-card-html-seo2.py",
    "path": "github_repo/scripts/01_core_ai_analysis/data_processing/suno-csv-card-html-seo2.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 136,
    "size": 3976,
    "docstring": "",
    "keywords": [
      "data_processing"
    ],
    "functions": [],
    "classes": [],
    "imports": [
      "datetime",
      "pandas"
    ],
    "preview": "from datetime import datetime\n\nimport pandas as pd\n\n# Read CSV data\ndf = pd.read_csv(\n    \"/Users/steven/Music/nocTurne MeLoDieS/Song-origins-html/Discography_Reformatted.csv\"\n)\n# Add this after reading the CSV\ndf[\"content\"] = df[\"content\"].fillna(\"\")  # Handle missing content\ndf[\"Analysis\"] = df[\"Analysis\"].fillna(\"\")  # Handle missing analysis\n\n# Then modify the bullet points section:\nbullet_points = (\n    \"\\n\".join(\n        [\n            f\"<li>{bp.strip()}</li>\"\n            for bp in str(row[\"content\"]).split(\"\u2022\")[1:]  # Ensure string conversion\n        ]\n    ),",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1720",
    "name": "suno-csv-card-html-seo.py",
    "path": "github_repo/scripts/01_core_ai_analysis/data_processing/suno-csv-card-html-seo.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 125,
    "size": 4323,
    "docstring": "",
    "keywords": [
      "data_processing"
    ],
    "functions": [],
    "classes": [],
    "imports": [
      "datetime",
      "pandas"
    ],
    "preview": "from datetime import datetime\n\nimport pandas as pd\n\n# Load CSV and fill missing data\ndf = pd.read_csv(\"Discography_Reformatted.csv\").fillna(\"\")\n\n# Add this after reading the CSV\ndf[\"content\"] = df[\"content\"].fillna(\"\").astype(str)\n\n# HTML template (escaped braces for CSS)\nhtml_template = \"\"\"\n<!DOCTYPE html>\n<html lang=\"en\">\n<head>\n    <meta charset=\"UTF-8\">\n    <title>Avatar Arts Full Discography</title>\n    <style>\n        body {{ font-family: -apple-system, BlinkMacSystemFont, \"Segoe UI\", sans-serif; \n               max-width: 1200px; margin: 0 auto; padding: 2rem; background: #f5f5f5; }}",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1721",
    "name": "docs3.py",
    "path": "github_repo/scripts/01_core_ai_analysis/data_processing/docs3.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 115,
    "size": 4879,
    "docstring": "",
    "keywords": [
      "data_processing"
    ],
    "functions": [],
    "classes": [],
    "imports": [
      "pandas"
    ],
    "preview": "import pandas as pd\n\n# ------------------------------------------------------------------------------\n# 1) Read the TWO old CSVs and COMBINE them into one DataFrame\n# ------------------------------------------------------------------------------\n\nOLD_CSV_1 = \"/Users/steven/Documents/DeepSeek/docs-03-29-17_49.csv\"\nOLD_CSV_2 = \"/Users/steven/clean/docs-03-29-17-49.csv\"\nOLD_CSV_3 =\"/Users/steven/clean/clean-csv/docs-03-28-18-52.csv\"\nOLD_CSV_4 =\"/Users/steven/clean/clean-csv/docs-03-28-18-57.csv\"\nOLD_CSV_5 =\"/Users/steven/clean/clean-csv/docs-03-29-06-11.csv\"\nOLD_CSV_6 =\"/Users/steven/clean/clean-csv/docs-03-29-11-47.csv\"\nOLD_CSV_7 =\"/Users/steven/clean/clean-csv/docs-03-29-17_49_1.csv\"\nOLD_CSV_8 =\"/Users/steven/clean/clean-csv/docs-03-29-17_49.csv\"\nOLD_CSV_9 =\"/Users/steven/clean/clean-csv/docs-03-29-17-49.csv\"\nOLD_CSV_10 =\"/Users/steven/clean/clean-csv/python-newcho.csv\"\n\ndf_old_1 = pd.read_csv(OLD_CSV_1)\ndf_old_2 = pd.read_csv(OLD_CSV_2)\n",
    "last_modified": "2025-03-30T13:14:06.848356"
  },
  {
    "id": "1722",
    "name": "resize.py",
    "path": "github_repo/scripts/01_core_ai_analysis/data_processing/resize.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 187,
    "size": 6431,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "get_closest_aspect_ratio",
      "resize_image_to_target_size",
      "process_image",
      "process_images_in_directory",
      "write_log_to_csv",
      "display_summary",
      "main"
    ],
    "classes": [],
    "imports": [
      "csv",
      "os",
      "datetime",
      "PIL",
      "tqdm"
    ],
    "preview": "import csv\nimport os\nfrom datetime import datetime\n\nfrom PIL import Image, UnidentifiedImageError\nfrom tqdm import tqdm\n\n# Constants\nTARGET_DPI = 300\nTARGET_MIN_FILE_SIZE_MB = 5  # Minimum file size in MB\nTARGET_MAX_FILE_SIZE_MB = 9  # Maximum file size in MB\nASPECT_RATIOS = [\n    (9, 16),\n    (16, 9),\n    (1, 1),\n    (2, 3),\n    (3, 2),\n    (4, 5),\n    (5, 4),\n    (1, 2),",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1723",
    "name": "euctwprober.py",
    "path": "github_repo/scripts/01_core_ai_analysis/data_processing/euctwprober.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 48,
    "size": 1753,
    "docstring": "",
    "keywords": [],
    "functions": [
      "__init__",
      "charset_name",
      "language"
    ],
    "classes": [
      "EUCTWProber"
    ],
    "imports": [
      "chardistribution",
      "codingstatemachine",
      "mbcharsetprober",
      "mbcssm"
    ],
    "preview": "######################## BEGIN LICENSE BLOCK ########################\n# The Original Code is mozilla.org code.\n#\n# The Initial Developer of the Original Code is\n# Netscape Communications Corporation.\n# Portions created by the Initial Developer are Copyright (C) 1998\n# the Initial Developer. All Rights Reserved.\n#\n# Contributor(s):\n#   Mark Pilgrim - port to Python\n#\n# This library is free software; you can redistribute it and/or\n# modify it under the terms of the GNU Lesser General Public\n# License as published by the Free Software Foundation; either\n# version 2.1 of the License, or (at your option) any later version.\n#\n# This library is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU\n# Lesser General Public License for more details.",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1724",
    "name": "suno.py",
    "path": "github_repo/scripts/01_core_ai_analysis/data_processing/suno.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 55,
    "size": 2013,
    "docstring": "",
    "keywords": [
      "data_processing"
    ],
    "functions": [],
    "classes": [],
    "imports": [
      "re",
      "pandas"
    ],
    "preview": "import re\n\nimport pandas as pd\n\n# Define the adjusted regex pattern\npattern = r'src=\"([^\"]+)\".*?title=\"([^\"]+)\".*?href=\"([^\"]+)\".*?href=\"([^\"]+)\">([^<]+)'\n\n# Prompt the user for HTML file paths\nprint(\"Enter the paths to your HTML files, separated by commas:\")\nfile_input = input(\"> \")\n\n# Convert the input into a list of file paths\nhtml_files = [path.strip() for path in file_input.split(\",\")]\n\n# Initialize a list to store the extracted results\nresults = []\n\n# Process each file to extract matches\nfor html_file in html_files:\n    try:",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1725",
    "name": "printful.py",
    "path": "github_repo/scripts/01_core_ai_analysis/data_processing/printful.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 82,
    "size": 4226,
    "docstring": "",
    "keywords": [
      "data_processing",
      "web_tools"
    ],
    "functions": [],
    "classes": [],
    "imports": [
      "base64",
      "os",
      "pandas",
      "requests"
    ],
    "preview": "import base64\nimport os\n\nimport pandas as pd\nimport requests\n\n# Set your API credentials\naccess_token = \"eyJ0eXAiOiJKV1QiLCJhbGciOiJSUzI1NiJ9.eyJhdWQiOiIzN2Q0YmQzMDM1ZmUxMWU5YTgwM2FiN2VlYjNjY2M5NyIsImp0aSI6ImVjYjNlYTBlMDk0OWIzZDA1Y2ZiODk4N2EwYzU5NzU5ZTE0NzBmMjIzYzQ0ODViN2M0OWU0YWFkNTY5MGMxODM4MTc5N2Y4Y2RkZDE0MzBjIiwiaWF0IjoxNzM4MTk4MTA4LjkwNDQxMiwibmJmIjoxNzM4MTk4MTA4LjkwNDQxNCwiZXhwIjoxNzY5NzM0MTA4Ljg5NzE0Nywic3ViIjoiMTEzMjQxNTMiLCJzY29wZXMiOlsic2hvcHMubWFuYWdlIiwic2hvcHMucmVhZCIsImNhdGFsb2cucmVhZCIsIm9yZGVycy5yZWFkIiwib3JkZXJzLndyaXRlIiwicHJvZHVjdHMucmVhZCIsInByb2R1Y3RzLndyaXRlIiwid2ViaG9va3MucmVhZCIsIndlYmhvb2tzLndyaXRlIiwidXBsb2Fkcy5yZWFkIiwidXBsb2Fkcy53cml0ZSIsInByaW50X3Byb3ZpZGVycy5yZWFkIiwidXNlci5pbmZvIl19.AHO8pARL74uPg4bASasg_De9T9X50a29R014NE7TwFkPpT2R58TfeCA2Ygsyj4TQSXhg0g54cou2uY0ifJg\"\n\n# Find your shop ID by running this: curl -X GET https://api.printify.com/v1/catalog/blueprints.json --header \"Authorization: Bearer eyJ0eXAiOiJKV1QiLCJhbGciOiJSUzI1NiJ9.eyJhdWQiOiIzN2Q0YmQzMDM1ZmUxMWU5YTgwM2FiN2VlYjNjY2M5NyIsImp0aSI6ImVjYjNlYTBlMDk0OWIzZDA1Y2ZiODk4N2EwYzU5NzU5ZTE0NzBmMjIzYzQ0ODViN2M0OWU0YWFkNTY5MGMxODM4MTc5N2Y4Y2RkZDE0MzBjIiwiaWF0IjoxNzM4MTk4MTA4LjkwNDQxMiwibmJmIjoxNzM4MTk4MTA4LjkwNDQxNCwiZXhwIjoxNzY5NzM0MTA4Ljg5NzE0Nywic3ViIjoiMTEzMjQxNTMiLCJzY29wZXMiOlsic2hvcHMubWFuYWdlIiwic2hvcHMucmVhZCIsImNhdGFsb2cucmVhZCIsIm9yZGVycy5yZWFkIiwib3JkZXJzLndyaXRlIiwicHJvZHVjdHMucmVhZCIsInByb2R1Y3RzLndyaXRlIiwid2ViaG9va3MucmVhZCIsIndlYmhvb2tzLndyaXRlIiwidXBsb2Fkcy5yZWFkIiwidXBsb2Fkcy53cml0ZSIsInByaW50X3Byb3ZpZGVycy5yZWFkIiwidXNlci5pbmZvIl19.AHO8pARL74uPg4bASasg_De9T9X50a29R014NE7TwFkPpT2R58TfeCA2Ygsyj4TQSXhg0g54cou2uY0ifJg\"\n\nshop_id = \"19528660\"\n\n# Set the URL for the API endpoints\nbase_url = \"https://api.printify.com/v1\"\nupload_url = f\"{base_url}/uploads/images.json\"\nproduct_url = f\"{base_url}/shops/{shop_id}/products.json\"\n\n# Load the CSV file\ncsv_path = \"product_information.csv\"  # Update this to your CSV file path",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1726",
    "name": "file_analyzer.py",
    "path": "github_repo/scripts/01_core_ai_analysis/data_processing/file_analyzer.py",
    "category": "01_core_ai_analysis",
    "type": "analysis",
    "lines": 124,
    "size": 4407,
    "docstring": "",
    "keywords": [
      "data_processing",
      "analysis"
    ],
    "functions": [
      "run_fdupes",
      "parse_fdupes_output",
      "run_diff",
      "is_text_file",
      "analyze_text_files",
      "generate_report",
      "main"
    ],
    "classes": [],
    "imports": [
      "subprocess",
      "os",
      "re",
      "pandas",
      "datetime",
      "pathlib"
    ],
    "preview": "import subprocess\nimport os\nimport re\nimport pandas as pd\nfrom datetime import datetime\nfrom pathlib import Path\n\n# Define base directory and file extensions to analyze\nBASE_DIR = \"/Users/steven/SUNO\"\nTEXT_EXTENSIONS = {'.md', '.html', '.txt', '.py', '.json', '.csv'}\nREPORT_FILE = \"suno_file_analysis_report.txt\"\n\ndef run_fdupes(directory):\n    \"\"\"Run fdupes to find duplicate files and return the output.\"\"\"\n    try:\n        result = subprocess.run(\n            ['fdupes', '-r', directory],\n            capture_output=True, text=True, check=True\n        )\n        return result.stdout",
    "last_modified": "2025-10-09T05:27:15.578398"
  },
  {
    "id": "1727",
    "name": "video-ocr.py",
    "path": "github_repo/scripts/01_core_ai_analysis/data_processing/video-ocr.py",
    "category": "01_core_ai_analysis",
    "type": "youtube",
    "lines": 58,
    "size": 2244,
    "docstring": "",
    "keywords": [
      "opencv",
      "analysis"
    ],
    "functions": [
      "extract_text_from_video",
      "process_videos_in_folder"
    ],
    "classes": [],
    "imports": [
      "os",
      "cv2",
      "pytesseract"
    ],
    "preview": "import os\n\nimport cv2\nimport pytesseract\n\n# If you encounter issues with pytesseract finding Tesseract-OCR, specify the path as shown below\n# pytesseract.pytesseract.tesseract_cmd = '/path/to/tesseract'  # Update this path if necessary\n\n\ndef extract_text_from_video(video_path, output_file):\n    \"\"\"\n    Extracts text from the given video file using OCR and saves the text to an output file.\n\n    Args:\n    video_path (str): The path to the video file.\n    output_file (file): An open file object to write the extracted text.\n    \"\"\"\n    cap = cv2.VideoCapture(video_path)\n    frameRate = cap.get(5)  # frame rate\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1728",
    "name": "batch-img.py",
    "path": "github_repo/scripts/01_core_ai_analysis/data_processing/batch-img.py",
    "category": "01_core_ai_analysis",
    "type": "image_processing",
    "lines": 73,
    "size": 2589,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "convert_image_to_jpg",
      "process_batch",
      "batch_process_file_list"
    ],
    "classes": [],
    "imports": [
      "os",
      "PIL"
    ],
    "preview": "import os\n\nfrom PIL import Image\n\n\ndef convert_image_to_jpg(file_path):\n    try:\n        with Image.open(file_path) as img:\n            # Convert to RGB (for JPEG compatibility)\n            rgb_img = img.convert(\"RGB\")\n            new_path = os.path.splitext(file_path)[0] + \".jpg\"\n            rgb_img.save(new_path, \"JPEG\")\n        os.remove(file_path)\n        return new_path\n    except Exception as e:\n        print(f\"\u274c Error converting {file_path}: {e}\")\n        return None\n\n\ndef process_batch(batch, batch_index, total_batches, start_file, total_files):",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1729",
    "name": "YTdownloadthumb copy.py",
    "path": "github_repo/scripts/01_core_ai_analysis/data_processing/YTdownloadthumb copy.py",
    "category": "01_core_ai_analysis",
    "type": "youtube",
    "lines": 58,
    "size": 1881,
    "docstring": "",
    "keywords": [
      "data_processing",
      "web_tools"
    ],
    "functions": [
      "fetch_video_details"
    ],
    "classes": [],
    "imports": [
      "os",
      "pandas",
      "requests",
      "googleapiclient.discovery"
    ],
    "preview": "import os\n\nimport pandas as pd  # This line is necessary to use pandas in your script\nimport requests\nfrom googleapiclient.discovery import build\n\n# Initialize YouTube API\napi_key = \"AIzaSyCyfGm1oF2syfzfsPeCRHb10tMKmC0dbCo\"\nyoutube = build(\"youtube\", \"v3\", developerKey=api_key)\n\n# Define paths\ncsv_path = \"/Users/steven/etsy-automation/ytube - youtube_videos.csv\"\nthumbnail_dir = \"/Users/steven/Downloads/Misc/Thumbnails\"\nos.makedirs(thumbnail_dir, exist_ok=True)\n\n# Load CSV\ndf = pd.read_csv(csv_path)\n\n# Function to fetch video details and download thumbnail\n",
    "last_modified": "2025-05-04T22:47:12"
  },
  {
    "id": "1730",
    "name": "YTdownloadthumb.py",
    "path": "github_repo/scripts/01_core_ai_analysis/data_processing/YTdownloadthumb.py",
    "category": "01_core_ai_analysis",
    "type": "youtube",
    "lines": 57,
    "size": 1880,
    "docstring": "",
    "keywords": [
      "data_processing",
      "web_tools"
    ],
    "functions": [
      "fetch_video_details"
    ],
    "classes": [],
    "imports": [
      "os",
      "pandas",
      "requests",
      "googleapiclient.discovery"
    ],
    "preview": "import os\n\nimport pandas as pd  # This line is necessary to use pandas in your script\nimport requests\nfrom googleapiclient.discovery import build\n\n# Initialize YouTube API\napi_key = \"AIzaSyCyfGm1oF2syfzfsPeCRHb10tMKmC0dbCo\"\nyoutube = build(\"youtube\", \"v3\", developerKey=api_key)\n\n# Define paths\ncsv_path = \"/Users/steven/etsy-automation/ytube - youtube_videos.csv\"\nthumbnail_dir = \"/Users/steven/Downloads/Misc/Thumbnails\"\nos.makedirs(thumbnail_dir, exist_ok=True)\n\n# Load CSV\ndf = pd.read_csv(csv_path)\n\n\n# Function to fetch video details and download thumbnail",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1731",
    "name": "taggy.py",
    "path": "github_repo/scripts/01_core_ai_analysis/data_processing/taggy.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 153,
    "size": 5230,
    "docstring": "",
    "keywords": [
      "opencv",
      "web_tools"
    ],
    "functions": [
      "get_clarifai_tags",
      "get_rekognition_tags",
      "get_imagga_tags",
      "extract_video_frame",
      "extract_and_tag"
    ],
    "classes": [],
    "imports": [
      "csv",
      "os",
      "subprocess",
      "tempfile",
      "zipfile",
      "boto3",
      "cv2",
      "requests",
      "clarifai.client.model",
      "dotenv"
    ],
    "preview": "import csv\nimport os\nimport subprocess\nimport tempfile\nimport zipfile\n\nimport boto3\nimport cv2\nimport requests\nfrom clarifai.client.model import Model\nfrom dotenv import load_dotenv\nfrom PIL import Image\n\nload_dotenv(os.path.expanduser(\"~/.env\"))\n\n# API keys\nCLARIFAI_PAT = os.getenv(\"CLARIFAI_PAT\")\nAWS_ACCESS_KEY_ID = os.getenv(\"AWS_ACCESS_KEY_ID\")\nAWS_SECRET_ACCESS_KEY = os.getenv(\"AWS_SECRET_ACCESS_KEY\")\nAWS_REGION = os.getenv(\"AWS_REGION\")",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1732",
    "name": "diag.py",
    "path": "github_repo/scripts/01_core_ai_analysis/data_processing/diag.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 55,
    "size": 2012,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "csv",
      "json"
    ],
    "preview": "import csv\nimport json\n\nINPUT_CSV = \"/Users/steven/Documents/python/clean/CSV/flexible_analyzed_image_data-05-30-22-21.csv\"\nOUTPUT_CSV = \"/Users/steven/Documents/python/clean/CSV/prompts_expanded_image_data-05-30-22-21.csv\"\nPROMPTS_FIELD = \"design_prompts_json\"\nLOG_FILE = \"/Users/steven/Documents/python/clean/CSV/prompt_expand_issues.log\"\n\n# First pass: find all unique styles\nall_styles = set()\nrows = []\nwith open(INPUT_CSV, newline=\"\", encoding=\"utf-8\") as infile:\n    reader = csv.DictReader(infile)\n    for row in reader:\n        prompt_json = row.get(PROMPTS_FIELD, \"\")\n        if prompt_json:\n            try:\n                prompts = json.loads(prompt_json)\n                if isinstance(prompts, dict):\n                    all_styles.update(prompts.keys())",
    "last_modified": "2025-08-02T18:25:57.950990"
  },
  {
    "id": "1733",
    "name": "yt-upload.py",
    "path": "github_repo/scripts/01_core_ai_analysis/data_processing/yt-upload.py",
    "category": "01_core_ai_analysis",
    "type": "youtube",
    "lines": 81,
    "size": 2423,
    "docstring": "",
    "keywords": [
      "data_processing",
      "youtube"
    ],
    "functions": [
      "authenticate",
      "upload_video",
      "main"
    ],
    "classes": [],
    "imports": [
      "os",
      "google_auth_oauthlib.flow",
      "googleapiclient.discovery",
      "googleapiclient.errors",
      "pandas"
    ],
    "preview": "import os\n\nimport google_auth_oauthlib.flow\nimport googleapiclient.discovery\nimport googleapiclient.errors\nimport pandas as pd\n\n# Path to the client secrets file\nCLIENT_SECRETS_FILE = \"/Users/steven/Documents/Python/Youtube/clientsecret.json\"\n\n# Define the required scope for uploading videos\nSCOPES = [\"https://www.googleapis.com/auth/youtube.upload\"]\n\n\ndef authenticate():\n    \"\"\"\n    Authenticates the user and returns a YouTube API client.\n    \"\"\"\n    # Get credentials and create an API client\n    flow = google_auth_oauthlib.flow.InstalledAppFlow.from_client_secrets_file(",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1734",
    "name": "suno 1.py",
    "path": "github_repo/scripts/01_core_ai_analysis/data_processing/suno 1.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 55,
    "size": 2013,
    "docstring": "",
    "keywords": [
      "data_processing"
    ],
    "functions": [],
    "classes": [],
    "imports": [
      "re",
      "pandas"
    ],
    "preview": "import re\n\nimport pandas as pd\n\n# Define the adjusted regex pattern\npattern = r'src=\"([^\"]+)\".*?title=\"([^\"]+)\".*?href=\"([^\"]+)\".*?href=\"([^\"]+)\">([^<]+)'\n\n# Prompt the user for HTML file paths\nprint(\"Enter the paths to your HTML files, separated by commas:\")\nfile_input = input(\"> \")\n\n# Convert the input into a list of file paths\nhtml_files = [path.strip() for path in file_input.split(\",\")]\n\n# Initialize a list to store the extracted results\nresults = []\n\n# Process each file to extract matches\nfor html_file in html_files:\n    try:",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1735",
    "name": "taggy 1.py",
    "path": "github_repo/scripts/01_core_ai_analysis/data_processing/taggy 1.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 153,
    "size": 5230,
    "docstring": "",
    "keywords": [
      "opencv",
      "web_tools"
    ],
    "functions": [
      "get_clarifai_tags",
      "get_rekognition_tags",
      "get_imagga_tags",
      "extract_video_frame",
      "extract_and_tag"
    ],
    "classes": [],
    "imports": [
      "csv",
      "os",
      "subprocess",
      "tempfile",
      "zipfile",
      "boto3",
      "cv2",
      "requests",
      "clarifai.client.model",
      "dotenv"
    ],
    "preview": "import csv\nimport os\nimport subprocess\nimport tempfile\nimport zipfile\n\nimport boto3\nimport cv2\nimport requests\nfrom clarifai.client.model import Model\nfrom dotenv import load_dotenv\nfrom PIL import Image\n\nload_dotenv(os.path.expanduser(\"~/.env\"))\n\n# API keys\nCLARIFAI_PAT = os.getenv(\"CLARIFAI_PAT\")\nAWS_ACCESS_KEY_ID = os.getenv(\"AWS_ACCESS_KEY_ID\")\nAWS_SECRET_ACCESS_KEY = os.getenv(\"AWS_SECRET_ACCESS_KEY\")\nAWS_REGION = os.getenv(\"AWS_REGION\")",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1736",
    "name": "gdrive.py",
    "path": "github_repo/scripts/01_core_ai_analysis/data_processing/gdrive.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 72,
    "size": 2066,
    "docstring": "",
    "keywords": [],
    "functions": [
      "convert_to_colab"
    ],
    "classes": [],
    "imports": [
      "os",
      "re"
    ],
    "preview": "import os\nimport re\n\n\ndef convert_to_colab(file_path, google_drive_path):\n    # Read the content of the specified file\n    with open(file_path, \"r\") as file:\n        python_code = file.read()\n\n    # Install necessary libraries\n    python_code = re.sub(\n        r\"^\\s*#\\s*Install\\s*required\\s*libraries\\s*\",\n        \"\",\n        python_code,\n        flags=re.MULTILINE,\n    )\n    python_code = re.sub(\n        r\"^\\s*!pip\\s*install\\s*(\\w+)\",\n        r\"!pip install \\1\",\n        python_code,",
    "last_modified": "2025-09-13T05:53:49.178052"
  },
  {
    "id": "1737",
    "name": "csv.py",
    "path": "github_repo/scripts/01_core_ai_analysis/data_processing/csv.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 27,
    "size": 627,
    "docstring": "",
    "keywords": [
      "data_processing"
    ],
    "functions": [
      "load_excel",
      "check_missing_data"
    ],
    "classes": [],
    "imports": [
      "pandas"
    ],
    "preview": "import pandas as pd\n\n\ndef load_excel(file_path):\n    try:\n        df = pd.read_excel(file_path)\n        print(\"Excel file loaded successfully.\")\n        return df\n    except Exception as e:\n        print(f\"Error loading Excel file: {e}\")\n        return None\n\n\ndef check_missing_data(df):\n    if df is not None:\n        print(\"Missing data in each column:\")\n        print(df.isnull().sum())\n    else:\n        print(\"DataFrame is empty or not loaded.\")\n",
    "last_modified": "2025-05-04T22:47:13.345048"
  },
  {
    "id": "1738",
    "name": "text.py",
    "path": "github_repo/scripts/01_core_ai_analysis/data_processing/text.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 59,
    "size": 2238,
    "docstring": "",
    "keywords": [
      "opencv",
      "analysis"
    ],
    "functions": [
      "extract_text_from_video",
      "process_videos_in_folder"
    ],
    "classes": [],
    "imports": [
      "os",
      "cv2",
      "pytesseract"
    ],
    "preview": "import os\n\nimport cv2\nimport pytesseract\n\n# If you encounter issues with pytesseract finding Tesseract-OCR, specify the path as shown below\n# pytesseract.pytesseract.tesseract_cmd = '/path/to/tesseract'  # Update\n# this path if necessary\n\n\ndef extract_text_from_video(video_path, output_file):\n    \"\"\"\n    Extracts text from the given video file using OCR and saves the text to an output file.\n\n    Args:\n    video_path (str): The path to the video file.\n    output_file (file): An open file object to write the extracted text.\n    \"\"\"\n    cap = cv2.VideoCapture(video_path)\n    frameRate = cap.get(5)  # frame rate",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1739",
    "name": "dthumb2.py",
    "path": "github_repo/scripts/01_core_ai_analysis/data_processing/dthumb2.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 58,
    "size": 1913,
    "docstring": "",
    "keywords": [
      "data_processing",
      "web_tools"
    ],
    "functions": [
      "fetch_video_details"
    ],
    "classes": [],
    "imports": [
      "os",
      "pandas",
      "requests",
      "googleapiclient.discovery"
    ],
    "preview": "import os\n\nimport pandas as pd  # This line is necessary to use pandas in your script\nimport requests\nfrom googleapiclient.discovery import build\n\n# Initialize YouTube API\napi_key = \"AIzaSyCyfGm1oF2syfzfsPeCRHb10tMKmC0dbCo\"\nyoutube = build(\"youtube\", \"v3\", developerKey=api_key)\n\n# Define paths\ncsv_path = \"/Users/steven/Downloads/Misc/ytube - youtube_videos.csv\"\nthumbnail_dir = \"/Users/steven/Downloads/Misc/Thumbnails/d2\"\nos.makedirs(thumbnail_dir, exist_ok=True)\n\n# Load CSV\ndf = pd.read_csv(csv_path)\n\n# Function to fetch video details and download thumbnail\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1740",
    "name": "upscaleuploadimages.py",
    "path": "github_repo/scripts/01_core_ai_analysis/data_processing/upscaleuploadimages.py",
    "category": "01_core_ai_analysis",
    "type": "youtube",
    "lines": 82,
    "size": 2857,
    "docstring": "",
    "keywords": [
      "data_processing",
      "web_tools"
    ],
    "functions": [],
    "classes": [],
    "imports": [
      "base64",
      "os",
      "pandas",
      "requests"
    ],
    "preview": "import base64\nimport os\n\nimport pandas as pd\nimport requests\n\n# Set your API credentials\naccess_token = \"your_printful_api_key\"\n\n# Find your shop ID by running this: curl -X GET https://api.printify.com/v1/shops.json --header \"Authorization: Bearer YOUR_PRINTIFY_API_KEY\"\n\nshop_id = \"your_shop_Id\"\n\n# Set the URL for the API endpoints\nbase_url = \"https://api.printify.com/v1\"\nupload_url = f\"{base_url}/uploads/images.json\"\nproduct_url = f\"{base_url}/shops/{shop_id}/products.json\"\n\n# Load the CSV file\ncsv_path = \"product_information.csv\"  # Update this to your CSV file path",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1741",
    "name": "YTubeDLthumbs copy 2.py",
    "path": "github_repo/scripts/01_core_ai_analysis/data_processing/YTubeDLthumbs copy 2.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 58,
    "size": 1913,
    "docstring": "",
    "keywords": [
      "data_processing",
      "web_tools"
    ],
    "functions": [
      "fetch_video_details"
    ],
    "classes": [],
    "imports": [
      "os",
      "pandas",
      "requests",
      "googleapiclient.discovery"
    ],
    "preview": "import os\n\nimport pandas as pd  # This line is necessary to use pandas in your script\nimport requests\nfrom googleapiclient.discovery import build\n\n# Initialize YouTube API\napi_key = \"AIzaSyCyfGm1oF2syfzfsPeCRHb10tMKmC0dbCo\"\nyoutube = build(\"youtube\", \"v3\", developerKey=api_key)\n\n# Define paths\ncsv_path = \"/Users/steven/Downloads/Misc/ytube - youtube_videos.csv\"\nthumbnail_dir = \"/Users/steven/Downloads/Misc/Thumbnails/d2\"\nos.makedirs(thumbnail_dir, exist_ok=True)\n\n# Load CSV\ndf = pd.read_csv(csv_path)\n\n# Function to fetch video details and download thumbnail\n",
    "last_modified": "2025-09-13T05:54:07.361191"
  },
  {
    "id": "1742",
    "name": "YTdownloadthumb copy 2.py",
    "path": "github_repo/scripts/01_core_ai_analysis/data_processing/YTdownloadthumb copy 2.py",
    "category": "01_core_ai_analysis",
    "type": "youtube",
    "lines": 58,
    "size": 1881,
    "docstring": "",
    "keywords": [
      "data_processing",
      "web_tools"
    ],
    "functions": [
      "fetch_video_details"
    ],
    "classes": [],
    "imports": [
      "os",
      "pandas",
      "requests",
      "googleapiclient.discovery"
    ],
    "preview": "import os\n\nimport pandas as pd  # This line is necessary to use pandas in your script\nimport requests\nfrom googleapiclient.discovery import build\n\n# Initialize YouTube API\napi_key = \"AIzaSyCyfGm1oF2syfzfsPeCRHb10tMKmC0dbCo\"\nyoutube = build(\"youtube\", \"v3\", developerKey=api_key)\n\n# Define paths\ncsv_path = \"/Users/steven/etsy-automation/ytube - youtube_videos.csv\"\nthumbnail_dir = \"/Users/steven/Downloads/Misc/Thumbnails\"\nos.makedirs(thumbnail_dir, exist_ok=True)\n\n# Load CSV\ndf = pd.read_csv(csv_path)\n\n# Function to fetch video details and download thumbnail\n",
    "last_modified": "2025-08-06T13:57:29.941892"
  },
  {
    "id": "1743",
    "name": "upscaler.py",
    "path": "github_repo/scripts/01_core_ai_analysis/data_processing/upscaler.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 168,
    "size": 5941,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "get_user_choice",
      "apply_dpi",
      "resize_image",
      "upscale_image",
      "process_batch",
      "process_images",
      "write_log",
      "main"
    ],
    "classes": [],
    "imports": [
      "csv",
      "os",
      "time",
      "datetime",
      "PIL",
      "tqdm"
    ],
    "preview": "import csv\nimport os\nimport time\nfrom datetime import datetime\n\nfrom PIL import Image, UnidentifiedImageError\nfrom tqdm import tqdm\n\n# \ud83d\ude80 Constants\nTARGET_DPI = 300\nUPSCALE_MULTIPLIER = 2  # How much to enlarge small images\nBATCH_SIZE = 50\nPAUSE_DURATION = 3  # Just for dramatic effect \ud83c\udfad\nSIZE_THRESHOLD_MB = 9  # The Holy Grail of size rules\n\n# \ud83d\udcdc Log Data\nlog_data = []\n\n\n# \ud83c\udfc6 Ask the user for the processing mode",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1744",
    "name": "2mentest.py",
    "path": "github_repo/scripts/01_core_ai_analysis/data_processing/2mentest.py",
    "category": "01_core_ai_analysis",
    "type": "testing",
    "lines": 39,
    "size": 1194,
    "docstring": "",
    "keywords": [],
    "functions": [
      "extract_sitemap_data",
      "main"
    ],
    "classes": [],
    "imports": [
      "random",
      "xml.etree.ElementTree"
    ],
    "preview": "import random\nimport xml.etree.ElementTree as ET\n\n\ndef extract_sitemap_data(xml_file_path, num_urls=200):\n    tree = ET.parse(xml_file_path)\n    root = tree.getroot()\n\n    namespaces = {\n        \"ns\": \"http://www.sitemaps.org/schemas/sitemap/0.9\",\n        \"image\": \"http://www.google.com/schemas/sitemap-image/1.1\",\n    }\n\n    all_products = []\n    for url in root.findall(\"ns:url\", namespaces):\n        loc = url.find(\"ns:loc\", namespaces).text\n        image = url.find(\"image:image\", namespaces)\n        if image is not None:\n            image_loc = image.find(\"image:loc\", namespaces).text\n            image_title = image.find(\"image:title\", namespaces).text",
    "last_modified": "2025-05-06T04:35:15.041354"
  },
  {
    "id": "1745",
    "name": "ythumb copy.py",
    "path": "github_repo/scripts/01_core_ai_analysis/data_processing/ythumb copy.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 47,
    "size": 1512,
    "docstring": "",
    "keywords": [
      "data_processing",
      "youtube",
      "web_tools"
    ],
    "functions": [
      "download_thumbnail"
    ],
    "classes": [],
    "imports": [
      "os",
      "pandas",
      "requests"
    ],
    "preview": "import os\n\nimport pandas as pd\nimport requests\n\n# YTUBE API: AIzaSyCyfGm1oF2syfzfsPeCRHb10tMKmC0dbCo\n# Path to the CSV file\ncsv_path = \"/Users/steven/Downloads/Misc/ytube - youtube_videos.csv\"\n\n# Load the CSV file containing your YouTube video data\ndf = pd.read_csv(csv_path)\n\n# Directory where you want to save the thumbnails\nthumbnail_dir = \"/Users/steven/Downloads/Misc/Thumbnails\"\nos.makedirs(thumbnail_dir, exist_ok=True)\n\n\ndef download_thumbnail(url, video_id):\n    # Construct the URL for the video's max resolution thumbnail\n    thumbnail_url = f\"https://img.youtube.com/vi/{video_id}/maxresdefault.jpg\"",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1746",
    "name": "song-info.py",
    "path": "github_repo/scripts/01_core_ai_analysis/data_processing/song-info.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 34,
    "size": 891,
    "docstring": "",
    "keywords": [
      "data_processing"
    ],
    "functions": [],
    "classes": [],
    "imports": [
      "re",
      "pandas",
      "ace_tools"
    ],
    "preview": "import re\n\nimport pandas as pd\n\n# Load HTML content from file\nfile_path = \"/Users/steven/Music/suno/1.html\"\nwith open(file_path, \"r\", encoding=\"utf-8\") as file:\n    html_content = file.read()\n\n# Adjusted regex pattern to extract src, title, song_href, style_href, and style\npattern = r'src=\"([^\"]+)\".*?title=\"([^\"]+)\".*?href=\"([^\"]+)\".*?href=\"([^\"]+)\">([^<]+)'\n\n# Extract matches\nmatches = re.findall(pattern, html_content)\n\n# Prepare results for output\nresults = []\nfor match in matches:\n    results.append(\n        {",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1747",
    "name": "extract_webm_paths.py",
    "path": "github_repo/scripts/01_core_ai_analysis/data_processing/extract_webm_paths.py",
    "category": "01_core_ai_analysis",
    "type": "web_tools",
    "lines": 23,
    "size": 672,
    "docstring": "",
    "keywords": [
      "data_processing"
    ],
    "functions": [],
    "classes": [],
    "imports": [
      "pandas"
    ],
    "preview": "import pandas as pd\n\n# Path to your input CSV file\ncsv_path = \"/Users/steven/vids-12-05-00:16.csv\"\n\n# Output paths for the new CSV and TXT files\noutput_csv = \"/Users/steven/webm_files.csv\"\noutput_txt = \"/Users/steven/webm_files.txt\"\n\n# Load the CSV file\ndata = pd.read_csv(csv_path)\n\n# Filter rows where the file path ends with `.webm`\nwebm_files = data[data[\"Original Path\"].str.endswith(\".webm\", na=False)]\n\n# Save to a new CSV file\nwebm_files[[\"Original Path\"]].to_csv(output_csv, index=False)\n\n# Save to a TXT file\nwebm_files[\"Original Path\"].to_csv(output_txt, index=False, header=False)",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1748",
    "name": "htmlouts.py",
    "path": "github_repo/scripts/01_core_ai_analysis/data_processing/htmlouts.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 116,
    "size": 3751,
    "docstring": "",
    "keywords": [
      "data_processing",
      "analysis"
    ],
    "functions": [
      "group_files_by_title",
      "generate_html_and_csv",
      "main"
    ],
    "classes": [],
    "imports": [
      "os",
      "collections",
      "pandas"
    ],
    "preview": "import os\nfrom collections import defaultdict\n\nimport pandas as pd\n\n\ndef group_files_by_title(directory):\n    \"\"\"\n    Groups files by base title (ignoring version-specific suffixes).\n    \"\"\"\n    file_groups = defaultdict(list)\n    for root, _, files in os.walk(directory):\n        for file in files:\n            if file.endswith((\".mp3\", \".txt\")):\n                base_title = (\n                    os.path.splitext(file)[0]\n                    .split(\"_analysis\")[0]\n                    .split(\"_lyrics\")[0]\n                    .split(\"_song\")[0]\n                    .strip()",
    "last_modified": "2025-09-13T05:54:54.780461"
  },
  {
    "id": "1749",
    "name": "sorty.py",
    "path": "github_repo/scripts/01_core_ai_analysis/data_processing/sorty.py",
    "category": "01_core_ai_analysis",
    "type": "organization",
    "lines": 69,
    "size": 1859,
    "docstring": "",
    "keywords": [
      "data_processing"
    ],
    "functions": [
      "categorize_platform"
    ],
    "classes": [],
    "imports": [
      "os",
      "pandas"
    ],
    "preview": "import os\n\nimport pandas as pd\n\n\n# \ud83d\ude80 Your Custom Sorting Logic \u2013 Because Machines Should Think for You\ndef categorize_platform(filename):\n    \"\"\"Determines if an image belongs on Etsy or TikTok based on filename magic.\"\"\"\n    etsy_keywords = [\n        \"vintage\",\n        \"handmade\",\n        \"rustic\",\n        \"cozy\",\n        \"aesthetic\",\n        \"custom\",\n        \"wedding\",\n        \"gift\",\n        \"doormat\",\n        \"candle\",\n    ]",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1750",
    "name": "deepseek_python_20250608130553.py",
    "path": "github_repo/scripts/01_core_ai_analysis/data_processing/deepseek_python_20250608130553.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 481,
    "size": 18382,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "__init__",
      "get_system_info",
      "analyze",
      "analyze_file",
      "analyze_path_relationships",
      "get_file_metrics",
      "ast_analysis",
      "run_analysis_tools",
      "complexity_analysis",
      "update_summary"
    ],
    "classes": [
      "AdvancedPythonAnalyzer"
    ],
    "imports": [
      "ast",
      "csv",
      "datetime",
      "json",
      "os",
      "platform",
      "subprocess",
      "sys",
      "collections",
      "matplotlib.pyplot"
    ],
    "preview": "import ast\nimport csv\nimport datetime\nimport json\nimport os\nimport platform\nimport subprocess\nimport sys\nfrom collections import defaultdict\n\nimport matplotlib.pyplot as plt\nimport networkx as nx\nimport radon\nfrom pylint import epylint as lint\nfrom radon.complexity import cc_visit\nfrom radon.metrics import mi_visit\n\n\nclass AdvancedPythonAnalyzer:\n    def __init__(self, directory):",
    "last_modified": "2025-09-13T05:53:27.850984"
  },
  {
    "id": "1751",
    "name": "9mb-up.py",
    "path": "github_repo/scripts/01_core_ai_analysis/data_processing/9mb-up.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 172,
    "size": 5516,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "setup_logging",
      "get_user_choice",
      "resize_image",
      "upscale_image",
      "process_batch",
      "process_images",
      "write_log",
      "main"
    ],
    "classes": [],
    "imports": [
      "csv",
      "logging",
      "os",
      "time",
      "datetime",
      "PIL",
      "tqdm"
    ],
    "preview": "#!/usr/bin/env python3\nimport csv\nimport logging\nimport os\nimport time\nfrom datetime import datetime\n\nfrom PIL import Image, UnidentifiedImageError\nfrom tqdm import tqdm\n\n# \ud83d\ude80 Constants\nTARGET_DPI = 300\nUPSCALE_MULTIPLIER = 2  # How much to enlarge small images\nBATCH_SIZE = 50\nPAUSE_DURATION = 3  # Seconds between batches\nSIZE_THRESHOLD_MB = 9  # Max image size threshold\nMAX_IMAGE_SIZE_MB = 9  # Holy grail max size\n\n\n# Configure logging",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1752",
    "name": "autodownloaderUI.py",
    "path": "github_repo/scripts/01_core_ai_analysis/data_processing/autodownloaderUI.py",
    "category": "01_core_ai_analysis",
    "type": "youtube",
    "lines": 371,
    "size": 14813,
    "docstring": "",
    "keywords": [
      "organization"
    ],
    "functions": [
      "cleanDatabase",
      "deleteClipsForFilter",
      "__init__",
      "closeEvent",
      "openFinishedVids",
      "openClipBin",
      "addNewFTPUser",
      "deleteFTPUser",
      "updateAccountInfo",
      "populateRemoveUserList"
    ],
    "classes": [
      "PassiveDownloaderWindow"
    ],
    "imports": [
      "os",
      "pickle",
      "sys",
      "threading",
      "time",
      "autodownloader",
      "database",
      "scriptwrapper",
      "server",
      "settings"
    ],
    "preview": "import os\nimport pickle\nimport sys\nfrom threading import Thread\nfrom time import sleep\n\nimport autodownloader\nimport database\nimport scriptwrapper\nimport server\nimport settings\nimport tiktok\nfrom filtercreator import FilterCreationWindow\nfrom PyQt5 import QtCore, QtGui, QtWidgets, uic\nfrom PyQt5.QtCore import *\nfrom PyQt5.QtCore import QDir, QObject, QPoint, QRect, Qt, QUrl, pyqtSignal\nfrom PyQt5.QtGui import QIcon\nfrom PyQt5.QtMultimedia import (\n    QAbstractVideoBuffer,\n    QAbstractVideoSurface,",
    "last_modified": "2025-09-13T05:53:31.800705"
  },
  {
    "id": "1753",
    "name": "quick_demo.py",
    "path": "github_repo/scripts/01_core_ai_analysis/data_processing/quick_demo.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 113,
    "size": 3745,
    "docstring": "quick_demo.ipynb",
    "keywords": [],
    "functions": [
      "on_change"
    ],
    "classes": [],
    "imports": [
      "glob",
      "ipywidgets",
      "matplotlib.pyplot",
      "os",
      "sys",
      "base64",
      "IPython.display"
    ],
    "preview": "# -*- coding: utf-8 -*-\n\"\"\"quick_demo.ipynb\n\nAutomatically generated by Colab.\n\nOriginal file is located at\n    https://colab.research.google.com/github/Winfredy/SadTalker/blob/main/quick_demo.ipynb\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Winfredy/SadTalker/blob/main/quick_demo.ipynb)\n\n### SadTalker\uff1aLearning Realistic 3D Motion Coefficients for Stylized Audio-Driven Single Image Talking Face Animation\n\n[arxiv](https://arxiv.org/abs/2211.12194) | [project](https://sadtalker.github.io) | [Github](https://github.com/Winfredy/SadTalker)\n\nWenxuan Zhang, Xiaodong Cun, Xuan Wang, Yong Zhang, Xi Shen, Yu Guo, Ying Shan, Fei Wang.\n\nXi'an Jiaotong University, Tencent AI Lab, Ant Group\n\nCVPR 2023\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1754",
    "name": "1500-cutout-orn-process.py",
    "path": "github_repo/scripts/01_core_ai_analysis/data_processing/1500-cutout-orn-process.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 71,
    "size": 2445,
    "docstring": "",
    "keywords": [
      "opencv",
      "analysis"
    ],
    "functions": [
      "apply_circular_mask",
      "process_images_in_directory"
    ],
    "classes": [],
    "imports": [
      "os",
      "cv2",
      "numpy"
    ],
    "preview": "import os\n\nimport cv2\nimport numpy as np\n\n\ndef apply_circular_mask(image_path, output_path):\n    \"\"\"\n    Applies a circular mask to the input image and saves it with transparency.\n\n    Args:\n        image_path (str): Path to the input image file.\n        output_path (str): Path to save the masked image.\n    \"\"\"\n    # Load the image\n    image = cv2.imread(image_path, cv2.IMREAD_UNCHANGED)\n\n    # If the image is not loaded correctly, skip it\n    if image is None:\n        print(f\"Failed to load image: {image_path}\")",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1755",
    "name": "xlsx.py",
    "path": "github_repo/scripts/01_core_ai_analysis/data_processing/xlsx.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 54,
    "size": 1776,
    "docstring": "",
    "keywords": [
      "data_processing",
      "web_tools"
    ],
    "functions": [
      "sanitize_title"
    ],
    "classes": [],
    "imports": [
      "pathlib",
      "pandas",
      "requests"
    ],
    "preview": "from pathlib import Path\n\nimport pandas as pd\nimport requests\n\n# Other necessary code ...\n\n\n# Function to sanitize the title\ndef sanitize_title(title):\n    return title.replace(\" \", \"_\").replace(\"/\", \"_\").replace(\"|\", \"\").replace(\",\", \"\")\n\n\n# Read the original CSV file\ncsv_file = Path(\"/Users/steven/Downloads/NeAt/Misc/reformatted_mydesigns - Sheet1.csv\")\ndf = pd.read_csv(csv_file)\n\n# Directory where images will be downloaded\nbase_dir = Path(\"/Users/steven/csv2/\")\nbase_dir.mkdir(exist_ok=True)",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1756",
    "name": "process.py",
    "path": "github_repo/scripts/01_core_ai_analysis/data_processing/process.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 85,
    "size": 2241,
    "docstring": "process",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "configure_loggers",
      "handle_exception",
      "remove_old_log_files",
      "process"
    ],
    "classes": [],
    "imports": [
      "datetime",
      "logging",
      "os",
      "sys",
      "time",
      "ytdl.downloadupload",
      "ytdl.oshelper",
      "ytdl.ytdlconfiguration"
    ],
    "preview": "\"process\"\n\nimport datetime\nimport logging\nimport os\nimport sys\nimport time\n\nfrom ytdl.downloadupload import Downloadupload\nfrom ytdl.oshelper import absolute_files, mkdir, remove\nfrom ytdl.ytdlconfiguration import Ytdlconfiguration\n\n\ndef configure_loggers(config):\n    \"Configure logger\"\n\n    logging.getLogger(\"\").handlers = []\n\n    mkdir(config.log_folder)\n    logs_file_name = os.path.join(config.log_folder, str(datetime.date.today()) + \"process.log\")",
    "last_modified": "2025-09-13T05:54:15.204254"
  },
  {
    "id": "1757",
    "name": "mamba_reset.py",
    "path": "github_repo/scripts/01_core_ai_analysis/data_processing/mamba_reset.py",
    "category": "01_core_ai_analysis",
    "type": "setup",
    "lines": 437,
    "size": 16095,
    "docstring": "Complete Mamba/Miniforge Reset & Setup Script\nRemoves old conda/anaconda installations and sets up fresh Miniforge with Mamba.",
    "keywords": [
      "youtube",
      "organization"
    ],
    "functions": [
      "print_colored",
      "print_step",
      "run_command",
      "backup_important_files",
      "find_conda_installations",
      "remove_conda_installations",
      "clean_shell_configs",
      "download_miniforge",
      "install_miniforge",
      "initialize_mamba"
    ],
    "classes": [
      "Colors"
    ],
    "imports": [
      "os",
      "sys",
      "shutil",
      "subprocess",
      "platform",
      "pathlib",
      "datetime"
    ],
    "preview": "#!/usr/bin/env python3\n\"\"\"\nComplete Mamba/Miniforge Reset & Setup Script\nRemoves old conda/anaconda installations and sets up fresh Miniforge with Mamba.\n\"\"\"\n\nimport os\nimport sys\nimport shutil\nimport subprocess\nimport platform\nfrom pathlib import Path\nfrom datetime import datetime\n\nclass Colors:\n    HEADER = '\\033[95m'\n    BLUE = '\\033[94m'\n    CYAN = '\\033[96m'\n    GREEN = '\\033[92m'\n    YELLOW = '\\033[93m'",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1758",
    "name": "yt-playlist.py",
    "path": "github_repo/scripts/01_core_ai_analysis/data_processing/yt-playlist.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 33,
    "size": 991,
    "docstring": "",
    "keywords": [
      "data_processing"
    ],
    "functions": [],
    "classes": [],
    "imports": [
      "pandas",
      "pytube"
    ],
    "preview": "import pandas as pd\nfrom pytube import Playlist\n\n# Replace 'YOUR_PLAYLIST_URL' with the URL of your YouTube playlist\nplaylist_url = (\n    \"https://www.youtube.com/playlist?list=PLfudK7D_bQIgAVsQUK5WtfVe3_kz9cXjA\"\n)\nplaylist = Playlist(playlist_url)\n\n# Dictionary to store video information\nvideos_info = {\n    \"Title\": [],\n    \"Video URL\": [],\n    \"Length (seconds)\": [],\n    \"Views\": [],\n    \"Thumbnail URL\": [],\n    \"Description\": [],\n}\n\n# Loop through all videos in the playlist",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1759",
    "name": "bubblespider_scraper.py",
    "path": "github_repo/scripts/01_core_ai_analysis/data_processing/bubblespider_scraper.py",
    "category": "01_core_ai_analysis",
    "type": "web_tools",
    "lines": 34,
    "size": 1244,
    "docstring": "",
    "keywords": [],
    "functions": [
      "extract_product_info_to_csv"
    ],
    "classes": [],
    "imports": [
      "csv",
      "re"
    ],
    "preview": "import csv\nimport re\n\n\ndef extract_product_info_to_csv(input_file_path, output_csv_path):\n    with open(input_file_path, \"r\", encoding=\"utf-8\") as file:\n        content = file.read()\n\n    # Regular expression to match product titles and image URLs\n    pattern = re.compile(\n        r\"(.*?)\\n(https://images-na\\.ssl-images-amazon\\.com/images/W/MEDIAX_792452-T2/images/I/[^\\s]+\\.jpg)\",\n        re.DOTALL,\n    )\n\n    # Write the extracted data to a CSV file\n    with open(output_csv_path, \"w\", newline=\"\", encoding=\"utf-8\") as csvfile:\n        csvwriter = csv.writer(csvfile)\n        for match in pattern.finditer(content):\n            title = match.group(1).strip()\n            urls = re.findall(",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1760",
    "name": "miniforge-reset.py",
    "path": "github_repo/scripts/01_core_ai_analysis/data_processing/miniforge-reset.py",
    "category": "01_core_ai_analysis",
    "type": "setup",
    "lines": 437,
    "size": 16095,
    "docstring": "Complete Mamba/Miniforge Reset & Setup Script\nRemoves old conda/anaconda installations and sets up fresh Miniforge with Mamba.",
    "keywords": [
      "youtube",
      "organization"
    ],
    "functions": [
      "print_colored",
      "print_step",
      "run_command",
      "backup_important_files",
      "find_conda_installations",
      "remove_conda_installations",
      "clean_shell_configs",
      "download_miniforge",
      "install_miniforge",
      "initialize_mamba"
    ],
    "classes": [
      "Colors"
    ],
    "imports": [
      "os",
      "sys",
      "shutil",
      "subprocess",
      "platform",
      "pathlib",
      "datetime"
    ],
    "preview": "#!/usr/bin/env python3\n\"\"\"\nComplete Mamba/Miniforge Reset & Setup Script\nRemoves old conda/anaconda installations and sets up fresh Miniforge with Mamba.\n\"\"\"\n\nimport os\nimport sys\nimport shutil\nimport subprocess\nimport platform\nfrom pathlib import Path\nfrom datetime import datetime\n\nclass Colors:\n    HEADER = '\\033[95m'\n    BLUE = '\\033[94m'\n    CYAN = '\\033[96m'\n    GREEN = '\\033[92m'\n    YELLOW = '\\033[93m'",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1761",
    "name": "eucjpprober.py",
    "path": "github_repo/scripts/01_core_ai_analysis/data_processing/eucjpprober.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 103,
    "size": 3934,
    "docstring": "",
    "keywords": [],
    "functions": [
      "__init__",
      "reset",
      "charset_name",
      "language",
      "feed",
      "get_confidence"
    ],
    "classes": [
      "EUCJPProber"
    ],
    "imports": [
      "typing",
      "chardistribution",
      "codingstatemachine",
      "enums",
      "jpcntx",
      "mbcharsetprober",
      "mbcssm"
    ],
    "preview": "######################## BEGIN LICENSE BLOCK ########################\n# The Original Code is mozilla.org code.\n#\n# The Initial Developer of the Original Code is\n# Netscape Communications Corporation.\n# Portions created by the Initial Developer are Copyright (C) 1998\n# the Initial Developer. All Rights Reserved.\n#\n# Contributor(s):\n#   Mark Pilgrim - port to Python\n#\n# This library is free software; you can redistribute it and/or\n# modify it under the terms of the GNU Lesser General Public\n# License as published by the Free Software Foundation; either\n# version 2.1 of the License, or (at your option) any later version.\n#\n# This library is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU\n# Lesser General Public License for more details.",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1762",
    "name": "magickbg.py",
    "path": "github_repo/scripts/01_core_ai_analysis/data_processing/magickbg.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 36,
    "size": 982,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "remove_background",
      "process_directory"
    ],
    "classes": [],
    "imports": [
      "os",
      "subprocess"
    ],
    "preview": "import os\nimport subprocess\n\n\ndef remove_background(input_path, output_path):\n    command = [\n        \"magick\",\n        input_path,\n        \"-fuzz\",\n        \"20%\",\n        \"-transparent\",\n        \"white\",\n        output_path,\n    ]\n    subprocess.run(command)\n\n\ndef process_directory(input_dir):\n    output_dir = os.path.join(input_dir, \"output\")\n    if not os.path.exists(output_dir):",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1763",
    "name": "idt copy.py",
    "path": "github_repo/scripts/01_core_ai_analysis/data_processing/idt copy.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 34,
    "size": 1067,
    "docstring": "",
    "keywords": [
      "data_processing"
    ],
    "functions": [],
    "classes": [],
    "imports": [
      "os",
      "pandas"
    ],
    "preview": "import os\n\nimport pandas as pd\n\n# Load your YouTube video data\ncsv_path = \"/Users/steven/Downloads/Misc/ytube - youtube_videos.csv\"\ndf = pd.read_csv(csv_path)\n\n# Directory containing the downloaded thumbnails\nthumbnail_dir = \"/Users/steven/Downloads/Misc/Thumbnails/idT\"\n\n# Add a new column for the thumbnail path if it doesn't exist\nif \"Thumbnail Path\" not in df.columns:\n    df[\"Thumbnail Path\"] = \"\"\n\n# Iterate through the thumbnails in the directory\nfor filename in os.listdir(thumbnail_dir):\n    if filename.endswith(\".jpg\") or filename.endswith(\".png\"):\n        # Extract the video ID from the filename\n        video_id = filename.split(\".\")[0]",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1764",
    "name": "generate_song_csv (1).py",
    "path": "github_repo/scripts/01_core_ai_analysis/data_processing/generate_song_csv (1).py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 94,
    "size": 3021,
    "docstring": "",
    "keywords": [
      "data_processing"
    ],
    "functions": [
      "prompt_for_files",
      "predefined_file_paths",
      "extract_song_details",
      "main"
    ],
    "classes": [],
    "imports": [
      "os",
      "pandas",
      "bs4"
    ],
    "preview": "import os\n\nimport pandas as pd\nfrom bs4 import BeautifulSoup\n\n\n# Option A: Prompt for file paths\ndef prompt_for_files():\n    print(\"Enter the file paths for the HTML files (comma-separated):\")\n    file_paths = input().split(\",\")\n    return [file.strip() for file in file_paths]\n\n\n# Option B: Predefined file paths\ndef predefined_file_paths():\n    return [\n        \"/Users/steven/Music/suno/1.html\",\n        \"/Users/steven/Music/suno/2.html\",\n        \"/Users/steven/Music/suno/3.html\",\n        \"/Users/steven/Music/suno/4.html\",",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1765",
    "name": "multi_platform_product_recommender.py",
    "path": "github_repo/scripts/01_core_ai_analysis/data_processing/multi_platform_product_recommender.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 281,
    "size": 9315,
    "docstring": "",
    "keywords": [
      "opencv",
      "analysis"
    ],
    "functions": [
      "iminfo",
      "ocr_text",
      "keywords_from",
      "pick_product",
      "seo_and_tags",
      "process_dir",
      "main"
    ],
    "classes": [],
    "imports": [
      "csv",
      "os",
      "time",
      "datetime",
      "cv2",
      "numpy",
      "pytesseract",
      "PIL"
    ],
    "preview": "# filename: multi_platform_product_recommender.py\nimport csv\nimport os\nimport time\nfrom datetime import datetime\n\ntry:\n    import cv2\n    import numpy as np\n    import pytesseract\n    from PIL import Image, UnidentifiedImageError\nexcept Exception as e:\n    raise SystemExit(\"Please `pip install pillow opencv-python pytesseract numpy`\") from e\n\npytesseract.pytesseract.tesseract_cmd = \"/usr/local/bin/tesseract\"  # adjust if needed\n\nCSV_FIELDS = [\n    \"Platform\",\n    \"Folder\",\n    \"File\",",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1766",
    "name": "generate-info.py",
    "path": "github_repo/scripts/01_core_ai_analysis/data_processing/generate-info.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 94,
    "size": 3021,
    "docstring": "",
    "keywords": [
      "data_processing"
    ],
    "functions": [
      "prompt_for_files",
      "predefined_file_paths",
      "extract_song_details",
      "main"
    ],
    "classes": [],
    "imports": [
      "os",
      "pandas",
      "bs4"
    ],
    "preview": "import os\n\nimport pandas as pd\nfrom bs4 import BeautifulSoup\n\n\n# Option A: Prompt for file paths\ndef prompt_for_files():\n    print(\"Enter the file paths for the HTML files (comma-separated):\")\n    file_paths = input().split(\",\")\n    return [file.strip() for file in file_paths]\n\n\n# Option B: Predefined file paths\ndef predefined_file_paths():\n    return [\n        \"/Users/steven/Music/suno/1.html\",\n        \"/Users/steven/Music/suno/2.html\",\n        \"/Users/steven/Music/suno/3.html\",\n        \"/Users/steven/Music/suno/4.html\",",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1767",
    "name": "myd-csv-download.py",
    "path": "github_repo/scripts/01_core_ai_analysis/data_processing/myd-csv-download.py",
    "category": "01_core_ai_analysis",
    "type": "youtube",
    "lines": 97,
    "size": 3180,
    "docstring": "",
    "keywords": [
      "data_processing",
      "analysis",
      "youtube",
      "web_tools"
    ],
    "functions": [
      "ensure_directories",
      "download_image",
      "process_images",
      "write_log_to_csv",
      "main"
    ],
    "classes": [],
    "imports": [
      "os",
      "datetime",
      "pandas",
      "requests"
    ],
    "preview": "import os\nfrom datetime import datetime\n\nimport pandas as pd\nimport requests\n\n\n# Ensure that CSV and output directories exist based on the user prompt\ndef ensure_directories(base_dir):\n    output_dir = os.path.join(base_dir, \"processed_images\")\n    os.makedirs(output_dir, exist_ok=True)\n    return output_dir\n\n\n# Function to download an image\ndef download_image(url, filename):\n    try:\n        response = requests.get(url, timeout=10)\n        if response.status_code == 200:\n            with open(filename, \"wb\") as f:",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1768",
    "name": "deepseek_python_20250608130226.py",
    "path": "github_repo/scripts/01_core_ai_analysis/data_processing/deepseek_python_20250608130226.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 497,
    "size": 17786,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "__init__",
      "get_system_info",
      "analyze",
      "analyze_file",
      "analyze_path_relationships",
      "get_file_metrics",
      "ast_analysis",
      "run_analysis_tools",
      "complexity_analysis",
      "update_summary"
    ],
    "classes": [
      "AdvancedPythonAnalyzer"
    ],
    "imports": [
      "ast",
      "csv",
      "datetime",
      "json",
      "os",
      "platform",
      "subprocess",
      "sys",
      "collections",
      "matplotlib.pyplot"
    ],
    "preview": "import ast\nimport csv\nimport datetime\nimport json\nimport os\nimport platform\nimport subprocess\nimport sys\nfrom collections import defaultdict\n\nimport matplotlib.pyplot as plt\nimport networkx as nx\nimport radon\nfrom pylint import epylint as lint\nfrom radon.complexity import cc_visit\nfrom radon.metrics import mi_visit\n\n\nclass AdvancedPythonAnalyzer:\n    def __init__(self, directory):",
    "last_modified": "2025-09-13T05:53:27.589738"
  },
  {
    "id": "1769",
    "name": "big5prober.py",
    "path": "github_repo/scripts/01_core_ai_analysis/data_processing/big5prober.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 48,
    "size": 1763,
    "docstring": "",
    "keywords": [],
    "functions": [
      "__init__",
      "charset_name",
      "language"
    ],
    "classes": [
      "Big5Prober"
    ],
    "imports": [
      "chardistribution",
      "codingstatemachine",
      "mbcharsetprober",
      "mbcssm"
    ],
    "preview": "######################## BEGIN LICENSE BLOCK ########################\n# The Original Code is Mozilla Communicator client code.\n#\n# The Initial Developer of the Original Code is\n# Netscape Communications Corporation.\n# Portions created by the Initial Developer are Copyright (C) 1998\n# the Initial Developer. All Rights Reserved.\n#\n# Contributor(s):\n#   Mark Pilgrim - port to Python\n#\n# This library is free software; you can redistribute it and/or\n# modify it under the terms of the GNU Lesser General Public\n# License as published by the Free Software Foundation; either\n# version 2.1 of the License, or (at your option) any later version.\n#\n# This library is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU\n# Lesser General Public License for more details.",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1770",
    "name": "sort-suno-regx.py",
    "path": "github_repo/scripts/01_core_ai_analysis/data_processing/sort-suno-regx.py",
    "category": "01_core_ai_analysis",
    "type": "organization",
    "lines": 55,
    "size": 2013,
    "docstring": "",
    "keywords": [
      "data_processing"
    ],
    "functions": [],
    "classes": [],
    "imports": [
      "re",
      "pandas"
    ],
    "preview": "import re\n\nimport pandas as pd\n\n# Define the adjusted regex pattern\npattern = r'src=\"([^\"]+)\".*?title=\"([^\"]+)\".*?href=\"([^\"]+)\".*?href=\"([^\"]+)\">([^<]+)'\n\n# Prompt the user for HTML file paths\nprint(\"Enter the paths to your HTML files, separated by commas:\")\nfile_input = input(\"> \")\n\n# Convert the input into a list of file paths\nhtml_files = [path.strip() for path in file_input.split(\",\")]\n\n# Initialize a list to store the extracted results\nresults = []\n\n# Process each file to extract matches\nfor html_file in html_files:\n    try:",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1771",
    "name": "pods2.py",
    "path": "github_repo/scripts/01_core_ai_analysis/data_processing/pods2.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 96,
    "size": 3263,
    "docstring": "",
    "keywords": [],
    "functions": [
      "extract_sections",
      "compare_sections",
      "merge_files"
    ],
    "classes": [],
    "imports": [
      "os",
      "re",
      "difflib"
    ],
    "preview": "import os\nimport re\nfrom difflib import unified_diff\n\n# Files to merge and analyze\nfiles_to_merge = [\n    \"/Users/steven/Documents/podcast/14-16-45-Podcast_Production_Assistance copy 2.md\",\n    \"/Users/steven/Documents/podcast/ChatGPT-Project_2025_Imagery_Design.html\",\n    \"/Users/steven/Documents/podcast/14-16-45-Podcast_Production_Assistance copy.md\",\n    \"/Users/steven/Documents/podcast/Content Plan & Strategy.md\",\n    \"/Users/steven/Documents/podcast/JusticeThomas.md\",\n    \"/Users/steven/Documents/podcast/podcast-palyerzs.md\",\n    \"/Users/steven/Documents/podcast/Podcast-Trump.md\",\n    \"/Users/steven/Documents/podcast/Transition from Donald Trump to The Messiah of Mar-a-Lago.md\",\n]\n\n# Output file path\noutput_file = \"/Users/steven/Documents/podcast/merged_unique_output.md\"\n\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1772",
    "name": "similar.py",
    "path": "github_repo/scripts/01_core_ai_analysis/data_processing/similar.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 20,
    "size": 817,
    "docstring": "",
    "keywords": [],
    "functions": [
      "merge_py_files"
    ],
    "classes": [],
    "imports": [
      "os",
      "pathlib"
    ],
    "preview": "import os\nfrom pathlib import Path\n\ndef merge_py_files(file_paths, output_path):\n    merged_content = []\n    for file_path in file_paths:\n        with open(file_path, 'r', encoding='utf-8') as f:\n            merged_content.extend(f.readlines())\n    merged_content = list(dict.fromkeys(merged_content))  # Remove duplicate lines\n    with open(output_path, 'w', encoding='utf-8') as out:\n        out.writelines(merged_content)\n    print(f\"Merged {file_paths} into {output_path}\")\n\n# Example: Merge analyze*.py files\nfiles_to_merge = [\n    \"/Users/steven/Documents/python/sphinx-docs/analyze 1.py\",\n    \"/Users/steven/Documents/python/sphinx-docs/analyze 2.py\",\n    \"/Users/steven/Documents/python/sphinx-docs/analyze 6.py\"\n]\nmerge_py_files(files_to_merge, \"/Users/steven/Documents/python/sphinx-docs/analyze_merged.py\")",
    "last_modified": "2025-10-08T06:50:53"
  },
  {
    "id": "1773",
    "name": "subject_tracker (1).py",
    "path": "github_repo/scripts/01_core_ai_analysis/data_processing/subject_tracker (1).py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 209,
    "size": 7148,
    "docstring": "Subject-aware reframing using face/pose detection for intelligent cropping",
    "keywords": [
      "opencv",
      "analysis"
    ],
    "functions": [
      "generate_smooth_path",
      "__init__",
      "track_segment",
      "_extract_frame",
      "_detect_faces",
      "__init__",
      "init_tracker",
      "update"
    ],
    "classes": [
      "SubjectTracker",
      "MotionTracker"
    ],
    "imports": [
      "__future__",
      "os",
      "subprocess",
      "tempfile",
      "typing",
      "cv2",
      "numpy"
    ],
    "preview": "\"\"\"\nSubject-aware reframing using face/pose detection for intelligent cropping\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nimport subprocess\nimport tempfile\nfrom typing import Any, Dict, List, Optional, Tuple\n\nimport cv2\nimport numpy as np\n\n\nclass SubjectTracker:\n    def __init__(self):\n        # Initialize face detection\n        try:\n            self.face_cascade = cv2.CascadeClassifier(",
    "last_modified": "2025-09-13T05:55:10.405594"
  },
  {
    "id": "1774",
    "name": "johabprober.py",
    "path": "github_repo/scripts/01_core_ai_analysis/data_processing/johabprober.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 48,
    "size": 1752,
    "docstring": "",
    "keywords": [],
    "functions": [
      "__init__",
      "charset_name",
      "language"
    ],
    "classes": [
      "JOHABProber"
    ],
    "imports": [
      "chardistribution",
      "codingstatemachine",
      "mbcharsetprober",
      "mbcssm"
    ],
    "preview": "######################## BEGIN LICENSE BLOCK ########################\n# The Original Code is mozilla.org code.\n#\n# The Initial Developer of the Original Code is\n# Netscape Communications Corporation.\n# Portions created by the Initial Developer are Copyright (C) 1998\n# the Initial Developer. All Rights Reserved.\n#\n# Contributor(s):\n#   Mark Pilgrim - port to Python\n#\n# This library is free software; you can redistribute it and/or\n# modify it under the terms of the GNU Lesser General Public\n# License as published by the Free Software Foundation; either\n# version 2.1 of the License, or (at your option) any later version.\n#\n# This library is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU\n# Lesser General Public License for more details.",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1775",
    "name": "my_csv_tools.py",
    "path": "github_repo/scripts/01_core_ai_analysis/data_processing/my_csv_tools.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 27,
    "size": 627,
    "docstring": "",
    "keywords": [
      "data_processing"
    ],
    "functions": [
      "load_excel",
      "check_missing_data"
    ],
    "classes": [],
    "imports": [
      "pandas"
    ],
    "preview": "import pandas as pd\n\n\ndef load_excel(file_path):\n    try:\n        df = pd.read_excel(file_path)\n        print(\"Excel file loaded successfully.\")\n        return df\n    except Exception as e:\n        print(f\"Error loading Excel file: {e}\")\n        return None\n\n\ndef check_missing_data(df):\n    if df is not None:\n        print(\"Missing data in each column:\")\n        print(df.isnull().sum())\n    else:\n        print(\"DataFrame is empty or not loaded.\")\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1776",
    "name": "GmailBot.py",
    "path": "github_repo/scripts/01_core_ai_analysis/data_processing/GmailBot.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 995,
    "size": 37149,
    "docstring": "",
    "keywords": [
      "data_processing",
      "analysis",
      "youtube",
      "web_tools"
    ],
    "functions": [
      "send_delayed_keys",
      "randomize",
      "check_proxy",
      "load_proxy",
      "download_driver",
      "get_driver",
      "quit_driver",
      "click_whatever",
      "check_username_taken",
      "create_database"
    ],
    "classes": [
      "bcolors"
    ],
    "imports": [
      "json",
      "logging",
      "os",
      "platform",
      "random",
      "re",
      "sqlite3",
      "string",
      "sys",
      "time"
    ],
    "preview": "import json\nimport logging\nimport os\nimport platform\nimport random\nimport re\nimport sqlite3\nimport string\nimport sys\nimport time\nimport zipfile\nfrom contextlib import closing\nfrom datetime import datetime\nfrom random import choice, choices, randint, uniform\n\nimport pandas as pd\nimport requests\nimport xlrd\nfrom fake_headers import Headers, browsers\nfrom selenium import webdriver",
    "last_modified": "2025-09-13T05:54:13.699296"
  },
  {
    "id": "1777",
    "name": "finetune.py",
    "path": "github_repo/scripts/01_core_ai_analysis/data_processing/finetune.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 368,
    "size": 12752,
    "docstring": "",
    "keywords": [],
    "functions": [
      "load_model",
      "finetune",
      "inference",
      "save_model",
      "check_if_huggingface_model_exists",
      "format_samples_sft",
      "format_samples_dpo"
    ],
    "classes": [],
    "imports": [
      "argparse",
      "os",
      "pathlib",
      "unsloth",
      "typing",
      "torch",
      "datasets",
      "huggingface_hub",
      "huggingface_hub.utils",
      "transformers"
    ],
    "preview": "import argparse\nimport os\nfrom pathlib import Path\n\nfrom unsloth import PatchDPOTrainer\n\nPatchDPOTrainer()\n\nfrom typing import Any, List, Literal, Optional  # noqa: E402\n\nimport torch  # noqa\nfrom datasets import concatenate_datasets, load_dataset  # noqa: E402\nfrom huggingface_hub import HfApi  # noqa: E402\nfrom huggingface_hub.utils import RepositoryNotFoundError  # noqa: E402\nfrom transformers import TextStreamer, TrainingArguments  # noqa: E402\nfrom trl import DPOConfig, DPOTrainer, SFTTrainer  # noqa: E402\nfrom unsloth import FastLanguageModel, is_bfloat16_supported  # noqa: E402\nfrom unsloth.chat_templates import get_chat_template  # noqa: E402\n\nalpaca_template = \"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.",
    "last_modified": "2025-09-13T05:53:42.427738"
  },
  {
    "id": "1778",
    "name": "download_yt-videos.py",
    "path": "github_repo/scripts/01_core_ai_analysis/data_processing/download_yt-videos.py",
    "category": "01_core_ai_analysis",
    "type": "youtube",
    "lines": 40,
    "size": 894,
    "docstring": "",
    "keywords": [
      "data_processing"
    ],
    "functions": [],
    "classes": [],
    "imports": [
      "os",
      "subprocess",
      "pandas"
    ],
    "preview": "import os\nimport subprocess\n\nimport pandas as pd\n\n# Path to your CSV file\ncsv_file_path = \"/Users/steven/Movies/SEO/rebrandy-vidiq-2025-04-29.csv\"\n\n# Load CSV data\ndf = pd.read_csv(csv_file_path)\n\n# Create output directory\noutput_dir = \"/Users/steven/Movies/SEO\"\nos.makedirs(output_dir, exist_ok=True)\n\n# Column in CSV with URLs (adjust if needed)\nurl_column = \"URL\"\n\n# yt-dlp format string for ~480-600p range\nformat_str = \"bestvideo[height<=600]+bestaudio/best[height<=600]\"",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1779",
    "name": "csv2.py",
    "path": "github_repo/scripts/01_core_ai_analysis/data_processing/csv2.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 67,
    "size": 2403,
    "docstring": "",
    "keywords": [
      "data_processing",
      "web_tools"
    ],
    "functions": [],
    "classes": [],
    "imports": [
      "pathlib",
      "pandas",
      "requests"
    ],
    "preview": "from pathlib import Path\n\nimport pandas as pd\nimport requests\n\n# Corrected CSV file path\ncsv_file = Path(\"/Users/steven/Downloads/NeAt/Misc/reformatted_mydesigns - Sheet1.csv\")\n\ndf = pd.read_csv(csv_file)\n\n# Base directory to save images and info\nbase_dir = Path(\"/Users/steven/csv2/\")\nbase_dir.mkdir(exist_ok=True)\n\n# Iterate through each row of the CSV and process the images and info\nfor index, row in df.iterrows():\n    title = row[\"TITLE\"]\n    description = row[\"DESCRIPTION\"]\n    tags = row[\"TAGS\"]\n    language = row.get(\"LANGUAGE\", \"EN\")  # Assuming 'EN' as default if not specified",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1780",
    "name": "create_csv_from_json-leonardo.py",
    "path": "github_repo/scripts/01_core_ai_analysis/data_processing/create_csv_from_json-leonardo.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 75,
    "size": 2203,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "process_json_to_csv",
      "combine_json_to_csv"
    ],
    "classes": [],
    "imports": [
      "csv",
      "gzip",
      "json",
      "os"
    ],
    "preview": "import csv\nimport gzip\nimport json\nimport os\n\n# Configuration\nOUTPUT_DIR = \"/Users/steven/Pictures/leodowns\"\nCSV_FILE = os.path.join(OUTPUT_DIR, \"leonardo.csv\")\n\n# Headers for CSV\nHEADERS = [\n    \"id\",\n    \"prompt\",\n    \"negativePrompt\",\n    \"motionStrength\",\n    \"createdAt\",\n    \"image_url\",\n    \"motion_url\",\n    \"local_image_path\",\n    \"local_motion_path\",",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1781",
    "name": "organize_files.py",
    "path": "github_repo/scripts/01_core_ai_analysis/data_processing/organize_files.py",
    "category": "01_core_ai_analysis",
    "type": "organization",
    "lines": 107,
    "size": 3312,
    "docstring": "",
    "keywords": [
      "analysis",
      "organization"
    ],
    "functions": [
      "get_creation_date",
      "custom_tags",
      "organize_files",
      "tag_files",
      "create_database",
      "insert_metadata"
    ],
    "classes": [],
    "imports": [
      "os",
      "shutil",
      "sqlite3",
      "datetime"
    ],
    "preview": "import os\nimport shutil\nimport sqlite3\nfrom datetime import datetime\n\n\ndef get_creation_date(filepath):\n    \"\"\"Get the creation date of the file.\"\"\"\n    return datetime.fromtimestamp(os.path.getctime(filepath)).strftime(\"%Y-%m-%d %H:%M:%S\")\n\n\ndef custom_tags(filename, filepath):\n    \"\"\"Determine custom tags based on file content or filename patterns.\"\"\"\n    custom_tag = None\n    if filename.endswith(\".py\"):\n        with open(filepath, \"r\") as file:\n            content = file.read()\n            if \"import pandas\" in content:\n                custom_tag = \"python_data_analysis\"\n            elif \"import tensorflow\" in content:",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1782",
    "name": "backupcsv.py",
    "path": "github_repo/scripts/01_core_ai_analysis/data_processing/backupcsv.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 69,
    "size": 2237,
    "docstring": "",
    "keywords": [
      "data_processing",
      "analysis"
    ],
    "functions": [
      "generate_dry_run_csv",
      "is_excluded"
    ],
    "classes": [],
    "imports": [
      "os",
      "re",
      "subprocess",
      "pandas"
    ],
    "preview": "import os\nimport re\nimport subprocess\n\nimport pandas as pd\n\n\ndef generate_dry_run_csv(directories, csv_path):\n    rows = []\n\n    # Regex patterns for exclusions\n    excluded_patterns = [\n        r\"^\\..*\",  # Hidden files and directories\n        r\".*/venv/.*\",  # venv directories\n        r\".*/\\.venv/.*\",  # .venv directories\n        r\".*/my_global_venv/.*\",  # my_global_venv directories\n        r\".*/simplegallery/.*\",\n        r\".*/avatararts/.*\",\n        r\".*/github/.*\",\n        r\".*/Documents/gitHub/.*\",  # Specific gitHub directory",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1783",
    "name": "re.py",
    "path": "github_repo/scripts/01_core_ai_analysis/data_processing/re.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 52,
    "size": 1731,
    "docstring": "",
    "keywords": [
      "data_processing",
      "web_tools"
    ],
    "functions": [
      "sanitize_title"
    ],
    "classes": [],
    "imports": [
      "pathlib",
      "pandas",
      "requests"
    ],
    "preview": "from pathlib import Path\n\nimport pandas as pd\nimport requests\n\n\ndef sanitize_title(title):\n    # Sanitize the title to create a valid file name\n    return title.replace(\" \", \"_\").replace(\"/\", \"_\").replace(\"|\", \"\").replace(\",\", \"\")\n\n\n# Read the original CSV file\ncsv_file = Path(\"/Users/steven/Downloads/NeAt/Misc/reformatted_mydesigns - Sheet1.csv\")\ndf = pd.read_csv(csv_file)\n\n# Directory where images will be downloaded\nbase_dir = Path(\"/Users/steven/csv2/\")\nbase_dir.mkdir(exist_ok=True)\n\n# Process each row in the DataFrame",
    "last_modified": "2025-05-04T22:47:12"
  },
  {
    "id": "1784",
    "name": "YTubeDLthumbs copy.py",
    "path": "github_repo/scripts/01_core_ai_analysis/data_processing/YTubeDLthumbs copy.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 58,
    "size": 1913,
    "docstring": "",
    "keywords": [
      "data_processing",
      "web_tools"
    ],
    "functions": [
      "fetch_video_details"
    ],
    "classes": [],
    "imports": [
      "os",
      "pandas",
      "requests",
      "googleapiclient.discovery"
    ],
    "preview": "import os\n\nimport pandas as pd  # This line is necessary to use pandas in your script\nimport requests\nfrom googleapiclient.discovery import build\n\n# Initialize YouTube API\napi_key = \"AIzaSyCyfGm1oF2syfzfsPeCRHb10tMKmC0dbCo\"\nyoutube = build(\"youtube\", \"v3\", developerKey=api_key)\n\n# Define paths\ncsv_path = \"/Users/steven/Downloads/Misc/ytube - youtube_videos.csv\"\nthumbnail_dir = \"/Users/steven/Downloads/Misc/Thumbnails/d2\"\nos.makedirs(thumbnail_dir, exist_ok=True)\n\n# Load CSV\ndf = pd.read_csv(csv_path)\n\n# Function to fetch video details and download thumbnail\n",
    "last_modified": "2025-09-13T05:54:13.128583"
  },
  {
    "id": "1785",
    "name": "test_environment.py",
    "path": "github_repo/scripts/01_core_ai_analysis/data_processing/test_environment.py",
    "category": "01_core_ai_analysis",
    "type": "testing",
    "lines": 16,
    "size": 405,
    "docstring": "",
    "keywords": [
      "data_processing"
    ],
    "functions": [],
    "classes": [],
    "imports": [
      "sys",
      "librosa",
      "pandas",
      "plotly.express",
      "sklearn.linear_model"
    ],
    "preview": "# test_environment.py\nimport sys\n\nimport librosa\nimport pandas as pd\nimport plotly.express as px\nfrom sklearn.linear_model import LinearRegression\n\nprint(f\"Python version: {sys.version}\")\nprint(f\"Pandas version: {pd.__version__}\")\nprint(f\"Librosa version: {librosa.__version__}\")\n\n# Test audio analysis\ny, sr = librosa.load(librosa.ex(\"trumpet\"))\nprint(f\"Audio sample loaded: {len(y)} samples at {sr}Hz\")\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1786",
    "name": "dblsort.py",
    "path": "github_repo/scripts/01_core_ai_analysis/data_processing/dblsort.py",
    "category": "01_core_ai_analysis",
    "type": "organization",
    "lines": 25,
    "size": 1140,
    "docstring": "",
    "keywords": [
      "data_processing"
    ],
    "functions": [],
    "classes": [],
    "imports": [
      "pandas",
      "ace_tools"
    ],
    "preview": "import pandas as pd\n\n# Load the CSV file\nfile_path = \"/Users/steven/Documents/Python/fdupes/detailed_duplicate_report.csv\"\nduplicate_report = pd.read_csv(file_path)\n\n# Sort by Duplicate Count in descending order\nsorted_by_duplicates = duplicate_report.sort_values(by=\"Duplicate Count\", ascending=False)\n\n# Filter to show only duplicates (where Duplicate Count > 1)\nonly_duplicates = sorted_by_duplicates[sorted_by_duplicates[\"Duplicate Count\"] > 1]\n\n# Sort by File Size (assuming sizes are in KB and removing the KB text for sorting)\nduplicate_report[\"File Size\"] = duplicate_report[\"File Size\"].str.replace(\" KB\", \"\").astype(float)\nsorted_by_size = duplicate_report.sort_values(by=\"File Size\", ascending=False)\n\n# Display the sorted and filtered dataframes to the user\nimport ace_tools as tools\n\ntools.display_dataframe_to_user(name=\"Sorted by Duplicate Count\", dataframe=sorted_by_duplicates)",
    "last_modified": "2025-09-13T05:54:29.167088"
  },
  {
    "id": "1787",
    "name": "ClipHandler.py",
    "path": "github_repo/scripts/01_core_ai_analysis/data_processing/ClipHandler.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 194,
    "size": 8089,
    "docstring": "",
    "keywords": [
      "youtube",
      "web_tools"
    ],
    "functions": [
      "__init__",
      "get_clips",
      "is_ingame_clip",
      "get_game_clips",
      "is_required_length",
      "handle_response_data",
      "download_clip",
      "filter_func"
    ],
    "classes": [
      "ClipHandler"
    ],
    "imports": [
      "logging",
      "os",
      "pathlib",
      "numpy",
      "requests",
      "streamlink",
      "tensorflow",
      "moviepy.editor",
      "src",
      "src.APIHandler"
    ],
    "preview": "import logging\nimport os\nfrom pathlib import Path\n\nimport numpy as np\nimport requests\nimport streamlink\nimport tensorflow as tf\nfrom moviepy.editor import VideoFileClip\nfrom src import utils\nfrom src.APIHandler import APIHandler\nfrom src.Clip import Clip\nfrom src.MetadataHandler import MetadataHandler\n\nimport config\n\n\nclass ClipHandler:\n    clips = []\n    retries = 3",
    "last_modified": "2025-09-13T05:53:45.547250"
  },
  {
    "id": "1788",
    "name": "reportbot.py",
    "path": "github_repo/scripts/01_core_ai_analysis/data_processing/reportbot.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 197,
    "size": 5134,
    "docstring": "",
    "keywords": [
      "video_processing"
    ],
    "functions": [
      "clear_screen",
      "chunks",
      "profile_attack_process",
      "video_attack_process",
      "video_attack",
      "profile_attack",
      "main"
    ],
    "classes": [],
    "imports": [
      "os",
      "os",
      "sys",
      "libs.check_modules",
      "os",
      "sys",
      "multiprocessing",
      "os",
      "time",
      "colorama"
    ],
    "preview": "import os\n\n\ndef clear_screen():\n    os.system(\"cls\")\n\n\nclear_screen()\n\nfrom os import _exit\nfrom sys import exit\n\nfrom libs.check_modules import check_modules\n\ncheck_modules()\n\nimport os\nimport sys\nfrom multiprocessing import Process\nfrom os import path",
    "last_modified": "2025-09-13T05:53:30.261669"
  },
  {
    "id": "1789",
    "name": "organizer.py",
    "path": "github_repo/scripts/01_core_ai_analysis/data_processing/organizer.py",
    "category": "01_core_ai_analysis",
    "type": "organization",
    "lines": 51,
    "size": 1506,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "process_files",
      "process_audio"
    ],
    "classes": [],
    "imports": [
      "argparse",
      "pathlib",
      "utils"
    ],
    "preview": "#!/usr/bin/env python3\nimport argparse\nfrom pathlib import Path\n\nfrom utils import EXCLUSIONS, SETTINGS, FileOrganizer\n\n\ndef process_files(file_type: str, directories: List[Path]):\n    processor = {\n        \"audio\": process_audio,\n        \"video\": process_video,\n        \"image\": process_image,\n        \"documents\": process_docs,\n        \"other\": process_other,\n    }[file_type]\n\n    results = []\n    for directory in directories:\n        for path in directory.rglob(\"*\"):\n            if path.is_file() and not FileOrganizer.should_exclude(path):",
    "last_modified": "2025-09-06T12:24:11.731941"
  },
  {
    "id": "1790",
    "name": "winterm.py",
    "path": "github_repo/scripts/01_core_ai_analysis/data_processing/winterm.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 200,
    "size": 7095,
    "docstring": "",
    "keywords": [],
    "functions": [
      "enable_vt_processing",
      "__init__",
      "get_attrs",
      "set_attrs",
      "reset_all",
      "fore",
      "back",
      "style",
      "set_console",
      "get_position"
    ],
    "classes": [
      "WinColor",
      "WinStyle",
      "WinTerm"
    ],
    "imports": [
      "msvcrt"
    ],
    "preview": "# Copyright Jonathan Hartley 2013. BSD 3-Clause license, see LICENSE file.\ntry:\n    from msvcrt import get_osfhandle\nexcept ImportError:\n\n    def get_osfhandle(_):\n        raise OSError(\"This isn't windows!\")\n\n\nfrom . import win32\n\n\n# from wincon.h\nclass WinColor(object):\n    BLACK = 0\n    BLUE = 1\n    GREEN = 2\n    CYAN = 3\n    RED = 4\n    MAGENTA = 5",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1791",
    "name": "sorts.py",
    "path": "github_repo/scripts/01_core_ai_analysis/data_processing/sorts.py",
    "category": "01_core_ai_analysis",
    "type": "organization",
    "lines": 287,
    "size": 9050,
    "docstring": "",
    "keywords": [
      "data_processing",
      "analysis"
    ],
    "functions": [
      "is_excluded",
      "compute_md5",
      "normalize_file_size",
      "get_creation_date",
      "get_image_metadata",
      "format_file_size",
      "generate_detailed_duplicate_report",
      "prompt_for_csv_files"
    ],
    "classes": [],
    "imports": [
      "csv",
      "hashlib",
      "os",
      "re",
      "collections",
      "datetime",
      "pandas",
      "PIL"
    ],
    "preview": "import csv\nimport hashlib\nimport os\nimport re\nfrom collections import defaultdict\nfrom datetime import datetime\n\nimport pandas as pd\nfrom PIL import Image\n\n\ndef is_excluded(path, patterns):\n    \"\"\"\n    Check if a given path matches any of the exclusion patterns.\n\n    Parameters:\n    path (str): The path to check.\n    patterns (list): A list of regex patterns for exclusion.\n\n    Returns:",
    "last_modified": "2025-09-13T05:54:29.675982"
  },
  {
    "id": "1792",
    "name": "check-python.py",
    "path": "github_repo/scripts/01_core_ai_analysis/data_processing/check-python.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 57,
    "size": 1661,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "check_with_pylint",
      "check_with_flake8",
      "process_directory"
    ],
    "classes": [],
    "imports": [
      "os",
      "subprocess"
    ],
    "preview": "import os\nimport subprocess\n\n\ndef check_with_pylint(file_path):\n    \"\"\"\n    Run pylint on the given Python file to check for errors and style issues.\n\n    :param file_path: Path to the Python file.\n    :return: None\n    \"\"\"\n    result = subprocess.run([\"pylint\", file_path], capture_output=True, text=True)\n    if result.returncode != 0:\n        print(f\"Pylint issues in {file_path}:\")\n        print(result.stdout)\n    else:\n        print(f\"No pylint issues found in {file_path}\")\n\n\ndef check_with_flake8(file_path):",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1793",
    "name": "unpacking.py",
    "path": "github_repo/scripts/01_core_ai_analysis/data_processing/unpacking.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 257,
    "size": 8820,
    "docstring": "Utilities related archives.",
    "keywords": [],
    "functions": [
      "current_umask",
      "split_leading_dir",
      "has_leading_dir",
      "is_within_directory",
      "set_extracted_file_to_default_mode_plus_executable",
      "zip_item_is_executable",
      "unzip_file",
      "untar_file",
      "unpack_file"
    ],
    "classes": [],
    "imports": [
      "logging",
      "os",
      "shutil",
      "stat",
      "tarfile",
      "zipfile",
      "typing",
      "zipfile",
      "pip._internal.exceptions",
      "pip._internal.utils.filetypes"
    ],
    "preview": "\"\"\"Utilities related archives.\"\"\"\n\nimport logging\nimport os\nimport shutil\nimport stat\nimport tarfile\nimport zipfile\nfrom typing import Iterable, List, Optional\nfrom zipfile import ZipInfo\n\nfrom pip._internal.exceptions import InstallationError\nfrom pip._internal.utils.filetypes import (\n    BZ2_EXTENSIONS,\n    TAR_EXTENSIONS,\n    XZ_EXTENSIONS,\n    ZIP_EXTENSIONS,\n)\nfrom pip._internal.utils.misc import ensure_dir\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1794",
    "name": "autofix.py",
    "path": "github_repo/scripts/01_core_ai_analysis/data_processing/autofix.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 89,
    "size": 2796,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "create_backup",
      "apply_autopep8",
      "run_pylint",
      "process_directory"
    ],
    "classes": [],
    "imports": [
      "os",
      "shutil",
      "subprocess",
      "datetime"
    ],
    "preview": "import os\nimport shutil\nimport subprocess\nfrom datetime import datetime\n\n\ndef create_backup(file_path, backup_dir):\n    \"\"\"\n    Create a backup of the given file in the specified backup directory.\n\n    :param file_path: Path to the original file.\n    :param backup_dir: Path to the backup directory.\n    :return: None\n    \"\"\"\n    os.makedirs(backup_dir, exist_ok=True)\n    file_name = os.path.basename(file_path)\n    backup_path = os.path.join(backup_dir, file_name)\n    shutil.copy(file_path, backup_path)\n    print(f\"Backup created for {file_path} at {backup_path}\")\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1795",
    "name": "merger.py",
    "path": "github_repo/scripts/01_core_ai_analysis/data_processing/merger.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 21,
    "size": 818,
    "docstring": "",
    "keywords": [],
    "functions": [
      "merge_py_files"
    ],
    "classes": [],
    "imports": [
      "os",
      "pathlib"
    ],
    "preview": "import os\nfrom pathlib import Path\n\ndef merge_py_files(file_paths, output_path):\n    merged_content = []\n    for file_path in file_paths:\n        with open(file_path, 'r', encoding='utf-8') as f:\n            merged_content.extend(f.readlines())\n    merged_content = list(dict.fromkeys(merged_content))  # Remove duplicate lines\n    with open(output_path, 'w', encoding='utf-8') as out:\n        out.writelines(merged_content)\n    print(f\"Merged {file_paths} into {output_path}\")\n\n# Example: Merge analyze*.py files\nfiles_to_merge = [\n    \"/Users/steven/Documents/python/sphinx-docs/analyze 1.py\",\n    \"/Users/steven/Documents/python/sphinx-docs/analyze 2.py\",\n    \"/Users/steven/Documents/python/sphinx-docs/analyze 6.py\"\n]\nmerge_py_files(files_to_merge, \"/Users/steven/Documents/python/sphinx-docs/analyze_merged.py\")",
    "last_modified": "2025-10-08T06:39:52.518525"
  },
  {
    "id": "1796",
    "name": "csv_from_json2.py",
    "path": "github_repo/scripts/01_core_ai_analysis/data_processing/csv_from_json2.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 72,
    "size": 2339,
    "docstring": "",
    "keywords": [
      "data_processing"
    ],
    "functions": [],
    "classes": [],
    "imports": [
      "json",
      "os",
      "glob",
      "pandas",
      "natsort"
    ],
    "preview": "import json\nimport os\nfrom glob import glob\n\nimport pandas as pd\nfrom natsort import natsorted\n\n# Set your root directory containing the JSON files\nROOT_DIR = \"/Users/steven/Documents/Conversation_JSONs/\"  # Adjust as needed\n\n# Find all JSON files recursively and sort them naturally\njson_files = glob(os.path.join(ROOT_DIR, \"**/*.json\"), recursive=True)\njson_files = natsorted(json_files)\n\n# Define the headers for the CSV\nHEADERS = [\"Title\", \"Node ID\", \"Parent\", \"Children\", \"Author Role\", \"Content\", \"Status\"]\n\ndata_list = []\n\nfor file in json_files:",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1797",
    "name": "pods.py",
    "path": "github_repo/scripts/01_core_ai_analysis/data_processing/pods.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 84,
    "size": 3213,
    "docstring": "",
    "keywords": [],
    "functions": [
      "extract_sections",
      "get_unique_sections",
      "merge_files"
    ],
    "classes": [],
    "imports": [
      "os",
      "re",
      "difflib"
    ],
    "preview": "import os\nimport re\nfrom difflib import unified_diff\n\n# List of files to merge\nfiles_to_merge = [\n    \"/Users/steven/Documents/podcast/14-16-45-Podcast_Production_Assistance copy 2.md\",\n    \"/Users/steven/Documents/podcast/ChatGPT-Project_2025_Imagery_Design.html\",\n    \"/Users/steven/Documents/podcast/14-16-45-Podcast_Production_Assistance copy.md\",\n    \"/Users/steven/Documents/podcast/Content Plan & Strategy.md\",\n    \"/Users/steven/Documents/podcast/JusticeThomas.md\",\n    \"/Users/steven/Documents/podcast/podcast-palyerzs.md\",\n    \"/Users/steven/Documents/podcast/Podcast-Trump.md\",\n    \"/Users/steven/Documents/podcast/Transition from Donald Trump to The Messiah of Mar-a-Lago.md\",\n]\n\n# Output file\noutput_file = \"/Users/steven/Documents/podcast/merged_unique_output.md\"\n\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1798",
    "name": "analyze.py.py",
    "path": "github_repo/scripts/01_core_ai_analysis/analyze.py_consolidated/analyze.py.py",
    "category": "01_core_ai_analysis",
    "type": "analysis",
    "lines": 57,
    "size": 1994,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis",
      "transcription"
    ],
    "functions": [
      "transcribe_audio",
      "analyze_text"
    ],
    "classes": [],
    "imports": [
      "openai",
      "os",
      "dotenv",
      "sys"
    ],
    "preview": "from openai import OpenAI\n\nclient = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\nimport os\n\nfrom dotenv import load_dotenv\n\n# Load environment variables from .env\nload_dotenv()\n\n\n# Function to transcribe audio using OpenAI Whisper\ndef transcribe_audio(file_path):\n    with open(file_path, \"rb\") as audio_file:\n        transcript = client.audio.transcribe(\"whisper-1\", audio_file)\n        return transcript.text\n\n\n# Function to analyze the transcript using GPT\ndef analyze_text(text):",
    "last_modified": "2025-05-04T22:47:11"
  },
  {
    "id": "1799",
    "name": "analyze.py_02.py",
    "path": "github_repo/scripts/01_core_ai_analysis/analyze.py_consolidated/analyze.py_02.py",
    "category": "01_core_ai_analysis",
    "type": "analysis",
    "lines": 42,
    "size": 1099,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis"
    ],
    "functions": [
      "analyze_text"
    ],
    "classes": [],
    "imports": [
      "openai",
      "os",
      "dotenv",
      "sys"
    ],
    "preview": "from openai import OpenAI\n\nclient = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\nimport os\n\nfrom dotenv import load_dotenv\n\n# Load environment variables from .env\nload_dotenv()\n\n\ndef analyze_text(text):\n    response = client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",  # Ensure you're using a chat model like gpt-3.5-turbo\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": \"You are a helpful assistant that analyzes song lyrics.\",\n            },\n            {",
    "last_modified": "2025-05-04T22:47:11"
  },
  {
    "id": "1800",
    "name": "analyzer.py_02.py",
    "path": "github_repo/scripts/01_core_ai_analysis/analyzer.py_consolidated/analyzer.py_02.py",
    "category": "01_core_ai_analysis",
    "type": "analysis",
    "lines": 42,
    "size": 1080,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis"
    ],
    "functions": [
      "analyze_text"
    ],
    "classes": [],
    "imports": [
      "os",
      "openai",
      "dotenv",
      "sys"
    ],
    "preview": "import os\n\nimport openai\nfrom dotenv import load_dotenv\n\n# Load environment variables from .env\nload_dotenv()\n\nopenai.api_key = os.getenv(\"OPENAI_API_KEY\")\n\n\ndef analyze_text(text):\n    response = openai.ChatCompletion.create(\n        model=\"gpt-3.5-turbo\",  # Ensure you're using a chat model like gpt-3.5-turbo\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": \"You are a helpful assistant that analyzes song lyrics.\",\n            },\n            {",
    "last_modified": "2025-05-06T04:35:14"
  },
  {
    "id": "1801",
    "name": "analyzer.py.py",
    "path": "github_repo/scripts/01_core_ai_analysis/analyzer.py_consolidated/analyzer.py.py",
    "category": "01_core_ai_analysis",
    "type": "analysis",
    "lines": 60,
    "size": 3430,
    "docstring": "",
    "keywords": [
      "openai",
      "analysis"
    ],
    "functions": [
      "analyze_text"
    ],
    "classes": [],
    "imports": [
      "shared.config",
      "openai",
      "os"
    ],
    "preview": "from shared.config import *\nfrom openai import OpenAI\n\nclient = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\nimport os\n\n# Load environment variables from .env\nenv_path = os.path.expanduser(\"~/.env\")\n# Update the path if needed\nload_dotenv(dotenv_path=env_path)\n\n# Error checking for openai API key\nif not openai.api_key:\n    raise EnvironmentError(\"openai API key not found. Please check your .env file.\")\n\n\ndef analyze_text(text):\n    response = client.chat.completions.create(\n        model=\"gpt-4o\",\n        messages=[",
    "last_modified": "2025-10-09T05:27:15.570663"
  },
  {
    "id": "1802",
    "name": "analyzer.py_03.py",
    "path": "github_repo/scripts/01_core_ai_analysis/analyzer.py_consolidated/analyzer.py_03.py",
    "category": "01_core_ai_analysis",
    "type": "analysis",
    "lines": 778,
    "size": 31333,
    "docstring": "This script is intended for use as a GYP_GENERATOR. It takes as input (by way of\nthe generator flag config_path) the path of a json file that dictates the files\nand targets to search for. The following keys are supported:\nfiles: list of paths (relative) of the files to search for.\ntest_targets: unqualified target names to search for. Any target in this list\nthat depends upon a file in |files| is output regardless of the type of target\nor chain of dependencies.\nadditional_compile_targets: Unqualified targets to search for in addition to\ntest_targets. Targets in the combined list that depend upon a file in |files|\nare not necessarily output. For example, if the target is of type none then the\ntarget is not output (but one of the descendants of the target will be).\n\nThe following is output:\nerror: only supplied if there is an error.\ncompile_targets: minimal set of targets that directly or indirectly (for\n  targets of type none) depend on the files in |files| and is one of the\n  supplied targets or a target that one of the supplied targets depends on.\n  The expectation is this set of targets is passed into a build step. This list\n  always contains the output of test_targets as well.\ntest_targets: set of targets from the supplied |test_targets| that either\n  directly or indirectly depend upon a file in |files|. This list if useful\n  if additional processing needs to be done for certain targets after the\n  build, such as running tests.\nstatus: outputs one of three values: none of the supplied files were found,\n  one of the include files changed so that it should be assumed everything\n  changed (in this case test_targets and compile_targets are not output) or at\n  least one file was found.\ninvalid_targets: list of supplied targets that were not found.\n\nExample:\nConsider a graph like the following:\n  A     D\n / B   C\nA depends upon both B and C, A is of type none and B and C are executables.\nD is an executable, has no dependencies and nothing depends on it.\nIf |additional_compile_targets| = [\"A\"], |test_targets| = [\"B\", \"C\"] and\nfiles = [\"b.cc\", \"d.cc\"] (B depends upon b.cc and D depends upon d.cc), then\nthe following is output:\n|compile_targets| = [\"B\"] B must built as it depends upon the changed file b.cc\nand the supplied target A depends upon it. A is not output as a build_target\nas it is of type none with no rules and actions.\n|test_targets| = [\"B\"] B directly depends upon the change file b.cc.\n\nEven though the file d.cc, which D depends upon, has changed D is not output\nas it was not supplied by way of |additional_compile_targets| or |test_targets|.\n\nIf the generator flag analyzer_output_path is specified, output is written\nthere. Otherwise output is written to stdout.\n\nIn Gyp the \"all\" target is shorthand for the root targets in the files passed\nto gyp. For example, if file \"a.gyp\" contains targets \"a1\" and\n\"a2\", and file \"b.gyp\" contains targets \"b1\" and \"b2\" and \"a2\" has a dependency\non \"b2\" and gyp is supplied \"a.gyp\" then \"all\" consists of \"a1\" and \"a2\".\nNotice that \"b1\" and \"b2\" are not in the \"all\" target as \"b.gyp\" was not\ndirectly supplied to gyp. OTOH if both \"a.gyp\" and \"b.gyp\" are supplied to gyp\nthen the \"all\" target includes \"b1\" and \"b2\".",
    "keywords": [],
    "functions": [
      "_ToGypPath",
      "_ResolveParent",
      "_AddSources",
      "_ExtractSourcesFromAction",
      "_ToLocalPath",
      "_ExtractSources",
      "_WasBuildFileModified",
      "_GetOrCreateTargetByName",
      "_DoesTargetTypeRequireBuild",
      "_GenerateTargets"
    ],
    "classes": [
      "Target",
      "Config",
      "TargetCalculator"
    ],
    "imports": [
      "json",
      "os",
      "posixpath",
      "gyp.common"
    ],
    "preview": "# Copyright (c) 2014 Google Inc. All rights reserved.\n# Use of this source code is governed by a BSD-style license that can be\n# found in the LICENSE file.\n\n\"\"\"\nThis script is intended for use as a GYP_GENERATOR. It takes as input (by way of\nthe generator flag config_path) the path of a json file that dictates the files\nand targets to search for. The following keys are supported:\nfiles: list of paths (relative) of the files to search for.\ntest_targets: unqualified target names to search for. Any target in this list\nthat depends upon a file in |files| is output regardless of the type of target\nor chain of dependencies.\nadditional_compile_targets: Unqualified targets to search for in addition to\ntest_targets. Targets in the combined list that depend upon a file in |files|\nare not necessarily output. For example, if the target is of type none then the\ntarget is not output (but one of the descendants of the target will be).\n\nThe following is output:\nerror: only supplied if there is an error.\ncompile_targets: minimal set of targets that directly or indirectly (for",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1803",
    "name": "main.py_03.py",
    "path": "github_repo/scripts/01_core_ai_analysis/main.py_consolidated/main.py_03.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 41,
    "size": 1399,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "os",
      "slugify",
      "config",
      "utils.textgenerator",
      "utils.videogenerator"
    ],
    "preview": "import os\n\nfrom slugify import slugify\n\nimport config\nfrom utils.textgenerator import generate_text_list\nfrom utils.videogenerator import VideoGenerator\n\nif __name__ == \"__main__\":\n    vg = VideoGenerator(\n        video_folder=config.VIDEO,\n        music_folder=config.MUSIC,\n        duration=config.DURATION,\n        size=config.SIZE,\n    )\n    with open(\"prompts.txt\", \"r\") as file:\n        prompts = file.readlines()\n        prompts = [line.strip() for line in prompts]\n\n    for prompt in prompts:",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1804",
    "name": "main.py.py",
    "path": "github_repo/scripts/01_core_ai_analysis/main.py_consolidated/main.py.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 19,
    "size": 558,
    "docstring": "",
    "keywords": [],
    "functions": [
      "main"
    ],
    "classes": [],
    "imports": [
      "csv",
      "generate_speech"
    ],
    "preview": "import csv\n\nfrom generate_speech import generate_speech\n\n\ndef main():\n    csv_path = \"quiz329.csv\"\n\n    with open(csv_path, newline=\"\", encoding=\"utf-8\") as csvfile:\n        reader = csv.DictReader(csvfile)\n        for i, row in enumerate(reader):\n            question_text = row[\"Question\"]  # Assuming 'Question' is the column name\n            output_path = f\"/Users/steven/Documents/quiz-talk/quiz329/question/question_{i+1}.mp3\"\n            generate_speech(question_text, voice=\"shimmer\", output_path=output_path)\n\n\nif __name__ == \"__main__\":\n    main()\n",
    "last_modified": "2025-09-13T05:53:51.047536"
  },
  {
    "id": "1805",
    "name": "main.py_02.py",
    "path": "github_repo/scripts/01_core_ai_analysis/main.py_consolidated/main.py_02.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 80,
    "size": 3116,
    "docstring": "",
    "keywords": [],
    "functions": [
      "str_to_bool"
    ],
    "classes": [],
    "imports": [
      "argparse",
      "twitchtube.video"
    ],
    "preview": "import argparse\n\nfrom twitchtube.video import make_video\n\nparser = argparse.ArgumentParser(description=\"\")\n\n\ndef str_to_bool(value):\n    if isinstance(value, bool):\n        return value\n    if value.lower() in (\"yes\", \"true\", \"t\", \"y\", \"1\"):\n        return True\n    elif value.lower() in (\"no\", \"false\", \"f\", \"n\", \"0\"):\n        return False\n    else:\n        raise argparse.ArgumentTypeError(\"Boolean value expected.\")\n\n\nparser.add_argument(\"--data\", type=str, help=\"\")\nparser.add_argument(\"--blacklist\", type=str, help=\"\")",
    "last_modified": "2025-03-28T18:37:10.706376"
  },
  {
    "id": "1806",
    "name": "final_video.py.py",
    "path": "github_repo/scripts/01_core_ai_analysis/final_video.py_consolidated/final_video.py.py",
    "category": "01_core_ai_analysis",
    "type": "youtube",
    "lines": 58,
    "size": 1810,
    "docstring": "",
    "keywords": [],
    "functions": [
      "make_final_video"
    ],
    "classes": [],
    "imports": [
      "moviepy.editor",
      "utils.console"
    ],
    "preview": "from moviepy.editor import (\n    AudioFileClip,\n    CompositeAudioClip,\n    CompositeVideoClip,\n    ImageClip,\n    VideoFileClip,\n    concatenate_audioclips,\n    concatenate_videoclips,\n)\n\nfrom utils.console import print_step\n\nW, H = 1080, 1920\n\n\ndef make_final_video(number_of_clips):\n    print_step(\"Creating the final video \ud83c\udfa5\")\n    VideoFileClip.reW = lambda clip: clip.resize(width=W)\n    VideoFileClip.reH = lambda clip: clip.resize(width=H)\n",
    "last_modified": "2025-09-13T05:53:59.421604"
  },
  {
    "id": "1807",
    "name": "final_video.py_02.py",
    "path": "github_repo/scripts/01_core_ai_analysis/final_video.py_consolidated/final_video.py_02.py",
    "category": "01_core_ai_analysis",
    "type": "youtube",
    "lines": 87,
    "size": 3056,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "createBackground",
      "finalize"
    ],
    "classes": [],
    "imports": [
      "os",
      "random",
      "mutagen.mp3",
      "moviepy.editor",
      "moviepy.video.tools.subtitles",
      "sub"
    ],
    "preview": "import os\nimport random\n\nimport mutagen.mp3 as MP3\nfrom moviepy.editor import (\n    AudioFileClip,\n    CompositeVideoClip,\n    TextClip,\n    VideoFileClip,\n    concatenate_videoclips,\n)\nfrom moviepy.video.tools.subtitles import SubtitlesClip\nfrom sub import *\n\n\ndef createBackground(\n    audio_path, videoPath\n):  # the path to the mp3 file, the path to the video FILES (folder)\n    print(\"Creating background for the video...Gathering Audio and Video files\")\n    audio = MP3.MP3(audio_path)",
    "last_modified": "2025-09-13T05:53:29.595240"
  },
  {
    "id": "1808",
    "name": "config.py.py",
    "path": "github_repo/scripts/01_core_ai_analysis/config.py_consolidated/config.py.py",
    "category": "01_core_ai_analysis",
    "type": "analysis",
    "lines": 33,
    "size": 987,
    "docstring": "Configuration settings for the Transcription Analyzer",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [],
    "preview": "\"\"\"\nConfiguration settings for the Transcription Analyzer\n\"\"\"\n\n# Whisper model options (smaller = faster, less accurate)\n# Options: \"tiny\", \"base\", \"small\", \"medium\", \"large\"\nWHISPER_MODEL = \"base\"\n\n# OpenAI model for analysis\nOPENAI_MODEL = \"gpt-4o\"\n\n# Analysis settings\nANALYSIS_TEMPERATURE = 0.3\nANALYSIS_MAX_TOKENS = 2000\n\n# File processing settings\nSUPPORTED_AUDIO_FORMATS = ['.mp3', '.mp4', '.MP3', '.MP4']\nAUDIO_QUALITY = \"medium\"  # Options: \"low\", \"medium\", \"high\"\n\n# Output settings",
    "last_modified": "2025-10-09T05:16:03.900549"
  },
  {
    "id": "1809",
    "name": "config.py_03.py",
    "path": "github_repo/scripts/01_core_ai_analysis/config.py_consolidated/config.py_03.py",
    "category": "01_core_ai_analysis",
    "type": "setup",
    "lines": 22,
    "size": 744,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [],
    "preview": "# MODEL SETTINGS\nMODEL = \"text-davinci-003\"\nAPI_PARAM = {\n    \"engine\": MODEL,\n    \"max_tokens\": 512,\n    \"temperature\": 0.77,\n    \"top_p\": 1,\n    \"frequency_penalty\": 0.28,\n    \"presence_penalty\": 0.13,\n}\n# VIDEO SETTINGS\nCHANNEL_NAME = \"historyfactstv\"\nDURATION = 8\nSIZE = (1080, 1920)\nFPS = 30\n# FOLDERS\nVIDEO = \"video\"\nMUSIC = \"music\"\n\nVID_TO_GENRATE = 12  # How many videos generate for each request",
    "last_modified": "2025-03-28T18:36:56"
  },
  {
    "id": "1810",
    "name": "config.py_02.py",
    "path": "github_repo/scripts/01_core_ai_analysis/config.py_consolidated/config.py_02.py",
    "category": "01_core_ai_analysis",
    "type": "setup",
    "lines": 18,
    "size": 359,
    "docstring": "Shared configuration for all Python projects",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "os",
      "dotenv"
    ],
    "preview": "\"\"\"\nShared configuration for all Python projects\n\"\"\"\nimport os\nfrom dotenv import load_dotenv\n\n# Load environment variables\nload_dotenv(os.path.expanduser(\"~/.env\"))\n\n# API Keys\nOPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\nYOUTUBE_API_KEY = os.getenv(\"YOUTUBE_API_KEY\")\n\n# Common settings\nDEFAULT_MODEL = \"gpt-4o\"\nDEFAULT_TEMPERATURE = 0.3\nLOG_LEVEL = \"INFO\"\n",
    "last_modified": "2025-10-09T05:27:15.568467"
  },
  {
    "id": "1811",
    "name": "settings.py_02.py",
    "path": "github_repo/scripts/01_core_ai_analysis/settings.py_consolidated/settings.py_02.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 73,
    "size": 2607,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "generateConfigFile",
      "loadValues"
    ],
    "classes": [],
    "imports": [
      "configparser",
      "os",
      "sys"
    ],
    "preview": "import configparser\nimport os\nfrom sys import platform\n\ncurrentPath = os.path.dirname(os.path.realpath(__file__))\n\n\naddress = \"127.0.0.1\"\nFTP_PORT = 2121\nHTTP_PORT = 8000\n\nFTP_USER = \"Tom\"\nFTP_PASSWORD = \"password\"\n\nautoLogin = False\n\nblock_size = 262144\n\nconfig = configparser.ConfigParser()\n",
    "last_modified": "2025-09-13T05:53:31.587377"
  },
  {
    "id": "1812",
    "name": "settings.py.py",
    "path": "github_repo/scripts/01_core_ai_analysis/settings.py_consolidated/settings.py.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 126,
    "size": 4000,
    "docstring": "",
    "keywords": [],
    "functions": [
      "OPENAI_MAX_TOKEN_WINDOW",
      "load_settings",
      "export"
    ],
    "classes": [
      "Settings"
    ],
    "imports": [
      "loguru",
      "pydantic_settings",
      "zenml.client",
      "zenml.exceptions"
    ],
    "preview": "from loguru import logger\nfrom pydantic_settings import BaseSettings, SettingsConfigDict\nfrom zenml.client import Client\nfrom zenml.exceptions import EntityExistsError\n\n\nclass Settings(BaseSettings):\n    model_config = SettingsConfigDict(env_file=\".env\", env_file_encoding=\"utf-8\")\n\n    # --- Required settings even when working locally. ---\n\n    # OpenAI API\n    OPENAI_MODEL_ID: str = \"gpt-4o-mini\"\n    OPENAI_API_KEY: str | None = None\n\n    # Huggingface API\n    HUGGINGFACE_ACCESS_TOKEN: str | None = None\n\n    # Comet ML (during training)\n    COMET_API_KEY: str | None = None",
    "last_modified": "2025-05-04T22:47:11.573896"
  },
  {
    "id": "1813",
    "name": "setup.py.py",
    "path": "github_repo/scripts/01_core_ai_analysis/setup.py_consolidated/setup.py.py",
    "category": "01_core_ai_analysis",
    "type": "analysis",
    "lines": 105,
    "size": 3527,
    "docstring": "Setup script for the Transcription Analyzer",
    "keywords": [],
    "functions": [
      "check_python_version",
      "install_requirements",
      "check_ffmpeg",
      "setup_env_file",
      "main"
    ],
    "classes": [],
    "imports": [
      "os",
      "sys",
      "subprocess",
      "pathlib"
    ],
    "preview": "#!/usr/bin/env python3\n\"\"\"\nSetup script for the Transcription Analyzer\n\"\"\"\n\nimport os\nimport sys\nimport subprocess\nfrom pathlib import Path\n\ndef check_python_version():\n    \"\"\"Check if Python version is 3.8 or higher.\"\"\"\n    if sys.version_info < (3, 8):\n        print(\"\u274c Python 3.8 or higher is required\")\n        print(f\"Current version: {sys.version}\")\n        return False\n    print(f\"\u2705 Python version: {sys.version.split()[0]}\")\n    return True\n\ndef install_requirements():",
    "last_modified": "2025-10-09T05:14:33.307201"
  },
  {
    "id": "1814",
    "name": "setup.py_02.py",
    "path": "github_repo/scripts/01_core_ai_analysis/setup.py_consolidated/setup.py_02.py",
    "category": "01_core_ai_analysis",
    "type": "setup",
    "lines": 27,
    "size": 614,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "setuptools"
    ],
    "preview": "from setuptools import setup\n\nsetup(\n    name=\"deepseek-python-analyzer\",\n    version=\"0.1.0\",\n    py_modules=[\"deepseek_python\"],\n    install_requires=[\n        \"matplotlib\",\n        \"networkx\",\n        \"radon\",\n        \"pylint\",\n        \"flake8\",\n        \"mypy\",\n        \"jsonschema\",\n    ],\n    entry_points={\n        \"console_scripts\": [\n            \"deepseek-python=deepseek_python:main\",\n        ],\n    },",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1815",
    "name": "categorize_html.py",
    "path": "github_repo/scripts/01_core_ai_analysis/ai_generation/categorize_html.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 130,
    "size": 3829,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "categorize_html",
      "generate_html",
      "save_html"
    ],
    "classes": [],
    "imports": [
      "os",
      "bs4"
    ],
    "preview": "import os\n\nfrom bs4 import BeautifulSoup\n\n# Directory containing HTML files\nHTML_DIR = \"/Users/steven/Documents/HTML/html\"\n\n# Define categories and keywords (you can customize these)\nCATEGORIES = {\n    \"Art\": [\n        \"illustration\",\n        \"painting\",\n        \"artistic\",\n        \"graphic design\",\n        \"creative artwork\",\n    ],\n    \"Technology\": [\n        \"API\",\n        \"automation\",\n        \"script\",",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1816",
    "name": "test_google_gallery_logic.py",
    "path": "github_repo/scripts/01_core_ai_analysis/ai_generation/test_google_gallery_logic.py",
    "category": "01_core_ai_analysis",
    "type": "testing",
    "lines": 65,
    "size": 2378,
    "docstring": "",
    "keywords": [
      "testing"
    ],
    "functions": [
      "test_create_thumbnails",
      "test_generate_images_data"
    ],
    "classes": [
      "GoogleGalleryTestCase"
    ],
    "imports": [
      "os",
      "unittest",
      "unittest",
      "simplegallery.test.helpers",
      "simplegallery.logic.variants.google_gallery_logic",
      "testfixtures"
    ],
    "preview": "import os\nimport unittest\nfrom unittest import mock\n\nimport simplegallery.test.helpers as helpers\nfrom simplegallery.logic.variants.google_gallery_logic import GoogleGalleryLogic\nfrom testfixtures import TempDirectory\n\n\nclass GoogleGalleryTestCase(unittest.TestCase):\n\n    remote_link = \"https://photos.app.goo.gl/cevaz94hQiF8Z5p67\"\n\n    @mock.patch(\"builtins.input\", side_effect=[\"\", \"\", \"\", \"\"])\n    def test_create_thumbnails(self, input):\n        with TempDirectory() as tempdir:\n            # Init files gallery logic\n            gallery_config = helpers.init_gallery_and_read_gallery_config(\n                tempdir.path, self.remote_link\n            )",
    "last_modified": "2025-09-13T05:53:52.853674"
  },
  {
    "id": "1817",
    "name": "testFonts.py",
    "path": "github_repo/scripts/01_core_ai_analysis/ai_generation/testFonts.py",
    "category": "01_core_ai_analysis",
    "type": "testing",
    "lines": 106,
    "size": 2840,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "create_text_slide",
      "create_intro",
      "create_transition",
      "create_outro"
    ],
    "classes": [],
    "imports": [
      "sys",
      "modules.clipEditor",
      "modules.configHandler",
      "PIL"
    ],
    "preview": "import sys\n\nfrom modules.clipEditor import Slide\nfrom modules.configHandler import *\nfrom PIL import Image, ImageDraw, ImageFont\n\n\ndef create_text_slide(slide):\n    # Create and save a text slide based on the Slide object that is passed\n    assert type(slide) == Slide\n\n    if slide.customBg:\n        image = Image.open(f\"./res/{slide.bgName}\")\n        if image.size != slide.size:\n            image = image.resize(slide.size)\n    else:\n        image = Image.new(\"RGB\", slide.size, color=slide.bgColor)\n    drawImage = ImageDraw.Draw(image)\n\n    # Taken from https://stackoverflow.com/a/61891053",
    "last_modified": "2025-03-28T18:37:01.441426"
  },
  {
    "id": "1818",
    "name": "VideoEdit.py",
    "path": "github_repo/scripts/01_core_ai_analysis/ai_generation/VideoEdit.py",
    "category": "01_core_ai_analysis",
    "type": "youtube",
    "lines": 72,
    "size": 2572,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "__init__",
      "create_dir",
      "create_movie"
    ],
    "classes": [
      "VideoEditor"
    ],
    "imports": [
      "os",
      "moviepy.editor",
      "PIL"
    ],
    "preview": "# This clas will hold the class that edits the images into video\n# TODO: Once complete, remove global import and only import required\n\nimport os  # used for some file grabbing\n\nfrom moviepy.editor import *  # movie editor\nfrom PIL import Image  # Image library\n\n\nclass VideoEditor:\n    def __init__(self, num_replies, video_name):\n        self.num_replies = num_replies\n        self.video_name = video_name\n        self.image_path = \"../images/\"\n        self.audio_path = \"../audio/\"\n        self.save_path = \"../edited_videos/\"\n\n        self.create_dir()  # Creates the edited videos dir if it doesnt exist\n\n        print(self.num_replies)",
    "last_modified": "2025-09-13T05:53:51.715010"
  },
  {
    "id": "1819",
    "name": "show.py",
    "path": "github_repo/scripts/01_core_ai_analysis/ai_generation/show.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 183,
    "size": 6352,
    "docstring": "",
    "keywords": [],
    "functions": [
      "search_packages_info",
      "print_results",
      "add_options",
      "run",
      "_get_requiring_packages"
    ],
    "classes": [
      "ShowCommand",
      "_PackageInfo"
    ],
    "imports": [
      "logging",
      "optparse",
      "typing",
      "pip._internal.cli.base_command",
      "pip._internal.cli.status_codes",
      "pip._internal.metadata",
      "pip._internal.utils.misc",
      "pip._vendor.packaging.utils"
    ],
    "preview": "import logging\nfrom optparse import Values\nfrom typing import Generator, Iterable, Iterator, List, NamedTuple, Optional\n\nfrom pip._internal.cli.base_command import Command\nfrom pip._internal.cli.status_codes import ERROR, SUCCESS\nfrom pip._internal.metadata import BaseDistribution, get_default_environment\nfrom pip._internal.utils.misc import write_output\nfrom pip._vendor.packaging.utils import canonicalize_name\n\nlogger = logging.getLogger(__name__)\n\n\nclass ShowCommand(Command):\n    \"\"\"\n    Show information about one or more installed packages.\n\n    The output is in RFC-compliant mail header format.\n    \"\"\"\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1820",
    "name": "run.py",
    "path": "github_repo/scripts/01_core_ai_analysis/ai_generation/run.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 44,
    "size": 1586,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "create_endpoint"
    ],
    "classes": [],
    "imports": [
      "loguru",
      "llm_engineering.model.utils",
      "llm_engineering.settings",
      "config",
      "sagemaker_huggingface",
      "sagemaker.enums",
      "sagemaker.huggingface"
    ],
    "preview": "from loguru import logger\n\ntry:\n    from sagemaker.enums import EndpointType\n    from sagemaker.huggingface import get_huggingface_llm_image_uri\nexcept ModuleNotFoundError:\n    logger.warning(\n        \"Couldn't load SageMaker imports. Run 'poetry install --with aws' to support AWS.\"\n    )\n\nfrom llm_engineering.model.utils import ResourceManager\nfrom llm_engineering.settings import settings\n\nfrom .config import hugging_face_deploy_config, model_resource_config\nfrom .sagemaker_huggingface import DeploymentService, SagemakerHuggingfaceStrategy\n\n\ndef create_endpoint(endpoint_type=EndpointType.INFERENCE_COMPONENT_BASED) -> None:\n    assert settings.AWS_ARN_ROLE is not None, \"AWS_ARN_ROLE is not set in the .env file.\"\n",
    "last_modified": "2025-09-13T05:53:41.967624"
  },
  {
    "id": "1821",
    "name": "onedrive_gallery_logic 2.py",
    "path": "github_repo/scripts/01_core_ai_analysis/ai_generation/onedrive_gallery_logic 2.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 108,
    "size": 3893,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "parse_photo_link",
      "create_thumbnails",
      "generate_images_data"
    ],
    "classes": [
      "OnedriveGalleryLogic"
    ],
    "imports": [
      "time",
      "pkg_resources",
      "simplegallery.common",
      "simplegallery.media",
      "selenium",
      "selenium.webdriver.firefox.options",
      "simplegallery.logic.base_gallery_logic"
    ],
    "preview": "import time\n\nimport pkg_resources\nimport simplegallery.common as spg_common\nimport simplegallery.media as spg_media\nfrom selenium import webdriver\nfrom selenium.webdriver.firefox.options import Options\nfrom simplegallery.logic.base_gallery_logic import BaseGalleryLogic\n\n\ndef parse_photo_link(photo_url):\n    \"\"\"\n    Extracts the base URL (URL without query parameters) and the photo name from a Onedrive photo URL\n    :param photo_url: photo URL\n    :return: base URL and photo name\n    \"\"\"\n    base_url = photo_url.split(\"?\")[0]\n    name = base_url.split(\"/\")[-1]\n\n    return base_url, name",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1822",
    "name": "media.py",
    "path": "github_repo/scripts/01_core_ai_analysis/ai_generation/media.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 257,
    "size": 8299,
    "docstring": "",
    "keywords": [
      "opencv",
      "analysis",
      "web_tools"
    ],
    "functions": [
      "rotate_image_by_orientation",
      "get_thumbnail_size",
      "create_image_thumbnail",
      "create_video_thumbnail",
      "create_thumbnail",
      "get_remote_image_size",
      "get_image_size",
      "get_video_size",
      "get_image_description",
      "parse_exif_datetime"
    ],
    "classes": [],
    "imports": [
      "os",
      "io",
      "cv2",
      "requests",
      "PIL",
      "datetime",
      "simplegallery.common"
    ],
    "preview": "import os\nfrom io import BytesIO\n\nimport cv2\nimport requests\nfrom PIL import ExifTags, Image, ImageFile\n\nImageFile.LOAD_TRUNCATED_IMAGES = True\nfrom datetime import datetime\n\nimport simplegallery.common as spg_common\n\n# Mapping of the string representation if an Exif tag to its id\nEXIF_TAG_MAP = {ExifTags.TAGS[tag]: tag for tag in ExifTags.TAGS}\n\n\ndef rotate_image_by_orientation(image):\n    \"\"\"\n    Rotates an image according to it's Orientation EXIF Tag\n    :param im: Image",
    "last_modified": "2025-09-13T05:53:52.709810"
  },
  {
    "id": "1823",
    "name": "MetadataHandler.py",
    "path": "github_repo/scripts/01_core_ai_analysis/ai_generation/MetadataHandler.py",
    "category": "01_core_ai_analysis",
    "type": "analysis",
    "lines": 199,
    "size": 8243,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "__init__",
      "create_metadata",
      "get_metadata_config",
      "get_metadata",
      "refresh_metadata",
      "get_youtube_description",
      "get_youtube_title",
      "get_number_in_series",
      "create_youtube_description",
      "create_youtube_title"
    ],
    "classes": [
      "MetadataHandler"
    ],
    "imports": [
      "logging",
      "os",
      "random",
      "datetime",
      "jinja2",
      "moviepy.editor",
      "moviepy.video.fx.resize",
      "moviepy.video.fx.rotate",
      "src",
      "src.APIHandler"
    ],
    "preview": "import logging\nimport os\nimport random\nfrom datetime import time, timedelta\n\nfrom jinja2 import Environment, FileSystemLoader\nfrom moviepy.editor import CompositeVideoClip, ImageClip, TextClip, VideoFileClip\nfrom moviepy.video.fx.resize import resize\nfrom moviepy.video.fx.rotate import rotate\nfrom src import Clip, utils\nfrom src.APIHandler import APIHandler\n\nimport config\n\n\nclass MetadataHandler:\n    def __init__(self, game: str, asset_path: str, output_path: str):\n        self.output_path = output_path\n        self.game = game\n        self.asset_path = asset_path",
    "last_modified": "2025-09-13T05:53:45.651344"
  },
  {
    "id": "1824",
    "name": "impo.py",
    "path": "github_repo/scripts/01_core_ai_analysis/ai_generation/impo.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 49,
    "size": 1529,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "create_thumbnail",
      "create_contact_sheet",
      "save_contact_sheet"
    ],
    "classes": [],
    "imports": [
      "os",
      "PIL"
    ],
    "preview": "import os\n\nfrom PIL import Image\n\n\ndef create_thumbnail(image_path, thumbnail_size=(100, 100)):\n    with Image.open(image_path) as img:\n        img.thumbnail(thumbnail_size)\n        return img\n\n\ndef create_contact_sheet(directory, thumbnail_size=(100, 100), sheet_size=(500, 500)):\n    thumbnails = []\n    for subdir, dirs, files in os.walk(directory):\n        for file in files:\n            if file.lower().endswith((\"png\", \"jpg\", \"jpeg\", \"gif\", \"bmp\")):\n                image_path = os.path.join(subdir, file)\n                thumbnail = create_thumbnail(image_path, thumbnail_size)\n                thumbnails.append(thumbnail)\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1825",
    "name": "process_leonardo_20250102104751.py",
    "path": "github_repo/scripts/01_core_ai_analysis/ai_generation/process_leonardo_20250102104751.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 52,
    "size": 1578,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "csv",
      "json"
    ],
    "preview": "import csv\nimport json\n\n# File paths\ninput_file_path = \"/Users/steven/Pictures/leonardo.json\"  # Path to the input JSON file\noutput_file_path = (\n    \"/Users/steven/Pictures/leonardo_generations_full.csv\"  # Path to the output CSV file\n)\n\n# Headers for the CSV file\nheaders = [\n    \"id\",\n    \"prompt\",\n    \"negativePrompt\",\n    \"url\",\n    \"motionMP4URL\",\n    \"motionStrength\",\n    \"height\",\n    \"width\",\n    \"duration\",",
    "last_modified": "2025-09-13T05:53:49.665378"
  },
  {
    "id": "1826",
    "name": "polly_main.py",
    "path": "github_repo/scripts/01_core_ai_analysis/ai_generation/polly_main.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 64,
    "size": 1788,
    "docstring": "",
    "keywords": [],
    "functions": [
      "polly",
      "tts_task_resp",
      "transcript_generator",
      "get_word_by_transcript"
    ],
    "classes": [],
    "imports": [
      "json",
      "boto3",
      "botocore.config",
      "utilities.const"
    ],
    "preview": "import json\n\nimport boto3\nfrom botocore.config import Config\nfrom utilities.const import AWS_ACCESS_KEY, AWS_SEC_KEY, S3_BUCKET, get_current_date\n\nmy_config = Config(\n    region_name=\"ap-south-1\",\n    signature_version=\"v4\",\n    retries={\"max_attempts\": 3, \"mode\": \"standard\"},\n)\n\nclient = boto3.client(\n    \"polly\",\n    config=my_config,\n    aws_access_key_id=AWS_ACCESS_KEY,\n    aws_secret_access_key=AWS_SEC_KEY,\n)\n\n",
    "last_modified": "2025-09-13T05:53:28.648046"
  },
  {
    "id": "1827",
    "name": "alphabet-html-gall.py",
    "path": "github_repo/scripts/01_core_ai_analysis/ai_generation/alphabet-html-gall.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 125,
    "size": 4184,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "generate_grouped_gallery"
    ],
    "classes": [],
    "imports": [
      "os"
    ],
    "preview": "import os\n\n\ndef generate_grouped_gallery(folder_path, output_html, group_size=4):\n    \"\"\"\n    Generates an HTML gallery grouped alphabetically by sets of images and MP4s.\n    \"\"\"\n    # Get all valid files\n    valid_extensions = (\".jpg\", \".jpeg\", \".png\", \".mp4\")\n    all_files = [f for f in os.listdir(folder_path) if f.lower().endswith(valid_extensions)]\n\n    # Sort files alphabetically\n    all_files.sort()\n\n    # Group files into sets of the specified size\n    grouped_files = [all_files[i : i + group_size] for i in range(0, len(all_files), group_size)]\n\n    # Start the HTML structure\n    html_content = \"\"\"\n    <!DOCTYPE html>",
    "last_modified": "2025-09-13T05:54:15.432728"
  },
  {
    "id": "1828",
    "name": "contacts.py",
    "path": "github_repo/scripts/01_core_ai_analysis/ai_generation/contacts.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 50,
    "size": 1836,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "create_contact_sheet"
    ],
    "classes": [],
    "imports": [
      "os",
      "PIL"
    ],
    "preview": "import os\n\nfrom PIL import Image\n\n\ndef create_contact_sheet(source_folder, output_size=(2000, 2000), images_per_row=2):\n    # List all the image files in the source folder with the correct extension\n    image_files = [\n        os.path.join(source_folder, f)\n        for f in os.listdir(source_folder)\n        if f.endswith((\".png\", \".jpg\", \".jpeg\"))\n    ]\n\n    # Create a new blank white image to serve as the contact sheet\n    contact_sheet = Image.new(\"RGB\", output_size, \"white\")\n\n    # Define the new width and height for each image\n    new_width = output_size[0] // images_per_row\n    # Calculate the height while maintaining aspect ratio\n    new_height = int(new_width / (16 / 9))",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1829",
    "name": "gallery_init_20241204123524.py",
    "path": "github_repo/scripts/01_core_ai_analysis/ai_generation/gallery_init_20241204123524.py",
    "category": "01_core_ai_analysis",
    "type": "setup",
    "lines": 292,
    "size": 9791,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "parse_args",
      "check_if_gallery_creation_possible",
      "check_if_gallery_already_exists",
      "create_gallery_folder_structure",
      "create_gallery_json",
      "main"
    ],
    "classes": [],
    "imports": [
      "argparse",
      "glob",
      "importlib.resources",
      "json",
      "os",
      "shutil",
      "sys",
      "distutils.dir_util",
      "simplegallery.common",
      "simplegallery.logic.gallery_logic"
    ],
    "preview": "import argparse\nimport glob\nimport importlib.resources\nimport json\nimport os\nimport shutil\nimport sys\nfrom distutils.dir_util import copy_tree\n\nimport simplegallery.common as spg_common\nimport simplegallery.logic.gallery_logic as gallery_logic\n\n\ndef parse_args():\n    \"\"\"\n    Configures the argument parser\n    :return: Parsed arguments\n    \"\"\"\n\n    description = \"\"\"Initializes a new Simple Photo Gallery in the specified folder (default is the current folder).",
    "last_modified": "2025-09-13T05:55:18.370327"
  },
  {
    "id": "1830",
    "name": "mklabels.py",
    "path": "github_repo/scripts/01_core_ai_analysis/ai_generation/mklabels.py",
    "category": "01_core_ai_analysis",
    "type": "web_tools",
    "lines": 63,
    "size": 1293,
    "docstring": "webencodings.mklabels\n~~~~~~~~~~~~~~~~~~~~~\n\nRegenarate the webencodings.labels module.\n\n:copyright: Copyright 2012 by Simon Sapin\n:license: BSD, see LICENSE for details.",
    "keywords": [
      "testing",
      "analysis"
    ],
    "functions": [
      "assert_lower",
      "generate"
    ],
    "classes": [],
    "imports": [
      "json",
      "urllib",
      "urllib.request"
    ],
    "preview": "\"\"\"\n\nwebencodings.mklabels\n~~~~~~~~~~~~~~~~~~~~~\n\nRegenarate the webencodings.labels module.\n\n:copyright: Copyright 2012 by Simon Sapin\n:license: BSD, see LICENSE for details.\n\n\"\"\"\n\nimport json\n\ntry:\n    from urllib import urlopen\nexcept ImportError:\n    from urllib.request import urlopen\n\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1831",
    "name": "playlist_20221230180403.py",
    "path": "github_repo/scripts/01_core_ai_analysis/ai_generation/playlist_20221230180403.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 212,
    "size": 6700,
    "docstring": "",
    "keywords": [
      "youtube",
      "analysis",
      "web_tools"
    ],
    "functions": [
      "generate_token",
      "get_youtube_url",
      "download_mp3",
      "last_fm_artist_info",
      "set_metadata",
      "write_tracks",
      "write_playlist",
      "split_spotify_uri",
      "get_os",
      "get_folder"
    ],
    "classes": [],
    "imports": [
      "argparse",
      "json",
      "os",
      "re",
      "subprocess",
      "urllib.parse",
      "urllib.request",
      "secret",
      "spotipy",
      "spotipy.oauth2"
    ],
    "preview": "import argparse\nimport json\nimport os\nimport re\nimport subprocess\nimport urllib.parse\nimport urllib.request\n\nimport secret\nimport spotipy\nimport spotipy.oauth2 as oauth2\nfrom requests.utils import quote\n\n\ndef generate_token():\n    credentials = oauth2.SpotifyClientCredentials(\n        client_id=secret.SPOTIFY_CLIENT_ID, client_secret=secret.SPOTIFY_CLIENT_SECRET\n    )\n    token = credentials.get_access_token()\n    return token",
    "last_modified": "2025-09-13T05:54:01.977236"
  },
  {
    "id": "1832",
    "name": "videogenerator.py",
    "path": "github_repo/scripts/01_core_ai_analysis/ai_generation/videogenerator.py",
    "category": "01_core_ai_analysis",
    "type": "youtube",
    "lines": 101,
    "size": 3278,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "__init__",
      "_pick_random_file",
      "make_audio",
      "make_video",
      "header",
      "content",
      "footer",
      "_make_main_clip",
      "generate_video"
    ],
    "classes": [
      "VideoGenerator"
    ],
    "imports": [
      "os",
      "random",
      "typing",
      "moviepy.editor"
    ],
    "preview": "import os\nimport random\nfrom typing import Tuple\n\nimport moviepy.editor as me\n\n\nclass VideoGenerator:\n\n    def __init__(\n        self,\n        video_folder: str = \"video\",\n        music_folder: str = \"music\",\n        duration: int = 8,\n        size: Tuple[int, int] = (1080, 1920),\n        channel_name: str = \"historyfactstv\",\n    ) -> None:\n        self.video_folder = video_folder\n        self.music_folder = music_folder\n        self.video_list = os.listdir(video_folder)",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1833",
    "name": "metadata.py",
    "path": "github_repo/scripts/01_core_ai_analysis/ai_generation/metadata.py",
    "category": "01_core_ai_analysis",
    "type": "analysis",
    "lines": 35,
    "size": 1407,
    "docstring": "Metadata generation logic for source distributions.",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "generate_metadata"
    ],
    "classes": [],
    "imports": [
      "os",
      "pip._internal.build_env",
      "pip._internal.exceptions",
      "pip._internal.utils.subprocess",
      "pip._internal.utils.temp_dir",
      "pip._vendor.pyproject_hooks"
    ],
    "preview": "\"\"\"Metadata generation logic for source distributions.\"\"\"\n\nimport os\n\nfrom pip._internal.build_env import BuildEnvironment\nfrom pip._internal.exceptions import InstallationSubprocessError, MetadataGenerationFailed\nfrom pip._internal.utils.subprocess import runner_with_spinner_message\nfrom pip._internal.utils.temp_dir import TempDirectory\nfrom pip._vendor.pyproject_hooks import BuildBackendHookCaller\n\n\ndef generate_metadata(\n    build_env: BuildEnvironment, backend: BuildBackendHookCaller, details: str\n) -> str:\n    \"\"\"Generate metadata using mechanisms described in PEP 517.\n\n    Returns the generated metadata directory.\n    \"\"\"\n    metadata_tmpdir = TempDirectory(kind=\"modern-metadata\", globally_managed=True)\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1834",
    "name": "batch_upload.py",
    "path": "github_repo/scripts/01_core_ai_analysis/ai_generation/batch_upload.py",
    "category": "01_core_ai_analysis",
    "type": "youtube",
    "lines": 62,
    "size": 2127,
    "docstring": "",
    "keywords": [
      "analysis",
      "youtube"
    ],
    "functions": [
      "authenticate",
      "upload_video",
      "generate_title_description",
      "main"
    ],
    "classes": [],
    "imports": [
      "os",
      "google_auth_oauthlib.flow",
      "googleapiclient.discovery",
      "googleapiclient.errors",
      "googleapiclient.http"
    ],
    "preview": "import os\n\nimport google_auth_oauthlib.flow\nimport googleapiclient.discovery\nimport googleapiclient.errors\nimport googleapiclient.http\n\n\ndef authenticate():\n    os.environ[\"OAUTHLIB_INSECURE_TRANSPORT\"] = \"1\"\n    api_service_name = \"youtube\"\n    api_version = \"v3\"\n    client_secrets_file = \"client_secrets.json\"\n    flow = google_auth_oauthlib.flow.InstalledAppFlow.from_client_secrets_file(\n        client_secrets_file, [\"https://www.googleapis.com/auth/youtube.upload\"]\n    )\n    credentials = flow.run_local_server()\n    youtube = googleapiclient.discovery.build(\n        api_service_name, api_version, credentials=credentials\n    )",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1835",
    "name": "gallery_init_20241204123444.py",
    "path": "github_repo/scripts/01_core_ai_analysis/ai_generation/gallery_init_20241204123444.py",
    "category": "01_core_ai_analysis",
    "type": "setup",
    "lines": 292,
    "size": 9791,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "parse_args",
      "check_if_gallery_creation_possible",
      "check_if_gallery_already_exists",
      "create_gallery_folder_structure",
      "create_gallery_json",
      "main"
    ],
    "classes": [],
    "imports": [
      "argparse",
      "glob",
      "importlib.resources",
      "json",
      "os",
      "shutil",
      "sys",
      "distutils.dir_util",
      "simplegallery.common",
      "simplegallery.logic.gallery_logic"
    ],
    "preview": "import argparse\nimport glob\nimport importlib.resources\nimport json\nimport os\nimport shutil\nimport sys\nfrom distutils.dir_util import copy_tree\n\nimport simplegallery.common as spg_common\nimport simplegallery.logic.gallery_logic as gallery_logic\n\n\ndef parse_args():\n    \"\"\"\n    Configures the argument parser\n    :return: Parsed arguments\n    \"\"\"\n\n    description = \"\"\"Initializes a new Simple Photo Gallery in the specified folder (default is the current folder).",
    "last_modified": "2025-09-13T05:55:18.122361"
  },
  {
    "id": "1836",
    "name": "analyzer 1.py",
    "path": "github_repo/scripts/01_core_ai_analysis/ai_generation/analyzer 1.py",
    "category": "01_core_ai_analysis",
    "type": "analysis",
    "lines": 812,
    "size": 31839,
    "docstring": "This script is intended for use as a GYP_GENERATOR. It takes as input (by way of\nthe generator flag config_path) the path of a json file that dictates the files\nand targets to search for. The following keys are supported:\nfiles: list of paths (relative) of the files to search for.\ntest_targets: unqualified target names to search for. Any target in this list\nthat depends upon a file in |files| is output regardless of the type of target\nor chain of dependencies.\nadditional_compile_targets: Unqualified targets to search for in addition to\ntest_targets. Targets in the combined list that depend upon a file in |files|\nare not necessarily output. For example, if the target is of type none then the\ntarget is not output (but one of the descendants of the target will be).\n\nThe following is output:\nerror: only supplied if there is an error.\ncompile_targets: minimal set of targets that directly or indirectly (for\n  targets of type none) depend on the files in |files| and is one of the\n  supplied targets or a target that one of the supplied targets depends on.\n  The expectation is this set of targets is passed into a build step. This list\n  always contains the output of test_targets as well.\ntest_targets: set of targets from the supplied |test_targets| that either\n  directly or indirectly depend upon a file in |files|. This list if useful\n  if additional processing needs to be done for certain targets after the\n  build, such as running tests.\nstatus: outputs one of three values: none of the supplied files were found,\n  one of the include files changed so that it should be assumed everything\n  changed (in this case test_targets and compile_targets are not output) or at\n  least one file was found.\ninvalid_targets: list of supplied targets that were not found.\n\nExample:\nConsider a graph like the following:\n  A     D\n / B   C\nA depends upon both B and C, A is of type none and B and C are executables.\nD is an executable, has no dependencies and nothing depends on it.\nIf |additional_compile_targets| = [\"A\"], |test_targets| = [\"B\", \"C\"] and\nfiles = [\"b.cc\", \"d.cc\"] (B depends upon b.cc and D depends upon d.cc), then\nthe following is output:\n|compile_targets| = [\"B\"] B must built as it depends upon the changed file b.cc\nand the supplied target A depends upon it. A is not output as a build_target\nas it is of type none with no rules and actions.\n|test_targets| = [\"B\"] B directly depends upon the change file b.cc.\n\nEven though the file d.cc, which D depends upon, has changed D is not output\nas it was not supplied by way of |additional_compile_targets| or |test_targets|.\n\nIf the generator flag analyzer_output_path is specified, output is written\nthere. Otherwise output is written to stdout.\n\nIn Gyp the \"all\" target is shorthand for the root targets in the files passed\nto gyp. For example, if file \"a.gyp\" contains targets \"a1\" and\n\"a2\", and file \"b.gyp\" contains targets \"b1\" and \"b2\" and \"a2\" has a dependency\non \"b2\" and gyp is supplied \"a.gyp\" then \"all\" consists of \"a1\" and \"a2\".\nNotice that \"b1\" and \"b2\" are not in the \"all\" target as \"b.gyp\" was not\ndirectly supplied to gyp. OTOH if both \"a.gyp\" and \"b.gyp\" are supplied to gyp\nthen the \"all\" target includes \"b1\" and \"b2\".",
    "keywords": [],
    "functions": [
      "_ToGypPath",
      "_ResolveParent",
      "_AddSources",
      "_ExtractSourcesFromAction",
      "_ToLocalPath",
      "_ExtractSources",
      "_WasBuildFileModified",
      "_GetOrCreateTargetByName",
      "_DoesTargetTypeRequireBuild",
      "_GenerateTargets"
    ],
    "classes": [
      "Target",
      "Config",
      "TargetCalculator"
    ],
    "imports": [
      "json",
      "os",
      "posixpath",
      "gyp.common"
    ],
    "preview": "# Copyright (c) 2014 Google Inc. All rights reserved.\n# Use of this source code is governed by a BSD-style license that can be\n# found in the LICENSE file.\n\n\"\"\"\nThis script is intended for use as a GYP_GENERATOR. It takes as input (by way of\nthe generator flag config_path) the path of a json file that dictates the files\nand targets to search for. The following keys are supported:\nfiles: list of paths (relative) of the files to search for.\ntest_targets: unqualified target names to search for. Any target in this list\nthat depends upon a file in |files| is output regardless of the type of target\nor chain of dependencies.\nadditional_compile_targets: Unqualified targets to search for in addition to\ntest_targets. Targets in the combined list that depend upon a file in |files|\nare not necessarily output. For example, if the target is of type none then the\ntarget is not output (but one of the descendants of the target will be).\n\nThe following is output:\nerror: only supplied if there is an error.\ncompile_targets: minimal set of targets that directly or indirectly (for",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1837",
    "name": "metadata_editable.py",
    "path": "github_repo/scripts/01_core_ai_analysis/ai_generation/metadata_editable.py",
    "category": "01_core_ai_analysis",
    "type": "analysis",
    "lines": 35,
    "size": 1437,
    "docstring": "Metadata generation logic for source distributions.",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "generate_editable_metadata"
    ],
    "classes": [],
    "imports": [
      "os",
      "pip._internal.build_env",
      "pip._internal.exceptions",
      "pip._internal.utils.subprocess",
      "pip._internal.utils.temp_dir",
      "pip._vendor.pyproject_hooks"
    ],
    "preview": "\"\"\"Metadata generation logic for source distributions.\"\"\"\n\nimport os\n\nfrom pip._internal.build_env import BuildEnvironment\nfrom pip._internal.exceptions import InstallationSubprocessError, MetadataGenerationFailed\nfrom pip._internal.utils.subprocess import runner_with_spinner_message\nfrom pip._internal.utils.temp_dir import TempDirectory\nfrom pip._vendor.pyproject_hooks import BuildBackendHookCaller\n\n\ndef generate_editable_metadata(\n    build_env: BuildEnvironment, backend: BuildBackendHookCaller, details: str\n) -> str:\n    \"\"\"Generate metadata using mechanisms described in PEP 660.\n\n    Returns the generated metadata directory.\n    \"\"\"\n    metadata_tmpdir = TempDirectory(kind=\"modern-metadata\", globally_managed=True)\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1838",
    "name": "generate_album_pages.py",
    "path": "github_repo/scripts/01_core_ai_analysis/ai_generation/generate_album_pages.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 90,
    "size": 2534,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "read_text_file",
      "generate_album_html"
    ],
    "classes": [],
    "imports": [
      "os"
    ],
    "preview": "import os\n\nTEMPLATE = \"\"\"\n<!DOCTYPE html>\n<html lang=\"en\">\n<head>\n    <meta charset=\"UTF-8\">\n    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n    <title>{album_name}</title>\n    <link rel=\"stylesheet\" href=\"../css/styles.css\">\n</head>\n<body>\n    <div class=\"album-container\">\n        <div class=\"album\">\n            <img src=\"../albums/{folder_name}/album_cover.jpg\" alt=\"{album_name} Cover\">\n            <h3>{album_name}</h3>\n            <audio controls>\n                <source src=\"../albums/{folder_name}/{mp3_file}\" type=\"audio/mpeg\">\n                Your browser does not support the audio element.\n            </audio>",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1839",
    "name": "sort.py",
    "path": "github_repo/scripts/01_core_ai_analysis/ai_generation/sort.py",
    "category": "01_core_ai_analysis",
    "type": "organization",
    "lines": 173,
    "size": 6182,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "get_creation_date",
      "get_image_metadata",
      "get_video_metadata",
      "custom_tags",
      "contains_web_project_files",
      "generate_dry_run_csv"
    ],
    "classes": [],
    "imports": [
      "csv",
      "os",
      "shutil",
      "datetime",
      "ffmpeg",
      "PIL"
    ],
    "preview": "import csv\nimport os\nimport shutil\nfrom datetime import datetime\n\nimport ffmpeg\nfrom PIL import Image\n\n\ndef get_creation_date(filepath):\n    \"\"\"Get the creation date of the file.\"\"\"\n    return datetime.fromtimestamp(os.path.getctime(filepath)).strftime(\"%Y-%m-%d %H:%M:%S\")\n\n\ndef get_image_metadata(filepath):\n    \"\"\"Extract metadata from an image file.\"\"\"\n    with Image.open(filepath) as img:\n        return img.size, os.path.getsize(filepath)\n\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1840",
    "name": "lazy 2.py",
    "path": "github_repo/scripts/01_core_ai_analysis/ai_generation/lazy 2.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 161,
    "size": 6963,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "parse_album_data",
      "generate_html",
      "main"
    ],
    "classes": [],
    "imports": [
      "os"
    ],
    "preview": "import os\n\n\ndef parse_album_data(paths_file):\n    albums = {}\n    with open(paths_file, \"r\", encoding=\"utf-8\") as f:\n        for line in f:\n            line = line.strip()\n            if not line:\n                continue\n            # Process only if the line is in the mp4 folder structure\n            if \"/mp4/\" in line:\n                # Extract album folder name by splitting at \"/mp4/\" then taking first folder\n                parts = line.split(\"/mp4/\")[1].split(\"/\")\n                if not parts:\n                    continue\n                album_key = parts[0]\n                if album_key not in albums:\n                    albums[album_key] = {\n                        \"mp3\": None,",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1841",
    "name": "base_gallery_logic.py",
    "path": "github_repo/scripts/01_core_ai_analysis/ai_generation/base_gallery_logic.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 54,
    "size": 1937,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "__init__",
      "create_thumbnails",
      "generate_images_data",
      "create_images_data_file"
    ],
    "classes": [
      "BaseGalleryLogic"
    ],
    "imports": [
      "json",
      "os",
      "collections"
    ],
    "preview": "import json\nimport os\nfrom collections import OrderedDict\n\n\nclass BaseGalleryLogic:\n    \"\"\"\n    Base class for defining a gallery logic. Derived classes should implement the methods create_thumbnails and\n    generate_images_data to define the specific logic.\n    \"\"\"\n\n    def __init__(self, gallery_config):\n        \"\"\"\n        Initializes the gallery logic\n        :param gallery_config: Gallery config dictionary as read from the gallery.json\n        \"\"\"\n        self.gallery_config = gallery_config\n\n    def create_thumbnails(self, force=False):\n        \"\"\"",
    "last_modified": "2025-05-04T23:28:22"
  },
  {
    "id": "1842",
    "name": "oshelper.py",
    "path": "github_repo/scripts/01_core_ai_analysis/ai_generation/oshelper.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 125,
    "size": 3252,
    "docstring": "OS helper",
    "keywords": [],
    "functions": [
      "absolute_files",
      "absolute_dirs",
      "try_create_lock_file",
      "file_exists",
      "lock_file_exists",
      "try_delete_lock_file",
      "get_track_file",
      "get_album_art_file",
      "get_track_info_file",
      "isdir"
    ],
    "classes": [],
    "imports": [
      "errno",
      "logging",
      "os",
      "shutil"
    ],
    "preview": "\"OS helper\"\n\nimport errno\nimport logging\nimport os\nfrom shutil import copy2, rmtree\n\nDEFAULT_FILE_NAME = \"\"\nLOCK_FILE_NAME = \"CANT_TOUCH_THIS\"\n\n\ndef absolute_files(path):\n    \"Returns the files at this directory as absolute filepaths\"\n    if os.path.isdir(path):\n        files = [os.path.join(path, f) for f in os.listdir(path)]\n        return [f for f in files if os.path.isfile(f)]\n    return []\n\n\ndef absolute_dirs(path):",
    "last_modified": "2025-05-04T23:27:55"
  },
  {
    "id": "1843",
    "name": "process_leonardo_20250102110149.py",
    "path": "github_repo/scripts/01_core_ai_analysis/ai_generation/process_leonardo_20250102110149.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 52,
    "size": 1578,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "csv",
      "json"
    ],
    "preview": "import csv\nimport json\n\n# File paths\ninput_file_path = \"/Users/steven/Pictures/leonardo.json\"  # Path to the input JSON file\noutput_file_path = (\n    \"/Users/steven/Pictures/leonardo_generations_full.csv\"  # Path to the output CSV file\n)\n\n# Headers for the CSV file\nheaders = [\n    \"id\",\n    \"prompt\",\n    \"negativePrompt\",\n    \"url\",\n    \"motionMP4URL\",\n    \"motionStrength\",\n    \"height\",\n    \"width\",\n    \"duration\",",
    "last_modified": "2025-09-13T05:53:49.760131"
  },
  {
    "id": "1844",
    "name": "clipEditor.py",
    "path": "github_repo/scripts/01_core_ai_analysis/ai_generation/clipEditor.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 221,
    "size": 7185,
    "docstring": "",
    "keywords": [
      "opencv",
      "analysis"
    ],
    "functions": [
      "get_max_dimensions",
      "get_video_dimension",
      "get_max_fps",
      "create_text_slide",
      "create_intro",
      "create_transition",
      "create_outro",
      "create_video",
      "__init__"
    ],
    "classes": [
      "Slide"
    ],
    "imports": [
      "random",
      "modules.configHandler",
      "moviepy.editor",
      "PIL",
      "cv2",
      "cv2",
      "cv2"
    ],
    "preview": "#!/usr/bin/env python\nimport random\n\nfrom modules.configHandler import *\nfrom moviepy.editor import *\nfrom PIL import Image, ImageDraw, ImageFont\n\n\nclass Slide:\n\n    text: str  # Text Displayed in the Slide\n    size: tuple  # Size of the Slide as as a tuple (width,height)\n    bgColor: tuple  # Color of the background  as a tuple (R,G,B)\n    txtColor: tuple  # Color of the text as a tuple (R,G,B)\n    fileName: str  # File name of the Slide file , without extension\n    fontName: str  # Name of the Font file used for the text without the extension,\n    #   must be in the /res folder and be a ttf file\n    textRatio: float  # Font size of the text in the Slide\n    customBg = bool  # {True|False} Use Custom Background\n    bgName: (",
    "last_modified": "2025-09-13T05:53:44.981305"
  },
  {
    "id": "1845",
    "name": "conf.py",
    "path": "github_repo/scripts/01_core_ai_analysis/ai_generation/conf.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 155,
    "size": 4818,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "os",
      "sys",
      "savify"
    ],
    "preview": "#!/usr/bin/env python\n#\n# savify documentation build configuration file, created by\n# sphinx-quickstart on Fri Jun  9 13:47:02 2017.\n#\n# This file is execfile()d with the current directory set to its\n# containing dir.\n#\n# Note that not all possible configuration values are present in this\n# autogenerated file.\n#\n# All configuration values have a default; values that are commented out\n# serve to show the default.\n\n# If extensions (or modules to document with autodoc) are in another\n# directory, add these directories to sys.path here. If the directory is\n# relative to the documentation root, use os.path.abspath to make it\n# absolute, like shown here.\n#\nimport os",
    "last_modified": "2025-05-04T23:27:53.601941"
  },
  {
    "id": "1846",
    "name": "database.py",
    "path": "github_repo/scripts/01_core_ai_analysis/ai_generation/database.py",
    "category": "01_core_ai_analysis",
    "type": "analysis",
    "lines": 71,
    "size": 2454,
    "docstring": "MIT License\n\nCopyright (c) 2021-2022 MShawon\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "create_database",
      "update_database"
    ],
    "classes": [],
    "imports": [
      "os",
      "shutil",
      "sqlite3",
      "contextlib",
      "datetime"
    ],
    "preview": "\"\"\"\nMIT License\n\nCopyright (c) 2021-2022 MShawon\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,",
    "last_modified": "2025-05-04T23:27:45"
  },
  {
    "id": "1847",
    "name": "client.py",
    "path": "github_repo/scripts/01_core_ai_analysis/ai_generation/client.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 263,
    "size": 8174,
    "docstring": "",
    "keywords": [
      "testing",
      "youtube",
      "web_tools"
    ],
    "functions": [
      "requestGames",
      "requestClips",
      "testFTPConnection",
      "requestClipsWithoutClips",
      "uploadFile",
      "VideoGeneratorRenderStatus",
      "exportVideo",
      "requestFinishedVideoList",
      "downloadFinishedVideo"
    ],
    "classes": [],
    "imports": [
      "ftplib",
      "sys",
      "traceback",
      "clientUI",
      "requests",
      "scriptwrapper",
      "settings",
      "time"
    ],
    "preview": "import ftplib\nimport sys\nimport traceback\n\nimport clientUI\nimport requests\nimport scriptwrapper\nimport settings\n\nsettings.generateConfigFile()\nhttpaddress = \"%s:%s\" % (settings.address, settings.HTTP_PORT)\nmax_progress = None\ncurrent_progress = None\nrender_message = None\nmusic_categories = [\"None\"]\nmainMenuWindow = None\n\nfrom time import sleep\n\n",
    "last_modified": "2025-09-13T05:53:31.063273"
  },
  {
    "id": "1848",
    "name": "process_leonardo.py",
    "path": "github_repo/scripts/01_core_ai_analysis/ai_generation/process_leonardo.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 137,
    "size": 4831,
    "docstring": "",
    "keywords": [
      "youtube",
      "web_tools"
    ],
    "functions": [
      "download_image",
      "save_metadata_to_file",
      "fetch_and_download_all_generations"
    ],
    "classes": [],
    "imports": [
      "gzip",
      "json",
      "os",
      "concurrent.futures",
      "requests",
      "tqdm"
    ],
    "preview": "import gzip\nimport json\nimport os\nfrom concurrent.futures import ThreadPoolExecutor\n\nimport requests\nfrom tqdm import tqdm\n\n# Configuration\nBASE_URL = (\n    \"https://cloud.leonardo.ai/api/rest/v1/generations/user/f7bb8476-e3f0-4f1f-9a06-4600866cc49c\"\n)\nAUTH_TOKEN = \"Bearer 93043291-957d-4ec1-8c79-ee734abcb6e3\"\nOUTPUT_DIR = \"/Users/steven/Pictures/leodowns\"\nMAX_RECORDS_PER_FILE = 500  # Limit records per JSON file\nMAX_WORKERS = 5  # For parallel downloads\nINCLUDE_IMAGES = True  # Set False to skip image downloads\n\nHEADERS = {\n    \"accept\": \"application/json\",",
    "last_modified": "2025-09-13T05:53:50.665952"
  },
  {
    "id": "1849",
    "name": "toc-merge.py",
    "path": "github_repo/scripts/01_core_ai_analysis/ai_generation/toc-merge.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 71,
    "size": 1856,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "create_toc_pdf",
      "merge_pdfs",
      "add_bookmarks"
    ],
    "classes": [],
    "imports": [
      "pypdf",
      "pypdf",
      "PyPDF2",
      "reportlab.lib.pagesizes",
      "reportlab.pdfgen"
    ],
    "preview": "from pypdf import PdfReader as PdfReader_pypdf\nfrom pypdf import PdfWriter as PdfWriter_pypdf\nfrom PyPDF2 import PdfReader, PdfWriter\nfrom reportlab.lib.pagesizes import letter\nfrom reportlab.pdfgen import canvas\n\n\ndef create_toc_pdf(toc_items, output_path):\n    c = canvas.Canvas(output_path, pagesize=letter)\n    width, height = letter\n    c.setFont(\"Helvetica\", 12)\n\n    y_position = height - 40\n    for item in toc_items:\n        text = f\"{item['title']} ... {item['page']}\"\n        c.drawString(40, y_position, text)\n        y_position -= 20\n\n    c.save()\n",
    "last_modified": "2025-05-04T22:47:13.332114"
  },
  {
    "id": "1850",
    "name": "keywords.py",
    "path": "github_repo/scripts/01_core_ai_analysis/ai_generation/keywords.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 30,
    "size": 903,
    "docstring": "",
    "keywords": [],
    "functions": [
      "get_keywords"
    ],
    "classes": [],
    "imports": [
      "multi_rake"
    ],
    "preview": "from multi_rake import Rake\n\n\ndef get_keywords(text):\n\n    rake = Rake()\n\n    keywords = rake.apply(text)\n\n    return keywords\n\n\nif __name__ == \"__main__\":\n\n    text = (\n        \"Compatibility of systems of linear constraints over the set of \"\n        \"natural numbers. Criteria of compatibility of a system of linear \"\n        \"Diophantine equations, strict inequations, and nonstrict inequations \"\n        \"are considered. Upper bounds for components of a minimal set of \"\n        \"solutions and algorithms of construction of minimal generating sets \"",
    "last_modified": "2025-03-28T18:35:46.756178"
  },
  {
    "id": "1851",
    "name": "prompt_templates.py",
    "path": "github_repo/scripts/01_core_ai_analysis/ai_generation/prompt_templates.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 55,
    "size": 1993,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "separator",
      "create_template",
      "create_template"
    ],
    "classes": [
      "QueryExpansionTemplate",
      "SelfQueryTemplate"
    ],
    "imports": [
      "langchain.prompts",
      "base"
    ],
    "preview": "from langchain.prompts import PromptTemplate\n\nfrom .base import PromptTemplateFactory\n\n\nclass QueryExpansionTemplate(PromptTemplateFactory):\n    prompt: str = \"\"\"You are an AI language model assistant. Your task is to generate {expand_to_n}\n    different versions of the given user question to retrieve relevant documents from a vector\n    database. By generating multiple perspectives on the user question, your goal is to help\n    the user overcome some of the limitations of the distance-based similarity search.\n    Provide these alternative questions seperated by '{separator}'.\n    Original question: {question}\"\"\"\n\n    @property\n    def separator(self) -> str:\n        return \"#next-question#\"\n\n    def create_template(self, expand_to_n: int) -> PromptTemplate:\n        return PromptTemplate(\n            template=self.prompt,",
    "last_modified": "2025-05-04T22:47:11.562551"
  },
  {
    "id": "1852",
    "name": "backup-clone-move.py",
    "path": "github_repo/scripts/01_core_ai_analysis/ai_generation/backup-clone-move.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 34,
    "size": 1245,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "os",
      "shutil"
    ],
    "preview": "import os\nimport shutil\n\n# Path to the CSV file\ncsv_path = \"/Users/steven/image_list.csv\"\n\n# The root directory where you want to backup the files\nbackup_root = \"/Volumes/iMac/backup\"\n\n# The root directory where you want to move the files\ndestination_root = \"/Volumes/iMac/ogMac images\"\n\n# Open and read the CSV file line by line\nwith open(csv_path, \"r\") as file:\n    for line in file:\n        file_path = line.strip()\n        if file_path:\n            # For backup: Generate the backup path and copy the file\n            backup_path = os.path.join(\n                backup_root,",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1853",
    "name": "duplicate-appscript.py",
    "path": "github_repo/scripts/01_core_ai_analysis/ai_generation/duplicate-appscript.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 48,
    "size": 1510,
    "docstring": "",
    "keywords": [],
    "functions": [
      "hash_file",
      "find_duplicates"
    ],
    "classes": [],
    "imports": [
      "hashlib",
      "os",
      "collections",
      "sys"
    ],
    "preview": "import hashlib\nimport os\nfrom collections import defaultdict\n\n\ndef hash_file(file_path):\n    \"\"\"Generate MD5 hash for a file.\"\"\"\n    hasher = hashlib.md5()\n    try:\n        with open(file_path, \"rb\") as f:\n            buf = f.read(65536)  # Read in chunks to avoid large memory usage\n            while len(buf) > 0:\n                hasher.update(buf)\n                buf = f.read(65536)\n        return hasher.hexdigest()\n    except IOError:\n        print(f\"Error opening or reading file: {file_path}\")\n        return None\n\n",
    "last_modified": "2025-03-28T18:37:05"
  },
  {
    "id": "1854",
    "name": "retriever.py",
    "path": "github_repo/scripts/01_core_ai_analysis/ai_generation/retriever.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 118,
    "size": 4051,
    "docstring": "",
    "keywords": [],
    "functions": [
      "__init__",
      "search",
      "_search",
      "rerank",
      "_search_data_category"
    ],
    "classes": [
      "ContextRetriever"
    ],
    "imports": [
      "concurrent.futures",
      "opik",
      "llm_engineering.application",
      "llm_engineering.application.preprocessing.dispatchers",
      "llm_engineering.domain.embedded_chunks",
      "llm_engineering.domain.queries",
      "loguru",
      "qdrant_client.models",
      "query_expanison",
      "reranking"
    ],
    "preview": "import concurrent.futures\n\nimport opik\nfrom llm_engineering.application import utils\nfrom llm_engineering.application.preprocessing.dispatchers import EmbeddingDispatcher\nfrom llm_engineering.domain.embedded_chunks import (\n    EmbeddedArticleChunk,\n    EmbeddedChunk,\n    EmbeddedPostChunk,\n    EmbeddedRepositoryChunk,\n)\nfrom llm_engineering.domain.queries import EmbeddedQuery, Query\nfrom loguru import logger\nfrom qdrant_client.models import FieldCondition, Filter, MatchValue\n\nfrom .query_expanison import QueryExpansion\nfrom .reranking import Reranker\nfrom .self_query import SelfQuery\n\n",
    "last_modified": "2025-09-13T05:53:41.805315"
  },
  {
    "id": "1855",
    "name": "2025-vid.py",
    "path": "github_repo/scripts/01_core_ai_analysis/ai_generation/2025-vid.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 219,
    "size": 6894,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "get_creation_date",
      "get_video_metadata",
      "format_file_size",
      "format_duration",
      "generate_dry_run_csv",
      "write_csv",
      "get_unique_file_path",
      "save_last_directory",
      "load_last_directory"
    ],
    "classes": [],
    "imports": [
      "csv",
      "os",
      "re",
      "datetime",
      "mutagen.easymp4",
      "mutagen.mp4"
    ],
    "preview": "import csv\nimport os\nimport re\nfrom datetime import datetime\n\nfrom mutagen.easymp4 import EasyMP4\nfrom mutagen.mp4 import MP4\n\nLAST_DIRECTORY_FILE = \"vids.txt\"\n\n\n# Function to get the creation date of a file\ndef get_creation_date(filepath):\n    try:\n        return datetime.fromtimestamp(os.path.getctime(filepath)).strftime(\"%m-%d-%y\")\n    except Exception as e:\n        print(f\"Error getting creation date for {filepath}: {e}\")\n        return \"Unknown\"\n\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1856",
    "name": "test_onedrive_gallery_logic.py",
    "path": "github_repo/scripts/01_core_ai_analysis/ai_generation/test_onedrive_gallery_logic.py",
    "category": "01_core_ai_analysis",
    "type": "testing",
    "lines": 65,
    "size": 2147,
    "docstring": "",
    "keywords": [
      "testing"
    ],
    "functions": [
      "test_create_thumbnails",
      "test_generate_images_data"
    ],
    "classes": [
      "OnedriveGalleryTestCase"
    ],
    "imports": [
      "os",
      "unittest",
      "unittest",
      "simplegallery.test.helpers",
      "simplegallery.logic.variants.onedrive_gallery_logic",
      "testfixtures"
    ],
    "preview": "import os\nimport unittest\nfrom unittest import mock\n\nimport simplegallery.test.helpers as helpers\nfrom simplegallery.logic.variants.onedrive_gallery_logic import OnedriveGalleryLogic\nfrom testfixtures import TempDirectory\n\n\nclass OnedriveGalleryTestCase(unittest.TestCase):\n\n    remote_link = \"https://1drv.ms/u/s!AkD5kF--59kUf28rkcQAphGO668?e=ELiTW6\"\n\n    @mock.patch(\"builtins.input\", side_effect=[\"\", \"\", \"\", \"\"])\n    def test_create_thumbnails(self, input):\n        with TempDirectory() as tempdir:\n            # Init files gallery logic\n            gallery_config = helpers.init_gallery_and_read_gallery_config(\n                tempdir.path, self.remote_link\n            )",
    "last_modified": "2025-09-13T05:53:52.882942"
  },
  {
    "id": "1857",
    "name": "thumbnail.py",
    "path": "github_repo/scripts/01_core_ai_analysis/ai_generation/thumbnail.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 51,
    "size": 1481,
    "docstring": "",
    "keywords": [
      "opencv",
      "analysis"
    ],
    "functions": [
      "create_thumbnails",
      "create_thumbnail_frame"
    ],
    "classes": [],
    "imports": [
      "random",
      "cv2",
      "PIL",
      "utils"
    ],
    "preview": "import random\n\nimport cv2\nfrom PIL import Image, ImageDraw, ImageFont\n\nfrom .utils import resource_path\n\n\ndef create_thumbnails(video_filepath: str, options: dict = {}):\n    vidcap = cv2.VideoCapture(video_filepath)\n    total_frames = int(vidcap.get(cv2.CAP_PROP_FRAME_COUNT))\n\n    amount = options[\"amount\"]\n    font_size = options[\"font_size\"]\n    title = options[\"title\"]\n\n    results = []\n    for _ in range(amount):\n        random_frame = random.randint(total_frames // 10, total_frames * 9 // 10)\n        results.append(create_thumbnail_frame(vidcap, random_frame, title, font_size))",
    "last_modified": "2025-05-04T22:47:12"
  },
  {
    "id": "1858",
    "name": "pydoc.py",
    "path": "github_repo/scripts/01_core_ai_analysis/ai_generation/pydoc.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 19,
    "size": 595,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "generate_docs"
    ],
    "classes": [],
    "imports": [
      "os",
      "subprocess"
    ],
    "preview": "import os\nimport subprocess\n\n\ndef generate_docs(directory):\n    # Walk through the directory and its subdirectories\n    for root, dirs, files in os.walk(directory):\n        for file in files:\n            if file.endswith(\".py\"):\n                file_path = os.path.join(root, file)\n                print(f\"Generating documentation for {file_path}\")\n                # Generate documentation using pydoc\n                subprocess.run([\"python3\", \"-m\", \"pydoc\", \"-w\", file_path])\n\n\n# Specify the directory to generate docs for\ndirectory = \"/Users/steven/Documents/Python\"\ngenerate_docs(directory)\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1859",
    "name": "docs.py",
    "path": "github_repo/scripts/01_core_ai_analysis/ai_generation/docs.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 205,
    "size": 6494,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "get_creation_date",
      "format_file_size",
      "generate_dry_run_csv",
      "write_csv",
      "get_unique_file_path",
      "save_last_directory",
      "load_last_directory"
    ],
    "classes": [],
    "imports": [
      "csv",
      "os",
      "re",
      "datetime",
      "config"
    ],
    "preview": "import csv\nimport os\nimport re\nfrom datetime import datetime\n\nimport config  # Import the configuration\n\n# Constants\nLAST_DIRECTORY_FILE = \"docs.txt\"\n\n# Function to get the creation date of a file\n\n\ndef get_creation_date(filepath):\n    try:\n        return datetime.fromtimestamp(os.path.getctime(filepath)).strftime(\"%m-%d-%y\")\n    except Exception as e:\n        print(f\"Error getting creation date for {filepath}: {e}\")\n        return \"Unknown\"\n",
    "last_modified": "2025-09-13T05:53:48.416059"
  },
  {
    "id": "1860",
    "name": "other.py",
    "path": "github_repo/scripts/01_core_ai_analysis/ai_generation/other.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 235,
    "size": 7330,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "get_creation_date",
      "format_file_size",
      "generate_dry_run_csv",
      "write_csv",
      "get_unique_file_path",
      "save_last_directory",
      "load_last_directory"
    ],
    "classes": [],
    "imports": [
      "csv",
      "os",
      "re",
      "datetime",
      "config"
    ],
    "preview": "import csv\nimport os\nimport re\nfrom datetime import datetime\n\nimport config  # Import the configuration\n\n# Constants\nLAST_DIRECTORY_FILE = \"other.txt\"\n\n# Function to get the creation date of a file\n\n\ndef get_creation_date(filepath):\n    try:\n        return datetime.fromtimestamp(os.path.getctime(filepath)).strftime(\"%m-%d-%y\")\n    except Exception as e:\n        print(f\"Error getting creation date for {filepath}: {e}\")\n        return \"Unknown\"\n",
    "last_modified": "2025-09-13T05:53:48.616079"
  },
  {
    "id": "1861",
    "name": "archives.py",
    "path": "github_repo/scripts/01_core_ai_analysis/ai_generation/archives.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 33,
    "size": 1100,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "create_tar_gz"
    ],
    "classes": [],
    "imports": [
      "os",
      "tarfile"
    ],
    "preview": "import os\nimport tarfile\n\n# List of directories to include in the tar.gz archive\ndirectories = [\n    \"/Users/steven/Pictures/other-07-25-05:32.csv\",\n    \"/Users/steven/Pictures/audio-07-25-05:31.csv\",\n    \"/Users/steven/Pictures/docs-07-25-05:31.csv\",\n    \"/Users/steven/Pictures/image_data-07-25-05:32.csv\",\n    \"/Users/steven/Pictures/vids-07-25-05:34.csv\",\n]\n\n# Output tar.gz file\noutput_filename = \"/Users/steven/Pictures/data_archive.tar.gz\"\n\n\n# Function to create tar.gz archive\ndef create_tar_gz(output_filename, directories):\n    with tarfile.open(output_filename, \"w:gz\") as tar:\n        for dir_path in directories:",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1862",
    "name": "video_text_test.py",
    "path": "github_repo/scripts/01_core_ai_analysis/ai_generation/video_text_test.py",
    "category": "01_core_ai_analysis",
    "type": "youtube",
    "lines": 9,
    "size": 208,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "clips"
    ],
    "preview": "from clips import *\n\nclips = []\nclips.append(create_comment_clip(author=\"adsf\", content=\"Why is wrong with this?\"))\n\nvideo = concatenate_videoclips(clips)\n\nvideo.write_videofile(\"media/test_out.mp4\", fps=24)\n",
    "last_modified": "2025-03-28T18:35:48"
  },
  {
    "id": "1863",
    "name": "playlist_20221230180809.py",
    "path": "github_repo/scripts/01_core_ai_analysis/ai_generation/playlist_20221230180809.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 212,
    "size": 6700,
    "docstring": "",
    "keywords": [
      "youtube",
      "analysis",
      "web_tools"
    ],
    "functions": [
      "generate_token",
      "get_youtube_url",
      "download_mp3",
      "last_fm_artist_info",
      "set_metadata",
      "write_tracks",
      "write_playlist",
      "split_spotify_uri",
      "get_os",
      "get_folder"
    ],
    "classes": [],
    "imports": [
      "argparse",
      "json",
      "os",
      "re",
      "subprocess",
      "urllib.parse",
      "urllib.request",
      "secret",
      "spotipy",
      "spotipy.oauth2"
    ],
    "preview": "import argparse\nimport json\nimport os\nimport re\nimport subprocess\nimport urllib.parse\nimport urllib.request\n\nimport secret\nimport spotipy\nimport spotipy.oauth2 as oauth2\nfrom requests.utils import quote\n\n\ndef generate_token():\n    credentials = oauth2.SpotifyClientCredentials(\n        client_id=secret.SPOTIFY_CLIENT_ID, client_secret=secret.SPOTIFY_CLIENT_SECRET\n    )\n    token = credentials.get_access_token()\n    return token",
    "last_modified": "2025-09-13T05:54:02.085648"
  },
  {
    "id": "1864",
    "name": "process_leonardo_20250102110441.py",
    "path": "github_repo/scripts/01_core_ai_analysis/ai_generation/process_leonardo_20250102110441.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 137,
    "size": 4831,
    "docstring": "",
    "keywords": [
      "youtube",
      "web_tools"
    ],
    "functions": [
      "download_image",
      "save_metadata_to_file",
      "fetch_and_download_all_generations"
    ],
    "classes": [],
    "imports": [
      "gzip",
      "json",
      "os",
      "concurrent.futures",
      "requests",
      "tqdm"
    ],
    "preview": "import gzip\nimport json\nimport os\nfrom concurrent.futures import ThreadPoolExecutor\n\nimport requests\nfrom tqdm import tqdm\n\n# Configuration\nBASE_URL = (\n    \"https://cloud.leonardo.ai/api/rest/v1/generations/user/f7bb8476-e3f0-4f1f-9a06-4600866cc49c\"\n)\nAUTH_TOKEN = \"Bearer 93043291-957d-4ec1-8c79-ee734abcb6e3\"\nOUTPUT_DIR = \"/Users/steven/Pictures/leodowns\"\nMAX_RECORDS_PER_FILE = 500  # Limit records per JSON file\nMAX_WORKERS = 5  # For parallel downloads\nINCLUDE_IMAGES = True  # Set False to skip image downloads\n\nHEADERS = {\n    \"accept\": \"application/json\",",
    "last_modified": "2025-09-13T05:53:49.891924"
  },
  {
    "id": "1865",
    "name": "generate_album_pages 2.py",
    "path": "github_repo/scripts/01_core_ai_analysis/ai_generation/generate_album_pages 2.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 78,
    "size": 2806,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "os"
    ],
    "preview": "import os\n\n# Directory paths\nalbums_dir = \"/Users/steven/Music/nocTurneMeLoDieS/assests/all\"\noutput_dir = albums_dir  # Output HTML in the root music_site directory\ncss_path = \"/css/styles.css\"\n\n# Loop through each album folder\nfor album in os.listdir(albums_dir):\n    album_path = os.path.join(albums_dir, album)\n\n    # Check if it's a directory and contains the required files\n    if os.path.isdir(album_path):\n        cover_img = os.path.join(album_path, f\"{album}.png\")\n        audio_file = os.path.join(album_path, f\"{album}.mp3\")\n        lyrics_file = os.path.join(album_path, f\"{album}_transcript.txt\")\n        analysis_file = os.path.join(album_path, f\"{album}_analysis.txt\")\n\n        # Check if required files exist\n        if (",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1866",
    "name": "gallery_init.py",
    "path": "github_repo/scripts/01_core_ai_analysis/ai_generation/gallery_init.py",
    "category": "01_core_ai_analysis",
    "type": "setup",
    "lines": 292,
    "size": 9791,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "parse_args",
      "check_if_gallery_creation_possible",
      "check_if_gallery_already_exists",
      "create_gallery_folder_structure",
      "create_gallery_json",
      "main"
    ],
    "classes": [],
    "imports": [
      "argparse",
      "glob",
      "importlib.resources",
      "json",
      "os",
      "shutil",
      "sys",
      "distutils.dir_util",
      "simplegallery.common",
      "simplegallery.logic.gallery_logic"
    ],
    "preview": "import argparse\nimport glob\nimport importlib.resources\nimport json\nimport os\nimport shutil\nimport sys\nfrom distutils.dir_util import copy_tree\n\nimport simplegallery.common as spg_common\nimport simplegallery.logic.gallery_logic as gallery_logic\n\n\ndef parse_args():\n    \"\"\"\n    Configures the argument parser\n    :return: Parsed arguments\n    \"\"\"\n\n    description = \"\"\"Initializes a new Simple Photo Gallery in the specified folder (default is the current folder).",
    "last_modified": "2025-09-13T05:53:52.418972"
  },
  {
    "id": "1867",
    "name": "img.py",
    "path": "github_repo/scripts/01_core_ai_analysis/ai_generation/img.py",
    "category": "01_core_ai_analysis",
    "type": "image_processing",
    "lines": 217,
    "size": 7032,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "get_creation_date",
      "get_image_metadata",
      "format_file_size",
      "generate_csv",
      "write_csv",
      "get_unique_file_path",
      "save_last_directory",
      "load_last_directory"
    ],
    "classes": [],
    "imports": [
      "csv",
      "os",
      "re",
      "datetime",
      "PIL"
    ],
    "preview": "import csv\nimport os\nimport re\nfrom datetime import datetime\n\nfrom PIL import Image\n\n# Constants\nLAST_DIRECTORY_FILE = \"image_data.txt\"\n\n\n# Function to get the creation date of a file\ndef get_creation_date(filepath):\n    try:\n        return datetime.fromtimestamp(os.path.getctime(filepath)).strftime(\"%m-%d-%y\")\n    except Exception as e:\n        print(f\"Error getting creation date for {filepath}: {e}\")\n        return \"Unknown\"\n\n",
    "last_modified": "2025-09-13T05:53:48.506154"
  },
  {
    "id": "1868",
    "name": "process_leonardo_20250102110258.py",
    "path": "github_repo/scripts/01_core_ai_analysis/ai_generation/process_leonardo_20250102110258.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 137,
    "size": 4839,
    "docstring": "",
    "keywords": [
      "youtube",
      "web_tools"
    ],
    "functions": [
      "download_image",
      "save_metadata_to_file",
      "fetch_and_download_all_generations"
    ],
    "classes": [],
    "imports": [
      "gzip",
      "json",
      "os",
      "concurrent.futures",
      "requests",
      "tqdm"
    ],
    "preview": "import gzip\nimport json\nimport os\nfrom concurrent.futures import ThreadPoolExecutor\n\nimport requests\nfrom tqdm import tqdm\n\n# Configuration\nBASE_URL = (\n    \"https://cloud.leonardo.ai/api/rest/v1/generations/user/f7bb8476-e3f0-4f1f-9a06-4600866cc49c\"\n)\nAUTH_TOKEN = \"Bearer 93043291-957d-4ec1-8c79-ee734abcb6e3\"\nOUTPUT_DIR = \"/Users/steven/Downloads/leonardo_images\"\nMAX_RECORDS_PER_FILE = 500  # Limit records per JSON file\nMAX_WORKERS = 5  # For parallel downloads\nINCLUDE_IMAGES = True  # Set False to skip image downloads\n\nHEADERS = {\n    \"accept\": \"application/json\",",
    "last_modified": "2025-09-13T05:53:49.825950"
  },
  {
    "id": "1869",
    "name": "pics.py",
    "path": "github_repo/scripts/01_core_ai_analysis/ai_generation/pics.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 217,
    "size": 6519,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "get_creation_date",
      "get_image_metadata",
      "format_file_size",
      "generate_csv",
      "generate_dry_run_csv",
      "write_csv",
      "get_unique_file_path",
      "save_last_directory",
      "load_last_directory"
    ],
    "classes": [],
    "imports": [
      "csv",
      "os",
      "datetime",
      "PIL"
    ],
    "preview": "import csv\nimport os\nfrom datetime import datetime\n\nfrom PIL import Image\n\nLAST_DIRECTORY_FILE = \"images.txt\"\n\n# Function to get the creation date of a file\n\n\ndef get_creation_date(filepath):\n    try:\n        return datetime.fromtimestamp(os.path.getctime(filepath)).strftime(\"%m-%d-%y\")\n    except Exception as e:\n        print(f\"Error getting creation date for {filepath}: {e}\")\n        return \"Unknown\"\n\n\n# Function to extract metadata from an image file using PIL",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1870",
    "name": "generate_songs_csv 1.py",
    "path": "github_repo/scripts/01_core_ai_analysis/ai_generation/generate_songs_csv 1.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 64,
    "size": 2088,
    "docstring": "",
    "keywords": [],
    "functions": [
      "get_matching_files"
    ],
    "classes": [],
    "imports": [
      "csv",
      "os"
    ],
    "preview": "import csv\nimport os\n\n# Define the directories\nmp3_dir = \"/Users/steven/Music/suno/mp3\"\ntxt_dir = \"/Users/steven/Music/suno/txt\"\ncsv_output = \"/Users/steven/Music/suno/music_project/songs_data.csv\"\n\n# Collect the list of MP3 and text files\nmp3_files = [f for f in os.listdir(mp3_dir) if f.endswith(\".mp3\")]\ntxt_files = [f for f in os.listdir(txt_dir) if f.endswith(\".txt\")]\n\n\n# Function to match song with corresponding text files\ndef get_matching_files(song_title, txt_files):\n    analysis_file = None\n    transcript_file = None\n    base_title = song_title.replace(\".mp3\", \"\")\n    for txt in txt_files:\n        if base_title in txt:",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1871",
    "name": "voice.py",
    "path": "github_repo/scripts/01_core_ai_analysis/ai_generation/voice.py",
    "category": "01_core_ai_analysis",
    "type": "transcription",
    "lines": 43,
    "size": 1433,
    "docstring": "Synthesizes speech from the input string of text or ssml.\nMake sure to be working in a virtual environment.\n\nNote: ssml must be well-formed according to:\n    https://www.w3.org/TR/speech-synthesis/",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "generateVoice"
    ],
    "classes": [],
    "imports": [
      "os",
      "google.cloud"
    ],
    "preview": "\"\"\"Synthesizes speech from the input string of text or ssml.\nMake sure to be working in a virtual environment.\n\nNote: ssml must be well-formed according to:\n    https://www.w3.org/TR/speech-synthesis/\n\"\"\"\n\nimport os\n\nfrom google.cloud import texttospeech\n\nos.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"./credentials/key-gg-tts.json\"\n\n\ndef generateVoice(text, output):\n    # Instantiates a client\n    client = texttospeech.TextToSpeechClient()\n\n    # Set the text input to be synthesized\n    synthesis_input = texttospeech.SynthesisInput(text=text)",
    "last_modified": "2025-09-13T05:53:29.724622"
  },
  {
    "id": "1872",
    "name": "command_context.py",
    "path": "github_repo/scripts/01_core_ai_analysis/ai_generation/command_context.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 28,
    "size": 774,
    "docstring": "",
    "keywords": [],
    "functions": [
      "__init__",
      "main_context",
      "enter_context"
    ],
    "classes": [
      "CommandContextMixIn"
    ],
    "imports": [
      "contextlib",
      "typing"
    ],
    "preview": "from contextlib import ExitStack, contextmanager\nfrom typing import ContextManager, Generator, TypeVar\n\n_T = TypeVar(\"_T\", covariant=True)\n\n\nclass CommandContextMixIn:\n    def __init__(self) -> None:\n        super().__init__()\n        self._in_main_context = False\n        self._main_context = ExitStack()\n\n    @contextmanager\n    def main_context(self) -> Generator[None, None, None]:\n        assert not self._in_main_context\n\n        self._in_main_context = True\n        try:\n            with self._main_context:\n                yield",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1873",
    "name": "jobs.py",
    "path": "github_repo/scripts/01_core_ai_analysis/ai_generation/jobs.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 291,
    "size": 8457,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "get_unique_filename",
      "create_excerpt",
      "generate_job_cards",
      "write_html_job_cards",
      "generate_table_header",
      "generate_table_body",
      "write_html_table",
      "main"
    ],
    "classes": [],
    "imports": [
      "csv",
      "html",
      "os"
    ],
    "preview": "import csv\nimport html\nimport os\n\n\ndef get_unique_filename(filepath):\n    \"\"\"\n    If the filepath already exists, append _1, _2, ... until a unique filename is found.\n    Returns the unique filepath.\n    \"\"\"\n    base, ext = os.path.splitext(filepath)\n    counter = 1\n    unique_path = filepath\n    while os.path.exists(unique_path):\n        unique_path = f\"{base}_{counter}{ext}\"\n        counter += 1\n    return unique_path\n\n\n# Prompt the user for input CSV file path and output base name",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1874",
    "name": "vids.py",
    "path": "github_repo/scripts/01_core_ai_analysis/ai_generation/vids.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 239,
    "size": 7360,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "get_creation_date",
      "get_video_metadata",
      "format_file_size",
      "format_duration",
      "generate_dry_run_csv",
      "write_csv",
      "get_unique_file_path",
      "save_last_directory",
      "load_last_directory"
    ],
    "classes": [],
    "imports": [
      "csv",
      "os",
      "re",
      "datetime",
      "mutagen.easymp4",
      "mutagen.mp4",
      "config"
    ],
    "preview": "import csv\nimport os\nimport re\nfrom datetime import datetime\n\nfrom mutagen.easymp4 import EasyMP4\nfrom mutagen.mp4 import MP4\n\nimport config  # Import the configuration\n\nLAST_DIRECTORY_FILE = \"vids.txt\"\n\n# Function to get the creation date of a file\n\n\ndef get_creation_date(filepath):\n    try:\n        return datetime.fromtimestamp(os.path.getctime(filepath)).strftime(\"%m-%d-%y\")\n    except Exception as e:\n        print(f\"Error getting creation date for {filepath}: {e}\")",
    "last_modified": "2025-09-13T05:53:48.717566"
  },
  {
    "id": "1875",
    "name": "TextToSpeech.py",
    "path": "github_repo/scripts/01_core_ai_analysis/ai_generation/TextToSpeech.py",
    "category": "01_core_ai_analysis",
    "type": "transcription",
    "lines": 45,
    "size": 1420,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "__init__",
      "create_dir",
      "create_tts"
    ],
    "classes": [
      "TextToSpeech"
    ],
    "imports": [
      "os",
      "sys",
      "gtts"
    ],
    "preview": "# TextToSpeech.py\n# Called in run.py after RedditScrape.py has scraped all of the content\n# Creates an audio file for each scraped post\n#\n#\n\n\nimport os\nimport sys\n\n# including the text to speech API: subject to change\nfrom gtts import gTTS\n\nsys.path.append(\"../\")\n\n\nclass TextToSpeech:\n\n    def __init__(self):\n        self.audio_path = \"../audio/\"",
    "last_modified": "2025-05-04T23:28:21.785924"
  },
  {
    "id": "1876",
    "name": "img 2.py",
    "path": "github_repo/scripts/01_core_ai_analysis/ai_generation/img 2.py",
    "category": "01_core_ai_analysis",
    "type": "image_processing",
    "lines": 217,
    "size": 7032,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "get_creation_date",
      "get_image_metadata",
      "format_file_size",
      "generate_csv",
      "write_csv",
      "get_unique_file_path",
      "save_last_directory",
      "load_last_directory"
    ],
    "classes": [],
    "imports": [
      "csv",
      "os",
      "re",
      "datetime",
      "PIL"
    ],
    "preview": "import csv\nimport os\nimport re\nfrom datetime import datetime\n\nfrom PIL import Image\n\n# Constants\nLAST_DIRECTORY_FILE = \"image_data.txt\"\n\n\n# Function to get the creation date of a file\ndef get_creation_date(filepath):\n    try:\n        return datetime.fromtimestamp(os.path.getctime(filepath)).strftime(\"%m-%d-%y\")\n    except Exception as e:\n        print(f\"Error getting creation date for {filepath}: {e}\")\n        return \"Unknown\"\n\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1877",
    "name": "spinners.py",
    "path": "github_repo/scripts/01_core_ai_analysis/ai_generation/spinners.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 160,
    "size": 5118,
    "docstring": "",
    "keywords": [],
    "functions": [
      "open_spinner",
      "hidden_cursor",
      "spin",
      "finish",
      "__init__",
      "_write",
      "spin",
      "finish",
      "__init__",
      "_update"
    ],
    "classes": [
      "SpinnerInterface",
      "InteractiveSpinner",
      "NonInteractiveSpinner",
      "RateLimiter"
    ],
    "imports": [
      "contextlib",
      "itertools",
      "logging",
      "sys",
      "time",
      "typing",
      "pip._internal.utils.compat",
      "pip._internal.utils.logging"
    ],
    "preview": "import contextlib\nimport itertools\nimport logging\nimport sys\nimport time\nfrom typing import IO, Generator, Optional\n\nfrom pip._internal.utils.compat import WINDOWS\nfrom pip._internal.utils.logging import get_indentation\n\nlogger = logging.getLogger(__name__)\n\n\nclass SpinnerInterface:\n    def spin(self) -> None:\n        raise NotImplementedError()\n\n    def finish(self, final_status: str) -> None:\n        raise NotImplementedError()\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1878",
    "name": "autocompletion.py",
    "path": "github_repo/scripts/01_core_ai_analysis/ai_generation/autocompletion.py",
    "category": "01_core_ai_analysis",
    "type": "setup",
    "lines": 163,
    "size": 6591,
    "docstring": "Logic that powers autocompletion installed by ``pip completion``.",
    "keywords": [],
    "functions": [
      "autocomplete",
      "get_path_completion_type",
      "auto_complete_paths"
    ],
    "classes": [],
    "imports": [
      "optparse",
      "os",
      "sys",
      "itertools",
      "typing",
      "pip._internal.cli.main_parser",
      "pip._internal.commands",
      "pip._internal.metadata"
    ],
    "preview": "\"\"\"Logic that powers autocompletion installed by ``pip completion``.\"\"\"\n\nimport optparse\nimport os\nimport sys\nfrom itertools import chain\nfrom typing import Any, Iterable, List, Optional\n\nfrom pip._internal.cli.main_parser import create_main_parser\nfrom pip._internal.commands import commands_dict, create_command\nfrom pip._internal.metadata import get_default_environment\n\n\ndef autocomplete() -> None:\n    \"\"\"Entry Point for completion of main and subcommand options.\"\"\"\n    # Don't complete if user hasn't sourced bash_completion file.\n    if \"PIP_AUTO_COMPLETE\" not in os.environ:\n        return\n    cwords = os.environ[\"COMP_WORDS\"].split()[1:]\n    cword = int(os.environ[\"COMP_CWORD\"])",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1879",
    "name": "contact-sheet.py",
    "path": "github_repo/scripts/01_core_ai_analysis/ai_generation/contact-sheet.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 36,
    "size": 1125,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "create_contact_sheet"
    ],
    "classes": [],
    "imports": [
      "os",
      "PIL"
    ],
    "preview": "import os\n\nfrom PIL import Image\n\n\ndef create_contact_sheet(\n    directory, image_size=(600, 600), grid_size=(5, 5), sheet_size=(3000, 3000)\n):\n    contact_sheet = Image.new(\"RGB\", sheet_size)\n    x_offset, y_offset = 0, 0\n    count = 0\n\n    for subdir, dirs, files in os.walk(directory):\n        for file in files:\n            if file.lower().endswith((\"png\", \"jpg\", \"jpeg\", \"gif\", \"bmp\")):\n                image_path = os.path.join(subdir, file)\n                with Image.open(image_path) as img:\n                    img = img.resize(image_size)\n                    contact_sheet.paste(img, (x_offset, y_offset))\n                    x_offset += image_size[0]",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1880",
    "name": "setuptools_build.py",
    "path": "github_repo/scripts/01_core_ai_analysis/ai_generation/setuptools_build.py",
    "category": "01_core_ai_analysis",
    "type": "setup",
    "lines": 147,
    "size": 4435,
    "docstring": "",
    "keywords": [],
    "functions": [
      "make_setuptools_shim_args",
      "make_setuptools_bdist_wheel_args",
      "make_setuptools_clean_args",
      "make_setuptools_develop_args",
      "make_setuptools_egg_info_args"
    ],
    "classes": [],
    "imports": [
      "sys",
      "textwrap",
      "typing"
    ],
    "preview": "import sys\nimport textwrap\nfrom typing import List, Optional, Sequence\n\n# Shim to wrap setup.py invocation with setuptools\n# Note that __file__ is handled via two {!r} *and* %r, to ensure that paths on\n# Windows are correctly handled (it should be \"C:\\\\Users\" not \"C:\\Users\").\n_SETUPTOOLS_SHIM = textwrap.dedent(\n    \"\"\"\n    exec(compile('''\n    # This is <pip-setuptools-caller> -- a caller that pip uses to run setup.py\n    #\n    # - It imports setuptools before invoking setup.py, to enable projects that directly\n    #   import from `distutils.core` to work with newer packaging standards.\n    # - It provides a clear error message when setuptools is not installed.\n    # - It sets `sys.argv[0]` to the underlying `setup.py`, when invoking `setup.py` so\n    #   setuptools doesn't think the script is `-c`. This avoids the following warning:\n    #     manifest_maker: standard file '-c' not found\".\n    # - It generates a shim setup.py, for handling setup.cfg-only projects.\n    import os, sys, tokenize",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1881",
    "name": "pydoc_20250504224712.py",
    "path": "github_repo/scripts/01_core_ai_analysis/ai_generation/pydoc_20250504224712.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 19,
    "size": 595,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "generate_docs"
    ],
    "classes": [],
    "imports": [
      "os",
      "subprocess"
    ],
    "preview": "import os\nimport subprocess\n\n\ndef generate_docs(directory):\n    # Walk through the directory and its subdirectories\n    for root, dirs, files in os.walk(directory):\n        for file in files:\n            if file.endswith(\".py\"):\n                file_path = os.path.join(root, file)\n                print(f\"Generating documentation for {file_path}\")\n                # Generate documentation using pydoc\n                subprocess.run([\"python3\", \"-m\", \"pydoc\", \"-w\", file_path])\n\n\n# Specify the directory to generate docs for\ndirectory = \"/Users/steven/Documents/Python\"\ngenerate_docs(directory)\n",
    "last_modified": "2025-05-06T19:59:20.309260"
  },
  {
    "id": "1882",
    "name": "process_leonardo_20250102110442.py",
    "path": "github_repo/scripts/01_core_ai_analysis/ai_generation/process_leonardo_20250102110442.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 137,
    "size": 4831,
    "docstring": "",
    "keywords": [
      "youtube",
      "web_tools"
    ],
    "functions": [
      "download_image",
      "save_metadata_to_file",
      "fetch_and_download_all_generations"
    ],
    "classes": [],
    "imports": [
      "gzip",
      "json",
      "os",
      "concurrent.futures",
      "requests",
      "tqdm"
    ],
    "preview": "import gzip\nimport json\nimport os\nfrom concurrent.futures import ThreadPoolExecutor\n\nimport requests\nfrom tqdm import tqdm\n\n# Configuration\nBASE_URL = (\n    \"https://cloud.leonardo.ai/api/rest/v1/generations/user/f7bb8476-e3f0-4f1f-9a06-4600866cc49c\"\n)\nAUTH_TOKEN = \"Bearer 93043291-957d-4ec1-8c79-ee734abcb6e3\"\nOUTPUT_DIR = \"/Users/steven/Pictures/leodowns\"\nMAX_RECORDS_PER_FILE = 500  # Limit records per JSON file\nMAX_WORKERS = 5  # For parallel downloads\nINCLUDE_IMAGES = True  # Set False to skip image downloads\n\nHEADERS = {\n    \"accept\": \"application/json\",",
    "last_modified": "2025-09-13T05:53:49.959583"
  },
  {
    "id": "1883",
    "name": "reranking.py",
    "path": "github_repo/scripts/01_core_ai_analysis/ai_generation/reranking.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 32,
    "size": 1053,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "__init__",
      "generate"
    ],
    "classes": [
      "Reranker"
    ],
    "imports": [
      "opik",
      "llm_engineering.application.networks",
      "llm_engineering.domain.embedded_chunks",
      "llm_engineering.domain.queries",
      "base"
    ],
    "preview": "import opik\nfrom llm_engineering.application.networks import CrossEncoderModelSingleton\nfrom llm_engineering.domain.embedded_chunks import EmbeddedChunk\nfrom llm_engineering.domain.queries import Query\n\nfrom .base import RAGStep\n\n\nclass Reranker(RAGStep):\n    def __init__(self, mock: bool = False) -> None:\n        super().__init__(mock=mock)\n\n        self._model = CrossEncoderModelSingleton()\n\n    @opik.track(name=\"Reranker.generate\")\n    def generate(\n        self, query: Query, chunks: list[EmbeddedChunk], keep_top_k: int\n    ) -> list[EmbeddedChunk]:\n        if self._mock:\n            return chunks",
    "last_modified": "2025-05-06T04:35:14.982898"
  },
  {
    "id": "1884",
    "name": "generate_album_pages 1.py",
    "path": "github_repo/scripts/01_core_ai_analysis/ai_generation/generate_album_pages 1.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 90,
    "size": 2534,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "read_text_file",
      "generate_album_html"
    ],
    "classes": [],
    "imports": [
      "os"
    ],
    "preview": "import os\n\nTEMPLATE = \"\"\"\n<!DOCTYPE html>\n<html lang=\"en\">\n<head>\n    <meta charset=\"UTF-8\">\n    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n    <title>{album_name}</title>\n    <link rel=\"stylesheet\" href=\"../css/styles.css\">\n</head>\n<body>\n    <div class=\"album-container\">\n        <div class=\"album\">\n            <img src=\"../albums/{folder_name}/album_cover.jpg\" alt=\"{album_name} Cover\">\n            <h3>{album_name}</h3>\n            <audio controls>\n                <source src=\"../albums/{folder_name}/{mp3_file}\" type=\"audio/mpeg\">\n                Your browser does not support the audio element.\n            </audio>",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1885",
    "name": "google_gallery_logic.py",
    "path": "github_repo/scripts/01_core_ai_analysis/ai_generation/google_gallery_logic.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 101,
    "size": 3724,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "parse_photo_link",
      "create_thumbnails",
      "generate_images_data"
    ],
    "classes": [
      "GoogleGalleryLogic"
    ],
    "imports": [
      "time",
      "pkg_resources",
      "simplegallery.common",
      "simplegallery.media",
      "selenium",
      "selenium.webdriver.firefox.options",
      "simplegallery.logic.base_gallery_logic"
    ],
    "preview": "import time\n\nimport pkg_resources\nimport simplegallery.common as spg_common\nimport simplegallery.media as spg_media\nfrom selenium import webdriver\nfrom selenium.webdriver.firefox.options import Options\nfrom simplegallery.logic.base_gallery_logic import BaseGalleryLogic\n\n\ndef parse_photo_link(photo_url):\n    \"\"\"\n    Extracts the base URL (URL without query parameters) and the photo name from a Onedrive photo URL\n    :param photo_url: photo URL\n    :return: base URL and photo name\n    \"\"\"\n    base_url = photo_url.split(\"=\")[0]\n    name = base_url.split(\"/\")[-1]\n\n    return base_url, name",
    "last_modified": "2025-09-13T05:53:52.572855"
  },
  {
    "id": "1886",
    "name": "MakeCompilation.py",
    "path": "github_repo/scripts/01_core_ai_analysis/ai_generation/MakeCompilation.py",
    "category": "01_core_ai_analysis",
    "type": "youtube",
    "lines": 70,
    "size": 1821,
    "docstring": "2. Compile video out of clips",
    "keywords": [
      "testing",
      "analysis"
    ],
    "functions": [
      "select_videos",
      "create_video",
      "create_description",
      "test_mod"
    ],
    "classes": [],
    "imports": [
      "os",
      "random",
      "shutil",
      "moviepy",
      "moviepy.editor"
    ],
    "preview": "\"\"\"\n2. Compile video out of clips\n\"\"\"\n\nimport os\nimport random\nimport shutil\n\nimport moviepy\nfrom moviepy.editor import VideoFileClip, concatenate_videoclips\n\nOUTPUT_DIR = r\"directory where the compiled video will be saved\"\nDAILY_TRENDING_DIR = r\"directory location for downloaded tiktok videos\"\nTOP_TEN_VIDS_DIR = r\"directory location for top 10 videos\"\n\n\ndef select_videos(daily_trending_videos, top_videos):\n    os.chdir(daily_trending_videos)\n    trending_list = os.listdir(daily_trending_videos)\n    daily_vids = []",
    "last_modified": "2025-05-04T23:28:21"
  },
  {
    "id": "1887",
    "name": "playlist.py",
    "path": "github_repo/scripts/01_core_ai_analysis/ai_generation/playlist.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 214,
    "size": 6717,
    "docstring": "",
    "keywords": [
      "youtube",
      "analysis",
      "web_tools"
    ],
    "functions": [
      "generate_token",
      "get_youtube_url",
      "download_mp3",
      "last_fm_artist_info",
      "set_metadata",
      "write_tracks",
      "write_playlist",
      "split_spotify_uri",
      "get_os",
      "get_folder"
    ],
    "classes": [],
    "imports": [
      "argparse",
      "json",
      "os",
      "re",
      "subprocess",
      "urllib.parse",
      "urllib.request",
      "secret",
      "spotipy",
      "spotipy.oauth2"
    ],
    "preview": "import argparse\nimport json\nimport os\nimport re\nimport subprocess\nimport urllib.parse\nimport urllib.request\n\nimport secret\nimport spotipy\nimport spotipy.oauth2 as oauth2\nfrom requests.utils import quote\n\n\ndef generate_token():\n    credentials = oauth2.SpotifyClientCredentials(\n        client_id=secret.SPOTIFY_CLIENT_ID, client_secret=secret.SPOTIFY_CLIENT_SECRET\n    )\n    token = credentials.get_access_token()\n    return token",
    "last_modified": "2025-09-13T05:54:02.203892"
  },
  {
    "id": "1888",
    "name": "conda_optimizer.py",
    "path": "github_repo/scripts/01_core_ai_analysis/ai_generation/conda_optimizer.py",
    "category": "01_core_ai_analysis",
    "type": "organization",
    "lines": 315,
    "size": 10899,
    "docstring": "Conda Environment Optimizer\nProvides recommendations for cleaning up and optimizing conda environments.",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "load_report",
      "find_similar_environments",
      "find_unused_environments",
      "analyze_large_packages",
      "calculate_potential_savings",
      "generate_recommendations",
      "format_size",
      "generate_cleanup_script",
      "main"
    ],
    "classes": [],
    "imports": [
      "json",
      "pathlib",
      "collections"
    ],
    "preview": "#!/usr/bin/env python3\n\"\"\"\nConda Environment Optimizer\nProvides recommendations for cleaning up and optimizing conda environments.\n\"\"\"\n\nimport json\nfrom pathlib import Path\nfrom collections import defaultdict\n\ndef load_report(filename=\"conda_env_report.json\"):\n    \"\"\"Load the analysis report.\"\"\"\n    with open(filename) as f:\n        return json.load(f)\n\ndef find_similar_environments(environments):\n    \"\"\"Find environments with high package overlap.\"\"\"\n    existing_envs = [e for e in environments if e[\"exists\"]]\n    similar_pairs = []\n    ",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1889",
    "name": "lazy 1.py",
    "path": "github_repo/scripts/01_core_ai_analysis/ai_generation/lazy 1.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 161,
    "size": 6963,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "parse_album_data",
      "generate_html",
      "main"
    ],
    "classes": [],
    "imports": [
      "os"
    ],
    "preview": "import os\n\n\ndef parse_album_data(paths_file):\n    albums = {}\n    with open(paths_file, \"r\", encoding=\"utf-8\") as f:\n        for line in f:\n            line = line.strip()\n            if not line:\n                continue\n            # Process only if the line is in the mp4 folder structure\n            if \"/mp4/\" in line:\n                # Extract album folder name by splitting at \"/mp4/\" then taking first folder\n                parts = line.split(\"/mp4/\")[1].split(\"/\")\n                if not parts:\n                    continue\n                album_key = parts[0]\n                if album_key not in albums:\n                    albums[album_key] = {\n                        \"mp3\": None,",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1890",
    "name": "generate_album-pages.py",
    "path": "github_repo/scripts/01_core_ai_analysis/ai_generation/generate_album-pages.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 90,
    "size": 2534,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "read_text_file",
      "generate_album_html"
    ],
    "classes": [],
    "imports": [
      "os"
    ],
    "preview": "import os\n\nTEMPLATE = \"\"\"\n<!DOCTYPE html>\n<html lang=\"en\">\n<head>\n    <meta charset=\"UTF-8\">\n    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n    <title>{album_name}</title>\n    <link rel=\"stylesheet\" href=\"../css/styles.css\">\n</head>\n<body>\n    <div class=\"album-container\">\n        <div class=\"album\">\n            <img src=\"../albums/{folder_name}/album_cover.jpg\" alt=\"{album_name} Cover\">\n            <h3>{album_name}</h3>\n            <audio controls>\n                <source src=\"../albums/{folder_name}/{mp3_file}\" type=\"audio/mpeg\">\n                Your browser does not support the audio element.\n            </audio>",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1891",
    "name": "regexopt.py",
    "path": "github_repo/scripts/01_core_ai_analysis/ai_generation/regexopt.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 106,
    "size": 3225,
    "docstring": "pygments.regexopt\n~~~~~~~~~~~~~~~~~\n\nAn algorithm that generates optimized regexes for matching long lists of\nliteral strings.\n\n:copyright: Copyright 2006-2023 by the Pygments team, see AUTHORS.\n:license: BSD, see LICENSE for details.",
    "keywords": [],
    "functions": [
      "make_charset",
      "regex_opt_inner",
      "regex_opt"
    ],
    "classes": [],
    "imports": [
      "re",
      "itertools",
      "operator",
      "os.path",
      "re"
    ],
    "preview": "\"\"\"\npygments.regexopt\n~~~~~~~~~~~~~~~~~\n\nAn algorithm that generates optimized regexes for matching long lists of\nliteral strings.\n\n:copyright: Copyright 2006-2023 by the Pygments team, see AUTHORS.\n:license: BSD, see LICENSE for details.\n\"\"\"\n\nimport re\nfrom itertools import groupby\nfrom operator import itemgetter\nfrom os.path import commonprefix\nfrom re import escape\n\nCS_ESCAPE = re.compile(r\"[\\[\\^\\\\\\-\\]]\")\nFIRST_ELEMENT = itemgetter(0)\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1892",
    "name": "api_login.py",
    "path": "github_repo/scripts/01_core_ai_analysis/ai_generation/api_login.py",
    "category": "01_core_ai_analysis",
    "type": "web_tools",
    "lines": 405,
    "size": 21643,
    "docstring": "",
    "keywords": [
      "analysis",
      "web_tools"
    ],
    "functions": [
      "sync_device_features",
      "sync_launcher",
      "set_contact_point_prefill",
      "get_prefill_candidates",
      "get_account_family",
      "get_zr_token_result",
      "banyan",
      "igtv_browse_feed",
      "creatives_ar_class",
      "pre_login_flow"
    ],
    "classes": [],
    "imports": [
      "json",
      "os",
      "random",
      "time",
      "traceback",
      "requests",
      "requests.utils"
    ],
    "preview": "import json\nimport os\nimport random\nimport time\nimport traceback\n\nimport requests\nimport requests.utils\n\nfrom . import config, devices\n\n# ====== SYNC METHODS ====== #\n\n\ndef sync_device_features(self, login=None):\n    data = {\n        \"id\": self.uuid,\n        \"server_config_retrieval\": \"1\",\n        \"experiments\": config.LOGIN_EXPERIMENTS,\n    }",
    "last_modified": "2025-09-13T05:54:56.206413"
  },
  {
    "id": "1893",
    "name": "app.py",
    "path": "github_repo/scripts/01_core_ai_analysis/ai_generation/app.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 140,
    "size": 4413,
    "docstring": "",
    "keywords": [
      "analysis",
      "youtube",
      "organization"
    ],
    "functions": [
      "safe_filename",
      "main",
      "valid_post",
      "get_valid_posts",
      "download_assets",
      "create_directories",
      "create_directory",
      "clean_temp_dir"
    ],
    "classes": [
      "ttsvibelounge"
    ],
    "imports": [
      "argparse",
      "logging",
      "os",
      "re",
      "pathlib",
      "praw",
      "settings",
      "video",
      "yaml",
      "config"
    ],
    "preview": "import argparse\nimport logging\nimport os\nimport re\nfrom pathlib import Path\n\nimport praw\nimport settings\nimport video\nimport yaml\n\nimport config\n\nlogging.basicConfig(\n    format=\"%(asctime)s %(levelname)-8s %(message)s\",\n    level=logging.INFO,\n    datefmt=\"%Y-%m-%d %H:%M:%S\",\n    handlers=[logging.FileHandler(\"debug.log\"), logging.StreamHandler()],\n)\n",
    "last_modified": "2025-09-13T05:53:29.187021"
  },
  {
    "id": "1894",
    "name": "freeze.py",
    "path": "github_repo/scripts/01_core_ai_analysis/ai_generation/freeze.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 106,
    "size": 3126,
    "docstring": "",
    "keywords": [],
    "functions": [
      "_should_suppress_build_backends",
      "_dev_pkgs",
      "add_options",
      "run"
    ],
    "classes": [
      "FreezeCommand"
    ],
    "imports": [
      "sys",
      "optparse",
      "typing",
      "pip._internal.cli",
      "pip._internal.cli.base_command",
      "pip._internal.cli.status_codes",
      "pip._internal.operations.freeze",
      "pip._internal.utils.compat"
    ],
    "preview": "import sys\nfrom optparse import Values\nfrom typing import AbstractSet, List\n\nfrom pip._internal.cli import cmdoptions\nfrom pip._internal.cli.base_command import Command\nfrom pip._internal.cli.status_codes import SUCCESS\nfrom pip._internal.operations.freeze import freeze\nfrom pip._internal.utils.compat import stdlib_pkgs\n\n\ndef _should_suppress_build_backends() -> bool:\n    return sys.version_info < (3, 12)\n\n\ndef _dev_pkgs() -> AbstractSet[str]:\n    pkgs = {\"pip\"}\n\n    if _should_suppress_build_backends():\n        pkgs |= {\"setuptools\", \"distribute\", \"wheel\"}",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1895",
    "name": "sora-song.py",
    "path": "github_repo/scripts/01_core_ai_analysis/ai_generation/sora-song.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 118,
    "size": 4452,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "get_music_directory",
      "scan_music_directory",
      "generate_storyboard_csv"
    ],
    "classes": [],
    "imports": [
      "csv",
      "os"
    ],
    "preview": "import csv\nimport os\n\n\ndef get_music_directory():\n    \"\"\"Prompt the user to input the directory where MP3 files are stored.\"\"\"\n    music_directory = input(\n        \"Enter the path to your music directory (e.g., /Users/steven/Music/NocTurnE-meLoDieS/mp3/): \"\n    ).strip()\n\n    # Validate if the directory exists\n    if not os.path.isdir(music_directory):\n        print(\n            f\"Error: The directory '{music_directory}' does not exist. Please enter a valid directory.\"\n        )\n        return get_music_directory()\n\n    return music_directory\n\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1896",
    "name": "file_utils.py",
    "path": "github_repo/scripts/01_core_ai_analysis/ai_generation/file_utils.py",
    "category": "01_core_ai_analysis",
    "type": "organization",
    "lines": 28,
    "size": 832,
    "docstring": "",
    "keywords": [],
    "functions": [
      "get_creation_date",
      "format_size",
      "should_exclude"
    ],
    "classes": [
      "FileOrganizer"
    ],
    "imports": [
      "logging",
      "re",
      "datetime",
      "pathlib"
    ],
    "preview": "import logging\nimport re\nfrom datetime import datetime\nfrom pathlib import Path\n\n\nclass FileOrganizer:\n    @staticmethod\n    def get_creation_date(path: Path) -> str:\n        try:\n            return datetime.fromtimestamp(path.stat().st_ctime).strftime(\"%m-%d-%y\")\n        except Exception as e:\n            logging.error(f\"Creation date error: {str(e)}\")\n            return \"Unknown\"\n\n    @staticmethod\n    def format_size(size: int) -> str:\n        for unit in [\"B\", \"KB\", \"MB\", \"GB\", \"TB\"]:\n            if size < 1024:\n                return f\"{size:.2f} {unit}\"",
    "last_modified": "2025-05-04T22:47:12.611806"
  },
  {
    "id": "1897",
    "name": "img-origin-date.py",
    "path": "github_repo/scripts/01_core_ai_analysis/ai_generation/img-origin-date.py",
    "category": "01_core_ai_analysis",
    "type": "image_processing",
    "lines": 130,
    "size": 3978,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "get_creation_date",
      "get_image_metadata",
      "format_file_size",
      "generate_csv",
      "write_csv"
    ],
    "classes": [],
    "imports": [
      "csv",
      "os",
      "datetime",
      "PIL"
    ],
    "preview": "import csv\nimport os\nfrom datetime import datetime\n\nfrom PIL import Image\n\n\n# Function to get the creation date of a file\ndef get_creation_date(filepath):\n    try:\n        return datetime.fromtimestamp(os.path.getctime(filepath)).strftime(\"%y-%m-%d\")\n    except Exception as e:\n        print(f\"Error getting creation date for {filepath}: {e}\")\n        return \"Unknown\"\n\n\n# Function to extract metadata from an image file using PIL\ndef get_image_metadata(filepath):\n    try:\n        with Image.open(filepath) as img:",
    "last_modified": "2025-05-04T22:47:13.347944"
  },
  {
    "id": "1898",
    "name": "docs2.py",
    "path": "github_repo/scripts/01_core_ai_analysis/ai_generation/docs2.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 215,
    "size": 7169,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "generate_dry_run_csv",
      "write_csv",
      "format_file_size",
      "get_creation_date",
      "save_last_directory",
      "load_last_directory",
      "get_unique_file_path"
    ],
    "classes": [],
    "imports": [
      "csv",
      "logging",
      "os",
      "re",
      "pathlib",
      "datetime"
    ],
    "preview": "import csv\nimport logging\nimport os\nimport re\nfrom pathlib import Path\n\nlogger = logging.getLogger(__name__)\nlogging.basicConfig(level=logging.INFO)\n\n# Pre-compile all patterns for performance and clarity\nEXCLUDED_REGEXES = [\n    re.compile(pattern)\n    for pattern in [\n        r\"^\\..*\",  # Hidden files/directories at the root\n        r\".*\\/venv\\/.*\",  # venv directories\n        r\".*\\/\\.venv\\/.*\",  # .venv directories\n        r\".*\\/lib\\/.*\",  # library directories\n        r\".*\\/\\.lib\\/.*\",  # .lib directories\n        r\".*\\/my_global_venv\\/.*\",\n        r\".*\\/simplegallery\\/.*\",",
    "last_modified": "2025-09-06T12:24:11.724261"
  },
  {
    "id": "1899",
    "name": "audio.py",
    "path": "github_repo/scripts/01_core_ai_analysis/ai_generation/audio.py",
    "category": "01_core_ai_analysis",
    "type": "transcription",
    "lines": 239,
    "size": 7359,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "get_creation_date",
      "get_audio_metadata",
      "format_file_size",
      "format_duration",
      "generate_dry_run_csv",
      "write_csv",
      "get_unique_file_path",
      "save_last_directory",
      "load_last_directory"
    ],
    "classes": [],
    "imports": [
      "csv",
      "os",
      "re",
      "datetime",
      "mutagen.easyid3",
      "mutagen.mp3",
      "config"
    ],
    "preview": "import csv\nimport os\nimport re\nfrom datetime import datetime\n\nfrom mutagen.easyid3 import EasyID3\nfrom mutagen.mp3 import MP3\n\nimport config  # Import the configuration\n\n# Constants\nLAST_DIRECTORY_FILE = \"audio.txt\"\n\n# Function to get the creation date of a file\n\n\ndef get_creation_date(filepath):\n    try:\n        return datetime.fromtimestamp(os.path.getctime(filepath)).strftime(\"%m-%d-%y\")\n    except Exception as e:",
    "last_modified": "2025-09-13T05:53:48.331970"
  },
  {
    "id": "1900",
    "name": "generate_prompts.py",
    "path": "github_repo/scripts/01_core_ai_analysis/ai_generation/generate_prompts.py",
    "category": "01_core_ai_analysis",
    "type": "image_processing",
    "lines": 154,
    "size": 6462,
    "docstring": "Generate detailed, descriptive image prompts from a timestamped transcript.\n\nInput:\n  A transcript text file with lines like:\n    [HH:MM:SS] some narrative text...\nOutput:\n  - <base>_prompts.jsonl  (one JSON per prompt item)\n  - <base>_prompts.md     (nicely formatted, ready to paste into your generator)\nDesign:\n  For each line, we infer mood, setting, and symbolism, then produce a trio:\n    - Transition image (connective visual)\n    - Main image (cinematic narrative focus)\n    - Filler/typography (graphic/overlay for pacing)\nThe script uses lightweight heuristics and word-shape detection to avoid\nheavy dependencies while still yielding rich, cinematic prompts.",
    "keywords": [],
    "functions": [
      "parse_transcript_lines",
      "pick_tags",
      "enrich_prompt",
      "make_triplet",
      "write_jsonl_md",
      "main"
    ],
    "classes": [],
    "imports": [
      "argparse",
      "sys",
      "json",
      "re",
      "pathlib"
    ],
    "preview": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\nGenerate detailed, descriptive image prompts from a timestamped transcript.\n\nInput:\n  A transcript text file with lines like:\n    [HH:MM:SS] some narrative text...\nOutput:\n  - <base>_prompts.jsonl  (one JSON per prompt item)\n  - <base>_prompts.md     (nicely formatted, ready to paste into your generator)\nDesign:\n  For each line, we infer mood, setting, and symbolism, then produce a trio:\n    - Transition image (connective visual)\n    - Main image (cinematic narrative focus)\n    - Filler/typography (graphic/overlay for pacing)\nThe script uses lightweight heuristics and word-shape detection to avoid\nheavy dependencies while still yielding rich, cinematic prompts.\n\"\"\"\nimport argparse, sys, json, re",
    "last_modified": "2025-09-15T18:25:10"
  },
  {
    "id": "1901",
    "name": "input_handler.py",
    "path": "github_repo/scripts/01_core_ai_analysis/ai_generation/input_handler.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 270,
    "size": 7546,
    "docstring": "",
    "keywords": [],
    "functions": [
      "get_inputs",
      "get_time_period",
      "get_directory",
      "get_languages",
      "check_inputs",
      "select_clips"
    ],
    "classes": [],
    "imports": [
      "os",
      "tkinter",
      "tkinter.filedialog",
      "cmd_logs",
      "configHandler",
      "twitchClips"
    ],
    "preview": "# Local imports\n# Imports\nimport os\nfrom tkinter import Tk\nfrom tkinter.filedialog import askdirectory\n\nfrom .cmd_logs import *\nfrom .configHandler import *\nfrom .twitchClips import *\n\navailable_langs = [\n    \"eng\",\n    \"id\",\n    \"ca\",\n    \"da\",\n    \"de\",\n    \"es\",\n    \"fr\",\n    \"hu\",\n    \"nl\",",
    "last_modified": "2025-09-13T05:53:45.139003"
  },
  {
    "id": "1902",
    "name": "gallery_init_20241204123453.py",
    "path": "github_repo/scripts/01_core_ai_analysis/ai_generation/gallery_init_20241204123453.py",
    "category": "01_core_ai_analysis",
    "type": "setup",
    "lines": 292,
    "size": 9797,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "parse_args",
      "check_if_gallery_creation_possible",
      "check_if_gallery_already_exists",
      "create_gallery_folder_structure",
      "create_gallery_json",
      "main"
    ],
    "classes": [],
    "imports": [
      "argparse",
      "glob",
      "importlib.resources",
      "json",
      "os",
      "shutil",
      "sys",
      "distutils.dir_util",
      "simplegallery.common",
      "simplegallery.logic.gallery_logic"
    ],
    "preview": "import argparse\nimport glob\nimport importlib.resources\nimport json\nimport os\nimport shutil\nimport sys\nfrom distutils.dir_util import copy_tree\n\nimport simplegallery.common as spg_common\nimport simplegallery.logic.gallery_logic as gallery_logic\n\n\ndef parse_args():\n    \"\"\"\n    Configures the argument parser\n    :return: Parsed arguments\n    \"\"\"\n\n    description = \"\"\"Initializes a new Simple Photo Gallery in the specified folder (default is the current folder).",
    "last_modified": "2025-09-13T05:55:18.237910"
  },
  {
    "id": "1903",
    "name": "ansi.py",
    "path": "github_repo/scripts/01_core_ai_analysis/ai_generation/ansi.py",
    "category": "01_core_ai_analysis",
    "type": "web_tools",
    "lines": 111,
    "size": 2297,
    "docstring": "This module generates ANSI character codes to printing colors to terminals.\nSee: http://en.wikipedia.org/wiki/ANSI_escape_code",
    "keywords": [],
    "functions": [
      "code_to_chars",
      "set_title",
      "clear_screen",
      "clear_line",
      "__init__",
      "UP",
      "DOWN",
      "FORWARD",
      "BACK",
      "POS"
    ],
    "classes": [
      "AnsiCodes",
      "AnsiCursor",
      "AnsiFore",
      "AnsiBack",
      "AnsiStyle"
    ],
    "imports": [],
    "preview": "# Copyright Jonathan Hartley 2013. BSD 3-Clause license, see LICENSE file.\n\"\"\"\nThis module generates ANSI character codes to printing colors to terminals.\nSee: http://en.wikipedia.org/wiki/ANSI_escape_code\n\"\"\"\n\nCSI = \"\\033[\"\nOSC = \"\\033]\"\nBEL = \"\\a\"\n\n\ndef code_to_chars(code):\n    return CSI + str(code) + \"m\"\n\n\ndef set_title(title):\n    return OSC + \"2;\" + title + BEL\n\n\ndef clear_screen(mode=2):",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1904",
    "name": "contact-sheet2.py",
    "path": "github_repo/scripts/01_core_ai_analysis/ai_generation/contact-sheet2.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 49,
    "size": 1529,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "create_thumbnail",
      "create_contact_sheet",
      "save_contact_sheet"
    ],
    "classes": [],
    "imports": [
      "os",
      "PIL"
    ],
    "preview": "import os\n\nfrom PIL import Image\n\n\ndef create_thumbnail(image_path, thumbnail_size=(100, 100)):\n    with Image.open(image_path) as img:\n        img.thumbnail(thumbnail_size)\n        return img\n\n\ndef create_contact_sheet(directory, thumbnail_size=(100, 100), sheet_size=(500, 500)):\n    thumbnails = []\n    for subdir, dirs, files in os.walk(directory):\n        for file in files:\n            if file.lower().endswith((\"png\", \"jpg\", \"jpeg\", \"gif\", \"bmp\")):\n                image_path = os.path.join(subdir, file)\n                thumbnail = create_thumbnail(image_path, thumbnail_size)\n                thumbnails.append(thumbnail)\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1905",
    "name": "website.py",
    "path": "github_repo/scripts/01_core_ai_analysis/ai_generation/website.py",
    "category": "01_core_ai_analysis",
    "type": "web_tools",
    "lines": 188,
    "size": 5246,
    "docstring": "MIT License\n\nCopyright (c) 2021-2022 MShawon\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "create_graph_data",
      "create_dropdown_data",
      "shutdown_server",
      "start_server",
      "home",
      "update",
      "graph",
      "shutdown"
    ],
    "classes": [],
    "imports": [
      "calendar",
      "sqlite3",
      "warnings",
      "contextlib",
      "datetime",
      "flask"
    ],
    "preview": "\"\"\"\nMIT License\n\nCopyright (c) 2021-2022 MShawon\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,",
    "last_modified": "2025-05-04T23:27:45"
  },
  {
    "id": "1906",
    "name": "tocpdf.py",
    "path": "github_repo/scripts/01_core_ai_analysis/ai_generation/tocpdf.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 27,
    "size": 642,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "create_toc_pdf"
    ],
    "classes": [],
    "imports": [
      "reportlab.lib.pagesizes",
      "reportlab.pdfgen"
    ],
    "preview": "from reportlab.lib.pagesizes import letter\nfrom reportlab.pdfgen import canvas\n\n\ndef create_toc_pdf(toc_items, output_path):\n    c = canvas.Canvas(output_path, pagesize=letter)\n    width, height = letter\n    c.setFont(\"Helvetica\", 12)\n\n    y_position = height - 40\n    for item in toc_items:\n        text = f\"{item['title']} ... {item['page']}\"\n        c.drawString(40, y_position, text)\n        y_position -= 20\n\n    c.save()\n\n\ntoc_items = [\n    {\"title\": \"Introduction\", \"page\": 1},",
    "last_modified": "2025-05-04T22:47:13.332373"
  },
  {
    "id": "1907",
    "name": "progress_bars.py",
    "path": "github_repo/scripts/01_core_ai_analysis/ai_generation/progress_bars.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 68,
    "size": 1967,
    "docstring": "",
    "keywords": [],
    "functions": [
      "_rich_progress_bar",
      "get_download_progress_renderer"
    ],
    "classes": [],
    "imports": [
      "functools",
      "typing",
      "pip._internal.utils.logging",
      "pip._vendor.rich.progress"
    ],
    "preview": "import functools\nfrom typing import Callable, Generator, Iterable, Iterator, Optional, Tuple\n\nfrom pip._internal.utils.logging import get_indentation\nfrom pip._vendor.rich.progress import (\n    BarColumn,\n    DownloadColumn,\n    FileSizeColumn,\n    Progress,\n    ProgressColumn,\n    SpinnerColumn,\n    TextColumn,\n    TimeElapsedColumn,\n    TimeRemainingColumn,\n    TransferSpeedColumn,\n)\n\nDownloadProgressRenderer = Callable[[Iterable[bytes]], Iterator[bytes]]\n\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1908",
    "name": "dallecsv.py",
    "path": "github_repo/scripts/01_core_ai_analysis/ai_generation/dallecsv.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 42,
    "size": 995,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "csv"
    ],
    "preview": "import csv\n\n# Path to the input text file\ninput_file_path = \"/Users/steven/Pictures/DaLL-E/dalle/dalle.txt\"\n\n# Path to the output CSV file\noutput_csv_path = \"/Users/steven/Pictures/DaLL-E/dalle/dalle_output.csv\"\n\n# Read the content of the input file\nwith open(input_file_path, \"r\") as file:\n    lines = file.readlines()\n\n# Prepare the data for CSV\ndata = []\nurl = None\ninfo = None\n\nfor line in lines:\n    line = line.strip()\n    if line.startswith(\"https://\"):",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1909",
    "name": "onedrive_gallery_logic.py",
    "path": "github_repo/scripts/01_core_ai_analysis/ai_generation/onedrive_gallery_logic.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 101,
    "size": 3762,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "parse_photo_link",
      "create_thumbnails",
      "generate_images_data"
    ],
    "classes": [
      "OnedriveGalleryLogic"
    ],
    "imports": [
      "time",
      "pkg_resources",
      "simplegallery.common",
      "simplegallery.media",
      "selenium",
      "selenium.webdriver.firefox.options",
      "simplegallery.logic.base_gallery_logic"
    ],
    "preview": "import time\n\nimport pkg_resources\nimport simplegallery.common as spg_common\nimport simplegallery.media as spg_media\nfrom selenium import webdriver\nfrom selenium.webdriver.firefox.options import Options\nfrom simplegallery.logic.base_gallery_logic import BaseGalleryLogic\n\n\ndef parse_photo_link(photo_url):\n    \"\"\"\n    Extracts the base URL (URL without query parameters) and the photo name from a Onedrive photo URL\n    :param photo_url: photo URL\n    :return: base URL and photo name\n    \"\"\"\n    base_url = photo_url.split(\"?\")[0]\n    name = base_url.split(\"/\")[-1]\n\n    return base_url, name",
    "last_modified": "2025-09-13T05:53:52.615382"
  },
  {
    "id": "1910",
    "name": "lazy.py",
    "path": "github_repo/scripts/01_core_ai_analysis/ai_generation/lazy.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 161,
    "size": 6963,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "parse_album_data",
      "generate_html",
      "main"
    ],
    "classes": [],
    "imports": [
      "os"
    ],
    "preview": "import os\n\n\ndef parse_album_data(paths_file):\n    albums = {}\n    with open(paths_file, \"r\", encoding=\"utf-8\") as f:\n        for line in f:\n            line = line.strip()\n            if not line:\n                continue\n            # Process only if the line is in the mp4 folder structure\n            if \"/mp4/\" in line:\n                # Extract album folder name by splitting at \"/mp4/\" then taking first folder\n                parts = line.split(\"/mp4/\")[1].split(\"/\")\n                if not parts:\n                    continue\n                album_key = parts[0]\n                if album_key not in albums:\n                    albums[album_key] = {\n                        \"mp3\": None,",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1911",
    "name": "compatibility_tags.py",
    "path": "github_repo/scripts/01_core_ai_analysis/ai_generation/compatibility_tags.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 165,
    "size": 5376,
    "docstring": "Generate and work with PEP 425 Compatibility Tags.",
    "keywords": [],
    "functions": [
      "version_info_to_nodot",
      "_mac_platforms",
      "_custom_manylinux_platforms",
      "_get_custom_platforms",
      "_expand_allowed_platforms",
      "_get_python_version",
      "_get_custom_interpreter",
      "get_supported"
    ],
    "classes": [],
    "imports": [
      "re",
      "typing",
      "pip._vendor.packaging.tags"
    ],
    "preview": "\"\"\"Generate and work with PEP 425 Compatibility Tags.\"\"\"\n\nimport re\nfrom typing import List, Optional, Tuple\n\nfrom pip._vendor.packaging.tags import (\n    PythonVersion,\n    Tag,\n    compatible_tags,\n    cpython_tags,\n    generic_tags,\n    interpreter_name,\n    interpreter_version,\n    mac_platforms,\n)\n\n_osx_arch_pat = re.compile(r\"(.+)_(\\d+)_(\\d+)_(.+)\")\n\n\ndef version_info_to_nodot(version_info: Tuple[int, ...]) -> str:",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1912",
    "name": "test_file_gallery_logic.py",
    "path": "github_repo/scripts/01_core_ai_analysis/ai_generation/test_file_gallery_logic.py",
    "category": "01_core_ai_analysis",
    "type": "testing",
    "lines": 114,
    "size": 4710,
    "docstring": "",
    "keywords": [
      "testing"
    ],
    "functions": [
      "test_create_thumbnails",
      "test_generate_images_data"
    ],
    "classes": [
      "FileGalleryLogicTestCase"
    ],
    "imports": [
      "os",
      "unittest",
      "unittest",
      "simplegallery.media",
      "simplegallery.test.helpers",
      "simplegallery.logic.variants.files_gallery_logic",
      "testfixtures"
    ],
    "preview": "import os\nimport unittest\nfrom unittest import mock\n\nimport simplegallery.media as spg_media\nimport simplegallery.test.helpers as helpers\nfrom simplegallery.logic.variants.files_gallery_logic import FilesGalleryLogic\nfrom testfixtures import TempDirectory\n\n\nclass FileGalleryLogicTestCase(unittest.TestCase):\n    @mock.patch(\"builtins.input\", side_effect=[\"\", \"\", \"\", \"\"])\n    def test_create_thumbnails(self, input):\n        with TempDirectory() as tempdir:\n            helpers.create_mock_image(os.path.join(tempdir.path, \"photo.jpg\"), 1000, 500)\n            helpers.create_mock_image(os.path.join(tempdir.path, \"photo2.gif\"), 1000, 500)\n            helpers.create_mock_image(os.path.join(tempdir.path, \"photo3.png\"), 1000, 500)\n\n            thumbnail_path = os.path.join(\n                tempdir.path, \"public\", \"images\", \"thumbnails\", \"photo.jpg\"",
    "last_modified": "2025-09-13T05:53:52.828780"
  },
  {
    "id": "1913",
    "name": "files_gallery_logic.py",
    "path": "github_repo/scripts/01_core_ai_analysis/ai_generation/files_gallery_logic.py",
    "category": "01_core_ai_analysis",
    "type": "organization",
    "lines": 140,
    "size": 5667,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "check_correct_thumbnail_size",
      "get_thumbnail_name",
      "create_thumbnails",
      "format_image_date",
      "generate_images_data"
    ],
    "classes": [
      "FilesGalleryLogic"
    ],
    "imports": [
      "glob",
      "os",
      "datetime",
      "simplegallery.common",
      "simplegallery.media",
      "simplegallery.logic.base_gallery_logic"
    ],
    "preview": "import glob\nimport os\nfrom datetime import datetime\n\nimport simplegallery.common as spg_common\nimport simplegallery.media as spg_media\nfrom simplegallery.logic.base_gallery_logic import BaseGalleryLogic\n\n\ndef check_correct_thumbnail_size(thumbnail_path, expected_height):\n    \"\"\"\n    Check if a thumbnail has the correct height\n    :param thumbnail_path: Path to the thumbnail file\n    :param expected_height: Expected height of the thumbnail in pixels\n    :return: True if the height of the thumbnail equals the expected height, False otherwise\n    \"\"\"\n    return expected_height == spg_media.get_image_size(thumbnail_path)[1]\n\n\ndef get_thumbnail_name(thumbnails_path, photo_name):",
    "last_modified": "2025-09-13T05:53:52.534901"
  },
  {
    "id": "1914",
    "name": "metadata_legacy.py",
    "path": "github_repo/scripts/01_core_ai_analysis/ai_generation/metadata_legacy.py",
    "category": "01_core_ai_analysis",
    "type": "analysis",
    "lines": 72,
    "size": 2175,
    "docstring": "Metadata generation logic for legacy source distributions.",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "_find_egg_info",
      "generate_metadata"
    ],
    "classes": [],
    "imports": [
      "logging",
      "os",
      "pip._internal.build_env",
      "pip._internal.cli.spinners",
      "pip._internal.exceptions",
      "pip._internal.utils.setuptools_build",
      "pip._internal.utils.subprocess",
      "pip._internal.utils.temp_dir"
    ],
    "preview": "\"\"\"Metadata generation logic for legacy source distributions.\"\"\"\n\nimport logging\nimport os\n\nfrom pip._internal.build_env import BuildEnvironment\nfrom pip._internal.cli.spinners import open_spinner\nfrom pip._internal.exceptions import (\n    InstallationError,\n    InstallationSubprocessError,\n    MetadataGenerationFailed,\n)\nfrom pip._internal.utils.setuptools_build import make_setuptools_egg_info_args\nfrom pip._internal.utils.subprocess import call_subprocess\nfrom pip._internal.utils.temp_dir import TempDirectory\n\nlogger = logging.getLogger(__name__)\n\n\ndef _find_egg_info(directory: str) -> str:",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1915",
    "name": "pipgen.py",
    "path": "github_repo/scripts/01_core_ai_analysis/ai_generation/pipgen.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 11,
    "size": 282,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "subprocess"
    ],
    "preview": "# generate_requirements.py\nimport subprocess\n\n# Path to your project directory\nproject_path = \"/Users/steven/Documents/Python\"\n\n# Step 1: Generate requirements.txt using pipreqs\nsubprocess.run([\"pipreqs\", project_path, \"--force\"])\n\nprint(\"requirements.txt generated successfully.\")\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1916",
    "name": "test_gallery_build.py",
    "path": "github_repo/scripts/01_core_ai_analysis/ai_generation/test_gallery_build.py",
    "category": "01_core_ai_analysis",
    "type": "testing",
    "lines": 109,
    "size": 4020,
    "docstring": "",
    "keywords": [
      "testing",
      "analysis"
    ],
    "functions": [
      "create_mock_image",
      "test_nonexisting_gallery_config",
      "test_thumbnails_generation",
      "test_images_data_generation",
      "test_index_html"
    ],
    "classes": [
      "SPGBuildTestCase"
    ],
    "imports": [
      "json",
      "os",
      "sys",
      "unittest",
      "unittest",
      "simplegallery.gallery_build",
      "simplegallery.gallery_init",
      "simplegallery.media",
      "PIL",
      "testfixtures"
    ],
    "preview": "import json\nimport os\nimport sys\nimport unittest\nfrom unittest import mock\n\nimport simplegallery.gallery_build as gallery_build\nimport simplegallery.gallery_init as gallery_init\nimport simplegallery.media as spg_media\nfrom PIL import Image\nfrom testfixtures import TempDirectory\n\n\ndef create_mock_image(path, width, height):\n    img = Image.new(\"RGB\", (width, height), color=\"red\")\n    img.save(path)\n    img.close()\n\n\nclass SPGBuildTestCase(unittest.TestCase):",
    "last_modified": "2025-09-13T05:53:52.938426"
  },
  {
    "id": "1917",
    "name": "gallery_init_20241204123258.py",
    "path": "github_repo/scripts/01_core_ai_analysis/ai_generation/gallery_init_20241204123258.py",
    "category": "01_core_ai_analysis",
    "type": "setup",
    "lines": 292,
    "size": 9785,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "parse_args",
      "check_if_gallery_creation_possible",
      "check_if_gallery_already_exists",
      "create_gallery_folder_structure",
      "create_gallery_json",
      "main"
    ],
    "classes": [],
    "imports": [
      "argparse",
      "glob",
      "importlib.resources",
      "json",
      "os",
      "shutil",
      "sys",
      "distutils.dir_util",
      "simplegallery.common",
      "simplegallery.logic.gallery_logic"
    ],
    "preview": "import argparse\nimport glob\nimport importlib.resources\nimport json\nimport os\nimport shutil\nimport sys\nfrom distutils.dir_util import copy_tree\n\nimport simplegallery.common as spg_common\nimport simplegallery.logic.gallery_logic as gallery_logic\n\n\ndef parse_args():\n    \"\"\"\n    Configures the argument parser\n    :return: Parsed arguments\n    \"\"\"\n\n    description = \"\"\"Initializes a new Simple Photo Gallery in the specified folder (default is the current folder).",
    "last_modified": "2025-09-13T05:55:18.006675"
  },
  {
    "id": "1918",
    "name": "helpers.py",
    "path": "github_repo/scripts/01_core_ai_analysis/ai_generation/helpers.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 73,
    "size": 2558,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "init_gallery_and_read_gallery_config",
      "check_image_data",
      "create_mock_image"
    ],
    "classes": [],
    "imports": [
      "json",
      "os",
      "sys",
      "simplegallery.gallery_init",
      "PIL"
    ],
    "preview": "import json\nimport os\nimport sys\n\nimport simplegallery.gallery_init as gallery_init\nfrom PIL import Image\n\n\ndef init_gallery_and_read_gallery_config(path, remote_link=\"\"):\n    \"\"\"\n    Initializes a new gallery and reads the data from the gallery.json file\n    :param path: path to the folder where the gallery will be created\n    :param remote_link: optional remote link to initialize a remote gallery\n    :return: gallery config dict\n    \"\"\"\n\n    sys.argv = [\"gallery_init\", remote_link, \"-p\", path]\n    gallery_init.main()\n\n    with open(os.path.join(path, \"gallery.json\"), \"r\") as json_in:",
    "last_modified": "2025-09-13T05:53:52.742558"
  },
  {
    "id": "1919",
    "name": "csv-html-gen.py",
    "path": "github_repo/scripts/01_core_ai_analysis/ai_generation/csv-html-gen.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 277,
    "size": 8279,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "create_excerpt",
      "generate_job_cards",
      "write_html_job_cards",
      "generate_table_header",
      "generate_table_body",
      "write_html_table",
      "main"
    ],
    "classes": [],
    "imports": [
      "csv",
      "html",
      "os"
    ],
    "preview": "import csv\nimport html\nimport os\n\n# File paths \u2013 adjust as needed\ninput_csv = \"/Users/steven/Documents/QuantumForgeLabs/data.csv\"\noutput_html_job = \"/Users/steven/Documents/QuantumForgeLabs/data.html\"\noutput_html_table = \"/Users/steven/Documents/QuantumForgeLabs/linkd-scrape.html\"\n\n#########################################\n# PART 1: Generate job card HTML output #\n#########################################\njob_html_header = \"\"\"<!DOCTYPE html>\n<html lang=\"en\">\n<head>\n    <meta charset=\"UTF-8\" />\n    <title>LinkedIn Jobs Scraped - Job Cards</title>\n    <style>\n        body {\n            font-family: Arial, sans-serif;",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1920",
    "name": "job-template.py",
    "path": "github_repo/scripts/01_core_ai_analysis/ai_generation/job-template.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 154,
    "size": 4610,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "create_excerpt"
    ],
    "classes": [],
    "imports": [
      "csv",
      "html"
    ],
    "preview": "import csv\nimport html\n\n# Set the input/output paths; update these as necessary.\ninput_csv = \"/Users/steven/Documents/QuantumForgeLabs/data.csv\"\noutput_html = \"/Users/steven/Documents/QuantumForgeLabs/data.html\"\n\n# This is a template for an individual job listing; you can modify the styling or fields.\njob_template = \"\"\"\n<div class=\"job-listing\">\n  <div class=\"job-header\">\n    <div class=\"job-title\">\n      <a href=\"{apply_url}\" target=\"_blank\">{job_title}</a>\n    </div>\n    <div class=\"job-posted\">\n      Posted: <strong>{posted_time}</strong>\n    </div>\n  </div>\n  <div class=\"job-details\">\n    <div class=\"company\">",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1921",
    "name": "alphabet.py",
    "path": "github_repo/scripts/01_core_ai_analysis/ai_generation/alphabet.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 125,
    "size": 4184,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "generate_grouped_gallery"
    ],
    "classes": [],
    "imports": [
      "os"
    ],
    "preview": "import os\n\n\ndef generate_grouped_gallery(folder_path, output_html, group_size=4):\n    \"\"\"\n    Generates an HTML gallery grouped alphabetically by sets of images and MP4s.\n    \"\"\"\n    # Get all valid files\n    valid_extensions = (\".jpg\", \".jpeg\", \".png\", \".mp4\")\n    all_files = [f for f in os.listdir(folder_path) if f.lower().endswith(valid_extensions)]\n\n    # Sort files alphabetically\n    all_files.sort()\n\n    # Group files into sets of the specified size\n    grouped_files = [all_files[i : i + group_size] for i in range(0, len(all_files), group_size)]\n\n    # Start the HTML structure\n    html_content = \"\"\"\n    <!DOCTYPE html>",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1922",
    "name": "dupes.py",
    "path": "github_repo/scripts/01_core_ai_analysis/ai_generation/dupes.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 163,
    "size": 5472,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "is_excluded",
      "scan_directory",
      "compute_md5",
      "generate_detailed_duplicate_report",
      "prompt_for_directories"
    ],
    "classes": [],
    "imports": [
      "csv",
      "hashlib",
      "os",
      "re",
      "collections"
    ],
    "preview": "import csv\nimport hashlib\nimport os\nimport re\nfrom collections import defaultdict\n\n\ndef is_excluded(path, patterns):\n    \"\"\"\n    Check if a given path matches any of the exclusion patterns.\n\n    Parameters:\n    path (str): The path to check.\n    patterns (list): A list of regex patterns for exclusion.\n\n    Returns:\n    bool: True if path matches any pattern, False otherwise.\n    \"\"\"\n    for pattern in patterns:\n        if re.search(pattern, path):",
    "last_modified": "2025-09-13T05:54:29.371317"
  },
  {
    "id": "1923",
    "name": "base.py",
    "path": "github_repo/scripts/01_core_ai_analysis/ai_generation/base.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 22,
    "size": 510,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "create_template",
      "__init__",
      "generate"
    ],
    "classes": [
      "PromptTemplateFactory",
      "RAGStep"
    ],
    "imports": [
      "abc",
      "typing",
      "langchain.prompts",
      "llm_engineering.domain.queries",
      "pydantic"
    ],
    "preview": "from abc import ABC, abstractmethod\nfrom typing import Any\n\nfrom langchain.prompts import PromptTemplate\nfrom llm_engineering.domain.queries import Query\nfrom pydantic import BaseModel\n\n\nclass PromptTemplateFactory(ABC, BaseModel):\n    @abstractmethod\n    def create_template(self) -> PromptTemplate:\n        pass\n\n\nclass RAGStep(ABC):\n    def __init__(self, mock: bool = False) -> None:\n        self._mock = mock\n\n    @abstractmethod\n    def generate(self, query: Query, *args, **kwargs) -> Any:",
    "last_modified": "2025-05-06T04:35:14.981687"
  },
  {
    "id": "1924",
    "name": "gallery_build.py",
    "path": "github_repo/scripts/01_core_ai_analysis/ai_generation/gallery_build.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 163,
    "size": 5602,
    "docstring": "",
    "keywords": [],
    "functions": [
      "parse_args",
      "build_html",
      "main"
    ],
    "classes": [],
    "imports": [
      "argparse",
      "json",
      "os",
      "sys",
      "collections",
      "jinja2",
      "simplegallery.common",
      "simplegallery.logic.gallery_logic"
    ],
    "preview": "import argparse\nimport json\nimport os\nimport sys\nfrom collections import OrderedDict\n\nimport jinja2\nimport simplegallery.common as spg_common\nfrom simplegallery.logic.gallery_logic import get_gallery_logic\n\n\ndef parse_args():\n    \"\"\"\n    Configures the argument parser\n    :return: Parsed arguments\n    \"\"\"\n\n    description = \"\"\"Generated all files needed to display the gallery (thumbnails, image descriptions and HTML page).\n                    For detailed documentation please refer to https://github.com/haltakov/simple-photo-gallery.\"\"\"\n",
    "last_modified": "2025-09-13T05:53:52.319507"
  },
  {
    "id": "1925",
    "name": "img (1).py",
    "path": "github_repo/scripts/01_core_ai_analysis/ai_generation/img (1).py",
    "category": "01_core_ai_analysis",
    "type": "image_processing",
    "lines": 217,
    "size": 7032,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "get_creation_date",
      "get_image_metadata",
      "format_file_size",
      "generate_csv",
      "write_csv",
      "get_unique_file_path",
      "save_last_directory",
      "load_last_directory"
    ],
    "classes": [],
    "imports": [
      "csv",
      "os",
      "re",
      "datetime",
      "PIL"
    ],
    "preview": "import csv\nimport os\nimport re\nfrom datetime import datetime\n\nfrom PIL import Image\n\n# Constants\nLAST_DIRECTORY_FILE = \"image_data.txt\"\n\n\n# Function to get the creation date of a file\ndef get_creation_date(filepath):\n    try:\n        return datetime.fromtimestamp(os.path.getctime(filepath)).strftime(\"%m-%d-%y\")\n    except Exception as e:\n        print(f\"Error getting creation date for {filepath}: {e}\")\n        return \"Unknown\"\n\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1926",
    "name": "gallery_init_20241204120021.py",
    "path": "github_repo/scripts/01_core_ai_analysis/ai_generation/gallery_init_20241204120021.py",
    "category": "01_core_ai_analysis",
    "type": "setup",
    "lines": 292,
    "size": 9791,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "parse_args",
      "check_if_gallery_creation_possible",
      "check_if_gallery_already_exists",
      "create_gallery_folder_structure",
      "create_gallery_json",
      "main"
    ],
    "classes": [],
    "imports": [
      "argparse",
      "glob",
      "importlib.resources",
      "json",
      "os",
      "shutil",
      "sys",
      "distutils.dir_util",
      "simplegallery.common",
      "simplegallery.logic.gallery_logic"
    ],
    "preview": "import argparse\nimport glob\nimport importlib.resources\nimport json\nimport os\nimport shutil\nimport sys\nfrom distutils.dir_util import copy_tree\n\nimport simplegallery.common as spg_common\nimport simplegallery.logic.gallery_logic as gallery_logic\n\n\ndef parse_args():\n    \"\"\"\n    Configures the argument parser\n    :return: Parsed arguments\n    \"\"\"\n\n    description = \"\"\"Initializes a new Simple Photo Gallery in the specified folder (default is the current folder).",
    "last_modified": "2025-09-13T05:55:17.894305"
  },
  {
    "id": "1927",
    "name": "tag.py",
    "path": "github_repo/scripts/01_core_ai_analysis/ai_generation/tag.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 147,
    "size": 5372,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "get_creation_date",
      "get_image_metadata",
      "get_video_metadata",
      "custom_tags",
      "contains_web_project_files",
      "generate_dry_run_csv"
    ],
    "classes": [],
    "imports": [
      "csv",
      "os",
      "datetime",
      "ffmpeg",
      "PIL"
    ],
    "preview": "import csv\nimport os\nfrom datetime import datetime\n\nimport ffmpeg\nfrom PIL import Image\n\n\ndef get_creation_date(filepath):\n    \"\"\"Get the creation date of the file.\"\"\"\n    return datetime.fromtimestamp(os.path.getctime(filepath)).strftime(\"%Y-%m-%d %H:%M:%S\")\n\n\ndef get_image_metadata(filepath):\n    \"\"\"Extract metadata from an image file.\"\"\"\n    with Image.open(filepath) as img:\n        return img.size, os.path.getsize(filepath)\n\n\ndef get_video_metadata(filepath):",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1928",
    "name": "glibc.py",
    "path": "github_repo/scripts/01_core_ai_analysis/ai_generation/glibc.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 89,
    "size": 3113,
    "docstring": "",
    "keywords": [],
    "functions": [
      "glibc_version_string",
      "glibc_version_string_confstr",
      "glibc_version_string_ctypes",
      "libc_ver"
    ],
    "classes": [],
    "imports": [
      "os",
      "sys",
      "typing",
      "ctypes"
    ],
    "preview": "import os\nimport sys\nfrom typing import Optional, Tuple\n\n\ndef glibc_version_string() -> Optional[str]:\n    \"Returns glibc version string, or None if not using glibc.\"\n    return glibc_version_string_confstr() or glibc_version_string_ctypes()\n\n\ndef glibc_version_string_confstr() -> Optional[str]:\n    \"Primary implementation of glibc_version_string using os.confstr.\"\n    # os.confstr is quite a bit faster than ctypes.DLL. It's also less likely\n    # to be broken or missing. This strategy is used in the standard library\n    # platform module:\n    # https://github.com/python/cpython/blob/fcf1d003bf4f0100c9d0921ff3d70e1127ca1b71/Lib/platform.py#L175-L183\n    if sys.platform == \"win32\":\n        return None\n    try:\n        gnu_libc_version = os.confstr(\"CS_GNU_LIBC_VERSION\")",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1929",
    "name": "pydoc_20250506195920.py",
    "path": "github_repo/scripts/01_core_ai_analysis/ai_generation/pydoc_20250506195920.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 19,
    "size": 600,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "generate_docs"
    ],
    "classes": [],
    "imports": [
      "os",
      "subprocess"
    ],
    "preview": "import os\nimport subprocess\n\n\ndef generate_docs(directory):\n    # Walk through the directory and its subdirectories\n    for root, dirs, files in os.walk(directory):\n        for file in files:\n            if file.endswith(\".py\"):\n                file_path = os.path.join(root, file)\n                print(f\"Generating documentation for {file_path}\")\n                # Generate documentation using pydoc\n                subprocess.run([\"python3\", \"-m\", \"pydoc\", \"-w\", file_path])\n\n\n# Specify the directory to generate docs for\ndirectory = \"/Users/steven/Documents/python/docs\"\ngenerate_docs(directory)\n",
    "last_modified": "2025-05-06T19:59:20.357197"
  },
  {
    "id": "1930",
    "name": "generate_album_html-pages.py",
    "path": "github_repo/scripts/01_core_ai_analysis/ai_generation/generate_album_html-pages.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 132,
    "size": 3517,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "generate_album_html",
      "main"
    ],
    "classes": [],
    "imports": [
      "os"
    ],
    "preview": "import os\n\n# Define the base directory\nbase_dir = \"/Users/steven/Music/nocTurneMeLoDieS/mp4\"\noutput_file = \"/Users/steven/Music/nocTurneMeLoDieS/mp4/album-cover-html/index.html\"\n\n\n# Function to generate HTML for a single album\ndef generate_album_html(album_name):\n    album_path = os.path.join(base_dir, album_name)\n    cover_img = os.path.join(album_path, f\"{album_name}.png\")\n    audio_file = os.path.join(album_path, f\"{album_name}.mp3\")\n\n    # Use placeholder image if no cover image is found\n    cover_img_url = cover_img if os.path.exists(cover_img) else \"https://via.placeholder.com/150\"\n\n    # Generate HTML for the album\n    album_html = f\"\"\"\n        <div class=\"album\">\n            <img src=\"{cover_img_url}\" alt=\"{album_name} Cover\">",
    "last_modified": "2025-09-13T05:53:28.987258"
  },
  {
    "id": "1931",
    "name": "build_tracker.py",
    "path": "github_repo/scripts/01_core_ai_analysis/ai_generation/build_tracker.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 140,
    "size": 4832,
    "docstring": "",
    "keywords": [
      "organization"
    ],
    "functions": [
      "update_env_context_manager",
      "get_build_tracker",
      "__init__",
      "__enter__",
      "__exit__",
      "_entry_path",
      "add",
      "remove",
      "cleanup",
      "track"
    ],
    "classes": [
      "TrackerId",
      "BuildTracker"
    ],
    "imports": [
      "contextlib",
      "hashlib",
      "logging",
      "os",
      "types",
      "typing",
      "pip._internal.models.link",
      "pip._internal.req.req_install",
      "pip._internal.utils.temp_dir"
    ],
    "preview": "import contextlib\nimport hashlib\nimport logging\nimport os\nfrom types import TracebackType\nfrom typing import Dict, Generator, Optional, Set, Type, Union\n\nfrom pip._internal.models.link import Link\nfrom pip._internal.req.req_install import InstallRequirement\nfrom pip._internal.utils.temp_dir import TempDirectory\n\nlogger = logging.getLogger(__name__)\n\n\n@contextlib.contextmanager\ndef update_env_context_manager(**changes: str) -> Generator[None, None, None]:\n    target = os.environ\n\n    # Save values from the target and change them.\n    non_existent_marker = object()",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1932",
    "name": "filtercreator.py",
    "path": "github_repo/scripts/01_core_ai_analysis/ai_generation/filtercreator.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 163,
    "size": 5913,
    "docstring": "",
    "keywords": [],
    "functions": [
      "__init__",
      "__init__",
      "changeCategory",
      "updateDisplay",
      "attemptCreateFilter"
    ],
    "classes": [
      "Filter",
      "FilterCreationWindow"
    ],
    "imports": [
      "os",
      "database",
      "scriptwrapper",
      "PyQt5",
      "PyQt5.QtCore",
      "PyQt5.QtCore",
      "PyQt5.QtMultimedia",
      "PyQt5.QtWidgets"
    ],
    "preview": "import os\n\nimport database\nimport scriptwrapper\nfrom PyQt5 import QtCore, QtGui, QtWidgets, uic\nfrom PyQt5.QtCore import *\nfrom PyQt5.QtCore import QDir, QObject, QPoint, QRect, Qt, QUrl, pyqtSignal\nfrom PyQt5.QtMultimedia import (\n    QAbstractVideoBuffer,\n    QAbstractVideoSurface,\n    QMediaContent,\n    QMediaPlayer,\n    QMediaPlaylist,\n    QVideoFrame,\n    QVideoSurfaceFormat,\n)\nfrom PyQt5.QtWidgets import *\n\ncurrent_path = os.path.dirname(os.path.realpath(__file__))\n",
    "last_modified": "2025-09-13T05:53:32.100018"
  },
  {
    "id": "1933",
    "name": "img-prompt.py",
    "path": "github_repo/scripts/01_core_ai_analysis/ai_generation/img-prompt.py",
    "category": "01_core_ai_analysis",
    "type": "image_processing",
    "lines": 198,
    "size": 6318,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "get_creation_date",
      "get_image_metadata",
      "get_prompt",
      "generate_csv",
      "format_file_size",
      "write_csv",
      "get_unique_file_path",
      "save_last_directory",
      "load_last_directory"
    ],
    "classes": [],
    "imports": [
      "csv",
      "os",
      "re",
      "datetime",
      "PIL"
    ],
    "preview": "import csv\nimport os\nimport re\nfrom datetime import datetime\n\nfrom PIL import Image, UnidentifiedImageError\n\n# Constants\nLAST_DIRECTORY_FILE = \"image_data.txt\"\n\n\n# Function to get the creation date of a file\ndef get_creation_date(filepath):\n    try:\n        return datetime.fromtimestamp(os.path.getctime(filepath)).strftime(\"%m-%d-%y\")\n    except Exception as e:\n        print(f\"Error getting creation date for {filepath}: {e}\")\n        return \"Unknown\"\n\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1934",
    "name": "create_photo_grid.py",
    "path": "github_repo/scripts/01_core_ai_analysis/ai_generation/create_photo_grid.py",
    "category": "01_core_ai_analysis",
    "type": "image_processing",
    "lines": 65,
    "size": 2029,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "run_applescript",
      "create_photo_grid"
    ],
    "classes": [],
    "imports": [
      "math",
      "os",
      "osascript"
    ],
    "preview": "import math\nimport os\n\nimport osascript\n\n\ndef run_applescript(script, js_code):\n    \"\"\"Execute AppleScript from Python\"\"\"\n    full_script = script.format(js_code=js_code)\n    osascript.run(full_script)\n\n\ndef create_photo_grid(folder_path, grid_width, grid_height):\n    # Calculate the number of images and cell size\n    image_paths = [\n        os.path.join(folder_path, f)\n        for f in os.listdir(folder_path)\n        if f.endswith((\".png\", \".jpg\", \".jpeg\"))\n    ]\n    num_images = min(len(image_paths), grid_width * grid_height)",
    "last_modified": "2025-05-04T22:47:11.428050"
  },
  {
    "id": "1935",
    "name": "ClipCompilationCreator.py",
    "path": "github_repo/scripts/01_core_ai_analysis/ai_generation/ClipCompilationCreator.py",
    "category": "01_core_ai_analysis",
    "type": "utility",
    "lines": 146,
    "size": 5867,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "__init__",
      "create_compilation",
      "write_clips",
      "composite_clips",
      "generate_clip_text",
      "load_clips",
      "ffmpeg_compress_audio"
    ],
    "classes": [
      "ClipCompilationCreator"
    ],
    "imports": [
      "logging",
      "os",
      "moviepy.editor",
      "moviepy.tools",
      "src",
      "src.MetadataHandler",
      "config"
    ],
    "preview": "import logging\nimport os\n\nfrom moviepy.editor import (\n    CompositeVideoClip,\n    ImageClip,\n    TextClip,\n    VideoFileClip,\n    afx,\n    concatenate_videoclips,\n)\nfrom moviepy.tools import subprocess_call\nfrom src import utils\nfrom src.MetadataHandler import MetadataHandler\n\nimport config\n\n\nclass ClipCompilationCreator:\n    def __init__(self, game: str, asset_path: str, output_path: str):",
    "last_modified": "2025-09-13T05:53:45.443818"
  },
  {
    "id": "1936",
    "name": "run.py_02.py",
    "path": "github_repo/scripts/02_media_processing/run.py_consolidated/run.py_02.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 114,
    "size": 3200,
    "docstring": "",
    "keywords": [
      "web_tools"
    ],
    "functions": [
      "scrape"
    ],
    "classes": [],
    "imports": [
      "os",
      "time",
      "selenium",
      "selenium",
      "selenium.webdriver.common.action_chains",
      "selenium.webdriver.common.by",
      "selenium.webdriver.common.keys",
      "selenium.webdriver.support",
      "selenium.webdriver.support.ui",
      "webdriver_manager.chrome"
    ],
    "preview": "import os\nimport time\n\nimport selenium\nfrom selenium import webdriver\nfrom selenium.webdriver.common.action_chains import ActionChains\nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.common.keys import Keys\nfrom selenium.webdriver.support import expected_conditions as EC\nfrom selenium.webdriver.support.ui import WebDriverWait\nfrom webdriver_manager.chrome import ChromeDriverManager as CM\n\n# Complete these 2 fields ==================\nUSERNAME = \"your instagram username\"\nPASSWORD = \"your instagram password\"\n# ==========================================\n\nTIMEOUT = 15\n\n",
    "last_modified": "2025-09-13T05:53:38.691563"
  },
  {
    "id": "1937",
    "name": "run.py.py",
    "path": "github_repo/scripts/02_media_processing/run.py_consolidated/run.py.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 92,
    "size": 3313,
    "docstring": "",
    "keywords": [],
    "functions": [
      "main"
    ],
    "classes": [],
    "imports": [
      "argparse",
      "ImageCreator",
      "RedditScrape",
      "TextToSpeech",
      "VideoEdit"
    ],
    "preview": "# This file will be the main driver function and run the entire progam\n# This will import the Reddit Scraping Class and the Video Editing Class\n\nimport argparse  # Used to handle command line arguments\n\nfrom ImageCreator import ImageCreator  # Generates images of posts\nfrom RedditScrape import (\n    RedditScrape,\n)  # Importing reddit scraping class to acquire posts and authors\nfrom TextToSpeech import TextToSpeech  # Importing tts class to make mp3 of posts\nfrom VideoEdit import VideoEditor  # Edits all the tts mp3 and Images into a mp4 video\n\n\ndef main() -> int:\n\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"file\", help=\"file path of input file\")\n    args = parser.parse_args()\n\n    \"\"\" input_metadata holds the meta data from each entry in the input file",
    "last_modified": "2025-09-13T05:53:51.779488"
  },
  {
    "id": "1938",
    "name": "upload.py_02.py",
    "path": "github_repo/scripts/02_media_processing/upload.py_consolidated/upload.py_02.py",
    "category": "02_media_processing",
    "type": "youtube",
    "lines": 127,
    "size": 4493,
    "docstring": "",
    "keywords": [],
    "functions": [
      "initialize_upload",
      "resumable_upload",
      "print_progress"
    ],
    "classes": [],
    "imports": [
      "random",
      "sys",
      "time",
      "googleapiclient.discovery",
      "googleapiclient.errors",
      "googleapiclient.http",
      "constants",
      "presets"
    ],
    "preview": "import random\nimport sys\nimport time\n\nfrom googleapiclient.discovery import Resource\nfrom googleapiclient.errors import HttpError\nfrom googleapiclient.http import MediaFileUpload\n\nfrom .constants import MAX_RETRIES, RETRIABLE_EXCEPTIONS, RETRIABLE_STATUS_CODES\nfrom .presets import PresetOptions\n\n\ndef initialize_upload(youtube: Resource, options: PresetOptions):\n    body_status = {\"selfDeclaredMadeForKids\": False}\n    if options.publish_at == \"Now\":\n        body_status[\"privacyStatus\"] = \"public\"\n    else:\n        body_status[\"privacyStatus\"] = \"private\"\n        body_status[\"publishAt\"] = options.publish_at.isoformat()\n",
    "last_modified": "2025-09-13T05:53:47.358678"
  },
  {
    "id": "1939",
    "name": "upload.py.py",
    "path": "github_repo/scripts/02_media_processing/upload.py_consolidated/upload.py.py",
    "category": "02_media_processing",
    "type": "youtube",
    "lines": 105,
    "size": 2496,
    "docstring": "",
    "keywords": [
      "image_processing",
      "youtube",
      "web_tools"
    ],
    "functions": [
      "upload_image",
      "upscale_image",
      "get_upscaled_image",
      "main"
    ],
    "classes": [],
    "imports": [
      "json",
      "time",
      "requests"
    ],
    "preview": "import json\nimport time\n\nimport requests\n\n\ndef upload_image(api_key, image_file_path):\n headers = {\n \"accept\": \"application/json\",\n \"content-type\": \"application/json\",\n \"authorization\": f\"Bearer {api_key}\"\n }\n \n # Get a presigned URL for uploading an image\n url = \"https://cloud.leonardo.ai/api/rest/v1/init-image\"\n\n payload = {\"extension\": \"jpg\"}\n response = requests.post(url, json=payload, headers=headers)\n\n if response.status_code != 200:",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1940",
    "name": "exceptions.py.py",
    "path": "github_repo/scripts/02_media_processing/exceptions.py_consolidated/exceptions.py.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 68,
    "size": 1963,
    "docstring": "",
    "keywords": [],
    "functions": [
      "__init__",
      "__str__",
      "__init__",
      "__str__",
      "__init__",
      "__str__",
      "__init__",
      "__str__",
      "__init__",
      "__str__"
    ],
    "classes": [
      "SavifyError",
      "FFmpegNotInstalledError",
      "SpotifyApiCredentialsNotSetError",
      "UrlNotSupportedError",
      "YoutubeDlExtractionError",
      "InternetConnectionError"
    ],
    "imports": [],
    "preview": "class SavifyError(Exception):\n    def __init__(self, message=\"Savify ran into an error!\"):\n        self.message = message\n        super().__init__(self.message)\n\n    def __str__(self):\n        return self.message\n\n\nclass FFmpegNotInstalledError(SavifyError):\n    def __init__(\n        self,\n        message=\"FFmpeg must be installed to use Savify! [https://ffmpeg.org/download.html]\",\n    ):\n        self.message = message\n        super().__init__(self.message)\n\n    def __str__(self):\n        return self.message\n",
    "last_modified": "2025-05-04T23:27:53.632816"
  },
  {
    "id": "1941",
    "name": "exceptions.py_02.py",
    "path": "github_repo/scripts/02_media_processing/exceptions.py_consolidated/exceptions.py_02.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 15,
    "size": 362,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [
      "TwitchTubeError",
      "InvalidCategory",
      "VideoPathAlreadyExists",
      "NoClipsFound"
    ],
    "imports": [],
    "preview": "class TwitchTubeError(Exception):\n    \"\"\"General error class for TwitchTube.\"\"\"\n\n\nclass InvalidCategory(TwitchTubeError):\n    \"\"\"Error for when the specified category is invalid\"\"\"\n\n\nclass VideoPathAlreadyExists(TwitchTubeError):\n    \"\"\"Error for when a path already exists.\"\"\"\n\n\nclass NoClipsFound(TwitchTubeError):\n    \"\"\"Error for when no clips are found.\"\"\"\n",
    "last_modified": "2025-03-28T18:37:10.665090"
  },
  {
    "id": "1942",
    "name": "organize.py.py",
    "path": "github_repo/scripts/02_media_processing/organize.py_consolidated/organize.py.py",
    "category": "02_media_processing",
    "type": "organization",
    "lines": 35,
    "size": 1256,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "os",
      "shutil",
      "datetime"
    ],
    "preview": "import os\nimport shutil\nfrom datetime import datetime\n\n# Set the source directory where your images are stored\nsource_dir = \"/path/to/your/images\"\n\n# Loop through each file in the source directory\nfor filename in os.listdir(source_dir):\n    # Check for common image file extensions, case-insensitively\n    if filename.lower().endswith((\".png\", \".jpg\", \".jpeg\", \".gif\", \".bmp\")):\n        try:\n            # Get the full path to the file\n            file_path = os.path.join(source_dir, filename)\n\n            # Get the creation time and convert it to a year\n            creation_time = os.path.getctime(file_path)\n            year = datetime.fromtimestamp(creation_time).strftime(\"%Y\")\n\n            # Define the destination directory based on the year",
    "last_modified": "2025-03-28T18:37:05"
  },
  {
    "id": "1943",
    "name": "organize.py_02.py",
    "path": "github_repo/scripts/02_media_processing/organize.py_consolidated/organize.py_02.py",
    "category": "02_media_processing",
    "type": "organization",
    "lines": 29,
    "size": 1063,
    "docstring": "",
    "keywords": [],
    "functions": [
      "copy_files_with_structure"
    ],
    "classes": [],
    "imports": [
      "csv",
      "os",
      "shutil"
    ],
    "preview": "import csv\nimport os\nimport shutil\n\n\ndef copy_files_with_structure(csv_file_path, destination_base_path):\n    with open(csv_file_path, \"r\") as csv_file:\n        reader = csv.DictReader(csv_file)\n        for row in reader:\n            original_path = row[\"Original Path\"]\n            destination_path = os.path.join(destination_base_path, original_path.lstrip(os.sep))\n\n            # Create the destination directory if it doesn't exist\n            destination_dir = os.path.dirname(destination_path)\n            if not os.path.exists(destination_dir):\n                os.makedirs(destination_dir)\n\n            # Copy the file to the destination\n            shutil.copy2(original_path, destination_path)\n            print(f\"Copied {original_path} to {destination_path}\")",
    "last_modified": "2025-09-13T05:55:10.835843"
  },
  {
    "id": "1944",
    "name": "watch_user_likers_stories.py",
    "path": "github_repo/scripts/02_media_processing/format_conversion/watch_user_likers_stories.py",
    "category": "02_media_processing",
    "type": "organization",
    "lines": 84,
    "size": 2539,
    "docstring": "Watch user likers stories!\nThis script could be very useful to attract someone's\naudience to your account.\n\nIf you will not specify the user_id, the script will use\nyour likers as targets.\n\nDependencies:\n    pip install -U instabot\n\nNotes:\n    You can change file and add there your comments.",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "os",
      "random",
      "sys",
      "time",
      "instabot"
    ],
    "preview": "\"\"\"\nWatch user likers stories!\nThis script could be very useful to attract someone's\naudience to your account.\n\nIf you will not specify the user_id, the script will use\nyour likers as targets.\n\nDependencies:\n    pip install -U instabot\n\nNotes:\n    You can change file and add there your comments.\n\"\"\"\n\nimport os\nimport random\nimport sys\nimport time\n",
    "last_modified": "2025-09-13T05:54:55.835828"
  },
  {
    "id": "1945",
    "name": "bot_comment.py",
    "path": "github_repo/scripts/02_media_processing/format_conversion/bot_comment.py",
    "category": "02_media_processing",
    "type": "organization",
    "lines": 141,
    "size": 4898,
    "docstring": "Bot functions to generate and post a comments.\n\nInstructions to file with comments:\n    one line - one comment.\n\nExample:\n    lol\n    kek",
    "keywords": [],
    "functions": [
      "comment",
      "reply_to_comment",
      "comment_medias",
      "comment_hashtag",
      "comment_user",
      "comment_users",
      "comment_geotag",
      "is_commented"
    ],
    "classes": [],
    "imports": [
      "tqdm",
      "datetime"
    ],
    "preview": "\"\"\"\nBot functions to generate and post a comments.\n\nInstructions to file with comments:\n    one line - one comment.\n\nExample:\n    lol\n    kek\n\n\"\"\"\n\nfrom tqdm import tqdm\n\n\ndef comment(self, media_id, comment_text):\n    if self.is_commented(media_id):\n        return True\n    if not self.reached_limit(\"comments\"):\n        if self.blocked_actions[\"comments\"]:",
    "last_modified": "2025-09-13T05:54:57.203849"
  },
  {
    "id": "1946",
    "name": "envs.py",
    "path": "github_repo/scripts/02_media_processing/format_conversion/envs.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 26,
    "size": 689,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "json",
      "os",
      "dotenv"
    ],
    "preview": "import json\nimport os\n\nfrom dotenv import load_dotenv\n\n# Load .env variables\nload_dotenv(dotenv_path=os.path.expanduser(\"~/.env\"))\n\n# Retrieve API Key & Shop Data\nAPI_TOKEN = os.getenv(\"PRINTIFY_API_KEY\")\nSHOP_DATA_RAW = os.getenv(\"PRINTIFY_SHOPS\")\n\n# Ensure environment variable is loaded\nif SHOP_DATA_RAW is None:\n    print(\"\u274c ERROR: PRINTIFY_SHOPS not found in environment!\")\n    exit(1)\n\n# Convert JSON string to a dictionary\nSHOP_DATA = json.loads(SHOP_DATA_RAW)\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1947",
    "name": "subredditSentiment.py",
    "path": "github_repo/scripts/02_media_processing/format_conversion/subredditSentiment.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 50,
    "size": 1637,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "math",
      "praw",
      "textblob"
    ],
    "preview": "import math\n\nimport praw\nfrom textblob import TextBlob\n\nreddit = praw.Reddit(\n    client_id=\"uifJuvRl06uxTg\",\n    client_secret=\"Bf4eHDQhORvpJBw7syIvzat8gyA\",\n    user_agent=\"subSentiment\",\n)\n\n# opens file with subreddit names\nwith open(\"sb.txt\") as f:\n\n    for line in f:\n        subreddit = reddit.subreddit(line.strip())\n        # write web agent to get converter for datetime to epoch on a daily basis for updates\n        day_start = 1510635601\n        day_end = 1510721999\n",
    "last_modified": "2025-09-13T05:53:51.271538"
  },
  {
    "id": "1948",
    "name": "formatter.py",
    "path": "github_repo/scripts/02_media_processing/format_conversion/formatter.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 125,
    "size": 4158,
    "docstring": "pygments.formatter\n~~~~~~~~~~~~~~~~~~\n\nBase formatter class.\n\n:copyright: Copyright 2006-2023 by the Pygments team, see AUTHORS.\n:license: BSD, see LICENSE for details.",
    "keywords": [],
    "functions": [
      "_lookup_style",
      "__init__",
      "get_style_defs",
      "format"
    ],
    "classes": [
      "Formatter"
    ],
    "imports": [
      "codecs",
      "pip._vendor.pygments.styles",
      "pip._vendor.pygments.util"
    ],
    "preview": "\"\"\"\npygments.formatter\n~~~~~~~~~~~~~~~~~~\n\nBase formatter class.\n\n:copyright: Copyright 2006-2023 by the Pygments team, see AUTHORS.\n:license: BSD, see LICENSE for details.\n\"\"\"\n\nimport codecs\n\nfrom pip._vendor.pygments.styles import get_style_by_name\nfrom pip._vendor.pygments.util import get_bool_opt\n\n__all__ = [\"Formatter\"]\n\n\ndef _lookup_style(style):\n    if isinstance(style, str):",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1949",
    "name": "egg_link.py",
    "path": "github_repo/scripts/02_media_processing/format_conversion/egg_link.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 78,
    "size": 2450,
    "docstring": "",
    "keywords": [],
    "functions": [
      "_egg_link_names",
      "egg_link_path_from_sys_path",
      "egg_link_path_from_location"
    ],
    "classes": [],
    "imports": [
      "os",
      "re",
      "sys",
      "typing",
      "pip._internal.locations",
      "pip._internal.utils.virtualenv"
    ],
    "preview": "import os\nimport re\nimport sys\nfrom typing import List, Optional\n\nfrom pip._internal.locations import site_packages, user_site\nfrom pip._internal.utils.virtualenv import running_under_virtualenv, virtualenv_no_global\n\n__all__ = [\n    \"egg_link_path_from_sys_path\",\n    \"egg_link_path_from_location\",\n]\n\n\ndef _egg_link_names(raw_name: str) -> List[str]:\n    \"\"\"\n    Convert a Name metadata value to a .egg-link name, by applying\n    the same substitution as pkg_resources's safe_name function.\n    Note: we cannot use canonicalize_name because it has a different logic.\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1950",
    "name": "my_token.py",
    "path": "github_repo/scripts/02_media_processing/format_conversion/my_token.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 201,
    "size": 4840,
    "docstring": "pygments.token\n~~~~~~~~~~~~~~\n\nBasic token types and the standard tokens.\n\n:copyright: Copyright 2006-2023 by the Pygments team, see AUTHORS.\n:license: BSD, see LICENSE for details.",
    "keywords": [],
    "functions": [
      "is_token_subtype",
      "string_to_tokentype",
      "split",
      "__init__",
      "__contains__",
      "__getattr__",
      "__repr__",
      "__copy__",
      "__deepcopy__"
    ],
    "classes": [
      "_TokenType"
    ],
    "imports": [],
    "preview": "\"\"\"\npygments.token\n~~~~~~~~~~~~~~\n\nBasic token types and the standard tokens.\n\n:copyright: Copyright 2006-2023 by the Pygments team, see AUTHORS.\n:license: BSD, see LICENSE for details.\n\"\"\"\n\n\nclass _TokenType(tuple):\n    parent = None\n\n    def split(self):\n        buf = []\n        node = self\n        while node is not None:\n            buf.append(node)\n            node = node.parent",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1951",
    "name": "link-html.py",
    "path": "github_repo/scripts/02_media_processing/format_conversion/link-html.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 61,
    "size": 3065,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "pathlib",
      "markdown"
    ],
    "preview": "# file: generate_blog_from_markdown.py\n\nfrom pathlib import Path\n\nimport markdown\n\n# List of input markdown files\nmd_files = [\n    \"/Users/steven/Documents/DeepSeek/LinkedIn_SEO_and_Brand_Optimiz_2025-03-30_19_10_36.md\",\n    \"/Users/steven/Documents/DeepSeek/LinkedIn SEO and Brand Optimization Strategy_2025-03-30T18-35-10.md\",\n    \"/Users/steven/Documents/DeepSeek/LinkedIn_SEO_and_Brand_Optimiz_2025-03-30_18_38_16.md\",\n    \"/Users/steven/Documents/DeepSeek/LinkedIn SEO and Brand Optimization Strategy_2025-03-30T18-44-58.md\",\n    \"/Users/steven/Documents/DeepSeek/LinkedIn_SEO_and_Brand_Optimiz_2025-03-30_18_45_32.md\",\n    \"/Users/steven/Documents/DeepSeek/LinkedIn SEO and Brand Optimization Strategy_2025-03-30T18-46-35.md\",\n    \"/Users/steven/Documents/DeepSeek/LinkedIn SEO and Brand Optimization Strategy_2025-03-30T18-46-37.md\",\n    \"/Users/steven/Documents/DeepSeek/LinkedIn SEO and Brand Optimization Strategy_2025-03-30T18-47-49.md\",\n    \"/Users/steven/Documents/DeepSeek/LinkedIn SEO and Brand Optimization Strategy_2025-03-30T18-47-52.md\",\n    \"/Users/steven/Documents/DeepSeek/LinkedIn_SEO_and_Brand_Optimiz_2025-03-30_18_48_14.md\",\n    \"/Users/steven/Documents/DeepSeek/LinkedIn SEO and Brand Optimization Strategy_2025-03-30T18-48-31.md\",\n    \"/Users/steven/Documents/DeepSeek/LinkedIn_SEO_and_Brand_Optimiz_2025-03-30_18_52_16.md\",",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1952",
    "name": "data_warehouse.py",
    "path": "github_repo/scripts/02_media_processing/format_conversion/data_warehouse.py",
    "category": "02_media_processing",
    "type": "analysis",
    "lines": 106,
    "size": 3062,
    "docstring": "",
    "keywords": [],
    "functions": [
      "main",
      "__export",
      "__export_data_category",
      "__import",
      "__import_data_category"
    ],
    "classes": [],
    "imports": [
      "json",
      "pathlib",
      "click",
      "llm_engineering.domain.base.nosql",
      "llm_engineering.domain.documents",
      "loguru"
    ],
    "preview": "import json\nfrom pathlib import Path\n\nimport click\nfrom llm_engineering.domain.base.nosql import NoSQLBaseDocument\nfrom llm_engineering.domain.documents import (\n    ArticleDocument,\n    PostDocument,\n    RepositoryDocument,\n    UserDocument,\n)\nfrom loguru import logger\n\n\n@click.command()\n@click.option(\n    \"--export-raw-data\",\n    is_flag=True,\n    default=False,\n    help=\"Whether to export your data warehouse to a JSON file.\",",
    "last_modified": "2025-09-13T05:53:42.569473"
  },
  {
    "id": "1953",
    "name": "direct_url_helpers.py",
    "path": "github_repo/scripts/02_media_processing/format_conversion/direct_url_helpers.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 86,
    "size": 3184,
    "docstring": "",
    "keywords": [],
    "functions": [
      "direct_url_as_pep440_direct_reference",
      "direct_url_for_editable",
      "direct_url_from_link"
    ],
    "classes": [],
    "imports": [
      "typing",
      "pip._internal.models.direct_url",
      "pip._internal.models.link",
      "pip._internal.utils.urls",
      "pip._internal.vcs"
    ],
    "preview": "from typing import Optional\n\nfrom pip._internal.models.direct_url import ArchiveInfo, DirectUrl, DirInfo, VcsInfo\nfrom pip._internal.models.link import Link\nfrom pip._internal.utils.urls import path_to_url\nfrom pip._internal.vcs import vcs\n\n\ndef direct_url_as_pep440_direct_reference(direct_url: DirectUrl, name: str) -> str:\n    \"\"\"Convert a DirectUrl to a pip requirement string.\"\"\"\n    direct_url.validate()  # if invalid, this is a pip bug\n    requirement = name + \" @ \"\n    fragments = []\n    if isinstance(direct_url.info, VcsInfo):\n        requirement += \"{}+{}@{}\".format(\n            direct_url.info.vcs, direct_url.url, direct_url.info.commit_id\n        )\n    elif isinstance(direct_url.info, ArchiveInfo):\n        requirement += direct_url.url\n        if direct_url.info.hash:",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1954",
    "name": "bot_unlike.py",
    "path": "github_repo/scripts/02_media_processing/format_conversion/bot_unlike.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 61,
    "size": 1865,
    "docstring": "",
    "keywords": [],
    "functions": [
      "unlike",
      "unlike_comment",
      "unlike_media_comments",
      "unlike_medias",
      "unlike_user"
    ],
    "classes": [],
    "imports": [
      "tqdm"
    ],
    "preview": "from tqdm import tqdm\n\n\ndef unlike(self, media_id):\n    if not self.reached_limit(\"unlikes\"):\n        self.delay(\"unlike\")\n        if self.api.unlike(media_id):\n            self.total[\"unlikes\"] += 1\n            return True\n    else:\n        self.logger.info(\"Out of unlikes for today.\")\n    return False\n\n\ndef unlike_comment(self, comment_id):\n    if self.api.unlike_comment(comment_id):\n        return True\n    return False\n\n",
    "last_modified": "2025-09-13T05:54:58.099631"
  },
  {
    "id": "1955",
    "name": "rtf.py",
    "path": "github_repo/scripts/02_media_processing/format_conversion/rtf.py",
    "category": "02_media_processing",
    "type": "organization",
    "lines": 147,
    "size": 4932,
    "docstring": "pygments.formatters.rtf\n~~~~~~~~~~~~~~~~~~~~~~~\n\nA formatter that generates RTF files.\n\n:copyright: Copyright 2006-2023 by the Pygments team, see AUTHORS.\n:license: BSD, see LICENSE for details.",
    "keywords": [],
    "functions": [
      "__init__",
      "_escape",
      "_escape_text",
      "format_unencoded"
    ],
    "classes": [
      "RtfFormatter"
    ],
    "imports": [
      "pip._vendor.pygments.formatter",
      "pip._vendor.pygments.util"
    ],
    "preview": "\"\"\"\npygments.formatters.rtf\n~~~~~~~~~~~~~~~~~~~~~~~\n\nA formatter that generates RTF files.\n\n:copyright: Copyright 2006-2023 by the Pygments team, see AUTHORS.\n:license: BSD, see LICENSE for details.\n\"\"\"\n\nfrom pip._vendor.pygments.formatter import Formatter\nfrom pip._vendor.pygments.util import get_int_opt, surrogatepair\n\n__all__ = [\"RtfFormatter\"]\n\n\nclass RtfFormatter(Formatter):\n    \"\"\"\n    Format tokens as RTF markup. This formatter automatically outputs full RTF\n    documents with color information and other useful stuff. Perfect for Copy and",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1956",
    "name": "sources.py",
    "path": "github_repo/scripts/02_media_processing/format_conversion/sources.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 285,
    "size": 8687,
    "docstring": "",
    "keywords": [],
    "functions": [
      "_is_html_file",
      "build_source",
      "link",
      "page_candidates",
      "file_links",
      "__init__",
      "_scan_directory",
      "page_candidates",
      "project_name_to_urls",
      "__init__"
    ],
    "classes": [
      "LinkSource",
      "_FlatDirectoryToUrls",
      "_FlatDirectorySource",
      "_LocalFileSource",
      "_RemoteFileSource",
      "_IndexDirectorySource"
    ],
    "imports": [
      "logging",
      "mimetypes",
      "os",
      "collections",
      "typing",
      "pip._internal.models.candidate",
      "pip._internal.models.link",
      "pip._internal.utils.urls",
      "pip._internal.vcs",
      "pip._vendor.packaging.utils"
    ],
    "preview": "import logging\nimport mimetypes\nimport os\nfrom collections import defaultdict\nfrom typing import Callable, Dict, Iterable, List, Optional, Tuple\n\nfrom pip._internal.models.candidate import InstallationCandidate\nfrom pip._internal.models.link import Link\nfrom pip._internal.utils.urls import path_to_url, url_to_path\nfrom pip._internal.vcs import is_url\nfrom pip._vendor.packaging.utils import (\n    InvalidSdistFilename,\n    InvalidVersion,\n    InvalidWheelFilename,\n    canonicalize_name,\n    parse_sdist_filename,\n    parse_wheel_filename,\n)\n\nlogger = logging.getLogger(__name__)",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1957",
    "name": "json-to-csv.py",
    "path": "github_repo/scripts/02_media_processing/format_conversion/json-to-csv.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 62,
    "size": 2149,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "csv",
      "json",
      "os"
    ],
    "preview": "import csv\nimport json\nimport os\n\n# Prompt for the JSON input file path with a default value\ndefault_json_path = \"/Users/steven/Downloads/AI_Workflow_Automation_Summary.json\"\njson_file_path = (\n    input(f\"Enter JSON file path (default: {default_json_path}): \") or default_json_path\n)\n\n# Validate that the JSON file exists\nif not os.path.exists(json_file_path):\n    print(f\"Error: The file {json_file_path} does not exist.\")\n    exit(1)\n\n# Prompt for the CSV output file path with a default value\ndefault_csv_path = \"/Users/steven/Documents/output.csv\"\ncsv_file_path = (\n    input(f\"Enter CSV output file path (default: {default_csv_path}): \") or default_csv_path\n)",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1958",
    "name": "svg.py",
    "path": "github_repo/scripts/02_media_processing/format_conversion/svg.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 200,
    "size": 7337,
    "docstring": "pygments.formatters.svg\n~~~~~~~~~~~~~~~~~~~~~~~\n\nFormatter for SVG output.\n\n:copyright: Copyright 2006-2023 by the Pygments team, see AUTHORS.\n:license: BSD, see LICENSE for details.",
    "keywords": [],
    "functions": [
      "escape_html",
      "__init__",
      "format_unencoded",
      "_get_style"
    ],
    "classes": [
      "SvgFormatter"
    ],
    "imports": [
      "pip._vendor.pygments.formatter",
      "pip._vendor.pygments.token",
      "pip._vendor.pygments.util"
    ],
    "preview": "\"\"\"\npygments.formatters.svg\n~~~~~~~~~~~~~~~~~~~~~~~\n\nFormatter for SVG output.\n\n:copyright: Copyright 2006-2023 by the Pygments team, see AUTHORS.\n:license: BSD, see LICENSE for details.\n\"\"\"\n\nfrom pip._vendor.pygments.formatter import Formatter\nfrom pip._vendor.pygments.token import Comment\nfrom pip._vendor.pygments.util import get_bool_opt, get_int_opt\n\n__all__ = [\"SvgFormatter\"]\n\n\ndef escape_html(text):\n    \"\"\"Escape &, <, > as well as single and double quotes for HTML.\"\"\"\n    return (",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1959",
    "name": "packaging.py",
    "path": "github_repo/scripts/02_media_processing/format_conversion/packaging.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 56,
    "size": 2102,
    "docstring": "",
    "keywords": [],
    "functions": [
      "check_requires_python",
      "get_requirement",
      "safe_extra"
    ],
    "classes": [],
    "imports": [
      "functools",
      "logging",
      "re",
      "typing",
      "pip._vendor.packaging",
      "pip._vendor.packaging.requirements"
    ],
    "preview": "import functools\nimport logging\nimport re\nfrom typing import NewType, Optional, Tuple, cast\n\nfrom pip._vendor.packaging import specifiers, version\nfrom pip._vendor.packaging.requirements import Requirement\n\nNormalizedExtra = NewType(\"NormalizedExtra\", str)\n\nlogger = logging.getLogger(__name__)\n\n\ndef check_requires_python(requires_python: Optional[str], version_info: Tuple[int, ...]) -> bool:\n    \"\"\"\n    Check if the given Python version matches a \"Requires-Python\" specifier.\n\n    :param version_info: A 3-tuple of ints representing a Python\n        major-minor-micro version to check (e.g. `sys.version_info[:3]`).\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1960",
    "name": "sagemaker.py",
    "path": "github_repo/scripts/02_media_processing/format_conversion/sagemaker.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 76,
    "size": 2570,
    "docstring": "",
    "keywords": [],
    "functions": [
      "run_finetuning_on_sagemaker"
    ],
    "classes": [],
    "imports": [
      "pathlib",
      "huggingface_hub",
      "loguru",
      "llm_engineering.settings",
      "sagemaker.huggingface"
    ],
    "preview": "from pathlib import Path\n\nfrom huggingface_hub import HfApi\nfrom loguru import logger\n\ntry:\n    from sagemaker.huggingface import HuggingFace\nexcept ModuleNotFoundError:\n    logger.warning(\n        \"Couldn't load SageMaker imports. Run 'poetry install --with aws' to support AWS.\"\n    )\n\nfrom llm_engineering.settings import settings\n\nfinetuning_dir = Path(__file__).resolve().parent\nfinetuning_requirements_path = finetuning_dir / \"requirements.txt\"\n\n\ndef run_finetuning_on_sagemaker(\n    finetuning_type: str = \"sft\",",
    "last_modified": "2025-09-13T05:53:42.465710"
  },
  {
    "id": "1961",
    "name": "message_users.py",
    "path": "github_repo/scripts/02_media_processing/format_conversion/message_users.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 92,
    "size": 2813,
    "docstring": "instabot example\n\nWorkflow:\n1) Ask Message type\n2) Load messages CSV (if needed)\n3) Send message to each users",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "csv",
      "os",
      "sys",
      "time",
      "tqdm",
      "instabot"
    ],
    "preview": "\"\"\"\ninstabot example\n\nWorkflow:\n1) Ask Message type\n2) Load messages CSV (if needed)\n3) Send message to each users\n\"\"\"\n\nimport csv\nimport os\nimport sys\nimport time\n\nfrom tqdm import tqdm\n\nsys.path.append(os.path.join(sys.path[0], \"../\"))\nfrom instabot import Bot  # noqa: E402\n\ninstaUsers = [\"R1B4Z01D\", \"KoanMedia\"]",
    "last_modified": "2025-03-28T18:35:47"
  },
  {
    "id": "1962",
    "name": "search.py",
    "path": "github_repo/scripts/02_media_processing/format_conversion/search.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 173,
    "size": 5672,
    "docstring": "",
    "keywords": [],
    "functions": [
      "transform_hits",
      "print_dist_installation_info",
      "print_results",
      "highest_version",
      "add_options",
      "run",
      "search"
    ],
    "classes": [
      "SearchCommand",
      "TransformedHit"
    ],
    "imports": [
      "logging",
      "shutil",
      "sys",
      "textwrap",
      "xmlrpc.client",
      "collections",
      "optparse",
      "typing",
      "pip._internal.cli.base_command",
      "pip._internal.cli.req_command"
    ],
    "preview": "import logging\nimport shutil\nimport sys\nimport textwrap\nimport xmlrpc.client\nfrom collections import OrderedDict\nfrom optparse import Values\nfrom typing import TYPE_CHECKING, Dict, List, Optional\n\nfrom pip._internal.cli.base_command import Command\nfrom pip._internal.cli.req_command import SessionCommandMixin\nfrom pip._internal.cli.status_codes import NO_MATCHES_FOUND, SUCCESS\nfrom pip._internal.exceptions import CommandError\nfrom pip._internal.metadata import get_default_environment\nfrom pip._internal.models.index import PyPI\nfrom pip._internal.network.xmlrpc import PipXmlrpcTransport\nfrom pip._internal.utils.logging import indent_log\nfrom pip._internal.utils.misc import write_output\nfrom pip._vendor.packaging.version import parse as parse_version\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1963",
    "name": "filter.py",
    "path": "github_repo/scripts/02_media_processing/format_conversion/filter.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 78,
    "size": 1940,
    "docstring": "pygments.filter\n~~~~~~~~~~~~~~~\n\nModule that implements the default filter.\n\n:copyright: Copyright 2006-2023 by the Pygments team, see AUTHORS.\n:license: BSD, see LICENSE for details.",
    "keywords": [],
    "functions": [
      "apply_filters",
      "simplefilter",
      "_apply",
      "__init__",
      "filter",
      "__init__",
      "filter"
    ],
    "classes": [
      "Filter",
      "FunctionFilter"
    ],
    "imports": [],
    "preview": "\"\"\"\npygments.filter\n~~~~~~~~~~~~~~~\n\nModule that implements the default filter.\n\n:copyright: Copyright 2006-2023 by the Pygments team, see AUTHORS.\n:license: BSD, see LICENSE for details.\n\"\"\"\n\n\ndef apply_filters(stream, filters, lexer=None):\n    \"\"\"\n    Use this method to apply an iterable of filters to\n    a stream. If lexer is given it's forwarded to the\n    filter, otherwise the filter receives `None`.\n    \"\"\"\n\n    def _apply(filter_, stream):\n        yield from filter_.filter(lexer, stream)",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1964",
    "name": "bot_stats.py",
    "path": "github_repo/scripts/02_media_processing/format_conversion/bot_stats.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 50,
    "size": 1417,
    "docstring": "",
    "keywords": [],
    "functions": [
      "get_tsv_line",
      "get_header_line",
      "ensure_dir",
      "dump_data",
      "save_user_stats"
    ],
    "classes": [],
    "imports": [
      "datetime",
      "os"
    ],
    "preview": "import datetime\nimport os\n\n\ndef get_tsv_line(dictionary):\n    line = \"\"\n    for key in sorted(dictionary):\n        line += str(dictionary[key]) + \"\\t\"\n    return line[:-1] + \"\\n\"\n\n\ndef get_header_line(dictionary):\n    line = \"\\t\".join(sorted(dictionary))\n    return line + \"\\n\"\n\n\ndef ensure_dir(file_path):\n    directory = os.path.dirname(file_path)\n    if not os.path.exists(directory) and directory:\n        os.makedirs(directory)",
    "last_modified": "2025-03-28T18:35:47"
  },
  {
    "id": "1965",
    "name": "__init__.py_02.py",
    "path": "github_repo/scripts/02_media_processing/__init__.py_consolidated/__init__.py_02.py",
    "category": "02_media_processing",
    "type": "video_processing",
    "lines": 2,
    "size": 36,
    "docstring": "Unit test package for Savify.",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [],
    "preview": "\"\"\"Unit test package for Savify.\"\"\"\n",
    "last_modified": "2022-12-30T17:46:57"
  },
  {
    "id": "1966",
    "name": "__init__.py.py",
    "path": "github_repo/scripts/02_media_processing/__init__.py_consolidated/__init__.py.py",
    "category": "02_media_processing",
    "type": "analysis",
    "lines": 27,
    "size": 465,
    "docstring": "Savify\n\nDownload Spotify songs to mp3 with full metadata and cover art!\n\n:Copyright: 2020, Laurence Rawlings.\n:License: MIT (see /LICENSE).",
    "keywords": [],
    "functions": [
      "cli"
    ],
    "classes": [],
    "imports": [
      "savify",
      "types",
      "cli"
    ],
    "preview": "\"\"\"\nSavify\n\nDownload Spotify songs to mp3 with full metadata and cover art!\n\n:Copyright: 2020, Laurence Rawlings.\n:License: MIT (see /LICENSE).\n\"\"\"\n\nfrom .savify import *\nfrom .types import *\n\n__title__ = \"Savify\"\n__author__ = \"\"\"Laurence Rawlings\"\"\"\n__email__ = \"contact@laurencerawlings.com\"\n__version__ = \"2.3.4\"\n__license__ = \"MIT\"\n__docformat__ = \"restructuredtext en\"\n\n__all__ = [\"savify\", \"types\", \"utils\"]",
    "last_modified": "2025-05-04T23:27:53.526540"
  },
  {
    "id": "1967",
    "name": "copy.py_02.py",
    "path": "github_repo/scripts/02_media_processing/copy.py_consolidated/copy.py_02.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 39,
    "size": 1386,
    "docstring": "",
    "keywords": [],
    "functions": [
      "ensure_dir"
    ],
    "classes": [],
    "imports": [
      "csv",
      "os",
      "shutil"
    ],
    "preview": "import csv\nimport os\nimport shutil\n\n# Paths configuration\ncsv_file_path = \"/Users/steven/15days.csv\"\ndestination_root = \"/Volumes/iMac/15days\"\nbackup_root = os.path.join(destination_root, \"backup\")\n\n\ndef ensure_dir(directory):\n    \"\"\"Ensure the directory exists. If not, create it.\"\"\"\n    if not os.path.exists(directory):\n        os.makedirs(directory)\n\n\nwith open(csv_file_path, newline=\"\") as csvfile:\n    filereader = csv.reader(csvfile)\n    for row in filereader:\n        # Assuming each row has one column with the file path",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1968",
    "name": "copy.py.py",
    "path": "github_repo/scripts/02_media_processing/copy.py_consolidated/copy.py.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 91,
    "size": 3778,
    "docstring": "",
    "keywords": [],
    "functions": [
      "move_or_copy_files"
    ],
    "classes": [],
    "imports": [
      "csv",
      "logging",
      "os",
      "shutil"
    ],
    "preview": "import csv\nimport logging\nimport os\nimport shutil\n\n# Configure logging\nlogging.basicConfig(\n    filename=\"file_operations.log\",\n    level=logging.INFO,\n    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n)\n\n# Define the paths to your CSV files\ncsv_files = {\n    \"videos\": \"/Users/steven/Documents/Organize/vids-07-11-11_34.csv\",\n    \"audio\": \"/Users/steven/Documents/Organize/audio_files-07-11-11_34.csv\",\n    \"documents\": \"/Users/steven/Documents/Organize/docs-07-11-11_34.csv\",\n    \"images\": \"/Users/steven/Documents/Organize/images-07-11-11_34.csv\",\n    \"other\": \"/Users/steven/Documents/Organize/other-07-11-11_34.csv\",\n}",
    "last_modified": "2025-09-13T05:53:54.314007"
  },
  {
    "id": "1969",
    "name": "organize_albums 3.py_02.py",
    "path": "github_repo/scripts/02_media_processing/organize_albums 3.py_consolidated/organize_albums 3.py_02.py",
    "category": "02_media_processing",
    "type": "organization",
    "lines": 106,
    "size": 4065,
    "docstring": "",
    "keywords": [
      "organization"
    ],
    "functions": [
      "organize_files"
    ],
    "classes": [],
    "imports": [
      "os",
      "shutil"
    ],
    "preview": "import os\nimport shutil\n\n# Define the base directory\nbase_dir = \"/Users/steven/Music/nocTurneMeLoDieS/mp3\"\n\n\ndef organize_files():\n    # Check if the base directory exists\n    if not os.path.exists(base_dir):\n        print(f\"\u274c Error: The directory '{base_dir}' does not exist.\")\n        return\n\n    # List all files in the base directory\n    try:\n        files = os.listdir(base_dir)\n    except Exception as e:\n        print(f\"\u274c Error accessing directory '{base_dir}': {e}\")\n        return\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1970",
    "name": "organize_albums 3.py.py",
    "path": "github_repo/scripts/02_media_processing/organize_albums 3.py_consolidated/organize_albums 3.py.py",
    "category": "02_media_processing",
    "type": "organization",
    "lines": 62,
    "size": 2470,
    "docstring": "",
    "keywords": [
      "organization"
    ],
    "functions": [
      "organize_files"
    ],
    "classes": [],
    "imports": [
      "os",
      "shutil"
    ],
    "preview": "import os\nimport shutil\n\n# Define the base directory\nbase_dir = \"/Users/steven/Music/NocTurnE-meLoDieS/albums/\"\n\n\n# Helper function to create folders and move files\ndef organize_files():\n    # List all files in the base directory\n    files = os.listdir(base_dir)\n\n    # Process each file\n    for file in files:\n        # Skip directories\n        if os.path.isdir(os.path.join(base_dir, file)):\n            continue\n\n        # Extract the base name (album name) from the file\n        if file.endswith(\".mp3\"):",
    "last_modified": "2025-05-04T22:47:11"
  },
  {
    "id": "1971",
    "name": "imgconvert_colab.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/imgconvert_colab.py",
    "category": "02_media_processing",
    "type": "image_processing",
    "lines": 53,
    "size": 1729,
    "docstring": "",
    "keywords": [],
    "functions": [
      "convert_and_upscale_images",
      "main"
    ],
    "classes": [],
    "imports": [
      "os",
      "PIL"
    ],
    "preview": "import os\n\nfrom PIL import Image\n\n# Function to convert WebP images to PNG and upscale by 200% with 300 DPI\n\n\ndef convert_and_upscale_images(source_directory, destination_directory):\n    # Create the destination directory if it doesn't exist\n    os.makedirs(destination_directory, exist_ok=True)\n\n    for filename in os.listdir(source_directory):\n        if filename.endswith(\".png\"):\n            source_file = os.path.join(source_directory, filename)\n            filename_no_ext = os.path.splitext(filename)[0]\n            destination_file = os.path.join(destination_directory, f\"{filename_no_ext}.png\")\n\n            # Convert WebP to PNG and upscale by 200% with 300 DPI\n            im = Image.open(source_file)\n            width, height = im.size",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1972",
    "name": "vidgenUI.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/vidgenUI.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 128,
    "size": 4495,
    "docstring": "",
    "keywords": [
      "testing"
    ],
    "functions": [
      "__init__",
      "populateComboBox",
      "renderBackupFromName",
      "deleteBackupFromName",
      "closeEvent",
      "testServerFTP",
      "updateScriptScreen",
      "updateRenderProgress"
    ],
    "classes": [
      "renderingScreen"
    ],
    "imports": [
      "pickle",
      "shutil",
      "sys",
      "traceback",
      "distutils.dir_util",
      "server",
      "settings",
      "vidGen",
      "PyQt5",
      "PyQt5.QtCore"
    ],
    "preview": "import pickle\nimport shutil\nimport sys\nimport traceback\nfrom distutils.dir_util import copy_tree\n\nimport server\nimport settings\nimport vidGen\nfrom PyQt5 import QtCore, QtGui, QtWidgets, uic\nfrom PyQt5.QtCore import QDir, QObject, QPoint, QRect, Qt, QUrl, pyqtSignal\nfrom PyQt5.QtGui import QIcon\nfrom PyQt5.QtWidgets import *\n\n\nclass renderingScreen(QDialog):\n    script_queue_update = pyqtSignal()\n    render_progress = pyqtSignal()\n\n    update_backups = pyqtSignal()",
    "last_modified": "2025-09-13T05:53:32.772943"
  },
  {
    "id": "1973",
    "name": "scan_images_individual.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/scan_images_individual.py",
    "category": "02_media_processing",
    "type": "image_processing",
    "lines": 31,
    "size": 936,
    "docstring": "",
    "keywords": [],
    "functions": [
      "scan_directory",
      "main"
    ],
    "classes": [],
    "imports": [
      "os"
    ],
    "preview": "import os\n\n\ndef scan_directory(directory, file_types, min_size):\n    for root, dirs, files in os.walk(directory):\n        for file in files:\n            if file.lower().endswith(file_types):\n                file_path = os.path.join(root, file)\n                if os.path.getsize(file_path) >= min_size:\n                    yield file_path\n\n\ndef main():\n    file_types = (\".jpg\", \".jpeg\", \".png\", \".gif\", \".bmp\")\n    min_size = 1024 * 1024  # 1MB in bytes\n\n    drive = input(\"Enter the drive path to scan (e.g., /Volumes/4t): \")\n\n    # Automatically name the output file based on the drive\n    output_filename = f\"image_paths_{os.path.basename(drive)}.txt\"",
    "last_modified": "2025-05-04T22:47:11.913608"
  },
  {
    "id": "1974",
    "name": "imgmove.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/imgmove.py",
    "category": "02_media_processing",
    "type": "image_processing",
    "lines": 56,
    "size": 2189,
    "docstring": "",
    "keywords": [],
    "functions": [
      "is_excluded_path",
      "get_creation_date",
      "is_excluded_path"
    ],
    "classes": [],
    "imports": [
      "os",
      "shutil"
    ],
    "preview": "import os\nimport shutil\n\n\ndef is_excluded_path(path, excluded_paths):\n    return any(path.startswith(excluded_path) for excluded_path in excluded_paths)\n\ndef get_creation_date(file_path):\n    try:\n        creation_time = os.path.getctime(file_path)\n    except Exception:\n        creation_time = os.path.getmtime(file_path)\n    return datetime.fromtimestamp(creation_time).strftime('%Y_%m_%d')\n    def is_excluded_path(path, excluded_paths):\n    if any(path.startswith(excluded_path) for excluded_path in excluded_paths):\n        return True\n    for part in path.split(os.sep):\n        if part.startswith('.'):\n return True\n    return False",
    "last_modified": "2025-03-28T18:37:05"
  },
  {
    "id": "1975",
    "name": "youtube_dl_button 2.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/youtube_dl_button 2.py",
    "category": "02_media_processing",
    "type": "youtube",
    "lines": 386,
    "size": 15841,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "logging",
      "asyncio",
      "json",
      "math",
      "os",
      "shutil",
      "time",
      "datetime",
      "pyrogram",
      "translation"
    ],
    "preview": "import logging\n\nlogging.basicConfig(\n    level=logging.DEBUG, format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\n)\nlogger = logging.getLogger(__name__)\n\nimport asyncio\nimport json\nimport math\nimport os\nimport shutil\nimport time\nfrom datetime import datetime\n\n# the secret configuration specific things\nif bool(os.environ.get(\"WEBHOOK\", False)):\n    from sample_config import Config\nelse:\n    from config import Config",
    "last_modified": "2025-09-13T05:54:09.724031"
  },
  {
    "id": "1976",
    "name": "convert copy.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/convert copy.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 24,
    "size": 995,
    "docstring": "",
    "keywords": [],
    "functions": [
      "convert_to_direct_link"
    ],
    "classes": [],
    "imports": [],
    "preview": "# Automating the conversion of Google Drive shared links to direct image URLs\n\n# Extracted URLs from the provided file\ndrive_urls = [\n    \"https://drive.google.com/file/d/1w0YfYCEbBPZtQkzlf9X9pQHrzvbX46r2/view?usp=drivesdk\",\n    \"https://drive.google.com/file/d/1P0xd0htF4_90t_zvYjOWFnFsWr3NzZHw/view?usp=drivesdk\",\n    \"https://drive.google.com/file/d/1QKGv3oypeSSj4Q2AE928CsNfXoTh00fH/view?usp=drivesdk\",\n    \"https://drive.google.com/file/d/1YhjYSTgzrOBJGg6pWF24aNXqzyyuRIRh/view?usp=drivesdk\",\n    \"https://drive.google.com/file/d/1Rmtr7TMDtvuvXZ9Ir1FVM4mUPgk5us_p/view?usp=drivesdk\",\n    # Adding only a few for demonstration; the user can add more as needed\n]\n\n# Function to convert shared link to direct link\n\n\ndef convert_to_direct_link(shared_link):\n    file_id = shared_link.split(\"/d/\")[1].split(\"/\")[0]\n    return f\"https://drive.google.com/uc?export=view&id={file_id}\"\n\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1977",
    "name": "direct_send_photo.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/direct_send_photo.py",
    "category": "02_media_processing",
    "type": "image_processing",
    "lines": 24,
    "size": 564,
    "docstring": "instabot example\n\nSend photo to user",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "argparse",
      "os",
      "sys",
      "instabot"
    ],
    "preview": "\"\"\"\ninstabot example\n\nSend photo to user\n\"\"\"\n\nimport argparse\nimport os\nimport sys\n\nsys.path.append(os.path.join(sys.path[0], \"../\"))\nfrom instabot import Bot  # noqa: E402\n\nparser = argparse.ArgumentParser(add_help=True)\nparser.add_argument(\"-u\", type=str, help=\"username\")\nparser.add_argument(\"-p\", type=str, help=\"password\")\nparser.add_argument(\"user\", type=str, nargs=\"*\", help=\"user\")\nparser.add_argument(\"--filepath\", required=True)\nargs = parser.parse_args()\n",
    "last_modified": "2025-03-28T18:35:47"
  },
  {
    "id": "1978",
    "name": "up-down-old.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/up-down-old.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 124,
    "size": 5188,
    "docstring": "",
    "keywords": [],
    "functions": [
      "adjust_image_size",
      "convert_and_downscale_images_in_subfolders",
      "main"
    ],
    "classes": [],
    "imports": [
      "os",
      "PIL"
    ],
    "preview": "import os\n\nfrom PIL import Image, UnidentifiedImageError\n\n\ndef adjust_image_size(im, target_file_size, temp_file, target_dpi, upscale=False):\n    file_size = os.path.getsize(temp_file)\n\n    # Size limits: 4500x5400 max, 1024x1024 min\n    max_width, max_height = 4500, 5400\n    min_width, min_height = 1024, 1024\n\n    while (file_size > target_file_size) or (upscale and file_size < target_file_size):\n        # Downscale if image is too large\n        if file_size > target_file_size or im.size[0] > max_width or im.size[1] > max_height:\n            scale_factor = 0.9  # Downscale by 10%\n        # Upscale if image is too small\n        elif im.size[0] < min_width or im.size[1] < min_height:\n            scale_factor = 1.1  # Upscale by 10%\n        else:",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1979",
    "name": "2leomotion 2.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/2leomotion 2.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 90,
    "size": 3097,
    "docstring": "",
    "keywords": [
      "web_tools"
    ],
    "functions": [],
    "classes": [],
    "imports": [
      "time",
      "requests"
    ],
    "preview": "import time\n\nimport requests\n\napi_key = \"b5b99021-8e7a-42ef-8df9-4eca2c6efd3c\"\nauthorization = \"Bearer %s\" % api_key\n\nheaders = {\n    \"accept\": \"application/json\",\n    \"content-type\": \"application/json\",\n    \"authorization\": authorization,\n}\n\n# Generate an image\nurl = \"https://cloud.leonardo.ai/api/rest/v1/generations\"\n\npayload = {\n    \"height\": 960,\n    \"modelId\": \"ac614f96-1082-45bf-be9d-757f2d31c174\",\n    \"prompt\": \"A detailed photograph of a serious cyberpunk Hacker Cyborg transhumanist the past looking directly at the camera, standing straight, hands relaxed, square jaws, masculine face, dark scruff and no wrinkles, slightly buff looking, wearing a dark graphic t-shirt, detailed clothing texture realistic skin texture, black background, sharp focus, front view, waist up shot, high contrast, strong backlighting, action film dark color lut, cinematic luts\",",
    "last_modified": "2025-05-04T22:47:11"
  },
  {
    "id": "1980",
    "name": "imgmp4 copy.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/imgmp4 copy.py",
    "category": "02_media_processing",
    "type": "image_processing",
    "lines": 58,
    "size": 1968,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "get_cover_images",
      "convert_mp3_to_mp4_with_images",
      "process_directory"
    ],
    "classes": [],
    "imports": [
      "glob",
      "os",
      "moviepy.editor",
      "PIL",
      "sys"
    ],
    "preview": "import glob\nimport os\n\nfrom moviepy.editor import AudioFileClip, ImageSequenceClip\nfrom PIL import Image\n\n\ndef get_cover_images(file_name, cover_image_directory):\n    # Check for both JPG and PNG extensions\n    images = []\n    jpg_paths = glob.glob(os.path.join(cover_image_directory, f\"{file_name}*.jpg\"))\n    png_paths = glob.glob(os.path.join(cover_image_directory, f\"{file_name}*.png\"))\n\n    images.extend(jpg_paths)\n    images.extend(png_paths)\n\n    if images:\n        return images\n    else:\n        print(f\"Cover images not found for {file_name}. Please ensure the cover images exist.\")",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1981",
    "name": "over3.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/over3.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 53,
    "size": 1737,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "get_cover_image",
      "convert_mp3_to_mp4",
      "process_directory"
    ],
    "classes": [],
    "imports": [
      "glob",
      "os",
      "moviepy.editor",
      "PIL",
      "sys"
    ],
    "preview": "import glob\nimport os\n\nfrom moviepy.editor import AudioFileClip, ImageClip\nfrom PIL import Image\n\n\ndef get_cover_image(file_name, cover_image_directory):\n    # Check for both JPG and PNG extensions\n    jpg_path = os.path.join(cover_image_directory, f\"{file_name}.jpg\")\n    png_path = os.path.join(cover_image_directory, f\"{file_name}.png\")\n\n    if os.path.exists(jpg_path):\n        return jpg_path\n    elif os.path.exists(png_path):\n        return png_path\n    else:\n        print(f\"Cover image not found for {file_name}. Please ensure the cover image exists.\")\n        return None\n",
    "last_modified": "2025-09-13T05:53:55.368044"
  },
  {
    "id": "1982",
    "name": "test_bot_get.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/test_bot_get.py",
    "category": "02_media_processing",
    "type": "testing",
    "lines": 1097,
    "size": 37549,
    "docstring": "",
    "keywords": [
      "testing"
    ],
    "functions": [
      "test_get_media_owner",
      "test_get_media_info",
      "test_get_popular_medias",
      "test_get_timeline_medias",
      "test_get_timeline_users",
      "test_get_your_medias",
      "test_get_user_medias",
      "test_get_archived_medias",
      "test_search_users",
      "test_search_users_failed"
    ],
    "classes": [
      "TestBotGet"
    ],
    "imports": [
      "tempfile",
      "pytest",
      "responses",
      "instabot",
      "instabot.api.config",
      "test_bot",
      "test_variables",
      "unittest.mock",
      "mock"
    ],
    "preview": "import tempfile\n\nimport pytest\nimport responses\nfrom instabot import utils\nfrom instabot.api.config import API_URL, SIG_KEY_VERSION\n\nfrom .test_bot import TestBot\nfrom .test_variables import (\n    TEST_CAPTION_ITEM,\n    TEST_COMMENT_ITEM,\n    TEST_COMMENT_LIKER_ITEM,\n    TEST_FOLLOWER_ITEM,\n    TEST_FOLLOWING_ITEM,\n    TEST_INBOX_THREAD_ITEM,\n    TEST_LOCATION_ITEM,\n    TEST_MEDIA_LIKER,\n    TEST_MOST_RECENT_INVITER_ITEM,\n    TEST_PHOTO_ITEM,\n    TEST_SEARCH_USERNAME_ITEM,",
    "last_modified": "2025-09-13T05:54:59.155428"
  },
  {
    "id": "1983",
    "name": "organize_albums 16.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/organize_albums 16.py",
    "category": "02_media_processing",
    "type": "organization",
    "lines": 62,
    "size": 2470,
    "docstring": "",
    "keywords": [
      "organization"
    ],
    "functions": [
      "organize_files"
    ],
    "classes": [],
    "imports": [
      "os",
      "shutil"
    ],
    "preview": "import os\nimport shutil\n\n# Define the base directory\nbase_dir = \"/Users/steven/Music/NocTurnE-meLoDieS/albums/\"\n\n\n# Helper function to create folders and move files\ndef organize_files():\n    # List all files in the base directory\n    files = os.listdir(base_dir)\n\n    # Process each file\n    for file in files:\n        # Skip directories\n        if os.path.isdir(os.path.join(base_dir, file)):\n            continue\n\n        # Extract the base name (album name) from the file\n        if file.endswith(\".mp3\"):",
    "last_modified": "2025-08-02T18:25:58.493921"
  },
  {
    "id": "1984",
    "name": "config_20241213005656.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/config_20241213005656.py",
    "category": "02_media_processing",
    "type": "setup",
    "lines": 3,
    "size": 56,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [],
    "preview": "# config.py\nSOURCE_DIRECTORY = \"/Users/steven/Pictures\"\n",
    "last_modified": "2024-12-13T00:56:56.356783"
  },
  {
    "id": "1985",
    "name": "scan_images_individual--.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/scan_images_individual--.py",
    "category": "02_media_processing",
    "type": "image_processing",
    "lines": 31,
    "size": 936,
    "docstring": "",
    "keywords": [],
    "functions": [
      "scan_directory",
      "main"
    ],
    "classes": [],
    "imports": [
      "os"
    ],
    "preview": "import os\n\n\ndef scan_directory(directory, file_types, min_size):\n    for root, dirs, files in os.walk(directory):\n        for file in files:\n            if file.lower().endswith(file_types):\n                file_path = os.path.join(root, file)\n                if os.path.getsize(file_path) >= min_size:\n                    yield file_path\n\n\ndef main():\n    file_types = (\".jpg\", \".jpeg\", \".png\", \".gif\", \".bmp\")\n    min_size = 1024 * 1024  # 1MB in bytes\n\n    drive = input(\"Enter the drive path to scan (e.g., /Volumes/4t): \")\n\n    # Automatically name the output file based on the drive\n    output_filename = f\"image_paths_{os.path.basename(drive)}.txt\"",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1986",
    "name": "modeline.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/modeline.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 47,
    "size": 983,
    "docstring": "pygments.modeline\n~~~~~~~~~~~~~~~~~\n\nA simple modeline parser (based on pymodeline).\n\n:copyright: Copyright 2006-2023 by the Pygments team, see AUTHORS.\n:license: BSD, see LICENSE for details.",
    "keywords": [],
    "functions": [
      "get_filetype_from_line",
      "get_filetype_from_buffer"
    ],
    "classes": [],
    "imports": [
      "re"
    ],
    "preview": "\"\"\"\npygments.modeline\n~~~~~~~~~~~~~~~~~\n\nA simple modeline parser (based on pymodeline).\n\n:copyright: Copyright 2006-2023 by the Pygments team, see AUTHORS.\n:license: BSD, see LICENSE for details.\n\"\"\"\n\nimport re\n\n__all__ = [\"get_filetype_from_buffer\"]\n\n\nmodeline_re = re.compile(\n    r\"\"\"\n    (?: vi | vim | ex ) (?: [<=>]? \\d* )? :\n    .* (?: ft | filetype | syn | syntax ) = ( [^:\\s]+ )\n\"\"\",",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1987",
    "name": "leonardo_script copy 2.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/leonardo_script copy 2.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 129,
    "size": 4439,
    "docstring": "",
    "keywords": [
      "analysis",
      "youtube",
      "web_tools"
    ],
    "functions": [
      "convert_image_to_jpeg",
      "get_presigned_url",
      "upload_image",
      "upscale_image",
      "get_upscaled_image",
      "process_images"
    ],
    "classes": [],
    "imports": [
      "json",
      "os",
      "sys",
      "time",
      "requests",
      "PIL"
    ],
    "preview": "import json\nimport os\nimport sys\nimport time\n\nimport requests\nfrom PIL import Image\n\napi_key = \"de7c9cb8-022f-42f8-8bf7-a8f9caadfaee\"\nauthorization = f\"Bearer {api_key}\"\n\nheaders = {\n    \"accept\": \"application/json\",\n    \"content-type\": \"application/json\",\n    \"authorization\": authorization,\n}\n\n# Styles to apply\nstyles = [\"GENERAL\", \"CINEMATIC\", \"2D_ART_ILLUSTRATION\", \"PHOTOREALISTIC\"]\n",
    "last_modified": "2025-09-13T05:53:50.391821"
  },
  {
    "id": "1988",
    "name": "downs.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/downs.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 44,
    "size": 1314,
    "docstring": "",
    "keywords": [
      "youtube",
      "web_tools"
    ],
    "functions": [
      "download_and_convert_to_png"
    ],
    "classes": [],
    "imports": [
      "csv",
      "os",
      "io",
      "requests",
      "PIL"
    ],
    "preview": "import csv\nimport os\nfrom io import BytesIO\n\nimport requests\nfrom PIL import Image\n\n# Paths for input CSV and output directory\ninput_csv_path = \"/Users/steven/Downloads/Dalle-Aug2024 - Sheet1.csv\"\noutput_dir = (\n    \"/Users/steven/Downloads/output_images/\"  # You can change the output directory if needed\n)\n\n# Ensure the output directory exists\nos.makedirs(output_dir, exist_ok=True)\n\n# Read URLs from the CSV\nurls = []\nwith open(input_csv_path, newline=\"\") as csvfile:\n    reader = csv.DictReader(csvfile)",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1989",
    "name": "AskReddit_loop.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/AskReddit_loop.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 132,
    "size": 4743,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "random_title_msg",
      "write_to_log",
      "check_video_in_db",
      "create_submission_video"
    ],
    "classes": [],
    "imports": [
      "json",
      "time",
      "praw",
      "clips",
      "tinydb",
      "yt_upload"
    ],
    "preview": "#!./venv/bin/python\nimport json\nimport time\n\nimport praw\nfrom clips import *\nfrom tinydb import Query, TinyDB\nfrom yt_upload import upload_video\n\nwhile True:\n    try:\n        db = TinyDB(\"log/db.json\")\n        created_vids_db = db.table(\"created_videos\")\n        uploaded_vids_db = db.table(\"uploaded_vids\")\n\n        FPS = 30\n        # DURATION: int = 25\n        BACKGROUND_TRACK_VOLUME = 0.12\n        DURATION: int = 60 * 4\n        # DURATION: int = 60 * 10",
    "last_modified": "2025-09-13T05:53:51.492439"
  },
  {
    "id": "1990",
    "name": "ImageCreator.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/ImageCreator.py",
    "category": "02_media_processing",
    "type": "image_processing",
    "lines": 97,
    "size": 3353,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "create_image_for",
      "get_text_dimensions",
      "split_string"
    ],
    "classes": [
      "ImageCreator"
    ],
    "imports": [
      "os.path",
      "PIL"
    ],
    "preview": "# ImageCreator.py\n# Called after the Reddit Post content has been scraped\n#\n# Creates an image of the text for all posts, eg: title and replies\n#\n#\n\n\n# File holds some utility functions that may be called inside of the main\n\nimport os.path  # used to create image file path\n\nfrom PIL import Image, ImageDraw, ImageFont\n\n# Path to access images\nIMAGE_PATH = \"../images/\"\nFONT_PATH = \"../fonts/\"\n\n\nclass ImageCreator:",
    "last_modified": "2025-09-13T05:53:51.650509"
  },
  {
    "id": "1991",
    "name": "gallery_upload.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/gallery_upload.py",
    "category": "02_media_processing",
    "type": "youtube",
    "lines": 99,
    "size": 3026,
    "docstring": "",
    "keywords": [],
    "functions": [
      "parse_args",
      "main"
    ],
    "classes": [],
    "imports": [
      "argparse",
      "os",
      "sys",
      "simplegallery.common",
      "simplegallery.upload.uploader_factory"
    ],
    "preview": "import argparse\nimport os\nimport sys\n\nimport simplegallery.common as spg_common\nfrom simplegallery.upload.uploader_factory import get_uploader\n\n\ndef parse_args():\n    \"\"\"\n    Configures the argument parser\n    :return: Parsed arguments\n    \"\"\"\n\n    description = \"\"\"Uploads the gallery to a supported hosting provider. Currently supported: AWS S3 and Netlify.\n                    For detailed documentation please refer to https://github.com/haltakov/simple-photo-gallery.\"\"\"\n\n    parser = argparse.ArgumentParser(description=description)\n\n    parser.add_argument(\"hosting\", metavar=\"HOST\", default=\"\", help=\"Hosting provider: aws \")",
    "last_modified": "2025-09-13T05:53:52.460040"
  },
  {
    "id": "1992",
    "name": "converts copy.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/converts copy.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 62,
    "size": 2268,
    "docstring": "",
    "keywords": [],
    "functions": [
      "convert_and_upscale_images",
      "main"
    ],
    "classes": [],
    "imports": [
      "os",
      "PIL"
    ],
    "preview": "import os\n\nfrom PIL import Image\n\n\ndef convert_and_upscale_images(source_directory, destination_directory):\n    # Create the destination directory if it doesn't exist\n    os.makedirs(destination_directory, exist_ok=True)\n\n    for filename in os.listdir(source_directory):\n        if filename.endswith((\".tiff\", \".png\", \".jpg\", \".jpeg\")):\n            source_file = os.path.join(source_directory, filename)\n            filename_no_ext, file_ext = os.path.splitext(filename)\n            file_ext = file_ext.lower()\n\n            if file_ext == \".tiff\":\n                destination_file = os.path.join(destination_directory, f\"{filename_no_ext}.png\")\n            elif file_ext in [\".png\", \".jpg\", \".jpeg\"]:\n                destination_file = os.path.join(destination_directory, filename)\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1993",
    "name": "organize (1).py",
    "path": "github_repo/scripts/02_media_processing/image_tools/organize (1).py",
    "category": "02_media_processing",
    "type": "organization",
    "lines": 29,
    "size": 1052,
    "docstring": "",
    "keywords": [],
    "functions": [
      "copy_files_with_structure"
    ],
    "classes": [],
    "imports": [
      "csv",
      "os",
      "shutil"
    ],
    "preview": "import csv\nimport os\nimport shutil\n\n\ndef copy_files_with_structure(csv_file_path, destination_base_path):\n    with open(csv_file_path, \"r\") as csv_file:\n        reader = csv.DictReader(csv_file)\n        for row in reader:\n            original_path = row[\"Original Path\"]\n            destination_path = os.path.join(destination_base_path, original_path.lstrip(os.sep))\n\n            # Create the destination directory if it doesn't exist\n            destination_dir = os.path.dirname(destination_path)\n            if not os.path.exists(destination_dir):\n                os.makedirs(destination_dir)\n\n            # Copy the file to the destination\n            shutil.copy2(original_path, destination_path)\n            print(f\"Copied {original_path} to {destination_path}\")",
    "last_modified": "2025-09-13T05:55:10.806709"
  },
  {
    "id": "1994",
    "name": "console.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/console.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 127,
    "size": 4063,
    "docstring": "",
    "keywords": [],
    "functions": [
      "print_markdown",
      "print_step",
      "print_table",
      "print_substep",
      "handle_input"
    ],
    "classes": [],
    "imports": [
      "re",
      "rich.columns",
      "rich.console",
      "rich.markdown",
      "rich.padding",
      "rich.panel",
      "rich.text"
    ],
    "preview": "import re\n\nfrom rich.columns import Columns\nfrom rich.console import Console\nfrom rich.markdown import Markdown\nfrom rich.padding import Padding\nfrom rich.panel import Panel\nfrom rich.text import Text\n\nconsole = Console()\n\n\ndef print_markdown(text) -> None:\n    \"\"\"Prints a rich info message. Support Markdown syntax.\"\"\"\n\n    md = Padding(Markdown(text), 2)\n    console.print(md)\n\n\ndef print_step(text) -> None:",
    "last_modified": "2025-09-13T05:54:00.117665"
  },
  {
    "id": "1995",
    "name": "googlesets.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/googlesets.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 96,
    "size": 2569,
    "docstring": "",
    "keywords": [],
    "functions": [
      "__init__",
      "__str__",
      "html",
      "__init__",
      "get_results",
      "_maybe_raise",
      "_get_results_page",
      "_extract_results"
    ],
    "classes": [
      "GSError(Exception)",
      "GSParseError(Exception)",
      "GoogleSets(object)"
    ],
    "imports": [
      "random",
      "re",
      "urllib",
      "BeautifulSoup",
      "browser",
      "htmlentitydefs"
    ],
    "preview": "#!/usr/bin/python\n#\n# Peteris Krumins (peter@catonmat.net)\n# http://www.catonmat.net  --  good coders code, great reuse\n#\n# http://www.catonmat.net/blog/python-library-for-google-sets/\n#\n# Code is licensed under MIT license.\n#\n\nimport random\nimport re\nimport urllib\n\nfrom BeautifulSoup import BeautifulSoup\nfrom browser import Browser, BrowserError\nfrom htmlentitydefs import name2codepoint\n\n\nclass GSError(Exception):",
    "last_modified": "2025-05-04T23:28:20.724078"
  },
  {
    "id": "1996",
    "name": "upscale copy 2.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/upscale copy 2.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 85,
    "size": 2874,
    "docstring": "",
    "keywords": [],
    "functions": [
      "upscale_and_save_image",
      "compress_image_to_size"
    ],
    "classes": [],
    "imports": [
      "os",
      "PIL"
    ],
    "preview": "import os\n\nfrom PIL import Image\n\n# Input directory (no output directory needed since we're replacing the original images)\ninput_dir = \"/Users/steven/Pictures/etsy/Snowman_Action_Scenes\"\n\n# Max file size in bytes (9 MB = 9 * 1024 * 1024)\nmax_size = 9 * 1024 * 1024\n\n\ndef upscale_and_save_image(image_path, max_size=max_size, dpi=300):\n    \"\"\"\n    Upscale the image by 2x, set the DPI, and compress to ensure the file size is <= 9MB.\n    Args:\n        image_path (str): Path to the input image.\n        max_size (int): Maximum file size in bytes.\n        dpi (int): Target DPI (default is 300).\n    \"\"\"\n    # Open the image using PIL",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1997",
    "name": "webpConvert.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/webpConvert.py",
    "category": "02_media_processing",
    "type": "web_tools",
    "lines": 50,
    "size": 1736,
    "docstring": "",
    "keywords": [],
    "functions": [
      "convert_and_upscale_images",
      "main"
    ],
    "classes": [],
    "imports": [
      "os",
      "PIL"
    ],
    "preview": "import os\n\nfrom PIL import Image\n\n\n# Function to convert WebP images to PNG and upscale by 200% with 300 DPI\ndef convert_and_upscale_images(source_directory, destination_directory):\n    # Create the destination directory if it doesn't exist\n    os.makedirs(destination_directory, exist_ok=True)\n\n    for filename in os.listdir(source_directory):\n        if filename.endswith(\".webp\"):\n            source_file = os.path.join(source_directory, filename)\n            filename_no_ext = os.path.splitext(filename)[0]\n            destination_file = os.path.join(destination_directory, f\"{filename_no_ext}.jpg\")\n\n            # Convert WebP to PNG and upscale by 200% with 300 DPI\n            im = Image.open(source_file)\n            width, height = im.size\n            upscale_width = width * 2",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1998",
    "name": "midj-.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/midj-.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 28,
    "size": 650,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "json",
      "re"
    ],
    "preview": "import json\nimport re\n\n# Load the JSONL file.\nwith open(\n    \"/Users/steven/Pictures/midjourneyDownload_2023-10-13_1697181545353/metadata.jsonl\",\n    \"r\",\n) as f:\n    jsonl_data = json.load(f)\n\n# Compile the regular expression.\nregex = re.compile(r\"https:\\/\\/([\\w-]+.){1,}\\.(png)\")\n\n# Find all URLs in the JSONL file that match the regular expression.\nmatching_urls = []\nfor jsonl_object in jsonl_data:\n    url = jsonl_object.get(\"url\")\n    if regex.match(url):\n        matching_urls.append(url)\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "1999",
    "name": "repost_best_photos_from_users.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/repost_best_photos_from_users.py",
    "category": "02_media_processing",
    "type": "analysis",
    "lines": 114,
    "size": 3637,
    "docstring": "instabot example\nWorkflow:\nRepost best photos from users to your account\nBy default bot checks username_database.txt\nThe file should contain one username per line!",
    "keywords": [
      "organization"
    ],
    "functions": [
      "repost_best_photos",
      "sort_best_medias",
      "get_not_used_medias_from_users",
      "exists_in_posted_medias",
      "update_posted_medias",
      "repost_photo"
    ],
    "classes": [],
    "imports": [
      "argparse",
      "os",
      "random",
      "sys",
      "tqdm",
      "instabot"
    ],
    "preview": "\"\"\"\ninstabot example\nWorkflow:\nRepost best photos from users to your account\nBy default bot checks username_database.txt\nThe file should contain one username per line!\n\"\"\"\n\nimport argparse\nimport os\nimport random\nimport sys\n\nfrom tqdm import tqdm\n\nsys.path.append(os.path.join(sys.path[0], \"../\"))\nfrom instabot import Bot, utils  # noqa: E402\n\nUSERNAME_DATABASE = \"username_database.txt\"\nPOSTED_MEDIAS = \"posted_medias.txt\"",
    "last_modified": "2025-09-13T05:54:55.756914"
  },
  {
    "id": "2000",
    "name": "50.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/50.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 38,
    "size": 1418,
    "docstring": "",
    "keywords": [],
    "functions": [
      "split_csv",
      "write_chunk"
    ],
    "classes": [],
    "imports": [
      "csv"
    ],
    "preview": "import csv\n\n\ndef split_csv(input_file_path, output_file_prefix, chunk_size=50):\n    with open(input_file_path, \"r\", encoding=\"utf-8\") as input_file:\n        reader = csv.reader(input_file)\n        header = next(reader)  # Capture the header row\n\n        file_index = 0\n        current_chunk = []\n\n        for row in reader:\n            current_chunk.append(row)\n            if len(current_chunk) == chunk_size:\n                start_index = file_index * chunk_size + 1\n                end_index = start_index + chunk_size - 1\n                write_chunk(output_file_prefix, start_index, end_index, header, current_chunk)\n                file_index += 1\n                current_chunk = []\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2001",
    "name": "imove.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/imove.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 28,
    "size": 1079,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "csv",
      "os",
      "shutil"
    ],
    "preview": "import csv\nimport os\nimport shutil\n\n# Define the path to the CSV file\ncsv_file_path = \"/Users/steven/Sort/image_paths.csv\"\n# Define the destination directory based on your specification\ndestination_dir = \"/Volumes/Pics/ogPro\"\n\n# Ensure the destination directory exists\nos.makedirs(destination_dir, exist_ok=True)\n\n# Open the CSV file and read its contents\nwith open(csv_file_path, newline=\"\") as csvfile:\n    reader = csv.reader(csvfile)\n    next(reader, None)  # Skip the header row if your CSV has one\n    for row in reader:\n        # Assuming the file paths are in the first column\n        file_path = row[0].strip()  # Strip to remove any leading/trailing whitespace\n        # Define the destination path for the file",
    "last_modified": "2025-05-04T22:47:11.912806"
  },
  {
    "id": "2002",
    "name": "beatsaber.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/beatsaber.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 107,
    "size": 3495,
    "docstring": "",
    "keywords": [],
    "functions": [
      "taggify",
      "construct",
      "confirm",
      "confirm_thumbnail"
    ],
    "classes": [
      "BeatSaberPreset"
    ],
    "imports": [
      "re",
      "subprocess",
      "typing",
      "InquirerPy",
      "thumbnail",
      "utils",
      "preset"
    ],
    "preview": "import re\nimport subprocess\nfrom typing import override\n\nfrom InquirerPy import inquirer\n\nfrom ..thumbnail import create_thumbnails\nfrom ..utils import get_local_path\nfrom .preset import Preset\n\nDESCRIPTION_TEMPLATE = \"\"\"\\\n\ud83c\udfb5 __TITLE__\n\nDecided to post some VR content for fun. \n\nModding/Recording Guides:\nhttps://www.youtube.com/watch?v=DGRi8A0p1cY\nhttps://www.youtube.com/watch?v=bFK3XH_K9Jg\\\n\"\"\"\n",
    "last_modified": "2025-05-04T22:47:12"
  },
  {
    "id": "2003",
    "name": "rename_file.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/rename_file.py",
    "category": "02_media_processing",
    "type": "organization",
    "lines": 138,
    "size": 5177,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "logging",
      "os",
      "time",
      "pyrogram",
      "translation",
      "hachoir.metadata",
      "hachoir.parser",
      "helper_funcs.chat_base",
      "helper_funcs.display_progress",
      "PIL"
    ],
    "preview": "import logging\n\nlogging.basicConfig(\n    level=logging.DEBUG, format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\n)\nlogger = logging.getLogger(__name__)\n\nimport os\nimport time\n\n# the secret configuration specific things\nif bool(os.environ.get(\"WEBHOOK\", False)):\n    from sample_config import Config\nelse:\n    from config import Config\n\n# the Strings used for this \"thing\"\nimport pyrogram\nfrom translation import Translation\n",
    "last_modified": "2025-09-13T05:53:44.294462"
  },
  {
    "id": "2004",
    "name": "imgupscale 2.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/imgupscale 2.py",
    "category": "02_media_processing",
    "type": "image_processing",
    "lines": 69,
    "size": 2829,
    "docstring": "",
    "keywords": [],
    "functions": [
      "convert_and_upscale_images",
      "main"
    ],
    "classes": [],
    "imports": [
      "os",
      "PIL"
    ],
    "preview": "import os\n\nfrom PIL import Image, UnidentifiedImageError\n\n\n# Function to convert and upscale PNG and JPEG images by 200% with 300 DPI\ndef convert_and_upscale_images(source_directory, destination_directory, max_size_mb=8):\n    # Create the destination directory if it doesn't exist\n    os.makedirs(destination_directory, exist_ok=True)\n\n    for filename in os.listdir(source_directory):\n        if filename.lower().endswith((\".png\", \".jpeg\", \".jpg\")):\n            source_file = os.path.join(source_directory, filename)\n            filename_no_ext = os.path.splitext(filename)[0]\n            ext = filename.split(\".\")[-1].lower()\n            destination_file = os.path.join(destination_directory, f\"{filename_no_ext}.{ext}\")\n\n            try:\n                # Convert and upscale PNG or JPEG\n                with Image.open(source_file) as im:",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2005",
    "name": "organize_albums 12.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/organize_albums 12.py",
    "category": "02_media_processing",
    "type": "organization",
    "lines": 62,
    "size": 2470,
    "docstring": "",
    "keywords": [
      "organization"
    ],
    "functions": [
      "organize_files"
    ],
    "classes": [],
    "imports": [
      "os",
      "shutil"
    ],
    "preview": "import os\nimport shutil\n\n# Define the base directory\nbase_dir = \"/Users/steven/Music/NocTurnE-meLoDieS/albums/\"\n\n\n# Helper function to create folders and move files\ndef organize_files():\n    # List all files in the base directory\n    files = os.listdir(base_dir)\n\n    # Process each file\n    for file in files:\n        # Skip directories\n        if os.path.isdir(os.path.join(base_dir, file)):\n            continue\n\n        # Extract the base name (album name) from the file\n        if file.endswith(\".mp3\"):",
    "last_modified": "2025-08-02T18:25:58.473285"
  },
  {
    "id": "2006",
    "name": "scanner.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/scanner.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 103,
    "size": 3004,
    "docstring": "pygments.scanner\n~~~~~~~~~~~~~~~~\n\nThis library implements a regex based scanner. Some languages\nlike Pascal are easy to parse but have some keywords that\ndepend on the context. Because of this it's impossible to lex\nthat just by using a regular expression lexer like the\n`RegexLexer`.\n\nHave a look at the `DelphiLexer` to get an idea of how to use\nthis scanner.\n\n:copyright: Copyright 2006-2023 by the Pygments team, see AUTHORS.\n:license: BSD, see LICENSE for details.",
    "keywords": [
      "testing"
    ],
    "functions": [
      "__init__",
      "eos",
      "check",
      "test",
      "scan",
      "get_char",
      "__repr__"
    ],
    "classes": [
      "EndOfText",
      "Scanner"
    ],
    "imports": [
      "re"
    ],
    "preview": "\"\"\"\npygments.scanner\n~~~~~~~~~~~~~~~~\n\nThis library implements a regex based scanner. Some languages\nlike Pascal are easy to parse but have some keywords that\ndepend on the context. Because of this it's impossible to lex\nthat just by using a regular expression lexer like the\n`RegexLexer`.\n\nHave a look at the `DelphiLexer` to get an idea of how to use\nthis scanner.\n\n:copyright: Copyright 2006-2023 by the Pygments team, see AUTHORS.\n:license: BSD, see LICENSE for details.\n\"\"\"\n\nimport re\n\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2007",
    "name": "bot_filter.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/bot_filter.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 280,
    "size": 9465,
    "docstring": "Filter functions for media and user lists.",
    "keywords": [],
    "functions": [
      "filter_medias",
      "_filter_medias_not_liked",
      "_filter_medias_not_commented",
      "_filter_medias_nlikes",
      "_get_media_ids",
      "check_media",
      "search_stop_words_in_user",
      "search_blacklist_hashtags_in_media",
      "check_user",
      "check_not_bot"
    ],
    "classes": [],
    "imports": [],
    "preview": "\"\"\"\nFilter functions for media and user lists.\n\"\"\"\n\n\ndef filter_medias(self, media_items, filtration=True, quiet=False, is_comment=False):\n    if filtration:\n        if not quiet:\n            self.logger.info(\"Received {} medias.\".format(len(media_items)))\n        if not is_comment:\n            media_items = _filter_medias_not_liked(media_items)\n            if self.max_likes_to_like:\n                media_items = _filter_medias_nlikes(\n                    media_items, self.max_likes_to_like, self.min_likes_to_like\n                )\n        else:\n            media_items = _filter_medias_not_commented(self, media_items)\n        if not quiet:\n            msg = \"After filtration {} medias left.\"\n            self.logger.info(msg.format(len(media_items)))",
    "last_modified": "2025-09-13T05:54:57.472833"
  },
  {
    "id": "2008",
    "name": "imgg-test 1.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/imgg-test 1.py",
    "category": "02_media_processing",
    "type": "image_processing",
    "lines": 18,
    "size": 436,
    "docstring": "",
    "keywords": [
      "web_tools"
    ],
    "functions": [],
    "classes": [],
    "imports": [
      "os",
      "requests",
      "dotenv"
    ],
    "preview": "import os\n\nimport requests\nfrom dotenv import load_dotenv\n\n# Load .env file\nload_dotenv(\"~/.env\")\npat = os.getenv(\"CLARIFAI_PAT\")\n\nauth = (os.getenv(\"IMAGGA_API_KEY\"), os.getenv(\"IMAGGA_API_SECRET\"))\nurl = \"\"\n\nresponse = requests.get(f\"https://api.imagga.com/v2/tags?image_url={url}\", auth=auth)\n\nprint(\"\ud83d\udd16 Imagga Tags:\")\nfor tag in response.json()[\"result\"][\"tags\"][:10]:\n    print(f\"- {tag['tag']['en']} ({tag['confidence']:.2f})\")\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2009",
    "name": "organize_albums 9.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/organize_albums 9.py",
    "category": "02_media_processing",
    "type": "organization",
    "lines": 62,
    "size": 2466,
    "docstring": "",
    "keywords": [
      "organization"
    ],
    "functions": [
      "organize_files"
    ],
    "classes": [],
    "imports": [
      "os",
      "shutil"
    ],
    "preview": "import os\nimport shutil\n\n# Define the base directory\nbase_dir = \"/Users/steven/Music/NocTurnE-meLoDieS/mp3\"\n\n\n# Helper function to create folders and move files\ndef organize_files():\n    # List all files in the base directory\n    files = os.listdir(base_dir)\n\n    # Process each file\n    for file in files:\n        # Skip directories\n        if os.path.isdir(os.path.join(base_dir, file)):\n            continue\n\n        # Extract the base name (album name) from the file\n        if file.endswith(\".mp3\"):",
    "last_modified": "2025-05-31T00:15:07.991135"
  },
  {
    "id": "2010",
    "name": "society6-upload.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/society6-upload.py",
    "category": "02_media_processing",
    "type": "youtube",
    "lines": 39,
    "size": 1317,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "time",
      "pyautogui",
      "selenium"
    ],
    "preview": "import time\n\nimport pyautogui\nfrom selenium import webdriver\n\n# Initialize WebDriver\ndriver = webdriver.Chrome()\ndriver.get(\"https://www.society6.com/login\")\n\n# Log in to Society6\n# Note: Replace 'your_username' and 'your_password' with your actual login credentials\ndriver.find_element_by_id(\"sjchaplinski@gmail.com\").send_keys(\"sjchaplinski@gmail.com\")\ndriver.find_element_by_id(\"Zhil*0IPLma#\").send_keys(\"Zhil*0IPLma#\")\ndriver.find_element_by_id(\"login-button\").click()\ntime.sleep(5)  # Wait for the login process to complete\n\n# Navigate to the upload page\n# Note: Replace this URL with the actual URL of Society6's upload page\ndriver.get(\"https://www.society6.com/upload\")\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2011",
    "name": "imove--.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/imove--.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 28,
    "size": 1092,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "csv",
      "os",
      "shutil"
    ],
    "preview": "import csv\nimport os\nimport shutil\n\n# Define the path to the CSV file\ncsv_file_path = \"/Users/steven/Downloads/Misc/ogPro - Sheet1.csv\"\n# Define the destination directory based on your specification\ndestination_dir = \"/Volumes/iMac/ogImg\"\n\n# Ensure the destination directory exists\nos.makedirs(destination_dir, exist_ok=True)\n\n# Open the CSV file and read its contents\nwith open(csv_file_path, newline=\"\") as csvfile:\n    reader = csv.reader(csvfile)\n    next(reader, None)  # Skip the header row if your CSV has one\n    for row in reader:\n        # Assuming the file paths are in the first column\n        file_path = row[0].strip()  # Strip to remove any leading/trailing whitespace\n        # Define the destination path for the file",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2012",
    "name": "friends_last_post_likes_and_interact_with_user_based_on_hashtahs.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/friends_last_post_likes_and_interact_with_user_based_on_hashtahs.py",
    "category": "02_media_processing",
    "type": "analysis",
    "lines": 236,
    "size": 6550,
    "docstring": "Based in @jeremycjang and @boldestfortune\nThis config is meant to run with docker-compose inside a folder call z_{user}\n(Added to gitignore)\nFolder content:\n  - data.yaml\n  - docker-compose.yaml\n  - start.py (Containing this script)\n\nContent files examples (comments between parenthesis)\n\n::data.yaml::\nusername: user              # (instagram user)\npassword: password          # (instagram password)\nfriends_interaction: True   # (if True will like friendlist posts,\nFalse will avoid create friends session)\ndo_comments: True           # (if True will comment on user interaction)\ndo_follow: True             # (if True will follow on user interaction)\nuser_interact: True         # (if True will interact with user posts)\ndo_unfollow: True           # (if True will execution unfollow)\nfriendlist: ['friend1', 'friend2', 'friend3', 'friend4']\nhashtags: ['interest1', 'interest2', 'interest3', 'interest4']\n\n\n::docker-compose.yaml::\nversion: '3'\nservices:\n  web:\n    command: [\"./wait-for-selenium.sh\", \"http://selenium:4444/wd/hub\", \"--\",\n    \"python\", \"start.py\"]\n    environment:\n      - PYTHONUNBUFFERED=0\n    build:\n      context: ../\n      dockerfile: docker_conf/python/Dockerfile\n    depends_on:\n      - selenium\n    volumes:\n      - ./start.py:/code/start.py\n      - ./data.yaml:/code/data.yaml\n      - ./logs:/code/logs\n  selenium:\n    image: selenium/standalone-chrome\n    shm_size: 128M\n\n::HOW TO RUN::\nInside z_{user} directory:\n  run in background:\n    docker-compose down && docker-compose up -d --build\n  run with log in terminal:\n    docker-compose down && docker-compose up -d --build && docker-compose\n    logs -f",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "os",
      "random",
      "yaml",
      "instapy"
    ],
    "preview": "\"\"\"\nBased in @jeremycjang and @boldestfortune\nThis config is meant to run with docker-compose inside a folder call z_{user}\n(Added to gitignore)\nFolder content:\n  - data.yaml\n  - docker-compose.yaml\n  - start.py (Containing this script)\n\nContent files examples (comments between parenthesis)\n\n::data.yaml::\nusername: user              # (instagram user)\npassword: password          # (instagram password)\nfriends_interaction: True   # (if True will like friendlist posts,\nFalse will avoid create friends session)\ndo_comments: True           # (if True will comment on user interaction)\ndo_follow: True             # (if True will follow on user interaction)\nuser_interact: True         # (if True will interact with user posts)\ndo_unfollow: True           # (if True will execution unfollow)",
    "last_modified": "2025-03-28T18:36:56"
  },
  {
    "id": "2013",
    "name": "169-11.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/169-11.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 45,
    "size": 1302,
    "docstring": "",
    "keywords": [],
    "functions": [
      "upscale_and_replace_images",
      "main"
    ],
    "classes": [],
    "imports": [
      "os",
      "PIL"
    ],
    "preview": "import os\n\nfrom PIL import Image\n\n\ndef upscale_and_replace_images(directory):\n    for filename in os.listdir(directory):\n        if filename.endswith(\".png\"):\n            # Define the full path to the image\n            file_path = os.path.join(directory, filename)\n\n            # Load the image\n            with Image.open(file_path) as im:\n                width, height = im.size\n\n                # Upscale by 2x\n                upscale_width = width * 2\n                upscale_height = height * 2\n\n                # Resize the image",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2014",
    "name": "youtube_dl_button.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/youtube_dl_button.py",
    "category": "02_media_processing",
    "type": "youtube",
    "lines": 386,
    "size": 15841,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "logging",
      "asyncio",
      "json",
      "math",
      "os",
      "shutil",
      "time",
      "datetime",
      "pyrogram",
      "translation"
    ],
    "preview": "import logging\n\nlogging.basicConfig(\n    level=logging.DEBUG, format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\n)\nlogger = logging.getLogger(__name__)\n\nimport asyncio\nimport json\nimport math\nimport os\nimport shutil\nimport time\nfrom datetime import datetime\n\n# the secret configuration specific things\nif bool(os.environ.get(\"WEBHOOK\", False)):\n    from sample_config import Config\nelse:\n    from config import Config",
    "last_modified": "2025-09-13T05:53:44.585822"
  },
  {
    "id": "2015",
    "name": "leonardo_script 2.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/leonardo_script 2.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 129,
    "size": 4447,
    "docstring": "",
    "keywords": [
      "analysis",
      "youtube",
      "web_tools"
    ],
    "functions": [
      "convert_image_to_jpeg",
      "get_presigned_url",
      "upload_image",
      "upscale_image",
      "get_upscaled_image",
      "process_images"
    ],
    "classes": [],
    "imports": [
      "json",
      "os",
      "sys",
      "time",
      "requests",
      "PIL"
    ],
    "preview": "import json\nimport os\nimport sys\nimport time\n\nimport requests\nfrom PIL import Image\n\napi_key = \"de7c9cb8-022f-42f8-8bf7-a8f9caadfaee\"\nauthorization = f\"Bearer {api_key}\"\n\nheaders = {\n    \"accept\": \"application/json\",\n    \"content-type\": \"application/json\",\n    \"authorization\": authorization,\n}\n\n# Styles to apply\nstyles = [\"GENERAL\", \"CINEMATIC\", \"2D ART & ILLUSTRATION\", \"CG ART & GAME ASSETS\"]\n",
    "last_modified": "2025-09-13T05:53:50.331753"
  },
  {
    "id": "2016",
    "name": "midj--.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/midj--.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 28,
    "size": 650,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "json",
      "re"
    ],
    "preview": "import json\nimport re\n\n# Load the JSONL file.\nwith open(\n    \"/Users/steven/Pictures/midjourneyDownload_2023-10-13_1697181545353/metadata.jsonl\",\n    \"r\",\n) as f:\n    jsonl_data = json.load(f)\n\n# Compile the regular expression.\nregex = re.compile(r\"https:\\/\\/([\\w-]+.){1,}\\.(png)\")\n\n# Find all URLs in the JSONL file that match the regular expression.\nmatching_urls = []\nfor jsonl_object in jsonl_data:\n    url = jsonl_object.get(\"url\")\n    if regex.match(url):\n        matching_urls.append(url)\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2017",
    "name": "orn-up.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/orn-up.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 69,
    "size": 2253,
    "docstring": "",
    "keywords": [],
    "functions": [
      "upscale_and_compress_image",
      "compress_image_to_size"
    ],
    "classes": [],
    "imports": [
      "os",
      "PIL"
    ],
    "preview": "import os\n\nfrom PIL import Image\n\n# Input and output directories\ninput_dir = \"/Users/steven/Pictures/ornament/processed_output\"\noutput_dir = \"/Users/steven/Pictures/ornament/output\"\n\n# Max file size in bytes (9MB = 9 * 1024 * 1024)\nmax_size = 9 * 1024 * 1024\n\n# Target DPI\ntarget_dpi = 300\n\n# Create the output directory if it doesn't exist\nif not os.path.exists(output_dir):\n    os.makedirs(output_dir)\n\n\ndef upscale_and_compress_image(input_path, output_path, max_size, target_dpi):",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2018",
    "name": "convertupscale--.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/convertupscale--.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 50,
    "size": 1736,
    "docstring": "",
    "keywords": [],
    "functions": [
      "convert_and_upscale_images",
      "main"
    ],
    "classes": [],
    "imports": [
      "os",
      "PIL"
    ],
    "preview": "import os\n\nfrom PIL import Image\n\n\n# Function to convert WebP images to PNG and upscale by 200% with 300 DPI\ndef convert_and_upscale_images(source_directory, destination_directory):\n    # Create the destination directory if it doesn't exist\n    os.makedirs(destination_directory, exist_ok=True)\n\n    for filename in os.listdir(source_directory):\n        if filename.endswith(\".webp\"):\n            source_file = os.path.join(source_directory, filename)\n            filename_no_ext = os.path.splitext(filename)[0]\n            destination_file = os.path.join(destination_directory, f\"{filename_no_ext}.png\")\n\n            # Convert WebP to PNG and upscale by 200% with 300 DPI\n            im = Image.open(source_file)\n            width, height = im.size\n            upscale_width = width * 2",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2019",
    "name": "loop-upscale copy.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/loop-upscale copy.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 58,
    "size": 2241,
    "docstring": "",
    "keywords": [
      "web_tools"
    ],
    "functions": [],
    "classes": [],
    "imports": [
      "json",
      "os",
      "time",
      "requests"
    ],
    "preview": "import json\nimport os\nimport time\n\nimport requests\n\napi_key = \"3ca7a396-64c0-4e22-8cad-b00bc972d4b3\"\nauthorization = \"Bearer %s\" % api_key\n\nheaders = {\n    \"accept\": \"application/json\",\n    \"content-type\": \"application/json\",\n    \"authorization\": authorization,\n}\n\n# Directory containing images\ndirectory_path = \"/Users/steven/Pictures/CookiMonster/1\"\n\n# Loop through each file in the directory\nfor filename in os.listdir(directory_path):",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2020",
    "name": "scancsv.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/scancsv.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 36,
    "size": 930,
    "docstring": "",
    "keywords": [],
    "functions": [
      "scan_directory"
    ],
    "classes": [],
    "imports": [
      "csv",
      "os"
    ],
    "preview": "import csv\nimport os\n\n# List of drives to scan\ndrives = [\n    \"/Users/steven/Documents\",\n    \"/Users/steven/Music\",\n    \"/Users/steven/Movies/mine\",\n    \"/Users/steven/Pictures\",\n]\n\n# File types to look for\nfile_types = (\".jpg\", \".jpeg\", \".png\", \".gif\", \"webp\")\n\n\n# Function to scan a directory for image files\ndef scan_directory(directory):\n    for root, _, files in os.walk(directory):\n        for file in files:\n            if file.lower().endswith(file_types):",
    "last_modified": "2025-05-04T22:47:11.913987"
  },
  {
    "id": "2021",
    "name": "img-img-upscale copy.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/img-img-upscale copy.py",
    "category": "02_media_processing",
    "type": "image_processing",
    "lines": 36,
    "size": 1608,
    "docstring": "",
    "keywords": [],
    "functions": [
      "convert_and_upscale_images"
    ],
    "classes": [],
    "imports": [
      "os",
      "PIL"
    ],
    "preview": "import os\n\nfrom PIL import Image\n\n\ndef convert_and_upscale_images(directory):\n    # Walk through all directories and files within the specified directory\n    for root, _, files in os.walk(directory):\n        for filename in files:\n            if filename.lower().endswith(\".jpg\") or filename.lower().endswith(\".jpg\"):\n                # Construct full file path\n                file_path = os.path.join(root, filename)\n                # Open the .jpg image\n                with Image.open(file_path) as img:\n                    # Upscale the image by 2x\n                    img = img.resize((img.width * 2, img.height * 2), Image.LANCZOS)\n                    # Set DPI to 300\n                    img.info[\"dpi\"] = (300, 300)\n                    # Convert the image mode to RGB (if not already in that mode)\n                    if img.mode != \"RGB\":",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2022",
    "name": "encoding.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/encoding.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 37,
    "size": 1169,
    "docstring": "",
    "keywords": [],
    "functions": [
      "auto_decode"
    ],
    "classes": [],
    "imports": [
      "codecs",
      "locale",
      "re",
      "sys",
      "typing"
    ],
    "preview": "import codecs\nimport locale\nimport re\nimport sys\nfrom typing import List, Tuple\n\nBOMS: List[Tuple[bytes, str]] = [\n    (codecs.BOM_UTF8, \"utf-8\"),\n    (codecs.BOM_UTF16, \"utf-16\"),\n    (codecs.BOM_UTF16_BE, \"utf-16-be\"),\n    (codecs.BOM_UTF16_LE, \"utf-16-le\"),\n    (codecs.BOM_UTF32, \"utf-32\"),\n    (codecs.BOM_UTF32_BE, \"utf-32-be\"),\n    (codecs.BOM_UTF32_LE, \"utf-32-le\"),\n]\n\nENCODING_RE = re.compile(rb\"coding[:=]\\s*([-\\w.]+)\")\n\n\ndef auto_decode(data: bytes) -> str:",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2023",
    "name": "organize_albums 13.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/organize_albums 13.py",
    "category": "02_media_processing",
    "type": "organization",
    "lines": 62,
    "size": 2470,
    "docstring": "",
    "keywords": [
      "organization"
    ],
    "functions": [
      "organize_files"
    ],
    "classes": [],
    "imports": [
      "os",
      "shutil"
    ],
    "preview": "import os\nimport shutil\n\n# Define the base directory\nbase_dir = \"/Users/steven/Music/NocTurnE-meLoDieS/albums/\"\n\n\n# Helper function to create folders and move files\ndef organize_files():\n    # List all files in the base directory\n    files = os.listdir(base_dir)\n\n    # Process each file\n    for file in files:\n        # Skip directories\n        if os.path.isdir(os.path.join(base_dir, file)):\n            continue\n\n        # Extract the base name (album name) from the file\n        if file.endswith(\".mp3\"):",
    "last_modified": "2025-05-04T22:47:11"
  },
  {
    "id": "2024",
    "name": "convertupscale.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/convertupscale.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 50,
    "size": 1736,
    "docstring": "",
    "keywords": [],
    "functions": [
      "convert_and_upscale_images",
      "main"
    ],
    "classes": [],
    "imports": [
      "os",
      "PIL"
    ],
    "preview": "import os\n\nfrom PIL import Image\n\n\n# Function to convert WebP images to PNG and upscale by 200% with 300 DPI\ndef convert_and_upscale_images(source_directory, destination_directory):\n    # Create the destination directory if it doesn't exist\n    os.makedirs(destination_directory, exist_ok=True)\n\n    for filename in os.listdir(source_directory):\n        if filename.endswith(\".webp\"):\n            source_file = os.path.join(source_directory, filename)\n            filename_no_ext = os.path.splitext(filename)[0]\n            destination_file = os.path.join(destination_directory, f\"{filename_no_ext}.png\")\n\n            # Convert WebP to PNG and upscale by 200% with 300 DPI\n            im = Image.open(source_file)\n            width, height = im.size\n            upscale_width = width * 2",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2025",
    "name": "direct_url.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/direct_url.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 225,
    "size": 6736,
    "docstring": "PEP 610",
    "keywords": [],
    "functions": [
      "_get",
      "_get_required",
      "_exactly_one_of",
      "_filter_none",
      "__init__",
      "_from_dict",
      "_to_dict",
      "__init__",
      "hash",
      "hash"
    ],
    "classes": [
      "DirectUrlValidationError",
      "VcsInfo",
      "ArchiveInfo",
      "DirInfo",
      "DirectUrl"
    ],
    "imports": [
      "json",
      "re",
      "urllib.parse",
      "typing"
    ],
    "preview": "\"\"\"PEP 610\"\"\"\n\nimport json\nimport re\nimport urllib.parse\nfrom typing import Any, Dict, Iterable, Optional, Type, TypeVar, Union\n\n__all__ = [\n    \"DirectUrl\",\n    \"DirectUrlValidationError\",\n    \"DirInfo\",\n    \"ArchiveInfo\",\n    \"VcsInfo\",\n]\n\nT = TypeVar(\"T\")\n\nDIRECT_URL_METADATA_NAME = \"direct_url.json\"\nENV_VAR_RE = re.compile(r\"^\\$\\{[A-Za-z0-9-_]+\\}(:\\$\\{[A-Za-z0-9-_]+\\})?$\")\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2026",
    "name": "list.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/list.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 28,
    "size": 986,
    "docstring": "",
    "keywords": [],
    "functions": [
      "list_files_to_csv"
    ],
    "classes": [],
    "imports": [
      "csv",
      "os"
    ],
    "preview": "import csv\nimport os\n\n\ndef list_files_to_csv(startpath, output_file):\n    data = []\n    for root, dirs, files in os.walk(startpath):\n        if files:  # Ensure we write rows only for directories that contain files\n            row = [root]  # First column is the directory path\n            row.extend(os.path.join(root, f) for f in files)  # Append full paths of files\n            data.append(row)\n\n    # Find the maximum number of columns any row will have\n    max_cols = max(len(row) for row in data)\n\n    with open(output_file, \"w\", newline=\"\") as file:\n        writer = csv.writer(file)\n        writer.writerow(\n            [\"Folder\"] + [f\"IMG{i}\" for i in range(1, max_cols)]\n        )  # Create header row dynamically based on max_cols",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2027",
    "name": "readability.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/readability.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 34,
    "size": 718,
    "docstring": "",
    "keywords": [
      "web_tools"
    ],
    "functions": [
      "striphtml",
      "parse_url"
    ],
    "classes": [],
    "imports": [
      "re",
      "requests",
      "config"
    ],
    "preview": "import re\n\nimport requests\n\nfrom config import parse_config\n\nconfig = parse_config(\"local\")\n\nbase_url = \"https://www.readability.com/api/content/v1/parser\"\n\n\ndef striphtml(data):\n    p = re.compile(r\"<.*?>\")\n    return p.sub(\"\", data)\n\n\ndef parse_url(url):\n    url = base_url + \"?url=\" + url\n    params = {}\n    params[\"token\"] = config[\"readability_token\"]",
    "last_modified": "2025-09-13T05:53:51.210446"
  },
  {
    "id": "2028",
    "name": "organize_albums 8.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/organize_albums 8.py",
    "category": "02_media_processing",
    "type": "organization",
    "lines": 62,
    "size": 2466,
    "docstring": "",
    "keywords": [
      "organization"
    ],
    "functions": [
      "organize_files"
    ],
    "classes": [],
    "imports": [
      "os",
      "shutil"
    ],
    "preview": "import os\nimport shutil\n\n# Define the base directory\nbase_dir = \"/Users/steven/Music/NocTurnE-meLoDieS/mp3\"\n\n\n# Helper function to create folders and move files\ndef organize_files():\n    # List all files in the base directory\n    files = os.listdir(base_dir)\n\n    # Process each file\n    for file in files:\n        # Skip directories\n        if os.path.isdir(os.path.join(base_dir, file)):\n            continue\n\n        # Extract the base name (album name) from the file\n        if file.endswith(\".mp3\"):",
    "last_modified": "2025-05-04T22:47:13.021502"
  },
  {
    "id": "2029",
    "name": "get-pip.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/get-pip.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 33036,
    "size": 2635834,
    "docstring": "",
    "keywords": [],
    "functions": [
      "include_setuptools",
      "include_wheel",
      "determine_pip_install_arguments",
      "monkeypatch_for_cert",
      "bootstrap",
      "main",
      "cert_parse_args"
    ],
    "classes": [],
    "imports": [
      "argparse",
      "importlib",
      "os.path",
      "pkgutil",
      "shutil",
      "sys",
      "tempfile",
      "base64",
      "pip._internal.commands.install",
      "pip._internal.cli.main"
    ],
    "preview": "#!/usr/bin/env python\n#\n# Hi There!\n#\n# You may be wondering what this giant blob of binary data here is, you might\n# even be worried that we're up to something nefarious (good for you for being\n# paranoid!). This is a base85 encoding of a zip file, this zip file contains\n# an entire copy of pip (version 24.0).\n#\n# Pip is a thing that installs packages, pip itself is a package that someone\n# might want to install, especially if they're looking to run this get-pip.py\n# script. Pip has a lot of code to deal with the security of installing\n# packages, various edge cases on various platforms, and other such sort of\n# \"tribal knowledge\" that has been encoded in its code base. Because of this\n# we basically include an entire copy of pip inside this blob. We do this\n# because the alternatives are attempt to implement a \"minipip\" that probably\n# doesn't do things correctly and has weird edge cases, or compress pip itself\n# down into a single file.\n#\n# If you're wondering how this is created, it is generated using",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2030",
    "name": "convert-loop2.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/convert-loop2.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 163,
    "size": 5591,
    "docstring": "",
    "keywords": [
      "youtube",
      "web_tools"
    ],
    "functions": [
      "convert_image_to_jpeg",
      "get_presigned_url",
      "upload_image",
      "upscale_image",
      "get_upscaled_image"
    ],
    "classes": [],
    "imports": [
      "csv",
      "json",
      "os",
      "time",
      "datetime",
      "requests",
      "PIL"
    ],
    "preview": "import csv\nimport json\nimport os\nimport time\nfrom datetime import datetime\n\nimport requests\nfrom PIL import Image\n\napi_key = \"de7c9cb8-022f-42f8-8bf7-a8f9caadfaee\"\nauthorization = f\"Bearer {api_key}\"\n\nheaders = {\n    \"accept\": \"application/json\",\n    \"content-type\": \"application/json\",\n    \"authorization\": authorization,\n}\n\n# Directory containing images\ndirectory_path = \"/Users/steven/Pictures/TrashCaT/trashy-heartbreak\"",
    "last_modified": "2025-09-13T05:53:55.817963"
  },
  {
    "id": "2031",
    "name": "bot_get.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/bot_get.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 513,
    "size": 14400,
    "docstring": "All methods must return media_ids that can be\npassed into e.g. like() or comment() functions.",
    "keywords": [],
    "functions": [
      "get_user_stories",
      "get_self_story_viewers",
      "get_user_reel",
      "get_media_owner",
      "get_user_tags_medias",
      "get_popular_medias",
      "get_your_medias",
      "get_archived_medias",
      "get_timeline_medias",
      "get_user_medias"
    ],
    "classes": [],
    "imports": [
      "tqdm"
    ],
    "preview": "\"\"\"\nAll methods must return media_ids that can be\npassed into e.g. like() or comment() functions.\n\"\"\"\n\nfrom tqdm import tqdm\n\n# STORY\n\n\ndef get_user_stories(self, user_id):\n    self.api.get_user_stories(user_id)\n    try:\n        if int(self.api.last_json[\"reel\"][\"media_count\"]) > 0:\n            list_image = []\n            list_video = []\n            for item in self.api.last_json[\"reel\"][\"items\"]:\n                if int(item[\"media_type\"]) == 1:  # photo\n                    img = item[\"image_versions2\"][\"candidates\"][0][\"url\"]\n                    list_image.append(img)",
    "last_modified": "2025-09-13T05:54:57.909178"
  },
  {
    "id": "2032",
    "name": "style.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/style.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 198,
    "size": 6175,
    "docstring": "pygments.style\n~~~~~~~~~~~~~~\n\nBasic style object.\n\n:copyright: Copyright 2006-2023 by the Pygments team, see AUTHORS.\n:license: BSD, see LICENSE for details.",
    "keywords": [],
    "functions": [
      "__new__",
      "style_for_token",
      "list_styles",
      "styles_token",
      "__iter__",
      "__len__",
      "colorformat"
    ],
    "classes": [
      "StyleMeta",
      "Style"
    ],
    "imports": [
      "pip._vendor.pygments.token"
    ],
    "preview": "\"\"\"\npygments.style\n~~~~~~~~~~~~~~\n\nBasic style object.\n\n:copyright: Copyright 2006-2023 by the Pygments team, see AUTHORS.\n:license: BSD, see LICENSE for details.\n\"\"\"\n\nfrom pip._vendor.pygments.token import STANDARD_TYPES, Token\n\n# Default mapping of ansixxx to RGB colors.\n_ansimap = {\n    # dark\n    \"ansiblack\": \"000000\",\n    \"ansired\": \"7f0000\",\n    \"ansigreen\": \"007f00\",\n    \"ansiyellow\": \"7f7fe0\",\n    \"ansiblue\": \"00007f\",",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2033",
    "name": "upscale-.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/upscale-.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 33,
    "size": 1126,
    "docstring": "",
    "keywords": [],
    "functions": [
      "upscale_image"
    ],
    "classes": [],
    "imports": [
      "os",
      "PIL"
    ],
    "preview": "import os\n\nfrom PIL import Image\n\n\n# Function to upscale and set DPI of an image\ndef upscale_image(input_path, output_path, scale_factor=2, dpi=(300, 300)):\n    with Image.open(input_path) as img:\n        # Upscale the image\n        new_size = (img.width * scale_factor, img.height * scale_factor)\n        img_resized = img.resize(new_size, Image.ANTIALIAS)\n        # Set the DPI\n        img_resized.save(output_path, dpi=dpi)\n\n\n# Directory containing images to upscale\ninput_dir = input(\"Enter the path to the directory containing the images: \")\n# Directory to save the upscaled images\noutput_dir = input(\"Enter the path to the directory to save the upscaled images: \")\n",
    "last_modified": "2025-05-04T22:47:13.361779"
  },
  {
    "id": "2034",
    "name": "youtube_dl_button 3.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/youtube_dl_button 3.py",
    "category": "02_media_processing",
    "type": "youtube",
    "lines": 386,
    "size": 15841,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "logging",
      "asyncio",
      "json",
      "math",
      "os",
      "shutil",
      "time",
      "datetime",
      "pyrogram",
      "translation"
    ],
    "preview": "import logging\n\nlogging.basicConfig(\n    level=logging.DEBUG, format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\n)\nlogger = logging.getLogger(__name__)\n\nimport asyncio\nimport json\nimport math\nimport os\nimport shutil\nimport time\nfrom datetime import datetime\n\n# the secret configuration specific things\nif bool(os.environ.get(\"WEBHOOK\", False)):\n    from sample_config import Config\nelse:\n    from config import Config",
    "last_modified": "2025-09-13T05:54:09.895547"
  },
  {
    "id": "2035",
    "name": "UploadCompilation.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/UploadCompilation.py",
    "category": "02_media_processing",
    "type": "youtube",
    "lines": 47,
    "size": 1463,
    "docstring": "",
    "keywords": [
      "youtube"
    ],
    "functions": [
      "upload_compilation"
    ],
    "classes": [],
    "imports": [
      "datetime",
      "apikey",
      "google_auth_oauthlib.flow",
      "googleapiclient.discovery",
      "googleapiclient.http"
    ],
    "preview": "'''\n3. Upload video to YouTube\n'''\nimport datetime\n\nfrom apikey import apikey\nfrom google_auth_oauthlib.flow import InstalledAppFlow\nfrom googleapiclient.discovery import build\nfrom googleapiclient.http import MediaFileUpload\n\n\ndef upload_compilation(video_file, video_title, video_desc):\n    CLIENT_SECRET_FILE = 'directory to client_secret.json'\n    #https://www.googleapis.com/auth/youtube.upload\n    #https://www.googleapis.com/auth/youtube.force-ssl\n    SCOPES = ['https://www.googleapis.com/auth/youtube.upload']\n    flow = InstalledAppFlow.from_client_secrets_file(CLIENT_SECRET_FILE, SCOPES)\n    credentials = flow.run_console()\n    youtube = build('youtube', 'v3', credentials=credentials)\n",
    "last_modified": "2025-05-04T23:28:21"
  },
  {
    "id": "2036",
    "name": "like_hashtags.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/like_hashtags.py",
    "category": "02_media_processing",
    "type": "image_processing",
    "lines": 27,
    "size": 641,
    "docstring": "instabot example\n\nWorkflow:\n    Like last images with hashtag.",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "argparse",
      "os",
      "sys",
      "instabot"
    ],
    "preview": "\"\"\"\ninstabot example\n\nWorkflow:\n    Like last images with hashtag.\n\"\"\"\n\nimport argparse\nimport os\nimport sys\n\nsys.path.append(os.path.join(sys.path[0], \"../\"))\nfrom instabot import Bot  # noqa: E402\n\nparser = argparse.ArgumentParser(add_help=True)\nparser.add_argument(\"-u\", type=str, help=\"username\")\nparser.add_argument(\"-p\", type=str, help=\"password\")\nparser.add_argument(\"-proxy\", type=str, help=\"proxy\")\nparser.add_argument(\"hashtags\", type=str, nargs=\"+\", help=\"hashtags\")\nargs = parser.parse_args()",
    "last_modified": "2025-03-28T18:35:47"
  },
  {
    "id": "2037",
    "name": "csv-mydesgin.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/csv-mydesgin.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 31,
    "size": 956,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "csv",
      "os"
    ],
    "preview": "import csv\nimport os\n\n# Parent directory containing all the subdirectories\nparent_directory = \"/Users/steven/Pictures/V-Day CFab/tumnMuG/\"\n\n# The CSV file you want to create\ncsv_file = \"print_on_demand_data.csv\"\n\n# Define the fields for the CSV\nfields = [\"File Path\"]\n\n# List to store file details\nfile_details = []\n\n# Traverse through the parent directory and all its subdirectories\nfor subdir, dirs, files in os.walk(parent_directory):\n    for file in files:\n        if file.endswith((\"png\", \"svg\", \"dxf\", \"eps\")):\n            file_path = os.path.join(subdir, file)",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2038",
    "name": "leo-ups.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/leo-ups.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 178,
    "size": 5927,
    "docstring": "",
    "keywords": [
      "youtube",
      "web_tools"
    ],
    "functions": [
      "convert_image_to_jpeg",
      "get_presigned_url",
      "upload_image",
      "upscale_image",
      "get_upscaled_image",
      "initialize_csv",
      "log_to_csv"
    ],
    "classes": [],
    "imports": [
      "csv",
      "json",
      "os",
      "time",
      "datetime",
      "requests",
      "PIL"
    ],
    "preview": "import csv\nimport json\nimport os\nimport time\nfrom datetime import datetime\n\nimport requests\nfrom PIL import Image\n\napi_key = \"de7c9cb8-022f-42f8-8bf7-a8f9caadfaee\"\nauthorization = f\"Bearer {api_key}\"\n\nheaders = {\n    \"accept\": \"application/json\",\n    \"content-type\": \"application/json\",\n    \"authorization\": authorization,\n}\n\n# Directory containing images\ndirectory_path = \"/Users/steven/Pictures/TrashCaT/trashy-heartbreak\"",
    "last_modified": "2025-09-13T05:53:50.109890"
  },
  {
    "id": "2039",
    "name": "pics2pdfs-100album.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/pics2pdfs-100album.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 57,
    "size": 2179,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "collect_image_files",
      "create_pdf_volumes",
      "main"
    ],
    "classes": [],
    "imports": [
      "os",
      "PIL"
    ],
    "preview": "import os\n\nfrom PIL import Image, UnidentifiedImageError\n\n# Increase the maximum number of pixels allowed to prevent DecompressionBombWarning.\n# Note: Be cautious with this setting to avoid processing extremely large images that could exhaust system memory.\nImage.MAX_IMAGE_PIXELS = None  # This disables the limit. Alternatively, set it to a specific limit you're comfortable with.\n\n\ndef collect_image_files(source_directory):\n    \"\"\"Collects all PNG, JPG, JPEG files from the source directory and its subdirectories.\"\"\"\n    image_files = []\n    for root, dirs, files in os.walk(source_directory):\n        for file in files:\n            if file.lower().endswith((\".png\", \".jpg\", \".jpeg\")):\n                file_path = os.path.join(root, file)\n                image_files.append(file_path)\n    return image_files\n\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2040",
    "name": "leodown_20250102111003.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/leodown_20250102111003.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 51,
    "size": 1326,
    "docstring": "",
    "keywords": [
      "web_tools"
    ],
    "functions": [],
    "classes": [],
    "imports": [
      "json",
      "requests"
    ],
    "preview": "import json\n\nimport requests\n\n# Base URL for API\nbase_url = (\n    \"https://cloud.leonardo.ai/api/rest/v1/generations/user/f7bb8476-e3f0-4f1f-9a06-4600866cc49c\"\n)\nheaders = {\n    \"accept\": \"application/json\",\n    \"authorization\": \"Bearer f7bb8476-e3f0-4f1f-9a06-4600866cc49c\",  # Replace with your actual token\n}\n\n# Output file\noutput_file = \"/Users/steven/Pictures/leonardo_library.json\"\n\n# Pagination parameters\noffset = 0\nlimit = 10  # Maximum number of items per request\nall_generations = []",
    "last_modified": "2025-09-13T05:53:49.617002"
  },
  {
    "id": "2041",
    "name": "clips.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/clips.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 188,
    "size": 5843,
    "docstring": "",
    "keywords": [
      "youtube"
    ],
    "functions": [
      "get_data",
      "get_clip_data",
      "get_progress",
      "get_slug",
      "download_clip",
      "get_clips",
      "download_clips"
    ],
    "classes": [],
    "imports": [
      "datetime",
      "re",
      "urllib.request",
      "api",
      "logging",
      "utils"
    ],
    "preview": "import datetime\nimport re\nimport urllib.request\n\nfrom .api import get\nfrom .logging import Log as log\nfrom .utils import format_blacklist, is_blacklisted\n\n\ndef get_data(slug: str, oauth_token: str, client_id: str) -> dict:\n    \"\"\"\n    Gets the data from a given slug,\n    returns a JSON response from the Helix API endpoint\n    \"\"\"\n    response = get(\"data\", slug=slug, oauth_token=oauth_token, client_id=client_id)\n\n    try:\n        return response[\"data\"][0]\n    except KeyError as e:\n        log.error(f\"Ran into exception: {e}, {response}\")",
    "last_modified": "2025-09-13T05:53:56.187961"
  },
  {
    "id": "2042",
    "name": "organize_albums 17.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/organize_albums 17.py",
    "category": "02_media_processing",
    "type": "organization",
    "lines": 62,
    "size": 2470,
    "docstring": "",
    "keywords": [
      "organization"
    ],
    "functions": [
      "organize_files"
    ],
    "classes": [],
    "imports": [
      "os",
      "shutil"
    ],
    "preview": "import os\nimport shutil\n\n# Define the base directory\nbase_dir = \"/Users/steven/Music/NocTurnE-meLoDieS/albums/\"\n\n\n# Helper function to create folders and move files\ndef organize_files():\n    # List all files in the base directory\n    files = os.listdir(base_dir)\n\n    # Process each file\n    for file in files:\n        # Skip directories\n        if os.path.isdir(os.path.join(base_dir, file)):\n            continue\n\n        # Extract the base name (album name) from the file\n        if file.endswith(\".mp3\"):",
    "last_modified": "2025-05-05T01:38:21"
  },
  {
    "id": "2043",
    "name": "main_20221230233546.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/main_20221230233546.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 429,
    "size": 15514,
    "docstring": "",
    "keywords": [
      "transcription",
      "video_processing",
      "analysis",
      "youtube",
      "web_tools",
      "organization"
    ],
    "functions": [
      "downloadVideo",
      "scrapeVideos",
      "usedQuoteToDifferentFile",
      "getQuoteFromApi",
      "getQuoteFromTxtFile",
      "videoIntro",
      "createVideo",
      "audioClip",
      "randomBgMusic",
      "deleteTempFiles"
    ],
    "classes": [
      "bcolors"
    ],
    "imports": [
      "glob",
      "json",
      "os",
      "random",
      "subprocess",
      "gtts",
      "requests",
      "moviepy.editor",
      "moviepy.video.io.VideoFileClip",
      "mutagen.mp3"
    ],
    "preview": "import glob\nimport json\nimport os\nimport random\nimport subprocess\n\nimport gtts\nimport requests\nfrom moviepy.editor import *\nfrom moviepy.video.io.VideoFileClip import VideoFileClip\nfrom mutagen.mp3 import MP3\nfrom tqdm.auto import tqdm\n\n\n# download background video from pexels - https://www.pexels.com/api/documentation/#videos-search__parameters\ndef downloadVideo(id) -> str:\n    \"\"\"Downloads video from Pexels with the according video ID\"\"\"\n    url = \"https://www.pexels.com/video/\" + str(id) + \"/download.mp4\"\n    # Streaming, so we can iterate over the response.\n    response = requests.get(url, stream=True)",
    "last_modified": "2025-09-13T05:53:56.937195"
  },
  {
    "id": "2044",
    "name": "screenshot_downloader.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/screenshot_downloader.py",
    "category": "02_media_processing",
    "type": "youtube",
    "lines": 54,
    "size": 1965,
    "docstring": "",
    "keywords": [
      "youtube"
    ],
    "functions": [
      "download_screenshots_of_reddit_posts"
    ],
    "classes": [],
    "imports": [
      "pathlib",
      "playwright.sync_api",
      "rich.progress",
      "utils.console"
    ],
    "preview": "from pathlib import Path\n\nfrom playwright.sync_api import sync_playwright\nfrom rich.progress import track\n\nfrom utils.console import print_step, print_substep\n\n\ndef download_screenshots_of_reddit_posts(reddit_object, screenshot_num):\n    \"\"\"Downloads screenshots of reddit posts as they are seen on the web.\n\n    Args:\n        reddit_object: The Reddit Object you received in askreddit.py\n        screenshot_num: The number of screenshots you want to download.\n    \"\"\"\n    print_step(\"Downloading Screenshots of Reddit Posts \ud83d\udcf7\")\n\n    # ! Make sure the reddit screenshots folder exists\n    Path(\"assets/png\").mkdir(parents=True, exist_ok=True)\n",
    "last_modified": "2025-09-13T05:53:59.445634"
  },
  {
    "id": "2045",
    "name": "render.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/render.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 46,
    "size": 1411,
    "docstring": "",
    "keywords": [],
    "functions": [
      "blur",
      "render"
    ],
    "classes": [],
    "imports": [
      "moviepy.editor",
      "skimage.filters",
      "config"
    ],
    "preview": "from moviepy.editor import *\nfrom skimage.filters import gaussian\n\nimport config\n\n\ndef blur(image):\n    \"\"\"Returns a blurred (radius=2 pixels) version of the image\"\"\"\n    return gaussian(image.astype(float), sigma=25)\n\n\ndef render(\n    directory, clip_name, output_name, resolution\n):  # dir and names are strings, resolution is a tuple\n\n    blur = config.video[\"blur\"]\n\n    clip_dir = directory + clip_name\n    main_clip = VideoFileClip(clip_dir)\n",
    "last_modified": "2025-09-13T05:54:12.973310"
  },
  {
    "id": "2046",
    "name": "repost_photo.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/repost_photo.py",
    "category": "02_media_processing",
    "type": "image_processing",
    "lines": 53,
    "size": 1382,
    "docstring": "instabot example\n\nWorkflow:\n1) Repost photo to your account",
    "keywords": [],
    "functions": [
      "exists_in_posted_medias",
      "update_posted_medias",
      "repost_photo"
    ],
    "classes": [],
    "imports": [
      "os",
      "sys",
      "instabot",
      "instabot.bot.bot_support"
    ],
    "preview": "\"\"\"\ninstabot example\n\nWorkflow:\n1) Repost photo to your account\n\"\"\"\n\nimport os\nimport sys\n\nsys.path.append(os.path.join(sys.path[0], \"../\"))\nfrom instabot import Bot  # noqa: E402\nfrom instabot.bot.bot_support import read_list_from_file  # noqa: E402\n\n\ndef exists_in_posted_medias(new_media_id, path=\"config/posted_medias.txt\"):\n    medias = read_list_from_file(path)\n    return new_media_id in medias\n\n",
    "last_modified": "2025-03-28T18:35:47"
  },
  {
    "id": "2047",
    "name": "track.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/track.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 96,
    "size": 3405,
    "docstring": "",
    "keywords": [],
    "functions": [
      "_spotify_artist_names",
      "try_with_key_error",
      "__init__",
      "__repr__",
      "__str__"
    ],
    "classes": [
      "Track"
    ],
    "imports": [
      "typing",
      "uuid",
      "types"
    ],
    "preview": "from typing import Callable\nfrom uuid import uuid1\n\nfrom .types import Platform, Type\n\n\nclass Track:\n    def try_with_key_error(self, name: str, getter: Callable, default: str = \"\") -> None:\n        \"\"\"Wraps a try-except-assign statement.\"\"\"\n        try:\n            setattr(self, name, getter())\n        except KeyError:\n            setattr(self, name, default)\n\n    def __init__(self, spotify_data, track_type=Type.TRACK) -> None:\n        self.album_track_count = None\n        self.track_number = None\n        self.release_date = None\n        self.disc_number = None\n        self.playlist = None",
    "last_modified": "2025-09-13T05:55:15.904648"
  },
  {
    "id": "2048",
    "name": "convert-loop.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/convert-loop.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 132,
    "size": 4531,
    "docstring": "",
    "keywords": [
      "youtube",
      "web_tools"
    ],
    "functions": [
      "convert_image_to_jpeg",
      "get_presigned_url",
      "upload_image",
      "upscale_image",
      "get_upscaled_image"
    ],
    "classes": [],
    "imports": [
      "json",
      "os",
      "time",
      "requests",
      "PIL"
    ],
    "preview": "import json\nimport os\nimport time\n\nimport requests\nfrom PIL import Image\n\napi_key = \"de7c9cb8-022f-42f8-8bf7-a8f9caadfaee\"\nauthorization = f\"Bearer {api_key}\"\n\nheaders = {\n    \"accept\": \"application/json\",\n    \"content-type\": \"application/json\",\n    \"authorization\": authorization,\n}\n\n# Directory containing images\ndirectory_path = \"/Users/steven/Music/trashCaTs/in-this-alley-where-i-hide\"\n\n# Styles to apply",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2049",
    "name": "download_your_photos.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/download_your_photos.py",
    "category": "02_media_processing",
    "type": "youtube",
    "lines": 19,
    "size": 275,
    "docstring": "instabot example\n\nWorkflow:\n1) Downloads your medias",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "os",
      "sys",
      "instabot"
    ],
    "preview": "\"\"\"\ninstabot example\n\nWorkflow:\n1) Downloads your medias\n\n\"\"\"\n\nimport os\nimport sys\n\nsys.path.append(os.path.join(sys.path[0], \"../\"))\nfrom instabot import Bot  # noqa: E402\n\nbot = Bot()\nbot.login()\nmedias = bot.get_total_user_medias(bot.user_id)\nbot.download_photos(medias)\n",
    "last_modified": "2025-03-28T18:35:47"
  },
  {
    "id": "2050",
    "name": "motion-upload.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/motion-upload.py",
    "category": "02_media_processing",
    "type": "youtube",
    "lines": 58,
    "size": 1678,
    "docstring": "",
    "keywords": [
      "web_tools"
    ],
    "functions": [],
    "classes": [],
    "imports": [
      "json",
      "time",
      "requests"
    ],
    "preview": "import json\nimport time\n\nimport requests\n\napi_key = \"<YOUR_API_KEY>\"\nauthorization = \"Bearer %s\" % api_key\n\nheaders = {\n    \"accept\": \"application/json\",\n    \"content-type\": \"application/json\",\n    \"authorization\": authorization,\n}\n\n# Get a presigned URL for uploading an image\nurl = \"https://cloud.leonardo.ai/api/rest/v1/init-image\"\n\npayload = {\"extension\": \"jpg\"}\n\nresponse = requests.post(url, json=payload, headers=headers)",
    "last_modified": "2025-05-04T22:47:12.937267"
  },
  {
    "id": "2051",
    "name": "imgmp4.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/imgmp4.py",
    "category": "02_media_processing",
    "type": "image_processing",
    "lines": 58,
    "size": 1968,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "get_cover_images",
      "convert_mp3_to_mp4_with_images",
      "process_directory"
    ],
    "classes": [],
    "imports": [
      "glob",
      "os",
      "moviepy.editor",
      "PIL",
      "sys"
    ],
    "preview": "import glob\nimport os\n\nfrom moviepy.editor import AudioFileClip, ImageSequenceClip\nfrom PIL import Image\n\n\ndef get_cover_images(file_name, cover_image_directory):\n    # Check for both JPG and PNG extensions\n    images = []\n    jpg_paths = glob.glob(os.path.join(cover_image_directory, f\"{file_name}*.jpg\"))\n    png_paths = glob.glob(os.path.join(cover_image_directory, f\"{file_name}*.png\"))\n\n    images.extend(jpg_paths)\n    images.extend(png_paths)\n\n    if images:\n        return images\n    else:\n        print(f\"Cover images not found for {file_name}. Please ensure the cover images exist.\")",
    "last_modified": "2025-09-13T05:53:49.227378"
  },
  {
    "id": "2052",
    "name": "leoimg.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/leoimg.py",
    "category": "02_media_processing",
    "type": "image_processing",
    "lines": 38,
    "size": 1263,
    "docstring": "",
    "keywords": [
      "youtube",
      "web_tools"
    ],
    "functions": [
      "download_images"
    ],
    "classes": [],
    "imports": [
      "os",
      "requests",
      "colorama",
      "tqdm"
    ],
    "preview": "import os\n\nimport requests\nfrom colorama import Fore, Style\nfrom tqdm import tqdm\n\n\ndef download_images(source_file, destination_dir):\n    # Ensure destination directory exists\n    os.makedirs(destination_dir, exist_ok=True)\n\n    # Read URLs from the source file\n    with open(source_file, \"r\") as file:\n        urls = [line.strip() for line in file if line.strip()]\n\n    # Download each image with a progress bar\n    for url in tqdm(\n        urls,\n        desc=\"Downloading\",\n        bar_format=\"{l_bar}\" + Fore.GREEN + \"{bar}\" + Style.RESET_ALL + \"{r_bar}\",",
    "last_modified": "2025-05-04T22:47:12.933528"
  },
  {
    "id": "2053",
    "name": "categories.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/categories.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 69,
    "size": 2061,
    "docstring": "",
    "keywords": [],
    "functions": [
      "scan_directory",
      "export_to_csv",
      "main"
    ],
    "classes": [],
    "imports": [
      "csv",
      "os"
    ],
    "preview": "import csv\nimport os\n\n\ndef scan_directory(directory):\n    categories = {\n        \"Scripts\": [\".py\", \".ipynb\"],\n        \"Data Files\": [\".csv\", \".json\", \".xls\", \".xlsx\"],\n        \"Text Files\": [\".txt\", \".md\"],\n        \"Images\": [\".jpg\", \".jpeg\", \".png\", \".gif\", \".bmp\"],\n        \"Configuration Files\": [\".ini\", \".cfg\", \".yaml\", \".yml\", \".json\"],\n        \"Other\": [],\n    }\n\n    categorized_files = []\n\n    for root, _, files in os.walk(directory):\n        for filename in files:\n            file_path = os.path.join(root, filename)\n            file_extension = os.path.splitext(filename)[1].lower()",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2054",
    "name": "leodown_20250102105151.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/leodown_20250102105151.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 51,
    "size": 1326,
    "docstring": "",
    "keywords": [
      "web_tools"
    ],
    "functions": [],
    "classes": [],
    "imports": [
      "json",
      "requests"
    ],
    "preview": "import json\n\nimport requests\n\n# Base URL for API\nbase_url = (\n    \"https://cloud.leonardo.ai/api/rest/v1/generations/user/f7bb8476-e3f0-4f1f-9a06-4600866cc49c\"\n)\nheaders = {\n    \"accept\": \"application/json\",\n    \"authorization\": \"Bearer 93043291-957d-4ec1-8c79-ee734abcb6e3\",  # Replace with your actual token\n}\n\n# Output file\noutput_file = \"/Users/steven/Pictures/leonardo_library.json\"\n\n# Pagination parameters\noffset = 0\nlimit = 10  # Maximum number of items per request\nall_generations = []",
    "last_modified": "2025-09-13T05:53:49.599176"
  },
  {
    "id": "2055",
    "name": "YouTube Livestream Botter.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/YouTube Livestream Botter.py",
    "category": "02_media_processing",
    "type": "youtube",
    "lines": 171,
    "size": 7730,
    "docstring": "",
    "keywords": [
      "web_tools"
    ],
    "functions": [
      "bot",
      "__init__",
      "printservice",
      "update",
      "get_proxy",
      "FormatProxy",
      "__init__"
    ],
    "classes": [
      "main",
      "proxy"
    ],
    "imports": [
      "os",
      "platform",
      "random",
      "string",
      "threading",
      "time",
      "queue",
      "requests",
      "colorama"
    ],
    "preview": "import os\nimport platform\nimport random\nimport string\nimport threading\nimport time\nfrom queue import Queue\n\nimport requests\nfrom colorama import Fore, init\n\nintro = \"\"\"\n\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2588\u2557   \u2588\u2588\u2588\u2557      \u2588\u2588\u2588\u2588\u2588\u2588\u2557  \u2588\u2588\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2588\u2588\u2588\u2588\u2557 \n\u2588\u2588\u2554\u2550\u2550\u2550\u2550\u255d\u255a\u2550\u2550\u2588\u2588\u2554\u2550\u2550\u255d\u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2557\u2588\u2588\u2554\u2550\u2550\u2550\u2550\u255d\u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2557\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2588\u2588\u2551      \u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2557\u2588\u2588\u2554\u2550\u2550\u2550\u2588\u2588\u2557\u255a\u2550\u2550\u2588\u2588\u2554\u2550\u2550\u255d\u255a\u2550\u2550\u2588\u2588\u2554\u2550\u2550\u255d\u2588\u2588\u2554\u2550\u2550\u2550\u2550\u255d\u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2557\n\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557   \u2588\u2588\u2551   \u2588\u2588\u2588\u2588\u2588\u2588\u2554\u255d\u2588\u2588\u2588\u2588\u2588\u2557  \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2551\u2588\u2588\u2554\u2588\u2588\u2588\u2588\u2554\u2588\u2588\u2551\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2588\u2588\u2588\u2588\u2554\u255d\u2588\u2588\u2551   \u2588\u2588\u2551   \u2588\u2588\u2551      \u2588\u2588\u2551   \u2588\u2588\u2588\u2588\u2588\u2557  \u2588\u2588\u2588\u2588\u2588\u2588\u2554\u255d\n\u255a\u2550\u2550\u2550\u2550\u2588\u2588\u2551   \u2588\u2588\u2551   \u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2557\u2588\u2588\u2554\u2550\u2550\u255d  \u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2551\u2588\u2588\u2551\u255a\u2588\u2588\u2554\u255d\u2588\u2588\u2551\u255a\u2550\u2550\u2550\u2550\u255d\u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2557\u2588\u2588\u2551   \u2588\u2588\u2551   \u2588\u2588\u2551      \u2588\u2588\u2551   \u2588\u2588\u2554\u2550\u2550\u255d  \u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2557\n\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2551   \u2588\u2588\u2551   \u2588\u2588\u2551  \u2588\u2588\u2551\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2551  \u2588\u2588\u2551\u2588\u2588\u2551 \u255a\u2550\u255d \u2588\u2588\u2551      \u2588\u2588\u2588\u2588\u2588\u2588\u2554\u255d\u255a\u2588\u2588\u2588\u2588\u2588\u2588\u2554\u255d   \u2588\u2588\u2551      \u2588\u2588\u2551   \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2551  \u2588\u2588\u2551\n\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u255d   \u255a\u2550\u255d   \u255a\u2550\u255d  \u255a\u2550\u255d\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u255d\u255a\u2550\u255d  \u255a\u2550\u255d\u255a\u2550\u255d     \u255a\u2550\u255d      \u255a\u2550\u2550\u2550\u2550\u2550\u255d  \u255a\u2550\u2550\u2550\u2550\u2550\u255d    \u255a\u2550\u255d      \u255a\u2550\u255d   \u255a\u2550\u2550\u2550\u2550\u2550\u2550\u255d\u255a\u2550\u255d  \u255a\u2550\u255d\n\nhttps://github.com/KevinLage/YouTube-Livestream-Botter",
    "last_modified": "2025-05-04T23:28:20.826003"
  },
  {
    "id": "2056",
    "name": "organize_albums 7.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/organize_albums 7.py",
    "category": "02_media_processing",
    "type": "organization",
    "lines": 62,
    "size": 2470,
    "docstring": "",
    "keywords": [
      "organization"
    ],
    "functions": [
      "organize_files"
    ],
    "classes": [],
    "imports": [
      "os",
      "shutil"
    ],
    "preview": "import os\nimport shutil\n\n# Define the base directory\nbase_dir = \"/Users/steven/Music/NocTurnE-meLoDieS/albums/\"\n\n\n# Helper function to create folders and move files\ndef organize_files():\n    # List all files in the base directory\n    files = os.listdir(base_dir)\n\n    # Process each file\n    for file in files:\n        # Skip directories\n        if os.path.isdir(os.path.join(base_dir, file)):\n            continue\n\n        # Extract the base name (album name) from the file\n        if file.endswith(\".mp3\"):",
    "last_modified": "2025-05-04T22:47:11.429396"
  },
  {
    "id": "2057",
    "name": "sort.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/sort.py",
    "category": "02_media_processing",
    "type": "organization",
    "lines": 33,
    "size": 1090,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "os",
      "shutil"
    ],
    "preview": "import os\nimport shutil\n\n# Source directory containing your images\nsource_dir = \"/Volumes/baKs/105-mids\"\n\n# Destination directory where sorted folders will be created\ndestination_dir = \"/Volumes/baKs/105-mids\"\n\n# Create the destination directory if it doesn't exist\nos.makedirs(destination_dir, exist_ok=True)\n\n# Initialize variables\nimages_per_folder = 100\ncurrent_folder = None\nfolder_count = 0\n\n# Iterate through the source directory\nfor root, _, files in os.walk(source_dir):\n    for filename in files:",
    "last_modified": "2025-05-04T22:47:11.914841"
  },
  {
    "id": "2058",
    "name": "like_hashtags_from_file.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/like_hashtags_from_file.py",
    "category": "02_media_processing",
    "type": "image_processing",
    "lines": 33,
    "size": 812,
    "docstring": "instabot example\n\nWorkflow:\n    Like last images with hashtags from file.",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "argparse",
      "os",
      "sys",
      "instabot"
    ],
    "preview": "\"\"\"\ninstabot example\n\nWorkflow:\n    Like last images with hashtags from file.\n\"\"\"\n\nimport argparse\nimport os\nimport sys\n\nsys.path.append(os.path.join(sys.path[0], \"../\"))\nfrom instabot import Bot  # noqa: E402\n\nparser = argparse.ArgumentParser(add_help=True)\nparser.add_argument(\"-u\", type=str, help=\"username\")\nparser.add_argument(\"-p\", type=str, help=\"password\")\nparser.add_argument(\"-proxy\", type=str, help=\"proxy\")\nparser.add_argument(\"filename\", type=str, help=\"filename\")\nargs = parser.parse_args()",
    "last_modified": "2025-03-28T18:35:47"
  },
  {
    "id": "2059",
    "name": "upscale-dl.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/upscale-dl.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 107,
    "size": 3978,
    "docstring": "",
    "keywords": [
      "web_tools"
    ],
    "functions": [],
    "classes": [],
    "imports": [
      "json",
      "os",
      "time",
      "requests"
    ],
    "preview": "import json\nimport os\nimport time\n\nimport requests\n\napi_key = os.getenv(\"API_KEY\")\nif not api_key:\n    raise ValueError(\n        \"API key is not set. Please ensure the API_KEY environment variable is configured correctly.\"\n    )\n\nauthorization = f\"Bearer {api_key}\"\n\nheaders = {\n    \"accept\": \"application/json\",\n    \"content-type\": \"application/json\",\n    \"authorization\": authorization,\n}\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2060",
    "name": "upscale-dl 2.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/upscale-dl 2.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 92,
    "size": 3689,
    "docstring": "",
    "keywords": [
      "web_tools"
    ],
    "functions": [],
    "classes": [],
    "imports": [
      "json",
      "os",
      "time",
      "requests"
    ],
    "preview": "import json\nimport os\nimport time\n\nimport requests\n\napi_key = os.getenv(\"API_KEY\")\nif not api_key:\n    raise ValueError(\n        \"API key is not set. Please ensure the API_KEY environment variable is configured correctly.\"\n    )\n\nauthorization = f\"Bearer {api_key}\"\n\nheaders = {\n    \"accept\": \"application/json\",\n    \"content-type\": \"application/json\",\n    \"authorization\": authorization,\n}\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2061",
    "name": "scan_images.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/scan_images.py",
    "category": "02_media_processing",
    "type": "image_processing",
    "lines": 30,
    "size": 742,
    "docstring": "",
    "keywords": [],
    "functions": [
      "scan_directory"
    ],
    "classes": [],
    "imports": [
      "os"
    ],
    "preview": "import os\n\n# List of drives to scan\ndrives = [\n    \"/Users/steven/Downloads\",\n    \"/Users/steven/Documents\",\n    \"/Users/steven/Music\",\n    \"/Users/steven/Pictures\",\n]\n# File types to look for\nfile_types = (\".jpg\", \".jpeg\", \".png\")\n\n# Function to scan a directory for image files\n\n\ndef scan_directory(directory):\n    for root, dirs, files in os.walk(directory):\n        for file in files:\n            if file.lower().endswith(file_types):\n                yield os.path.join(root, file)",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2062",
    "name": "upscale 3.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/upscale 3.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 34,
    "size": 1127,
    "docstring": "",
    "keywords": [],
    "functions": [
      "upscale_image"
    ],
    "classes": [],
    "imports": [
      "os",
      "PIL"
    ],
    "preview": "import os\n\nfrom PIL import Image\n\n# Function to upscale and set DPI of an image\n\n\ndef upscale_image(input_path, output_path, scale_factor=2, dpi=(300, 300)):\n    with Image.open(input_path) as img:\n        # Upscale the image\n        new_size = (img.width * scale_factor, img.height * scale_factor)\n        img_resized = img.resize(new_size, Image.ANTIALIAS)\n        # Set the DPI\n        img_resized.save(output_path, dpi=dpi)\n\n\n# Directory containing images to upscale\ninput_dir = input(\"Enter the path to the directory containing the images: \")\n# Directory to save the upscaled images\noutput_dir = input(\"Enter the path to the directory to save the upscaled images: \")",
    "last_modified": "2025-05-04T22:47:13.381541"
  },
  {
    "id": "2063",
    "name": "test_bot_follow.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/test_bot_follow.py",
    "category": "02_media_processing",
    "type": "testing",
    "lines": 530,
    "size": 21951,
    "docstring": "",
    "keywords": [
      "testing"
    ],
    "functions": [
      "reset_files",
      "test_follow",
      "test_follow_users",
      "test_follow_followers",
      "test_follow_following",
      "test_sleep_feedback_successful",
      "test_sleep_feedback_unsuccessful"
    ],
    "classes": [
      "TestBotFilter"
    ],
    "imports": [
      "pytest",
      "responses",
      "instabot.api.config",
      "test_bot",
      "test_variables",
      "unittest.mock",
      "mock"
    ],
    "preview": "import pytest\nimport responses\nfrom instabot.api.config import API_URL, SIG_KEY_VERSION\n\nfrom .test_bot import TestBot\nfrom .test_variables import (\n    TEST_FOLLOWER_ITEM,\n    TEST_FOLLOWING_ITEM,\n    TEST_SEARCH_USERNAME_ITEM,\n    TEST_USERNAME_INFO_ITEM,\n)\n\ntry:\n    from unittest.mock import patch\nexcept ImportError:\n    from mock import patch\n\n\ndef reset_files(_bot):\n    for x in _bot.followed_file.list:",
    "last_modified": "2025-09-13T05:54:58.585345"
  },
  {
    "id": "2064",
    "name": "img-img.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/img-img.py",
    "category": "02_media_processing",
    "type": "image_processing",
    "lines": 56,
    "size": 2134,
    "docstring": "",
    "keywords": [],
    "functions": [
      "convert_and_upscale_images_in_subfolders",
      "main"
    ],
    "classes": [],
    "imports": [
      "os",
      "PIL"
    ],
    "preview": "import os\n\nfrom PIL import Image\n\n\ndef convert_and_upscale_images_in_subfolders(source_directory):\n    for root, dirs, files in os.walk(source_directory):\n        for filename in files:\n            if filename.endswith(\".png\"):\n                source_file = os.path.join(root, filename)\n                filename_no_ext = os.path.splitext(filename)[0]\n                temp_file = os.path.join(root, f\"{filename_no_ext}_temp.png\")\n\n                # Open the image and retrieve the original dimensions\n                im = Image.open(source_file)\n                width, height = im.size\n                print(f\"\ud83d\uddbc\ufe0f Processing {filename}: Original size: {width}x{height}\")\n\n                # Upscale the image by 2x\n                upscale_width = width * 2",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2065",
    "name": "Telegraph Image Downloader [RUS].py",
    "path": "github_repo/scripts/02_media_processing/image_tools/Telegraph Image Downloader [RUS].py",
    "category": "02_media_processing",
    "type": "youtube",
    "lines": 446,
    "size": 19134,
    "docstring": "",
    "keywords": [
      "youtube",
      "web_tools"
    ],
    "functions": [
      "getHTML",
      "validName",
      "print_percent",
      "download",
      "main"
    ],
    "classes": [],
    "imports": [
      "threading",
      "msvcrt",
      "os",
      "pathlib",
      "re",
      "subprocess",
      "sys",
      "time",
      "traceback",
      "urllib"
    ],
    "preview": "# Telegraph Image Downloader\n# \u0410\u0432\u0442\u043e\u0440: ARTEZON (vk.com/artez0n)\n#\n# \u0412\u0435\u0440\u0441\u0438\u044f 1.2.1\n#\n# --------------------------------------------------\n# -= \u041d\u0410\u0421\u0422\u0420\u041e\u0419\u041a\u0418 =-\n# --------------------------------------------------\n# \u042f\u0437\u044b\u043a (string)\n# \u0412\u043e\u0437\u043c\u043e\u0436\u043d\u044b\u0435 \u0432\u0430\u0440\u0438\u0430\u043d\u0442\u044b: ENG, RUS, JPN, CHN, KOR \u0438 \u0434\u0440\u0443\u0433\u0438\u0435\n# language = 'RUS'\n# --------------------------------------------------\n# \u0413\u0434\u0435 \u0441\u043e\u0445\u0440\u0430\u043d\u044f\u0442\u044c \u043c\u0435\u0442\u0430\u0434\u0430\u043d\u043d\u044b\u0435 (int)\n# 0 - \u041e\u0442\u043a\u043b\u044e\u0447\u0435\u043d\u043e\n# 1 - \u0412 \u043f\u0430\u043f\u043a\u0435 \"\u0417\u0430\u0433\u0440\u0443\u0437\u043a\u0438\" (\u043f\u043e \u0443\u043c\u043e\u043b\u0447\u0430\u043d\u0438\u044e)\n# 2 - \u0420\u044f\u0434\u043e\u043c \u0441 \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u044f\u043c\u0438\nmetadataLocation = 1\n# --------------------------------------------------\n# \u041c\u0430\u043a\u0441\u0438\u043c\u0430\u043b\u044c\u043d\u043e\u0435 \u0447\u0438\u0441\u043b\u043e \u043e\u0434\u043d\u043e\u0432\u0440\u0435\u043c\u0435\u043d\u043d\u044b\u0445 \u0437\u0430\u0433\u0440\u0443\u0437\u043e\u043a (int)\n# \u041f\u043e \u0443\u043c\u043e\u043b\u0447\u0430\u043d\u0438\u044e: 10",
    "last_modified": "2025-09-13T05:55:24.813336"
  },
  {
    "id": "2066",
    "name": "target_followers_of_similar_accounts_and_influencers.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/target_followers_of_similar_accounts_and_influencers.py",
    "category": "02_media_processing",
    "type": "analysis",
    "lines": 153,
    "size": 3981,
    "docstring": "This template is written by @Nuzzo235\n\nWhat does this quickstart script aim to do?\n- This script is targeting followers of similar accounts and influencers.\n- This is my starting point for a conservative approach: Interact with the\naudience of influencers in your niche with the help of 'Target-Lists' and\n'randomization'.\n\nNOTES:\n- For the ease of use most of the relevant data is retrieved in the upper part.",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "random",
      "instapy"
    ],
    "preview": "\"\"\"\nThis template is written by @Nuzzo235\n\nWhat does this quickstart script aim to do?\n- This script is targeting followers of similar accounts and influencers.\n- This is my starting point for a conservative approach: Interact with the\naudience of influencers in your niche with the help of 'Target-Lists' and\n'randomization'.\n\nNOTES:\n- For the ease of use most of the relevant data is retrieved in the upper part.\n\"\"\"\n\nimport random\n\nfrom instapy import InstaPy, smart_run\n\n# login credentials\ninsta_username = \"username\"\ninsta_password = \"password\"",
    "last_modified": "2025-03-28T18:36:56"
  },
  {
    "id": "2067",
    "name": "vanceai-removebg.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/vanceai-removebg.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 71,
    "size": 2580,
    "docstring": "",
    "keywords": [
      "analysis",
      "youtube",
      "web_tools"
    ],
    "functions": [
      "remove_background",
      "download_result",
      "process_directory"
    ],
    "classes": [],
    "imports": [
      "os",
      "time",
      "requests"
    ],
    "preview": "import os\nimport time\n\nimport requests\n\n# Get the API key from the environment variable\nAPI_KEY = os.getenv(\"VANCEAI_API_KEY\")\nAPI_URL = \"https://api-service.vanceai.com/web_api/v1/removebg\"\nPROGRESS_URL = \"https://api-service.vanceai.com/web_api/v1/progress\"\n\n\ndef remove_background(input_path, output_path):\n    with open(input_path, \"rb\") as file:\n        response = requests.post(API_URL, files={\"image_file\": file}, headers={\"api_key\": API_KEY})\n\n    if response.status_code == 200:\n        result = response.json()\n        trans_id = result[\"trans_id\"]\n        download_result(trans_id, output_path)\n    else:",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2068",
    "name": "clientUI.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/clientUI.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 842,
    "size": 33330,
    "docstring": "",
    "keywords": [
      "video_processing",
      "opencv",
      "youtube"
    ],
    "functions": [
      "__init__",
      "attemptLogin",
      "loginSuccess",
      "__init__",
      "updateRenderProgress",
      "downloadFinishedVideo",
      "populateFinishedVideos",
      "getFinishedVideos",
      "startEditingVideo",
      "updateDownload"
    ],
    "classes": [
      "LoginWindow",
      "MainMenu",
      "ClipDownloadMenu",
      "ClipUploadMenu",
      "clipEditor"
    ],
    "imports": [
      "os",
      "pickle",
      "threading",
      "client",
      "cv2",
      "scriptwrapper",
      "settings",
      "pymediainfo",
      "PyQt5",
      "PyQt5.QtCore"
    ],
    "preview": "import os\nimport pickle\nfrom threading import Thread\n\nimport client\nimport cv2\nimport scriptwrapper\nimport settings\nfrom pymediainfo import MediaInfo\nfrom PyQt5 import QtCore, QtGui, QtWidgets, uic\nfrom PyQt5.QtCore import *\nfrom PyQt5.QtCore import QDir, QObject, QPoint, QRect, Qt, QUrl, pyqtSignal\nfrom PyQt5.QtGui import QIcon\nfrom PyQt5.QtMultimedia import (\n    QAbstractVideoBuffer,\n    QAbstractVideoSurface,\n    QMediaContent,\n    QMediaPlayer,\n    QMediaPlaylist,\n    QVideoFrame,",
    "last_modified": "2025-09-13T05:53:31.455753"
  },
  {
    "id": "2069",
    "name": "download_stickers.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/download_stickers.py",
    "category": "02_media_processing",
    "type": "youtube",
    "lines": 117,
    "size": 3983,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "logging",
      "os",
      "time",
      "pyrogram",
      "translation",
      "helper_funcs.chat_base",
      "helper_funcs.display_progress",
      "sample_config",
      "config"
    ],
    "preview": "import logging\n\nlogging.basicConfig(\n    level=logging.DEBUG, format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\n)\nlogger = logging.getLogger(__name__)\n\nimport os\nimport time\n\n# the secret configuration specific things\nif bool(os.environ.get(\"WEBHOOK\", False)):\n    from sample_config import Config\nelse:\n    from config import Config\n\n# the Strings used for this \"thing\"\nimport pyrogram\nfrom translation import Translation\n",
    "last_modified": "2025-09-13T05:53:44.081939"
  },
  {
    "id": "2070",
    "name": "up-down.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/up-down.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 125,
    "size": 5234,
    "docstring": "",
    "keywords": [],
    "functions": [
      "adjust_image_size",
      "convert_and_downscale_images_in_subfolders",
      "main"
    ],
    "classes": [],
    "imports": [
      "os",
      "PIL"
    ],
    "preview": "import os\n\nfrom PIL import Image, UnidentifiedImageError\n\n\ndef adjust_image_size(im, target_file_size, temp_file, target_dpi, upscale=False):\n    file_size = os.path.getsize(temp_file)\n\n    # Size limits: 4500x5400 max, 1024x1024 min\n    max_width, max_height = 4500, 5400\n    min_width, min_height = 1024, 1024\n\n    while (file_size > target_file_size) or (upscale and file_size < target_file_size):\n        # Downscale if image is too large\n        if file_size > target_file_size or im.size[0] > max_width or im.size[1] > max_height:\n            scale_factor = 0.9  # Downscale by 10%\n        # Upscale if image is too small\n        elif im.size[0] < min_width or im.size[1] < min_height:\n            scale_factor = 1.1  # Upscale by 10%\n        else:",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2071",
    "name": "convert_to_video.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/convert_to_video.py",
    "category": "02_media_processing",
    "type": "youtube",
    "lines": 141,
    "size": 5334,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "logging",
      "os",
      "time",
      "pyrogram",
      "translation",
      "hachoir.metadata",
      "hachoir.parser",
      "helper_funcs.chat_base",
      "helper_funcs.display_progress",
      "PIL"
    ],
    "preview": "import logging\n\nlogging.basicConfig(\n    level=logging.DEBUG, format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\n)\nlogger = logging.getLogger(__name__)\n\nimport os\nimport time\n\n# the secret configuration specific things\nif bool(os.environ.get(\"WEBHOOK\", False)):\n    from sample_config import Config\nelse:\n    from config import Config\n\n# the Strings used for this \"thing\"\nimport pyrogram\nfrom translation import Translation\n",
    "last_modified": "2025-09-13T05:53:43.785319"
  },
  {
    "id": "2072",
    "name": "csvp.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/csvp.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 36,
    "size": 1274,
    "docstring": "",
    "keywords": [],
    "functions": [
      "compile_image_info"
    ],
    "classes": [],
    "imports": [
      "csv",
      "os"
    ],
    "preview": "import csv\nimport os\n\n\ndef compile_image_info(root_directory, master_csv_file_path):\n    with open(master_csv_file_path, mode=\"w\", newline=\"\", encoding=\"utf-8\") as master_file:\n        master_writer = csv.writer(master_file)\n        master_writer.writerow(\n            [\n                \"Subfolder\",\n                \"Product Name\",\n                \"Product Link\",\n                \"Est. Monthly Revenue\",\n                \"Reviews\",\n                \"Listing Age\",\n                \"Favorites\",\n                \"Est. Total Sales\",\n                \"Price\",\n                \"Tags\",\n            ]",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2073",
    "name": "convert.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/convert.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 32,
    "size": 1158,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "os",
      "PIL"
    ],
    "preview": "import os\n\nfrom PIL import Image\n\n# Your messy, no-folder, unstructured digital wasteland\ninput_dir = \"'/Users/steven/Pictures/etsy/PanoramicIndexJuice\"\n\n# Supported input formats because variety is the spice of life\ninput_formats = (\".webp\", \".tiff\", \".tif\")\n\n# Go through all files, because why not?\nfor filename in os.listdir(input_dir):\n    if filename.lower().endswith(input_formats):\n        input_path = os.path.join(input_dir, filename)\n        output_path = (\n            os.path.splitext(input_path)[0] + \".jpg\"\n        )  # Keep same name, just betray the format\n\n        # Open and mercilessly convert\n        with Image.open(input_path) as img:",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2074",
    "name": "test_bot_like.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/test_bot_like.py",
    "category": "02_media_processing",
    "type": "testing",
    "lines": 1270,
    "size": 46609,
    "docstring": "",
    "keywords": [
      "testing"
    ],
    "functions": [
      "test_bot_like",
      "test_bot_like_comment",
      "test_like_media_comments",
      "test_like_user",
      "test_like_users",
      "test_sleep_feedback_successful",
      "test_sleep_feedback_unsuccessful",
      "test_like_feedback",
      "test_like_medias",
      "test_like_hashtag"
    ],
    "classes": [
      "TestBotGet"
    ],
    "imports": [
      "pytest",
      "responses",
      "instabot.api.config",
      "test_bot",
      "test_variables",
      "unittest.mock",
      "mock"
    ],
    "preview": "import pytest\nimport responses\nfrom instabot.api.config import API_URL, SIG_KEY_VERSION\n\nfrom .test_bot import TestBot\nfrom .test_variables import (\n    TEST_CAPTION_ITEM,\n    TEST_COMMENT_ITEM,\n    TEST_FOLLOWER_ITEM,\n    TEST_FOLLOWING_ITEM,\n    TEST_PHOTO_ITEM,\n    TEST_SEARCH_USERNAME_ITEM,\n    TEST_TIMELINE_PHOTO_ITEM,\n    TEST_USERNAME_INFO_ITEM,\n)\n\ntry:\n    from unittest.mock import patch\nexcept ImportError:\n    from mock import patch",
    "last_modified": "2025-09-13T05:54:59.970121"
  },
  {
    "id": "2075",
    "name": "listcsv.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/listcsv.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 23,
    "size": 597,
    "docstring": "",
    "keywords": [],
    "functions": [
      "list_files",
      "save_to_csv"
    ],
    "classes": [],
    "imports": [
      "csv",
      "os"
    ],
    "preview": "import csv\nimport os\n\n\ndef list_files(directory):\n    for root, dirs, files in os.walk(directory):\n        for file in files:\n            yield os.path.join(root, file)\n\n\ndef save_to_csv(file_list, output_csv):\n    with open(output_csv, \"w\", newline=\"\") as csvfile:\n        filewriter = csv.writer(csvfile)\n        # Write each file path as a new column in the same row\n        filewriter.writerow(file_list)\n\n\nif __name__ == \"__main__\":\n    directory = \"/Users/steven/Pictures/3dMothers\"\n    output_csv = \"file_list.csv\"",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2076",
    "name": "infinity_hashtags_liker.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/infinity_hashtags_liker.py",
    "category": "02_media_processing",
    "type": "image_processing",
    "lines": 32,
    "size": 723,
    "docstring": "instabot example\n\nWorkflow:\n    Like last images with hashtag.",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "argparse",
      "os",
      "sys",
      "time",
      "instabot"
    ],
    "preview": "\"\"\"\ninstabot example\n\nWorkflow:\n    Like last images with hashtag.\n\"\"\"\n\nimport argparse\nimport os\nimport sys\nimport time\n\nsys.path.append(os.path.join(sys.path[0], \"../\"))\nfrom instabot import Bot  # noqa: E402\n\nparser = argparse.ArgumentParser(add_help=True)\nparser.add_argument(\"-u\", type=str, help=\"username\")\nparser.add_argument(\"-p\", type=str, help=\"password\")\nparser.add_argument(\"-proxy\", type=str, help=\"proxy\")\nparser.add_argument(\"hashtags\", type=str, nargs=\"+\", help=\"hashtags\")",
    "last_modified": "2025-03-28T18:35:47"
  },
  {
    "id": "2077",
    "name": "generate_album_html-pages_fixed 2.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/generate_album_html-pages_fixed 2.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 157,
    "size": 4917,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "os",
      "pathlib"
    ],
    "preview": "import os\nfrom pathlib import Path\n\nalbums_dir = Path(\"/Users/steven/Music/nocTurneMeLoDieS/Media\")\noutput_file = albums_dir / \"discography.html\"\n\nhtml_header = \"\"\"<!DOCTYPE html>\n<html lang=\"en\">\n<head>\n    <meta charset=\"UTF-8\">\n    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n    <title>Discography with MP3</title>\n    <style>\n        body { font-family: Arial, sans-serif; background-color: #f4f4f4; margin: 0; padding: 0; }\n        h1 { text-align: center; margin-top: 20px; font-size: 32px; color: #333; }\n        .grid-container {\n            display: grid;\n            grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));\n            gap: 20px;\n            padding: 20px;",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2078",
    "name": "imgconvert 2.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/imgconvert 2.py",
    "category": "02_media_processing",
    "type": "image_processing",
    "lines": 50,
    "size": 1736,
    "docstring": "",
    "keywords": [],
    "functions": [
      "convert_and_upscale_images",
      "main"
    ],
    "classes": [],
    "imports": [
      "os",
      "PIL"
    ],
    "preview": "import os\n\nfrom PIL import Image\n\n\n# Function to convert WebP images to PNG and upscale by 200% with 300 DPI\ndef convert_and_upscale_images(source_directory, destination_directory):\n    # Create the destination directory if it doesn't exist\n    os.makedirs(destination_directory, exist_ok=True)\n\n    for filename in os.listdir(source_directory):\n        if filename.endswith(\".webp\"):\n            source_file = os.path.join(source_directory, filename)\n            filename_no_ext = os.path.splitext(filename)[0]\n            destination_file = os.path.join(destination_directory, f\"{filename_no_ext}.png\")\n\n            # Convert WebP to PNG and upscale by 200% with 300 DPI\n            im = Image.open(source_file)\n            width, height = im.size\n            upscale_width = width * 2",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2079",
    "name": "simple_interaction_good_for_beginners.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/simple_interaction_good_for_beginners.py",
    "category": "02_media_processing",
    "type": "image_processing",
    "lines": 73,
    "size": 2159,
    "docstring": "This template is written by @Tachenz\n\nWhat does this quickstart script aim to do?\n- Interact with user followers, liking 3 pictures, doing 1-2 comment - and\n25% chance of follow (ratios which work the best for my account)\n\nNOTES:\n- This is used in combination with putting a 40 sec sleep delay after every\nlike the script does. It runs 24/7 at rather slower speed, but without\nproblems (so far).",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "instapy"
    ],
    "preview": "\"\"\"\nThis template is written by @Tachenz\n\nWhat does this quickstart script aim to do?\n- Interact with user followers, liking 3 pictures, doing 1-2 comment - and\n25% chance of follow (ratios which work the best for my account)\n\nNOTES:\n- This is used in combination with putting a 40 sec sleep delay after every\nlike the script does. It runs 24/7 at rather slower speed, but without\nproblems (so far).\n\"\"\"\n\nfrom instapy import InstaPy, smart_run\n\n# get a session!\nsession = InstaPy(username=\"\", password=\"\")\n\nphoto_comments = [\n    \"Nice shot! @{}\",",
    "last_modified": "2025-03-28T18:36:56"
  },
  {
    "id": "2080",
    "name": "remove.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/remove.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 32,
    "size": 929,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "io",
      "os",
      "PIL",
      "rembg"
    ],
    "preview": "import io\nimport os\n\nfrom PIL import Image\nfrom rembg import remove\n\ninput_dir = \"/Users/steven/Pictures/orn/Dream\"\noutput_dir = \"/Users/steven/Pictures/orn/Dream/processed_output\"\n\n# Create output directory if it doesn't exist\nif not os.path.exists(output_dir):\n    os.makedirs(output_dir)\n\n# Process all images in the input directory\nfor filename in os.listdir(input_dir):\n    if filename.lower().endswith((\".png\", \".jpg\", \".jpeg\")):\n        input_path = os.path.join(input_dir, filename)\n        output_path = os.path.join(output_dir, filename)\n\n        # Open the input image",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2081",
    "name": "2leomotion.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/2leomotion.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 90,
    "size": 3097,
    "docstring": "",
    "keywords": [
      "web_tools"
    ],
    "functions": [],
    "classes": [],
    "imports": [
      "time",
      "requests"
    ],
    "preview": "import time\n\nimport requests\n\napi_key = \"b5b99021-8e7a-42ef-8df9-4eca2c6efd3c\"\nauthorization = \"Bearer %s\" % api_key\n\nheaders = {\n    \"accept\": \"application/json\",\n    \"content-type\": \"application/json\",\n    \"authorization\": authorization,\n}\n\n# Generate an image\nurl = \"https://cloud.leonardo.ai/api/rest/v1/generations\"\n\npayload = {\n    \"height\": 960,\n    \"modelId\": \"ac614f96-1082-45bf-be9d-757f2d31c174\",\n    \"prompt\": \"A detailed photograph of a serious cyberpunk Hacker Cyborg transhumanist the past looking directly at the camera, standing straight, hands relaxed, square jaws, masculine face, dark scruff and no wrinkles, slightly buff looking, wearing a dark graphic t-shirt, detailed clothing texture realistic skin texture, black background, sharp focus, front view, waist up shot, high contrast, strong backlighting, action film dark color lut, cinematic luts\",",
    "last_modified": "2025-08-06T14:26:12.425397"
  },
  {
    "id": "2082",
    "name": "universe4 2.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/universe4 2.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 79,
    "size": 3098,
    "docstring": "",
    "keywords": [
      "web_tools"
    ],
    "functions": [],
    "classes": [],
    "imports": [
      "json",
      "os",
      "time",
      "requests"
    ],
    "preview": "import json\nimport os\nimport time\n\nimport requests\n\napi_key = \"2a59398a-fa31-47da-b771-64308e38bcc3\"\nauthorization = \"Bearer %s\" % api_key\n\nheaders = {\n    \"accept\": \"application/json\",\n    \"content-type\": \"application/json\",\n    \"authorization\": authorization,\n}\n\n# Directory containing images\ndirectory_path = \"/Users/steven/Pictures/Bcovers/\"\n\n# Styles to apply\n# Update the styles list according to your needs",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2083",
    "name": "png-jpg.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/png-jpg.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 42,
    "size": 1267,
    "docstring": "",
    "keywords": [],
    "functions": [
      "convert_images_to_jpeg",
      "main"
    ],
    "classes": [],
    "imports": [
      "os",
      "PIL"
    ],
    "preview": "import os\n\nfrom PIL import Image\n\n\ndef convert_images_to_jpeg(source_directory):\n    for root, dirs, files in os.walk(source_directory):\n        for filename in files:\n            if filename.endswith(\".png\"):\n                source_file = os.path.join(root, filename)\n                filename_no_ext = os.path.splitext(filename)[0]\n                jpeg_file = os.path.join(root, f\"{filename_no_ext}.jpg\")\n\n                # Open the PNG image\n                im = Image.open(source_file)\n\n                # Convert the image to RGB mode (JPEG doesn't support transparency)\n                im = im.convert(\"RGB\")\n\n                # Save the image as JPEG",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2084",
    "name": "download_photos_by_user.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/download_photos_by_user.py",
    "category": "02_media_processing",
    "type": "youtube",
    "lines": 27,
    "size": 546,
    "docstring": "instabot example\n\nWorkflow:\n    Download the specified user's medias",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "argparse",
      "os",
      "sys",
      "instabot"
    ],
    "preview": "\"\"\"\ninstabot example\n\nWorkflow:\n    Download the specified user's medias\n\n\"\"\"\n\nimport argparse\nimport os\nimport sys\n\nsys.path.append(os.path.join(sys.path[0], \"../\"))\nfrom instabot import Bot  # noqa: E402\n\nparser = argparse.ArgumentParser(add_help=True)\nparser.add_argument(\"username\", type=str, help=\"@username\")\nargs = parser.parse_args()\n\nif args.username[0] != \"@\":  # if first character isn't \"@\"",
    "last_modified": "2025-03-28T18:35:47"
  },
  {
    "id": "2085",
    "name": "savify.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/savify.py",
    "category": "02_media_processing",
    "type": "video_processing",
    "lines": 393,
    "size": 12897,
    "docstring": "Main module for Savify.",
    "keywords": [
      "youtube",
      "web_tools"
    ],
    "functions": [
      "_sort_dir",
      "_progress",
      "__init__",
      "check_for_updates",
      "_parse_query",
      "download",
      "_download"
    ],
    "classes": [
      "Savify"
    ],
    "imports": [
      "time",
      "multiprocessing",
      "multiprocessing.dummy",
      "pathlib",
      "shutil",
      "shutil",
      "urllib.error",
      "requests",
      "tldextract",
      "validators"
    ],
    "preview": "\"\"\"Main module for Savify.\"\"\"\n\n__all__ = [\"Savify\"]\n\nimport time\nfrom multiprocessing import cpu_count\nfrom multiprocessing.dummy import Pool as ThreadPool\nfrom pathlib import Path\nfrom shutil import Error as ShutilError\nfrom shutil import move\nfrom urllib.error import URLError\n\nimport requests\nimport tldextract\nimport validators\nfrom ffmpy import FFmpeg, FFRuntimeError\nfrom youtube_dl import YoutubeDL\n\nfrom .exceptions import (\n    FFmpegNotInstalledError,",
    "last_modified": "2025-09-13T05:55:15.753381"
  },
  {
    "id": "2086",
    "name": "upscale.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/upscale.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 55,
    "size": 2017,
    "docstring": "",
    "keywords": [],
    "functions": [
      "upscale_image"
    ],
    "classes": [],
    "imports": [
      "os",
      "PIL",
      "tqdm"
    ],
    "preview": "import os\n\nfrom PIL import Image\nfrom tqdm import tqdm\n\n\n# Function to upscale an image by 2x and set resolution to 300 DPI\ndef upscale_image(input_path, output_path):\n    \"\"\"\n    Upscale an image by 2x and set resolution to 300 DPI.\n\n    Args:\n        input_path (str): Path to the input image file.\n        output_path (str): Path to save the upscaled image.\n\n    Returns:\n        None\n    \"\"\"\n    image = Image.open(input_path)\n    upscaled_image = image.resize((image.width * 2, image.height * 2), Image.BICUBIC)",
    "last_modified": "2025-05-04T22:47:13.362289"
  },
  {
    "id": "2087",
    "name": "motion-upload (1).py",
    "path": "github_repo/scripts/02_media_processing/image_tools/motion-upload (1).py",
    "category": "02_media_processing",
    "type": "youtube",
    "lines": 57,
    "size": 1679,
    "docstring": "",
    "keywords": [
      "web_tools"
    ],
    "functions": [],
    "classes": [],
    "imports": [
      "json",
      "time",
      "requests"
    ],
    "preview": "import json\nimport time\n\nimport requests\n\napi_key = \"<YOUR_API_KEY>\"\nauthorization = \"Bearer %s\" % api_key\n\nheaders = {\n    \"accept\": \"application/json\",\n    \"content-type\": \"application/json\",\n    \"authorization\": authorization,\n}\n\n# Get a presigned URL for uploading an image\nurl = \"https://cloud.leonardo.ai/api/rest/v1/init-image\"\n\npayload = {\"extension\": \"jpg\"}\n\nresponse = requests.post(url, json=payload, headers=headers)",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2088",
    "name": "bing.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/bing.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 87,
    "size": 2594,
    "docstring": "",
    "keywords": [
      "web_tools"
    ],
    "functions": [
      "get_soup",
      "get_images"
    ],
    "classes": [],
    "imports": [
      "http.cookiejar",
      "json",
      "logging",
      "os",
      "re",
      "sys",
      "urllib.error",
      "urllib.parse",
      "urllib.request",
      "requests"
    ],
    "preview": "#!/usr/bin/env python3\nimport http.cookiejar\nimport json\nimport logging\nimport os\nimport re\nimport sys\nimport urllib.error\nimport urllib.parse\nimport urllib.request\n\nimport requests\nfrom bs4 import BeautifulSoup\n\nlogging.basicConfig(\n    format=\"%(asctime)s %(levelname)-8s %(message)s\",\n    level=logging.INFO,\n    datefmt=\"%Y-%m-%d %H:%M:%S\",\n    handlers=[logging.FileHandler(\"debug.log\"), logging.StreamHandler()],\n)",
    "last_modified": "2025-09-13T05:53:29.237077"
  },
  {
    "id": "2089",
    "name": "thumbnail.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/thumbnail.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 38,
    "size": 1435,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "create_thumbnail"
    ],
    "classes": [],
    "imports": [
      "PIL"
    ],
    "preview": "from PIL import ImageDraw, ImageFont\n\n\ndef create_thumbnail(thumbnail, font_family, font_size, font_color, width, height, title):\n    font = ImageFont.truetype(font_family + \".ttf\", font_size)\n    Xaxis = width - (width * 0.2)  # 20% of the width\n    sizeLetterXaxis = font_size * 0.5  # 50% of the font size\n    XaxisLetterQty = round(\n        Xaxis / sizeLetterXaxis\n    )  # Quantity of letters that can fit in the X axis\n    MarginYaxis = height * 0.12  # 12% of the height\n    MarginXaxis = width * 0.05  # 5% of the width\n    # 1.1 rem\n    LineHeight = font_size * 1.1\n    # rgb = \"255,255,255\" transform to list\n    rgb = font_color.split(\",\")\n    rgb = (int(rgb[0]), int(rgb[1]), int(rgb[2]))\n\n    arrayTitle = []\n    for word in title.split():",
    "last_modified": "2025-09-13T05:54:00.502218"
  },
  {
    "id": "2090",
    "name": "upscale copy.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/upscale copy.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 57,
    "size": 2021,
    "docstring": "",
    "keywords": [],
    "functions": [
      "upscale_image"
    ],
    "classes": [],
    "imports": [
      "os",
      "PIL",
      "tqdm"
    ],
    "preview": "import os\n\nfrom PIL import Image\nfrom tqdm import tqdm\n\n# Function to upscale an image by 2x and set resolution to 300 DPI\n\n\ndef upscale_image(input_path, output_path):\n    \"\"\"\n    Upscale an image by 2x and set resolution to 300 DPI.\n\n    Args:\n        input_path (str): Path to the input image file.\n        output_path (str): Path to save the upscaled image.\n\n    Returns:\n        None\n    \"\"\"\n    image = Image.open(input_path)",
    "last_modified": "2025-05-04T22:47:13.381792"
  },
  {
    "id": "2091",
    "name": "bot_direct.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/bot_direct.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 231,
    "size": 7591,
    "docstring": "",
    "keywords": [],
    "functions": [
      "send_message",
      "send_messages",
      "send_media",
      "send_medias",
      "send_hashtag",
      "send_profile",
      "send_like",
      "send_photo",
      "_get_user_ids",
      "approve_pending_thread_requests"
    ],
    "classes": [],
    "imports": [
      "os",
      "mimetypes",
      "tqdm"
    ],
    "preview": "import os\nfrom mimetypes import guess_type\n\nfrom tqdm import tqdm\n\n\ndef send_message(self, text, user_ids, thread_id=None):\n    \"\"\"\n    :param self: bot\n    :param text: text of message\n    :param user_ids: list of user_ids for creating group or\n    one user_id for send to one person\n    :param thread_id: thread_id\n    \"\"\"\n    user_ids = _get_user_ids(self, user_ids)\n    if not isinstance(text, str) and isinstance(user_ids, (list, str)):\n        self.logger.error(\"Text must be an string, user_ids must be an list or string\")\n        return False\n\n    if self.reached_limit(\"messages\"):",
    "last_modified": "2025-09-13T05:54:57.345261"
  },
  {
    "id": "2092",
    "name": "configHandler.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/configHandler.py",
    "category": "02_media_processing",
    "type": "setup",
    "lines": 347,
    "size": 8298,
    "docstring": "",
    "keywords": [],
    "functions": [
      "config_init",
      "get_output_title",
      "get_cmd_only",
      "get_out_path",
      "get_intro_slide",
      "get_intro_time",
      "get_intro_font_name",
      "get_intro_text_ratio",
      "get_intro_custom_bg",
      "get_intro_bg_name"
    ],
    "classes": [],
    "imports": [
      "os",
      "configparser",
      "cmd_logs"
    ],
    "preview": "#!/usr/bin/env python\nimport os\nfrom configparser import ConfigParser\n\nfrom .cmd_logs import *\n\nPATH = \"./res/config.ini\"\n\n\ndef config_init(bypass: bool = False, verbose: bool = False) -> None:\n    # Initialize Config file\n    # If bypass=True set config file to default values\n    global PATH\n    if os.path.isfile(PATH) and not bypass:\n        if verbose:\n            info(\"Config file already exists, add argument bypass=True to overwrite it\")\n        return\n    config_object = ConfigParser()\n    config_object[\"OUTPUT\"] = {\n        \"title\": \"clipsMontage\",",
    "last_modified": "2025-03-28T18:37:01"
  },
  {
    "id": "2093",
    "name": "ups.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/ups.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 68,
    "size": 2552,
    "docstring": "",
    "keywords": [],
    "functions": [
      "convert_and_upscale_images",
      "main"
    ],
    "classes": [],
    "imports": [
      "os",
      "PIL"
    ],
    "preview": "import os\n\nfrom PIL import Image\n\n\ndef convert_and_upscale_images(source_directory, destination_directory):\n    # Create the destination directory if it doesn't exist\n    os.makedirs(destination_directory, exist_ok=True)\n\n    for root, dirs, files in os.walk(source_directory):\n        # Replicate the directory structure in the destination\n        relative_path = os.path.relpath(root, source_directory)\n        dest_dir = os.path.join(destination_directory, relative_path)\n        os.makedirs(dest_dir, exist_ok=True)\n\n        for filename in files:\n            if filename.endswith((\".tiff\", \".png\", \".jpg\", \".jpeg\")):\n                source_file = os.path.join(root, filename)\n                filename_no_ext, file_ext = os.path.splitext(filename)\n                file_ext = file_ext.lower()",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2094",
    "name": "playing_around_with_quota_supervisor.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/playing_around_with_quota_supervisor.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 152,
    "size": 4076,
    "docstring": "This template is written by @boldestfortune\n\nWhat does this quickstart script aim to do?\n- Just started playing around with Quota Supervisor, so I'm still tweaking\nthese settings",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "random",
      "instapy"
    ],
    "preview": "\"\"\"\nThis template is written by @boldestfortune\n\nWhat does this quickstart script aim to do?\n- Just started playing around with Quota Supervisor, so I'm still tweaking\nthese settings\n\"\"\"\n\nimport random\n\nfrom instapy import InstaPy, smart_run\n\n# get a session!\nsession = InstaPy(username=\"\", password=\"\")\n\n# let's go! :>\nwith smart_run(session):\n    # general settings\n    session.set_quota_supervisor(\n        enabled=True,",
    "last_modified": "2025-03-28T18:36:56"
  },
  {
    "id": "2095",
    "name": "api_story.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/api_story.py",
    "category": "02_media_processing",
    "type": "web_tools",
    "lines": 92,
    "size": 2937,
    "docstring": "",
    "keywords": [
      "youtube",
      "web_tools"
    ],
    "functions": [
      "download_story",
      "upload_story_photo",
      "configure_story"
    ],
    "classes": [],
    "imports": [
      "__future__",
      "json",
      "os",
      "shutil",
      "time",
      "random",
      "requests_toolbelt",
      "api_photo"
    ],
    "preview": "from __future__ import unicode_literals\n\nimport json\nimport os\nimport shutil\nimport time\nfrom random import randint\n\nfrom requests_toolbelt import MultipartEncoder\n\nfrom . import config\nfrom .api_photo import get_image_size, stories_shaper\n\n\ndef download_story(self, filename, story_url, username):\n    path = \"stories/{}\".format(username)\n    if not os.path.exists(path):\n        os.makedirs(path)\n    fname = os.path.join(path, filename)\n    if os.path.exists(fname):  # already exists",
    "last_modified": "2025-09-13T05:54:56.636685"
  },
  {
    "id": "2096",
    "name": "leo-url-downloader.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/leo-url-downloader.py",
    "category": "02_media_processing",
    "type": "youtube",
    "lines": 38,
    "size": 1263,
    "docstring": "",
    "keywords": [
      "youtube",
      "web_tools"
    ],
    "functions": [
      "download_images"
    ],
    "classes": [],
    "imports": [
      "os",
      "requests",
      "colorama",
      "tqdm"
    ],
    "preview": "import os\n\nimport requests\nfrom colorama import Fore, Style\nfrom tqdm import tqdm\n\n\ndef download_images(source_file, destination_dir):\n    # Ensure destination directory exists\n    os.makedirs(destination_dir, exist_ok=True)\n\n    # Read URLs from the source file\n    with open(source_file, \"r\") as file:\n        urls = [line.strip() for line in file if line.strip()]\n\n    # Download each image with a progress bar\n    for url in tqdm(\n        urls,\n        desc=\"Downloading\",\n        bar_format=\"{l_bar}\" + Fore.GREEN + \"{bar}\" + Style.RESET_ALL + \"{r_bar}\",",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2097",
    "name": "format.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/format.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 29,
    "size": 742,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "csv",
      "json",
      "os"
    ],
    "preview": "import csv\nimport json\nimport os\n\n# Define the input CSV file\ncsv_file = \"/Users/steven/Pictures/DaLLe/csv-test/Etsy-12-2-2024.csv\"\n\n# Generate JSON file path based on the CSV file\njson_file = os.path.splitext(csv_file)[0] + \".json\"\n\n# Read the CSV and convert to JSON\ndata = []\ntry:\n    with open(csv_file, \"r\") as f:\n        reader = csv.DictReader(f)\n        for row in reader:\n            data.append(row)\nexcept FileNotFoundError:\n    print(f\"Error: The file {csv_file} was not found.\")\n    exit(1)",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2098",
    "name": "stylish_unfollow_tips_and_like_by_tags.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/stylish_unfollow_tips_and_like_by_tags.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 138,
    "size": 4387,
    "docstring": "This template is written by @Nocturnal-2\n\nWhat does this quickstart script aim to do?\n- I do some unfollow and like by tags mostly\n\nNOTES:\n- I am an one month old InstaPy user, with a small following. So my numbers\nin settings are bit conservative.",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "instapy"
    ],
    "preview": "\"\"\"\nThis template is written by @Nocturnal-2\n\nWhat does this quickstart script aim to do?\n- I do some unfollow and like by tags mostly\n\nNOTES:\n- I am an one month old InstaPy user, with a small following. So my numbers\nin settings are bit conservative.\n\"\"\"\n\nfrom instapy import InstaPy, smart_run\n\n# get a session!\nsession = InstaPy(username=\"\", password=\"\")\n\n# let's go! :>\nwith smart_run(session):\n    \"\"\"Start of parameter setting\"\"\"\n    # don't like if a post already has more than 150 likes",
    "last_modified": "2025-03-28T18:36:56"
  },
  {
    "id": "2099",
    "name": "botLike.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/botLike.py",
    "category": "02_media_processing",
    "type": "web_tools",
    "lines": 196,
    "size": 5434,
    "docstring": "Created in 12/2019\n@Author: Paulo https://github.com/alpdias",
    "keywords": [],
    "functions": [
      "functionLike",
      "botlogin",
      "findhashtag",
      "like"
    ],
    "classes": [],
    "imports": [
      "os",
      "random",
      "pathlib",
      "time",
      "art",
      "selenium",
      "selenium.webdriver.common.keys"
    ],
    "preview": "# -*- coding: utf-8 -*-\n\n\"\"\"\nCreated in 12/2019\n@Author: Paulo https://github.com/alpdias\n\"\"\"\n\n# imported libraries\nimport os\nimport random\nfrom pathlib import Path\nfrom time import sleep\n\nimport art\nfrom selenium import webdriver\nfrom selenium.webdriver.common.keys import Keys\n\n\ndef functionLike(mySystem):\n    \"\"\"",
    "last_modified": "2025-09-13T05:54:07.011516"
  },
  {
    "id": "2100",
    "name": "playlist_20221230180508.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/playlist_20221230180508.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 167,
    "size": 6416,
    "docstring": "",
    "keywords": [
      "youtube",
      "analysis",
      "web_tools"
    ],
    "functions": [
      "generate_token",
      "get_youtube_url",
      "download_mp3",
      "last_fm_artist_info",
      "set_metadata",
      "write_tracks",
      "write_playlist",
      "split_spotify_uri",
      "get_os",
      "get_folder"
    ],
    "classes": [],
    "imports": [
      "argparse",
      "json",
      "os",
      "re",
      "subprocess",
      "urllib.parse",
      "urllib.request",
      "secret",
      "spotipy",
      "spotipy.oauth2"
    ],
    "preview": "import argparse\nimport json\nimport os\nimport re\nimport subprocess\nimport urllib.parse\nimport urllib.request\n\nimport secret\nimport spotipy\nimport spotipy.oauth2 as oauth2\nfrom requests.utils import quote\n\n\ndef generate_token():\n    credentials = oauth2.SpotifyClientCredentials(\n        client_id=secret.28b20556906f4b75874c4ae98320c81d,\n        client_secret=secret.SPOTIFY_CLIENT_SECRET)\n    token = credentials.get_access_token()\n    return token",
    "last_modified": "2025-05-04T23:28:21.387315"
  },
  {
    "id": "2101",
    "name": "leocsv.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/leocsv.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 186,
    "size": 6195,
    "docstring": "",
    "keywords": [
      "youtube",
      "web_tools"
    ],
    "functions": [
      "convert_image_to_jpeg",
      "get_presigned_url",
      "upload_image",
      "upscale_image",
      "get_upscaled_image",
      "initialize_csv",
      "log_to_csv"
    ],
    "classes": [],
    "imports": [
      "csv",
      "json",
      "os",
      "time",
      "datetime",
      "requests",
      "PIL"
    ],
    "preview": "import csv\nimport json\nimport os\nimport time\nfrom datetime import datetime\n\nimport requests\nfrom PIL import Image\n\napi_key = \"de7c9cb8-022f-42f8-8bf7-a8f9caadfaee\"\nauthorization = f\"Bearer {api_key}\"\n\nheaders = {\n    \"accept\": \"application/json\",\n    \"content-type\": \"application/json\",\n    \"authorization\": authorization,\n}\n\n# Directory containing images\ndirectory_path = \"/Users/steven/Pictures/TrashCaT/trashy-heartbreak\"",
    "last_modified": "2025-09-13T05:53:50.187982"
  },
  {
    "id": "2102",
    "name": "html-auto-img-gallery.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/html-auto-img-gallery.py",
    "category": "02_media_processing",
    "type": "image_processing",
    "lines": 81,
    "size": 2230,
    "docstring": "",
    "keywords": [],
    "functions": [
      "csv_to_html"
    ],
    "classes": [],
    "imports": [
      "csv"
    ],
    "preview": "import csv\n\n\ndef csv_to_html(csv_file, output_html):\n    # Start the HTML structure\n    html_content = \"\"\"\n    <!DOCTYPE html>\n    <html lang=\"en\">\n    <head>\n        <meta charset=\"UTF-8\">\n        <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n        <title>Image Gallery</title>\n        <style>\n            body {\n                font-family: Arial, sans-serif;\n                background-color: #f0f0f0;\n                padding: 20px;\n            }\n            h1 {\n                text-align: center;",
    "last_modified": "2025-09-13T05:53:55.182254"
  },
  {
    "id": "2103",
    "name": "upscale-sub.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/upscale-sub.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 68,
    "size": 2591,
    "docstring": "",
    "keywords": [],
    "functions": [
      "convert_and_upscale_images",
      "main"
    ],
    "classes": [],
    "imports": [
      "os",
      "PIL"
    ],
    "preview": "import os\n\nfrom PIL import Image\n\n\ndef convert_and_upscale_images(source_directory, destination_directory):\n    # Create the destination directory if it doesn't exist\n    os.makedirs(destination_directory, exist_ok=True)\n\n    for root, dirs, files in os.walk(source_directory):\n        # Replicate the directory structure in the destination\n        relative_path = os.path.relpath(root, source_directory)\n        dest_dir = os.path.join(destination_directory, relative_path)\n        os.makedirs(dest_dir, exist_ok=True)\n\n        for filename in files:\n            if filename.endswith((\".tiff\", \".png\", \".jpg\", \".jpeg\")):\n                source_file = os.path.join(root, filename)\n                filename_no_ext, file_ext = os.path.splitext(filename)\n                file_ext = file_ext.lower()",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2104",
    "name": "convert_to_audio.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/convert_to_audio.py",
    "category": "02_media_processing",
    "type": "transcription",
    "lines": 143,
    "size": 5501,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "logging",
      "os",
      "time",
      "pyrogram",
      "translation",
      "hachoir.metadata",
      "hachoir.parser",
      "helper_funcs.chat_base",
      "helper_funcs.display_progress",
      "PIL"
    ],
    "preview": "import logging\n\nlogging.basicConfig(\n    level=logging.DEBUG, format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\n)\nlogger = logging.getLogger(__name__)\n\nimport os\nimport time\n\n# the secret configuration specific things\nif bool(os.environ.get(\"WEBHOOK\", False)):\n    from sample_config import Config\nelse:\n    from config import Config\n\n# the Strings used for this \"thing\"\nimport pyrogram\nfrom translation import Translation\n",
    "last_modified": "2025-09-13T05:53:43.718276"
  },
  {
    "id": "2105",
    "name": "img2img.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/img2img.py",
    "category": "02_media_processing",
    "type": "image_processing",
    "lines": 47,
    "size": 1754,
    "docstring": "",
    "keywords": [],
    "functions": [
      "convert_and_upscale_images",
      "main"
    ],
    "classes": [],
    "imports": [
      "os",
      "PIL"
    ],
    "preview": "import os\n\nfrom PIL import Image\n\n\ndef convert_and_upscale_images(source_directory, destination_directory):\n    os.makedirs(destination_directory, exist_ok=True)\n\n    for filename in os.listdir(source_directory):\n        if filename.endswith(\".jpg\"):\n            source_file = os.path.join(source_directory, filename)\n            filename_no_ext = os.path.splitext(filename)[0]\n            destination_file = os.path.join(destination_directory, f\"{filename_no_ext}.jpg\")\n\n            im = Image.open(source_file)\n            width, height = im.size\n            upscale_width = width * 2\n            upscale_height = height * 2\n            im_resized = im.resize((upscale_width, upscale_height))\n",
    "last_modified": "2025-09-13T05:55:28.400073"
  },
  {
    "id": "2106",
    "name": "imagenarator.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/imagenarator.py",
    "category": "02_media_processing",
    "type": "image_processing",
    "lines": 86,
    "size": 2942,
    "docstring": "",
    "keywords": [
      "image_processing"
    ],
    "functions": [
      "draw_multiple_line_text",
      "imagemaker"
    ],
    "classes": [],
    "imports": [
      "os",
      "re",
      "textwrap",
      "PIL",
      "rich.progress",
      "TTS.engine_wrapper"
    ],
    "preview": "import os\nimport re\nimport textwrap\n\nfrom PIL import Image, ImageDraw, ImageFont\nfrom rich.progress import track\nfrom TTS.engine_wrapper import process_text\n\n\ndef draw_multiple_line_text(\n    image, text, font, text_color, padding, wrap=50, transparent=False\n) -> None:\n    \"\"\"\n    Draw multiline text over given image\n    \"\"\"\n    draw = ImageDraw.Draw(image)\n    Fontperm = font.getsize(text)\n    image_width, image_height = image.size\n    lines = textwrap.wrap(text, width=wrap)\n    y = (image_height / 2) - (",
    "last_modified": "2025-09-13T05:54:00.322776"
  },
  {
    "id": "2107",
    "name": "leodown.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/leodown.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 51,
    "size": 1326,
    "docstring": "",
    "keywords": [
      "web_tools"
    ],
    "functions": [],
    "classes": [],
    "imports": [
      "json",
      "requests"
    ],
    "preview": "import json\n\nimport requests\n\n# Base URL for API\nbase_url = (\n    \"https://cloud.leonardo.ai/api/rest/v1/generations/user/f7bb8476-e3f0-4f1f-9a06-4600866cc49c\"\n)\nheaders = {\n    \"accept\": \"application/json\",\n    \"authorization\": \"Bearer 93043291-957d-4ec1-8c79-ee734abcb6e3\",  # Replace with your actual token\n}\n\n# Output file\noutput_file = \"/Users/steven/Pictures/leonardo_library.json\"\n\n# Pagination parameters\noffset = 0\nlimit = 10  # Maximum number of items per request\nall_generations = []",
    "last_modified": "2025-09-13T05:53:50.205950"
  },
  {
    "id": "2108",
    "name": "basic_follow-unfollow_activity.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/basic_follow-unfollow_activity.py",
    "category": "02_media_processing",
    "type": "organization",
    "lines": 108,
    "size": 3186,
    "docstring": "This template is written by @cormo1990\n\nWhat does this quickstart script aim to do?\n- Basic follow/unfollow activity.\n\nNOTES:\n- I don't want to automate comment and too much likes because I want to do\nthis only for post that I really like the content so at the moment I only\nuse the function follow/unfollow.\n- I use two files \"quickstart\", one for follow and one for unfollow.\n- I noticed that the most important thing is that the account from where I\nget followers has similar contents to mine in order to be sure that my\ncontent could be appreciated. After the following step, I start unfollowing\nthe user that don't followed me back.\n- At the end I clean my account unfollowing all the users followed with\nInstaPy.",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "instapy"
    ],
    "preview": "\"\"\"\nThis template is written by @cormo1990\n\nWhat does this quickstart script aim to do?\n- Basic follow/unfollow activity.\n\nNOTES:\n- I don't want to automate comment and too much likes because I want to do\nthis only for post that I really like the content so at the moment I only\nuse the function follow/unfollow.\n- I use two files \"quickstart\", one for follow and one for unfollow.\n- I noticed that the most important thing is that the account from where I\nget followers has similar contents to mine in order to be sure that my\ncontent could be appreciated. After the following step, I start unfollowing\nthe user that don't followed me back.\n- At the end I clean my account unfollowing all the users followed with\nInstaPy.\n\"\"\"\n\n# imports",
    "last_modified": "2025-09-13T05:53:49.274698"
  },
  {
    "id": "2109",
    "name": "bot_story.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/bot_story.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 62,
    "size": 2237,
    "docstring": "",
    "keywords": [
      "youtube"
    ],
    "functions": [
      "download_stories",
      "upload_story_photo",
      "watch_users_reels"
    ],
    "classes": [],
    "imports": [],
    "preview": "def download_stories(self, username):\n    user_id = self.get_user_id_from_username(username)\n    list_image, list_video = self.get_user_stories(user_id)\n    if list_image == [] and list_video == []:\n        self.logger.error(\n            (\"Make sure that '{}' is NOT private and that \" \"posted some stories\").format(username)\n        )\n        return False\n    self.logger.info(\"Downloading stories...\")\n    for story_url in list_image:\n        filename = story_url.split(\"/\")[-1].split(\".\")[0] + \".jpg\"\n        self.api.download_story(filename, story_url, username)\n    for story_url in list_video:\n        filename = story_url.split(\"/\")[-1].split(\".\")[0] + \".mp4\"\n        self.api.download_story(filename, story_url, username)\n\n\ndef upload_story_photo(self, photo, upload_id=None):\n    self.small_delay()\n    if self.api.upload_story_photo(photo, upload_id):",
    "last_modified": "2025-09-13T05:54:57.992603"
  },
  {
    "id": "2110",
    "name": "organize_albums 6.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/organize_albums 6.py",
    "category": "02_media_processing",
    "type": "organization",
    "lines": 62,
    "size": 2470,
    "docstring": "",
    "keywords": [
      "organization"
    ],
    "functions": [
      "organize_files"
    ],
    "classes": [],
    "imports": [
      "os",
      "shutil"
    ],
    "preview": "import os\nimport shutil\n\n# Define the base directory\nbase_dir = \"/Users/steven/Music/NocTurnE-meLoDieS/albums/\"\n\n\n# Helper function to create folders and move files\ndef organize_files():\n    # List all files in the base directory\n    files = os.listdir(base_dir)\n\n    # Process each file\n    for file in files:\n        # Skip directories\n        if os.path.isdir(os.path.join(base_dir, file)):\n            continue\n\n        # Extract the base name (album name) from the file\n        if file.endswith(\".mp3\"):",
    "last_modified": "2025-05-04T22:47:11"
  },
  {
    "id": "2111",
    "name": "leo.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/leo.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 26,
    "size": 1313,
    "docstring": "",
    "keywords": [
      "web_tools"
    ],
    "functions": [],
    "classes": [],
    "imports": [
      "requests"
    ],
    "preview": "import requests\n\n# API URL\nurl = \"https://cloud.leonardo.ai/api/rest/v1/generations\"\n\n# Payload for the POST request\npayload = {\n    \"height\": 512,\n    \"modelId\": \"6bef9f1b-29cb-40c7-b9df-32b51c1f67d3\",\n    \"prompt\": \"Create a vibrant and lively image featuring a front-facing, centered group of the CoTTonWooDs, the adorable inhabitants of Itchy Isle. These fluffy creatures should be depicted in a range of pastel colors, each exuding unique personality and joy. Imagine them with soft, cotton-like textures, big, sparkling eyes full of wonder, and wide, beaming smiles. They should have playful tufts of hair styled in whimsical ways, resembling colorful strands of yarn. The background should be a cheerful blur of Itchy Isle's signature landmarks - macrame houses, yarn trees, and a rainbow-streaked sky. The overall atmosphere of the image should be one of happiness, friendship, and the magical essence of a carefree, enchanting world.\",\n    \"width\": 512,\n}\n\n# Headers for the POST request\nheaders = {\n    \"accept\": \"application/json\",\n    \"content-type\": \"application/json\",\n    \"authorization\": \"Bearer 7ccf0307-636e-4334-9a61-814202374698\",\n}\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2112",
    "name": "latin1prober.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/latin1prober.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 146,
    "size": 5356,
    "docstring": "",
    "keywords": [],
    "functions": [
      "__init__",
      "reset",
      "charset_name",
      "language",
      "feed",
      "get_confidence"
    ],
    "classes": [
      "Latin1Prober"
    ],
    "imports": [
      "typing",
      "charsetprober",
      "enums"
    ],
    "preview": "######################## BEGIN LICENSE BLOCK ########################\n# The Original Code is Mozilla Universal charset detector code.\n#\n# The Initial Developer of the Original Code is\n# Netscape Communications Corporation.\n# Portions created by the Initial Developer are Copyright (C) 2001\n# the Initial Developer. All Rights Reserved.\n#\n# Contributor(s):\n#   Mark Pilgrim - port to Python\n#   Shy Shalom - original C code\n#\n# This library is free software; you can redistribute it and/or\n# modify it under the terms of the GNU Lesser General Public\n# License as published by the Free Software Foundation; either\n# version 2.1 of the License, or (at your option) any later version.\n#\n# This library is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2113",
    "name": "like_by_tag_interact_unfollow.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/like_by_tag_interact_unfollow.py",
    "category": "02_media_processing",
    "type": "image_processing",
    "lines": 146,
    "size": 2994,
    "docstring": "This template is written by @timgrossmann\n\nWhat does this quickstart script aim to do?\n- This script is automatically executed every 6h on my server via cron",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "random",
      "instapy"
    ],
    "preview": "\"\"\"\nThis template is written by @timgrossmann\n\nWhat does this quickstart script aim to do?\n- This script is automatically executed every 6h on my server via cron\n\"\"\"\n\nimport random\n\nfrom instapy import InstaPy, smart_run\n\n# login credentials\ninsta_username = \"\"\ninsta_password = \"\"\n\ndont_likes = [\n    \"sex\",\n    \"nude\",\n    \"naked\",\n    \"beef\",",
    "last_modified": "2025-09-13T05:53:49.464857"
  },
  {
    "id": "2114",
    "name": "ultimate.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/ultimate.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 144,
    "size": 4897,
    "docstring": "",
    "keywords": [
      "youtube"
    ],
    "functions": [
      "stats",
      "like_hashtags",
      "like_timeline",
      "like_followers_from_random_user_file",
      "follow_followers",
      "comment_medias",
      "unfollow_non_followers",
      "follow_users_from_hashtag_file",
      "comment_hashtag",
      "upload_pictures"
    ],
    "classes": [],
    "imports": [
      "argparse",
      "os",
      "sys",
      "threading",
      "time",
      "glob",
      "config",
      "schedule",
      "instabot"
    ],
    "preview": "# -*- coding: utf-8 -*-\n\nimport argparse\nimport os\nimport sys\nimport threading\nimport time\nfrom glob import glob\n\nimport config\n\nsys.path.append(os.path.join(sys.path[0], \"../../\"))\nimport schedule  # noqa: E402\nfrom instabot import Bot, utils  # noqa: E402\n\nbot = Bot(\n    comments_file=config.COMMENTS_FILE,\n    blacklist_file=config.BLACKLIST_FILE,\n    whitelist_file=config.WHITELIST_FILE,\n    friends_file=config.FRIENDS_FILE,",
    "last_modified": "2025-09-13T05:54:55.924954"
  },
  {
    "id": "2115",
    "name": "upscale 2.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/upscale 2.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 76,
    "size": 1849,
    "docstring": "",
    "keywords": [
      "web_tools"
    ],
    "functions": [],
    "classes": [],
    "imports": [
      "json",
      "os",
      "time",
      "requests"
    ],
    "preview": "import json\nimport os\nimport time\n\nimport requests\n\napi_key = os.getenv(\"API_KEY\")\nif not api_key:\n    raise ValueError(\n        \"API key is not set. Please ensure the API_KEY environment variable is configured correctly.\"\n    )\n\nauthorization = f\"Bearer {api_key}\"\n\nheaders = {\n    \"accept\": \"application/json\",\n    \"content-type\": \"application/json\",\n    \"authorization\": authorization,\n}\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2116",
    "name": "charsetprober.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/charsetprober.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 146,
    "size": 5414,
    "docstring": "",
    "keywords": [],
    "functions": [
      "__init__",
      "reset",
      "charset_name",
      "language",
      "feed",
      "state",
      "get_confidence",
      "filter_high_byte_only",
      "filter_international_words",
      "remove_xml_tags"
    ],
    "classes": [
      "CharSetProber"
    ],
    "imports": [
      "logging",
      "re",
      "typing",
      "enums"
    ],
    "preview": "######################## BEGIN LICENSE BLOCK ########################\n# The Original Code is Mozilla Universal charset detector code.\n#\n# The Initial Developer of the Original Code is\n# Netscape Communications Corporation.\n# Portions created by the Initial Developer are Copyright (C) 2001\n# the Initial Developer. All Rights Reserved.\n#\n# Contributor(s):\n#   Mark Pilgrim - port to Python\n#   Shy Shalom - original C code\n#\n# This library is free software; you can redistribute it and/or\n# modify it under the terms of the GNU Lesser General Public\n# License as published by the Free Software Foundation; either\n# version 2.1 of the License, or (at your option) any later version.\n#\n# This library is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2117",
    "name": "upscale--.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/upscale--.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 55,
    "size": 2017,
    "docstring": "",
    "keywords": [],
    "functions": [
      "upscale_image"
    ],
    "classes": [],
    "imports": [
      "os",
      "PIL",
      "tqdm"
    ],
    "preview": "import os\n\nfrom PIL import Image\nfrom tqdm import tqdm\n\n\n# Function to upscale an image by 2x and set resolution to 300 DPI\ndef upscale_image(input_path, output_path):\n    \"\"\"\n    Upscale an image by 2x and set resolution to 300 DPI.\n\n    Args:\n        input_path (str): Path to the input image file.\n        output_path (str): Path to save the upscaled image.\n\n    Returns:\n        None\n    \"\"\"\n    image = Image.open(input_path)\n    upscaled_image = image.resize((image.width * 2, image.height * 2), Image.BICUBIC)",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2118",
    "name": "botComment.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/botComment.py",
    "category": "02_media_processing",
    "type": "web_tools",
    "lines": 224,
    "size": 6739,
    "docstring": "Created in 12/2019\n@Author: Paulo https://github.com/alpdias",
    "keywords": [],
    "functions": [
      "functionComment",
      "botlogin",
      "findhashtag",
      "typephrase",
      "likecomment"
    ],
    "classes": [],
    "imports": [
      "os",
      "random",
      "pathlib",
      "time",
      "art",
      "selenium",
      "selenium.webdriver.common.keys"
    ],
    "preview": "# -*- coding: utf-8 -*-\n\n\"\"\"\nCreated in 12/2019\n@Author: Paulo https://github.com/alpdias\n\"\"\"\n\n# imported libraries\nimport os\nimport random\nfrom pathlib import Path\nfrom time import sleep\n\nimport art\nfrom selenium import webdriver\nfrom selenium.webdriver.common.keys import Keys\n\n\ndef functionComment(mySystem):\n    \"\"\"",
    "last_modified": "2025-09-13T05:54:06.694749"
  },
  {
    "id": "2119",
    "name": "img.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/img.py",
    "category": "02_media_processing",
    "type": "image_processing",
    "lines": 48,
    "size": 1316,
    "docstring": "",
    "keywords": [],
    "functions": [
      "is_system_path"
    ],
    "classes": [],
    "imports": [
      "datetime",
      "os"
    ],
    "preview": "import datetime\nimport os\n\n\ndef is_system_path(path):\n    system_paths = [\n        \"~/Desktop\",\n        \"/System\",\n        \"/Documents/Git\" \"/Applications\",\n        \"/Library\",\n        \"/usr\",\n        \"/bin\",\n        \"/sbin\",\n        \"/var\",\n        \"/private\",\n        \"/etc\",\n        \"/tmp\",\n        \"/.\",\n        \"/Python\",\n    ]",
    "last_modified": "2025-09-13T05:54:14.192869"
  },
  {
    "id": "2120",
    "name": "names.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/names.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 33,
    "size": 1139,
    "docstring": "",
    "keywords": [
      "web_tools"
    ],
    "functions": [],
    "classes": [],
    "imports": [
      "csv",
      "os",
      "requests",
      "slugify"
    ],
    "preview": "import csv\nimport os\n\nimport requests\nfrom slugify import slugify  # Install with: pip install python-slugify\n\n# Read CSV\nwith open(\"/Users/steven/Pictures/etsy/cookie/combined_csv.csv\", \"r\", encoding=\"utf-8\") as csvfile:\n    reader = csv.DictReader(csvfile)\n    for row in reader:\n        # Extract data\n        file_id = row[\"ID\"]\n        title = row[\"Listing.Title\"]\n        image_url = row[\"Main file_slot_image_url\"]  # Use the relevant URL column\n\n        # Skip rows without an image URL\n        if not image_url.strip():\n            continue\n\n        # Sanitize title for filename",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2121",
    "name": "main_20221230223947.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/main_20221230223947.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 127,
    "size": 3754,
    "docstring": "",
    "keywords": [
      "opencv",
      "analysis",
      "web_tools"
    ],
    "functions": [
      "generate"
    ],
    "classes": [],
    "imports": [
      "os",
      "sys",
      "time",
      "urllib.request",
      "io",
      "chromedriver_autoinstaller",
      "cv2",
      "imutils",
      "requests",
      "bs4"
    ],
    "preview": "import os\nimport sys\nimport time\nimport urllib.request\nfrom io import BytesIO, StringIO\n\n# Optional\nimport chromedriver_autoinstaller\nimport cv2\nimport imutils\nimport requests\nfrom bs4 import BeautifulSoup\nfrom selenium import webdriver\nfrom wand.display import display\nfrom wand.image import Image\n\nchromedriver_autoinstaller.install()\n\nopts = webdriver.ChromeOptions()\nopts.headless = True",
    "last_modified": "2025-09-13T05:54:14.142637"
  },
  {
    "id": "2122",
    "name": "web-png-upscale.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/web-png-upscale.py",
    "category": "02_media_processing",
    "type": "web_tools",
    "lines": 42,
    "size": 2008,
    "docstring": "",
    "keywords": [],
    "functions": [
      "convert_and_upscale_images"
    ],
    "classes": [],
    "imports": [
      "os",
      "PIL"
    ],
    "preview": "import os\n\nfrom PIL import Image\n\n\ndef convert_and_upscale_images(directory):\n    # Walk through all directories and files within the specified directory\n    for root, _, files in os.walk(directory):\n        print(f\"Checking directory: {root}\")  # Debug statement\n        for filename in files:\n            print(f\"Found file: {filename}\")  # Debug statement\n            if filename.lower().endswith(\".tiff\") or filename.lower().endswith(\".tif\"):\n                print(f\"Processing file: {filename}\")  # Debug statement\n                # Construct full file path\n                file_path = os.path.join(root, filename)\n                try:\n                    # Open the .tiff image\n                    with Image.open(file_path) as img:\n                        # Upscale the image by 2x\n                        img = img.resize((img.width * 2, img.height * 2), Image.LANCZOS)",
    "last_modified": "2025-09-13T05:55:28.646451"
  },
  {
    "id": "2123",
    "name": "attack.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/attack.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 308,
    "size": 9831,
    "docstring": "",
    "keywords": [
      "web_tools"
    ],
    "functions": [
      "random_str",
      "report_profile_attack",
      "report_video_attack"
    ],
    "classes": [],
    "imports": [
      "pprint",
      "random",
      "string",
      "sys",
      "libs.user_agents",
      "libs.utils",
      "requests"
    ],
    "preview": "import pprint\nimport random\nimport string\nfrom sys import exit\n\nfrom libs.user_agents import get_user_agent\nfrom libs.utils import ask_question, parse_proxy_file, print_error, print_status, print_success\nfrom requests import Session\n\npage_headers = {\n    \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\",\n    \"Accept-Encoding\": \"gzip, deflate\",\n    \"Accept-Language\": \"tr-TR,tr;q=0.8,en-US;q=0.5,en;q=0.3\",\n    \"Cache-Control\": \"no-cache\",\n    \"Connection\": \"keep-alive\",\n    \"DNT\": \"1\",\n}\n\nreport_headers = {\n    \"Accept\": \"*/*\",",
    "last_modified": "2025-09-13T05:53:40.882764"
  },
  {
    "id": "2124",
    "name": "upscale-sub copy.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/upscale-sub copy.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 68,
    "size": 2591,
    "docstring": "",
    "keywords": [],
    "functions": [
      "convert_and_upscale_images",
      "main"
    ],
    "classes": [],
    "imports": [
      "os",
      "PIL"
    ],
    "preview": "import os\n\nfrom PIL import Image\n\n\ndef convert_and_upscale_images(source_directory, destination_directory):\n    # Create the destination directory if it doesn't exist\n    os.makedirs(destination_directory, exist_ok=True)\n\n    for root, dirs, files in os.walk(source_directory):\n        # Replicate the directory structure in the destination\n        relative_path = os.path.relpath(root, source_directory)\n        dest_dir = os.path.join(destination_directory, relative_path)\n        os.makedirs(dest_dir, exist_ok=True)\n\n        for filename in files:\n            if filename.endswith((\".tiff\", \".png\", \".jpg\", \".jpeg\")):\n                source_file = os.path.join(root, filename)\n                filename_no_ext, file_ext = os.path.splitext(filename)\n                file_ext = file_ext.lower()",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2125",
    "name": "main_20221230234008.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/main_20221230234008.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 429,
    "size": 15518,
    "docstring": "",
    "keywords": [
      "transcription",
      "video_processing",
      "analysis",
      "youtube",
      "web_tools",
      "organization"
    ],
    "functions": [
      "downloadVideo",
      "scrapeVideos",
      "usedQuoteToDifferentFile",
      "getQuoteFromApi",
      "getQuoteFromTxtFile",
      "videoIntro",
      "createVideo",
      "audioClip",
      "randomBgMusic",
      "deleteTempFiles"
    ],
    "classes": [
      "bcolors"
    ],
    "imports": [
      "glob",
      "json",
      "os",
      "random",
      "subprocess",
      "gtts",
      "requests",
      "moviepy.editor",
      "moviepy.video.io.VideoFileClip",
      "mutagen.mp3"
    ],
    "preview": "import glob\nimport json\nimport os\nimport random\nimport subprocess\n\nimport gtts\nimport requests\nfrom moviepy.editor import *\nfrom moviepy.video.io.VideoFileClip import VideoFileClip\nfrom mutagen.mp3 import MP3\nfrom tqdm.auto import tqdm\n\n\n# download background video from pexels - https://www.pexels.com/api/documentation/#videos-search__parameters\ndef downloadVideo(id) -> str:\n    \"\"\"Downloads video from Pexels with the according video ID\"\"\"\n    url = \"https://www.pexels.com/video/\" + str(id) + \"/download.mp4\"\n    # Streaming, so we can iterate over the response.\n    response = requests.get(url, stream=True)",
    "last_modified": "2025-09-13T05:53:57.329705"
  },
  {
    "id": "2126",
    "name": "playlist_20221230180628.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/playlist_20221230180628.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 168,
    "size": 6451,
    "docstring": "",
    "keywords": [
      "youtube",
      "analysis",
      "web_tools"
    ],
    "functions": [
      "generate_token",
      "get_youtube_url",
      "download_mp3",
      "last_fm_artist_info",
      "set_metadata",
      "write_tracks",
      "write_playlist",
      "split_spotify_uri",
      "get_os",
      "get_folder"
    ],
    "classes": [],
    "imports": [
      "argparse",
      "json",
      "os",
      "re",
      "subprocess",
      "urllib.parse",
      "urllib.request",
      "secret",
      "spotipy",
      "spotipy.oauth2"
    ],
    "preview": "import argparse\nimport json\nimport os\nimport re\nimport subprocess\nimport urllib.parse\nimport urllib.request\n\nimport secret\nimport spotipy\nimport spotipy.oauth2 as oauth2\nfrom requests.utils import quote\n\n\ndef generate_token():\n    credentials = oauth2.SpotifyClientCredentials(\n        client_id=secret.28b20556906f4b75874c4ae98320c81d,\n        client_secret=secret.c7033fd14e1247cfb9eef73874dd2365 \n)\n    token = credentials.get_access_token()",
    "last_modified": "2025-05-04T23:28:21.394054"
  },
  {
    "id": "2127",
    "name": "list_csv.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/list_csv.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 16,
    "size": 497,
    "docstring": "",
    "keywords": [],
    "functions": [
      "list_files_to_csv"
    ],
    "classes": [],
    "imports": [
      "csv",
      "os"
    ],
    "preview": "import csv\nimport os\n\n\ndef list_files_to_csv(startpath, output_file):\n    with open(output_file, \"w\", newline=\"\") as file:\n        writer = csv.writer(file)\n        writer.writerow([\"Folder Path\", \"File Name\"])  # Optional: headers for the CSV columns\n        for root, dirs, files in os.walk(startpath):\n            for f in files:\n                writer.writerow([root, f])\n\n\n# Replace the path with your actual directory path\nlist_files_to_csv(\"/Users/steven/Pictures/3dMothers\", \"output.csv\")\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2128",
    "name": "leodown_20250102105147.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/leodown_20250102105147.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 51,
    "size": 1326,
    "docstring": "",
    "keywords": [
      "web_tools"
    ],
    "functions": [],
    "classes": [],
    "imports": [
      "json",
      "requests"
    ],
    "preview": "import json\n\nimport requests\n\n# Base URL for API\nbase_url = (\n    \"https://cloud.leonardo.ai/api/rest/v1/generations/user/f7bb8476-e3f0-4f1f-9a06-4600866cc49c\"\n)\nheaders = {\n    \"accept\": \"application/json\",\n    \"authorization\": \"Bearer 93043291-957d-4ec1-8c79-ee734abcb6e3\",  # Replace with your actual token\n}\n\n# Output file\noutput_file = \"/Users/steven/Pictures/leonardo_library.json\"\n\n# Pagination parameters\noffset = 0\nlimit = 10  # Maximum number of items per request\nall_generations = []",
    "last_modified": "2025-09-13T05:53:49.560267"
  },
  {
    "id": "2129",
    "name": "upload (1).py",
    "path": "github_repo/scripts/02_media_processing/image_tools/upload (1).py",
    "category": "02_media_processing",
    "type": "youtube",
    "lines": 115,
    "size": 2815,
    "docstring": "",
    "keywords": [
      "image_processing",
      "youtube",
      "web_tools"
    ],
    "functions": [
      "upload_image",
      "upscale_image",
      "get_upscaled_image",
      "main"
    ],
    "classes": [],
    "imports": [
      "json",
      "time",
      "requests"
    ],
    "preview": "import json\nimport time\n\nimport requests\n\n\ndef upload_image(api_key, image_file_path):\n    headers = {\n        \"accept\": \"application/json\",\n        \"content-type\": \"application/json\",\n        \"authorization\": f\"Bearer {api_key}\"\n    }\n\n    # Get a presigned URL for uploading an image\n    url = \"https://cloud.leonardo.ai/api/rest/v1/init-image\"\n\n    payload = {\"extension\": \"jpg\"}\n    response = requests.post(url, json=payload, headers=headers)\n\n    if response.status_code != 200:",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2130",
    "name": "mp3-mp4-coverimg copy.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/mp3-mp4-coverimg copy.py",
    "category": "02_media_processing",
    "type": "image_processing",
    "lines": 53,
    "size": 1737,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "get_cover_image",
      "convert_mp3_to_mp4",
      "process_directory"
    ],
    "classes": [],
    "imports": [
      "glob",
      "os",
      "moviepy.editor",
      "PIL",
      "sys"
    ],
    "preview": "import glob\nimport os\n\nfrom moviepy.editor import AudioFileClip, ImageClip\nfrom PIL import Image\n\n\ndef get_cover_image(file_name, cover_image_directory):\n    # Check for both JPG and PNG extensions\n    jpg_path = os.path.join(cover_image_directory, f\"{file_name}.jpg\")\n    png_path = os.path.join(cover_image_directory, f\"{file_name}.png\")\n\n    if os.path.exists(jpg_path):\n        return jpg_path\n    elif os.path.exists(png_path):\n        return png_path\n    else:\n        print(f\"Cover image not found for {file_name}. Please ensure the cover image exists.\")\n        return None\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2131",
    "name": "youtube_dl_echo 3.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/youtube_dl_echo 3.py",
    "category": "02_media_processing",
    "type": "youtube",
    "lines": 319,
    "size": 13139,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "logging",
      "asyncio",
      "json",
      "math",
      "os",
      "time",
      "pyrogram",
      "translation",
      "helper_funcs.chat_base",
      "helper_funcs.display_progress"
    ],
    "preview": "import logging\n\nlogging.basicConfig(\n    level=logging.DEBUG, format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\n)\nlogger = logging.getLogger(__name__)\n\nimport asyncio\nimport json\nimport math\nimport os\nimport time\n\n# the secret configuration specific things\nif bool(os.environ.get(\"WEBHOOK\", False)):\n    from sample_config import Config\nelse:\n    from config import Config\n\n# the Strings used for this \"thing\"",
    "last_modified": "2025-09-13T05:54:10.389097"
  },
  {
    "id": "2132",
    "name": "imgupscale.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/imgupscale.py",
    "category": "02_media_processing",
    "type": "image_processing",
    "lines": 69,
    "size": 2829,
    "docstring": "",
    "keywords": [],
    "functions": [
      "convert_and_upscale_images",
      "main"
    ],
    "classes": [],
    "imports": [
      "os",
      "PIL"
    ],
    "preview": "import os\n\nfrom PIL import Image, UnidentifiedImageError\n\n\n# Function to convert and upscale PNG and JPEG images by 200% with 300 DPI\ndef convert_and_upscale_images(source_directory, destination_directory, max_size_mb=8):\n    # Create the destination directory if it doesn't exist\n    os.makedirs(destination_directory, exist_ok=True)\n\n    for filename in os.listdir(source_directory):\n        if filename.lower().endswith((\".png\", \".jpeg\", \".jpg\")):\n            source_file = os.path.join(source_directory, filename)\n            filename_no_ext = os.path.splitext(filename)[0]\n            ext = filename.split(\".\")[-1].lower()\n            destination_file = os.path.join(destination_directory, f\"{filename_no_ext}.{ext}\")\n\n            try:\n                # Convert and upscale PNG or JPEG\n                with Image.open(source_file) as im:",
    "last_modified": "2025-09-13T05:53:55.286828"
  },
  {
    "id": "2133",
    "name": "create_video.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/create_video.py",
    "category": "02_media_processing",
    "type": "youtube",
    "lines": 84,
    "size": 2657,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "select_images_based_on_analysis",
      "create_video"
    ],
    "classes": [],
    "imports": [
      "os",
      "random",
      "moviepy.editor",
      "PIL",
      "sys"
    ],
    "preview": "import os\nimport random\n\nfrom moviepy.editor import *\nfrom PIL import Image  # Ensure Pillow is installed\n\n# Check Pillow version to avoid ANTIALIAS issues\nif hasattr(Image, \"Resampling\"):\n    ANTIALIAS = Image.Resampling.LANCZOS  # For Pillow 10+\nelse:\n    ANTIALIAS = Image.ANTIALIAS  # Fallback for older versions\n\n\ndef select_images_based_on_analysis(image_dir, keywords):\n    images = [\n        os.path.join(image_dir, img)\n        for img in os.listdir(image_dir)\n        if img.endswith((\".png\", \".jpg\"))\n    ]\n    if len(images) == 0:",
    "last_modified": "2025-09-13T05:53:42.804912"
  },
  {
    "id": "2134",
    "name": "parser.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/parser.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 80,
    "size": 2293,
    "docstring": "",
    "keywords": [],
    "functions": [
      "get_arg_parser"
    ],
    "classes": [],
    "imports": [
      "argparse"
    ],
    "preview": "import argparse\n\n\ndef get_arg_parser() -> argparse.ArgumentParser:\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        \"-g\",\n        \"--game\",\n        type=str,\n        default=\"fortnite\",\n        help=\"Declares for which game the compilation should be created. It uses fortnite as default\",\n        required=False,\n    )\n    parser.add_argument(\n        \"-ap\",\n        \"--asset_path\",\n        type=str,\n        default=\"assets\",\n        help=\"Path to the assets folder. If not declared it uses './assets' as default\",\n        required=False,",
    "last_modified": "2025-03-28T18:37:12.016423"
  },
  {
    "id": "2135",
    "name": "organize_albums 1.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/organize_albums 1.py",
    "category": "02_media_processing",
    "type": "organization",
    "lines": 74,
    "size": 3111,
    "docstring": "",
    "keywords": [
      "organization"
    ],
    "functions": [
      "organize_files"
    ],
    "classes": [],
    "imports": [
      "os",
      "shutil"
    ],
    "preview": "import os\nimport shutil\n\n# Define the base directory\nbase_dir = \"/Users/steven/Music/nocTurneMeLoDieS/MP3\"\n\n\n# Helper function to create folders and move files\ndef organize_files():\n    # List all files in the base directory\n    files = os.listdir(base_dir)\n\n    # Process each file\n    for file in files:\n        # Skip directories\n        if os.path.isdir(os.path.join(base_dir, file)):\n            continue\n\n        # Extract the base name (album name) from the file\n        if file.endswith(\".mp3\"):",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2136",
    "name": "twitchClips.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/twitchClips.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 223,
    "size": 7292,
    "docstring": "",
    "keywords": [
      "youtube",
      "web_tools"
    ],
    "functions": [
      "is_channel",
      "is_category",
      "get_loaded_page_content",
      "fetch_clips_category",
      "fetch_clips_channel",
      "remove_all_clips",
      "download_clip",
      "__init__",
      "print_info"
    ],
    "classes": [
      "Clip"
    ],
    "imports": [
      "logging",
      "os",
      "time",
      "urllib.request",
      "requests",
      "bs4",
      "moviepy.editor",
      "selenium",
      "selenium.webdriver.support.ui"
    ],
    "preview": "#!/usr/bin/env python\nimport logging\nimport os\nimport time\nimport urllib.request\n\nimport requests as rs\nfrom bs4 import BeautifulSoup as bs\nfrom moviepy.editor import *\nfrom selenium import webdriver\nfrom selenium.webdriver.support.ui import WebDriverWait\n\n\nclass Clip:\n    url: str  # Url of the clip\n    title: str  # Title of the clip\n    channelName: str  # Name of the channel\n    duration: str  # Duration in seconds of the clip\n\n    def __init__(self, url: str, title: str, channelName: str, duration: str):",
    "last_modified": "2025-09-13T05:53:45.254860"
  },
  {
    "id": "2137",
    "name": "loop-upscale.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/loop-upscale.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 58,
    "size": 2241,
    "docstring": "",
    "keywords": [
      "web_tools"
    ],
    "functions": [],
    "classes": [],
    "imports": [
      "json",
      "os",
      "time",
      "requests"
    ],
    "preview": "import json\nimport os\nimport time\n\nimport requests\n\napi_key = \"3ca7a396-64c0-4e22-8cad-b00bc972d4b3\"\nauthorization = \"Bearer %s\" % api_key\n\nheaders = {\n    \"accept\": \"application/json\",\n    \"content-type\": \"application/json\",\n    \"authorization\": authorization,\n}\n\n# Directory containing images\ndirectory_path = \"/Users/steven/Pictures/CookiMonster/1\"\n\n# Loop through each file in the directory\nfor filename in os.listdir(directory_path):",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2138",
    "name": "img-img-upscale (1).py",
    "path": "github_repo/scripts/02_media_processing/image_tools/img-img-upscale (1).py",
    "category": "02_media_processing",
    "type": "image_processing",
    "lines": 36,
    "size": 1608,
    "docstring": "",
    "keywords": [],
    "functions": [
      "convert_and_upscale_images"
    ],
    "classes": [],
    "imports": [
      "os",
      "PIL"
    ],
    "preview": "import os\n\nfrom PIL import Image\n\n\ndef convert_and_upscale_images(directory):\n    # Walk through all directories and files within the specified directory\n    for root, _, files in os.walk(directory):\n        for filename in files:\n            if filename.lower().endswith(\".jpg\") or filename.lower().endswith(\".jpg\"):\n                # Construct full file path\n                file_path = os.path.join(root, filename)\n                # Open the .jpg image\n                with Image.open(file_path) as img:\n                    # Upscale the image by 2x\n                    img = img.resize((img.width * 2, img.height * 2), Image.LANCZOS)\n                    # Set DPI to 300\n                    img.info[\"dpi\"] = (300, 300)\n                    # Convert the image mode to RGB (if not already in that mode)\n                    if img.mode != \"RGB\":",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2139",
    "name": "leoup.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/leoup.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 76,
    "size": 1849,
    "docstring": "",
    "keywords": [
      "web_tools"
    ],
    "functions": [],
    "classes": [],
    "imports": [
      "json",
      "os",
      "time",
      "requests"
    ],
    "preview": "import json\nimport os\nimport time\n\nimport requests\n\napi_key = os.getenv(\"API_KEY\")\nif not api_key:\n    raise ValueError(\n        \"API key is not set. Please ensure the API_KEY environment variable is configured correctly.\"\n    )\n\nauthorization = f\"Bearer {api_key}\"\n\nheaders = {\n    \"accept\": \"application/json\",\n    \"content-type\": \"application/json\",\n    \"authorization\": authorization,\n}\n",
    "last_modified": "2025-05-04T22:47:12.936647"
  },
  {
    "id": "2140",
    "name": "main_20221230223427.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/main_20221230223427.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 127,
    "size": 3768,
    "docstring": "",
    "keywords": [
      "opencv",
      "analysis",
      "web_tools"
    ],
    "functions": [
      "generate"
    ],
    "classes": [],
    "imports": [
      "os",
      "sys",
      "time",
      "urllib.request",
      "io",
      "chromedriver_autoinstaller",
      "cv2",
      "imutils",
      "requests",
      "bs4"
    ],
    "preview": "import os\nimport sys\nimport time\nimport urllib.request\nfrom io import BytesIO, StringIO\n\n# Optional\nimport chromedriver_autoinstaller\nimport cv2\nimport imutils\nimport requests\nfrom bs4 import BeautifulSoup\nfrom selenium import webdriver\nfrom wand.display import display\nfrom wand.image import Image\n\nchromedriver_autoinstaller.install()\n\nopts = webdriver.ChromeOptions()\nopts.headless = True",
    "last_modified": "2025-09-13T05:54:14.071402"
  },
  {
    "id": "2141",
    "name": "csv-url-down.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/csv-url-down.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 36,
    "size": 1258,
    "docstring": "",
    "keywords": [
      "web_tools"
    ],
    "functions": [],
    "classes": [],
    "imports": [
      "csv",
      "os",
      "requests"
    ],
    "preview": "import csv\nimport os\n\nimport requests\n\n# Set the output directory\noutput_directory = \"downloaded_files\"\nos.makedirs(output_directory, exist_ok=True)\n\n# Path to the CSV file containing URLs\ncsv_file_path = \"/Users/steven/Pictures/etsy/printify/mydesigns-export.CSV\"\n\n# Column name in the CSV that contains the URLs\nurl_column_name = \"url\"  # Adjust to match your CSV file's structure\n\n# Read the CSV file and download each URL\nwith open(csv_file_path, newline=\"\", encoding=\"utf-8\") as csvfile:\n    reader = csv.DictReader(csvfile)\n    for row in reader:\n        url = row[url_column_name]",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2142",
    "name": "mp3_to_mp4.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/mp3_to_mp4.py",
    "category": "02_media_processing",
    "type": "video_processing",
    "lines": 18,
    "size": 622,
    "docstring": "",
    "keywords": [],
    "functions": [
      "convert_to_mp4"
    ],
    "classes": [],
    "imports": [
      "os"
    ],
    "preview": "import os\n\n\ndef convert_to_mp4(image, audio, output):\n    cmd = f\"ffmpeg -loop 1 -i {image} -i {audio} -c:v libx264 -c:a aac -strict experimental -b:a 192k -shortest {output}\"\n    os.system(cmd)\n\n\n# Prompt the user for file paths\nimage_file = input(\"Enter the path to the image file (e.g., cover_image.jpg): \")\naudio_file = input(\"Enter the path to the MP3 file (e.g., your_music.mp3): \")\noutput_file = input(\"Enter the desired output file name (e.g., your_video.mp4): \")\n\n# Convert MP3 to MP4\nconvert_to_mp4(image_file, audio_file, output_file)\n\nprint(\"Conversion Complete. The MP4 file is ready for upload to YouTube.\")\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2143",
    "name": "YouTubeBot.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/YouTubeBot.py",
    "category": "02_media_processing",
    "type": "youtube",
    "lines": 164,
    "size": 4928,
    "docstring": "",
    "keywords": [
      "web_tools"
    ],
    "functions": [
      "fetch",
      "filter",
      "duration_split",
      "start"
    ],
    "classes": [],
    "imports": [
      "time",
      "tkinter",
      "tkinter.ttk",
      "pyautogui",
      "PIL",
      "selenium",
      "requests"
    ],
    "preview": "import time\nimport tkinter as tk\nimport tkinter.ttk as ttk\n\nimport pyautogui\nfrom PIL import Image, ImageTk\nfrom selenium import webdriver\n\nheight = pyautogui.size()[1]\nwidth = pyautogui.size()[0]\nprint(\"resolution = \" + str(width) + \", \" + str(height))\nwindow = tk.Tk()\nwindow.title(\"YouTube Bot\")\n\nwindow.resizable(0, 0)\nwindow.configure(background=\"white\")\nwindow.rowconfigure([0], minsize=round(width / 96), weight=0)\nwindow.columnconfigure([0, 2], minsize=round(width / 24), weight=0)\nwindow.columnconfigure(1, minsize=round(width / 2.13), weight=0)\n",
    "last_modified": "2025-08-10T20:33:59.047658"
  },
  {
    "id": "2144",
    "name": "lexica.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/lexica.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 36,
    "size": 1102,
    "docstring": "",
    "keywords": [
      "web_tools"
    ],
    "functions": [
      "get_image"
    ],
    "classes": [],
    "imports": [
      "json",
      "logging",
      "os",
      "time",
      "urllib",
      "pathlib",
      "urllib.request",
      "requests",
      "settings"
    ],
    "preview": "import json\nimport logging\nimport os\nimport time\nimport urllib\nfrom pathlib import Path\nfrom urllib.request import Request, urlopen\n\nimport requests\nimport settings\n\n\ndef get_image(file_path, sentence, number_of_images=1):\n\n    if settings.download_enabled:\n        if not os.path.exists(file_path):\n            safe_query = urllib.parse.quote(sentence.strip())\n            lexica_url = f\"https://lexica.art/api/v1/search?q={safe_query}\"\n            logging.info(f\"Downloading Image : {str(id)} - {sentence}\")\n            r = requests.get(lexica_url)",
    "last_modified": "2025-05-04T23:28:20.223267"
  },
  {
    "id": "2145",
    "name": "gallery_logic.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/gallery_logic.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 55,
    "size": 2267,
    "docstring": "",
    "keywords": [],
    "functions": [
      "get_gallery_logic",
      "get_gallery_type"
    ],
    "classes": [],
    "imports": [
      "simplegallery.common",
      "simplegallery.logic.variants.files_gallery_logic",
      "simplegallery.logic.variants.google_gallery_logic",
      "simplegallery.logic.variants.onedrive_gallery_logic"
    ],
    "preview": "import simplegallery.common as spg_common\nfrom simplegallery.logic.variants.files_gallery_logic import FilesGalleryLogic\nfrom simplegallery.logic.variants.google_gallery_logic import GoogleGalleryLogic\nfrom simplegallery.logic.variants.onedrive_gallery_logic import OnedriveGalleryLogic\n\n\ndef get_gallery_logic(gallery_config):\n    \"\"\"\n    Factory function that returns an object of a class derived from BaseGalleryLogic based on the gallery config.\n    Supported gallery logics:\n    - FilesGalleryLogic - logic for local files gallery\n    - OneDriveGallerLogic - logic for shared album from OneDrive\n    - GoogleGallerLogic - logic for shared album from Google Photos\n\n    :param gallery_config: gallery config dictionary as read from the gallery.json\n    :return: gallery logic object\n    \"\"\"\n    if \"remote_gallery_type\" not in gallery_config:\n        return FilesGalleryLogic(gallery_config)\n    elif not gallery_config[\"remote_gallery_type\"]:",
    "last_modified": "2025-09-13T05:53:52.487946"
  },
  {
    "id": "2146",
    "name": "YouTube Livestream Botter 2.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/YouTube Livestream Botter 2.py",
    "category": "02_media_processing",
    "type": "youtube",
    "lines": 171,
    "size": 7730,
    "docstring": "",
    "keywords": [
      "web_tools"
    ],
    "functions": [
      "bot",
      "__init__",
      "printservice",
      "update",
      "get_proxy",
      "FormatProxy",
      "__init__"
    ],
    "classes": [
      "main",
      "proxy"
    ],
    "imports": [
      "os",
      "platform",
      "random",
      "string",
      "threading",
      "time",
      "queue",
      "requests",
      "colorama"
    ],
    "preview": "import os\nimport platform\nimport random\nimport string\nimport threading\nimport time\nfrom queue import Queue\n\nimport requests\nfrom colorama import Fore, init\n\nintro = \"\"\"\n\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2588\u2557   \u2588\u2588\u2588\u2557      \u2588\u2588\u2588\u2588\u2588\u2588\u2557  \u2588\u2588\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2588\u2588\u2588\u2588\u2557 \n\u2588\u2588\u2554\u2550\u2550\u2550\u2550\u255d\u255a\u2550\u2550\u2588\u2588\u2554\u2550\u2550\u255d\u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2557\u2588\u2588\u2554\u2550\u2550\u2550\u2550\u255d\u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2557\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2588\u2588\u2551      \u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2557\u2588\u2588\u2554\u2550\u2550\u2550\u2588\u2588\u2557\u255a\u2550\u2550\u2588\u2588\u2554\u2550\u2550\u255d\u255a\u2550\u2550\u2588\u2588\u2554\u2550\u2550\u255d\u2588\u2588\u2554\u2550\u2550\u2550\u2550\u255d\u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2557\n\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557   \u2588\u2588\u2551   \u2588\u2588\u2588\u2588\u2588\u2588\u2554\u255d\u2588\u2588\u2588\u2588\u2588\u2557  \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2551\u2588\u2588\u2554\u2588\u2588\u2588\u2588\u2554\u2588\u2588\u2551\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2588\u2588\u2588\u2588\u2554\u255d\u2588\u2588\u2551   \u2588\u2588\u2551   \u2588\u2588\u2551      \u2588\u2588\u2551   \u2588\u2588\u2588\u2588\u2588\u2557  \u2588\u2588\u2588\u2588\u2588\u2588\u2554\u255d\n\u255a\u2550\u2550\u2550\u2550\u2588\u2588\u2551   \u2588\u2588\u2551   \u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2557\u2588\u2588\u2554\u2550\u2550\u255d  \u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2551\u2588\u2588\u2551\u255a\u2588\u2588\u2554\u255d\u2588\u2588\u2551\u255a\u2550\u2550\u2550\u2550\u255d\u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2557\u2588\u2588\u2551   \u2588\u2588\u2551   \u2588\u2588\u2551      \u2588\u2588\u2551   \u2588\u2588\u2554\u2550\u2550\u255d  \u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2557\n\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2551   \u2588\u2588\u2551   \u2588\u2588\u2551  \u2588\u2588\u2551\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2551  \u2588\u2588\u2551\u2588\u2588\u2551 \u255a\u2550\u255d \u2588\u2588\u2551      \u2588\u2588\u2588\u2588\u2588\u2588\u2554\u255d\u255a\u2588\u2588\u2588\u2588\u2588\u2588\u2554\u255d   \u2588\u2588\u2551      \u2588\u2588\u2551   \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2551  \u2588\u2588\u2551\n\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u255d   \u255a\u2550\u255d   \u255a\u2550\u255d  \u255a\u2550\u255d\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u255d\u255a\u2550\u255d  \u255a\u2550\u255d\u255a\u2550\u255d     \u255a\u2550\u255d      \u255a\u2550\u2550\u2550\u2550\u2550\u255d  \u255a\u2550\u2550\u2550\u2550\u2550\u255d    \u255a\u2550\u255d      \u255a\u2550\u255d   \u255a\u2550\u2550\u2550\u2550\u2550\u2550\u255d\u255a\u2550\u255d  \u255a\u2550\u255d\n\nhttps://github.com/KevinLage/YouTube-Livestream-Botter",
    "last_modified": "2025-08-06T14:23:44.339405"
  },
  {
    "id": "2147",
    "name": "upload_photos.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/upload_photos.py",
    "category": "02_media_processing",
    "type": "youtube",
    "lines": 96,
    "size": 2909,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "__future__",
      "argparse",
      "os",
      "sys",
      "captions_for_medias",
      "instabot",
      "glob",
      "random"
    ],
    "preview": "#!/usr/bin/python\n# - * - coding: utf-8 - * -\nfrom __future__ import unicode_literals\n\nimport argparse\nimport os\nimport sys\n\nimport captions_for_medias\n\nsys.path.append(os.path.join(sys.path[0], \"../../\"))\nfrom instabot import Bot  # noqa: E402\n\nparser = argparse.ArgumentParser(add_help=True)\nparser.add_argument(\"-u\", type=str, help=\"username\")\nparser.add_argument(\"-p\", type=str, help=\"password\")\nparser.add_argument(\"-proxy\", type=str, help=\"proxy\")\nparser.add_argument(\"-photo\", type=str, help=\"photo name\")\nparser.add_argument(\"-caption\", type=str, help=\"caption for photo\")\nparser.add_argument(\"-tag\", action=\"append\", help=\"taged user id\")",
    "last_modified": "2025-09-13T05:54:55.699322"
  },
  {
    "id": "2148",
    "name": "mp3-mp4-coverimg.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/mp3-mp4-coverimg.py",
    "category": "02_media_processing",
    "type": "image_processing",
    "lines": 53,
    "size": 1737,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "get_cover_image",
      "convert_mp3_to_mp4",
      "process_directory"
    ],
    "classes": [],
    "imports": [
      "glob",
      "os",
      "moviepy.editor",
      "PIL",
      "sys"
    ],
    "preview": "import glob\nimport os\n\nfrom moviepy.editor import AudioFileClip, ImageClip\nfrom PIL import Image\n\n\ndef get_cover_image(file_name, cover_image_directory):\n    # Check for both JPG and PNG extensions\n    jpg_path = os.path.join(cover_image_directory, f\"{file_name}.jpg\")\n    png_path = os.path.join(cover_image_directory, f\"{file_name}.png\")\n\n    if os.path.exists(jpg_path):\n        return jpg_path\n    elif os.path.exists(png_path):\n        return png_path\n    else:\n        print(f\"Cover image not found for {file_name}. Please ensure the cover image exists.\")\n        return None\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2149",
    "name": "codingstatemachine.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/codingstatemachine.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 91,
    "size": 3732,
    "docstring": "",
    "keywords": [],
    "functions": [
      "__init__",
      "reset",
      "next_state",
      "get_current_charlen",
      "get_coding_state_machine",
      "language"
    ],
    "classes": [
      "CodingStateMachine"
    ],
    "imports": [
      "logging",
      "codingstatemachinedict",
      "enums"
    ],
    "preview": "######################## BEGIN LICENSE BLOCK ########################\n# The Original Code is mozilla.org code.\n#\n# The Initial Developer of the Original Code is\n# Netscape Communications Corporation.\n# Portions created by the Initial Developer are Copyright (C) 1998\n# the Initial Developer. All Rights Reserved.\n#\n# Contributor(s):\n#   Mark Pilgrim - port to Python\n#\n# This library is free software; you can redistribute it and/or\n# modify it under the terms of the GNU Lesser General Public\n# License as published by the Free Software Foundation; either\n# version 2.1 of the License, or (at your option) any later version.\n#\n# This library is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU\n# Lesser General Public License for more details.",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2150",
    "name": "img2pdf.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/img2pdf.py",
    "category": "02_media_processing",
    "type": "image_processing",
    "lines": 39,
    "size": 1276,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "collect_image_files",
      "create_pdf_volumes",
      "main"
    ],
    "classes": [],
    "imports": [
      "os",
      "PIL"
    ],
    "preview": "import os\n\nfrom PIL import Image\n\n\ndef collect_image_files(source_directory):\n    image_files = []\n    for root, dirs, files in os.walk(source_directory):\n        for file in files:\n            if file.lower().endswith((\".png\", \".jpg\", \".jpeg\")):\n                file_path = os.path.join(root, file)\n                image_files.append(file_path)\n    return image_files\n\n\ndef create_pdf_volumes(image_files, target_directory, volume_size=100):\n    for i in range(0, len(image_files), volume_size):\n        volume_image_files = image_files[i : i + volume_size]\n        images = [Image.open(img).convert(\"RGB\") for img in volume_image_files]\n        volume_path = os.path.join(target_directory, f\"Image_Volume_{i//volume_size + 1}.pdf\")",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2151",
    "name": "scan_images--.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/scan_images--.py",
    "category": "02_media_processing",
    "type": "image_processing",
    "lines": 24,
    "size": 702,
    "docstring": "",
    "keywords": [],
    "functions": [
      "scan_directory"
    ],
    "classes": [],
    "imports": [
      "os"
    ],
    "preview": "import os\n\n# List of drives to scan\ndrives = [\"/Volumes/4t\", \"/Volumes/baKs\", \"/Volumes/iMac\", \"/Volumes/ogCho\"]\n# File types to look for\nfile_types = (\".jpg\", \".jpeg\", \".png\", \".gif\", \".bmp\")\n\n\n# Function to scan a directory for image files\ndef scan_directory(directory):\n    for root, dirs, files in os.walk(directory):\n        for file in files:\n            if file.lower().endswith(file_types):\n                yield os.path.join(root, file)\n\n\n# Open a file to write the paths\nwith open(\"image_paths.txt\", \"w\") as file:\n    for drive in drives:\n        for image_path in scan_directory(drive):",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2152",
    "name": "bot.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/bot.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 98,
    "size": 2704,
    "docstring": "",
    "keywords": [
      "web_tools"
    ],
    "functions": [
      "DLimage"
    ],
    "classes": [],
    "imports": [
      "math",
      "time",
      "urllib.request",
      "keyboard",
      "praw",
      "requests",
      "InstagramAPI",
      "PIL"
    ],
    "preview": "import math\nimport time\nimport urllib.request\n\nimport keyboard\nimport praw\nimport requests\nfrom InstagramAPI import InstagramAPI\nfrom PIL import Image\n\n# put it IG username/password\napi = InstagramAPI(\"username\", \"password\")\napi.login()\n\n\n# make a reddit acount and look up how to find this stuff. its called PRAW\nreddit = praw.Reddit(client_id=\"\", client_secret=\"\", username=\"\", password=\"\", user_agent=\"chrome\")\n\n\ndef DLimage(url, filePath, fileName):",
    "last_modified": "2025-09-13T05:55:13.854469"
  },
  {
    "id": "2153",
    "name": "best-csv.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/best-csv.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 139,
    "size": 5042,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "categorize_image",
      "process_images",
      "main"
    ],
    "classes": [],
    "imports": [
      "csv",
      "os",
      "datetime",
      "PIL"
    ],
    "preview": "import csv\nimport os\nfrom datetime import datetime\n\nfrom PIL import Image, UnidentifiedImageError\n\n# \ud83c\udfaf Bestselling Product Categories for Etsy & TikTok\nPLATFORMS = {\n    \"tiktok\": {\n        \"hoodie\": [\"bold colors\", \"dark tones\", \"statement text\"],\n        \"t-shirt\": [\"minimalist\", \"memes\", \"high contrast\"],\n        \"tote bag\": [\"artistic\", \"neutral tones\", \"simple graphics\"],\n        \"phone case\": [\"vibrant\", \"pop culture\", \"sharp details\"],\n        \"sticker\": [\"high contrast\", \"small details\", \"text-heavy\"],\n        \"candle\": [\"aesthetic\", \"soft colors\", \"cozy themes\"],\n        \"plush blanket\": [\"soft tones\", \"cozy aesthetics\", \"neutral patterns\"],\n    },\n    \"etsy\": {\n        \"ceramic mug\": [\"custom text\", \"personalized gifts\", \"vintage aesthetics\"],\n        \"cotton tee\": [\"affordable\", \"durable\", \"versatile for daily wear\"],",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2154",
    "name": "cli.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/cli.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 181,
    "size": 7544,
    "docstring": "",
    "keywords": [
      "youtube"
    ],
    "functions": [
      "main"
    ],
    "classes": [],
    "imports": [
      "argparse",
      "logging",
      "os",
      "pkg_resources",
      "youtube_bulk_upload"
    ],
    "preview": "#!/usr/bin/env python\nimport argparse\nimport logging\nimport os\n\nimport pkg_resources\nfrom youtube_bulk_upload import YouTubeBulkUpload\n\n\ndef main():\n    logger = logging.getLogger(__name__)\n    log_handler = logging.StreamHandler()\n    log_formatter = logging.Formatter(\n        fmt=\"%(asctime)s.%(msecs)03d - %(levelname)s - %(module)s - %(message)s\",\n        datefmt=\"%Y-%m-%d %H:%M:%S\",\n    )\n    log_handler.setFormatter(log_formatter)\n    logger.addHandler(log_handler)\n\n    package_version = pkg_resources.get_distribution(\"youtube-bulk-upload\").version",
    "last_modified": "2025-09-13T05:53:46.490664"
  },
  {
    "id": "2155",
    "name": "common.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/common.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 35,
    "size": 792,
    "docstring": "",
    "keywords": [],
    "functions": [
      "log",
      "read_gallery_config",
      "__init__"
    ],
    "classes": [
      "SPGException"
    ],
    "imports": [
      "json"
    ],
    "preview": "import json\n\n\nclass SPGException(Exception):\n    \"\"\"Exception raised for errors during the createion of the Simple Photo Gallery.\n\n    Attributes:\n        message -- explanation of the error that will be shown to the user\n    \"\"\"\n\n    def __init__(self, message):\n        super().__init__()\n        self.message = message\n\n\ndef log(message):\n    \"\"\"\n    Log a message to the console\n    :param message: message string\n    \"\"\"",
    "last_modified": "2025-05-04T22:47:13"
  },
  {
    "id": "2156",
    "name": "image_test.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/image_test.py",
    "category": "02_media_processing",
    "type": "image_processing",
    "lines": 12,
    "size": 615,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "clips"
    ],
    "preview": "from clips import *\n\nimg = Image.fromarray(\n    gen_comment_image(\n        author=\"me\",\n        content=\"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Pellentesque vestibulum quis diam nec faucibus. Ut aliquet massa justo. Pellentesque habitant morbi tristique senectus et netus et malesuada fames ac turpis egestas. Pellentesque lacinia ipsum non suscipit ultrices. Cras finibus erat vel nulla ullamcorper tempor. Quisque laoreet imperdiet urna non ultrices. Nulla nibh diam, feugiat sit amet est eget, tincidunt hendrerit massa. Ut molestie urna eget ornare pulvinar.\",\n    ),\n    \"RGB\",\n)\n\nimg.show()\n",
    "last_modified": "2025-05-04T23:27:53.330588"
  },
  {
    "id": "2157",
    "name": "organize_albums 5.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/organize_albums 5.py",
    "category": "02_media_processing",
    "type": "organization",
    "lines": 62,
    "size": 2470,
    "docstring": "",
    "keywords": [
      "organization"
    ],
    "functions": [
      "organize_files"
    ],
    "classes": [],
    "imports": [
      "os",
      "shutil"
    ],
    "preview": "import os\nimport shutil\n\n# Define the base directory\nbase_dir = \"/Users/steven/Music/NocTurnE-meLoDieS/albums/\"\n\n\n# Helper function to create folders and move files\ndef organize_files():\n    # List all files in the base directory\n    files = os.listdir(base_dir)\n\n    # Process each file\n    for file in files:\n        # Skip directories\n        if os.path.isdir(os.path.join(base_dir, file)):\n            continue\n\n        # Extract the base name (album name) from the file\n        if file.endswith(\".mp3\"):",
    "last_modified": "2025-05-04T22:47:11.429396"
  },
  {
    "id": "2158",
    "name": "gemini_grabber.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/gemini_grabber.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 285,
    "size": 9033,
    "docstring": "",
    "keywords": [
      "youtube",
      "web_tools"
    ],
    "functions": [
      "sanitize",
      "extract_image_urls",
      "download_assets",
      "load_urls",
      "main"
    ],
    "classes": [],
    "imports": [
      "asyncio",
      "argparse",
      "re",
      "time",
      "pathlib",
      "urllib.parse",
      "requests",
      "bs4",
      "playwright.async_api",
      "json"
    ],
    "preview": "import asyncio\nimport argparse\nimport re\nimport time\nfrom pathlib import Path\nfrom urllib.parse import urlparse, urljoin\n\nimport requests\nfrom bs4 import BeautifulSoup\nfrom playwright.async_api import async_playwright, TimeoutError as PWTimeout\n\nWAIT_STRATEGIES = [\"networkidle\", \"load\", \"domcontentloaded\"]\n\nBUTTON_TEXTS = [\n    \"Expand\",\n    \"See more\",\n    \"Show more\",\n    \"Continue\",\n    \"Next\",\n    \"More\",",
    "last_modified": "2025-09-13T05:54:30.982854"
  },
  {
    "id": "2159",
    "name": "BeautifulSoup.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/BeautifulSoup.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 1934,
    "size": 76366,
    "docstring": "Beautiful Soup",
    "keywords": [],
    "functions": [
      "setup",
      "replaceWith",
      "extract",
      "_lastRecursiveChild",
      "insert",
      "append",
      "findNext",
      "findAllNext",
      "findNextSibling",
      "findNextSiblings"
    ],
    "classes": [
      "PageElement",
      "NavigableString(unicode, PageElement)",
      "CData(NavigableString)",
      "ProcessingInstruction(NavigableString)",
      "Comment(NavigableString)",
      "Declaration(NavigableString)",
      "Tag(PageElement)",
      "SoupStrainer",
      "ResultSet(list)",
      "BeautifulStoneSoup(Tag, SGMLParser)",
      "has some tricks for dealing with some HTML that kills",
      "BeautifulSoup(BeautifulStoneSoup)",
      "StopParsing(Exception)",
      "ICantBelieveItsBeautifulSoup(BeautifulSoup)",
      "MinimalSoup(BeautifulSoup)",
      "BeautifulSOAP(BeautifulStoneSoup)",
      "RobustXMLParser(BeautifulStoneSoup)",
      "RobustHTMLParser(BeautifulSoup)",
      "RobustWackAssHTMLParser(ICantBelieveItsBeautifulSoup)",
      "RobustInsanelyWackAssHTMLParser(MinimalSoup)",
      "SimplifyingSOAPParser(BeautifulSOAP)",
      "UnicodeDammit"
    ],
    "imports": [
      "__future__",
      "codecs",
      "re",
      "types",
      "sgmllib",
      "sgmllib",
      "htmlentitydefs",
      "chardet",
      "cjkcodecs.aliases",
      "iconv_codec"
    ],
    "preview": "\"\"\"Beautiful Soup\nElixir and Tonic\n\"The Screen-Scraper's Friend\"\nhttp://www.crummy.com/software/BeautifulSoup/\n\nBeautiful Soup parses a (possibly invalid) XML or HTML document into a\ntree representation. It provides methods and Pythonic idioms that make\nit easy to navigate, search, and modify the tree.\n\nA well-formed XML/HTML document yields a well-formed data\nstructure. An ill-formed XML/HTML document yields a correspondingly\nill-formed data structure. If your document is only locally\nwell-formed, you can use this library to find and process the\nwell-formed part of it.\n\nBeautiful Soup works with Python 2.2 and up. It has no external\ndependencies, but you'll have more success at converting data to UTF-8\nif you also install these three packages:\n\n* chardet, for auto-detecting character encodings",
    "last_modified": "2025-05-04T23:28:20.748307"
  },
  {
    "id": "2160",
    "name": "imgconvert copy.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/imgconvert copy.py",
    "category": "02_media_processing",
    "type": "image_processing",
    "lines": 53,
    "size": 1729,
    "docstring": "",
    "keywords": [],
    "functions": [
      "convert_and_upscale_images",
      "main"
    ],
    "classes": [],
    "imports": [
      "os",
      "PIL"
    ],
    "preview": "import os\n\nfrom PIL import Image\n\n# Function to convert WebP images to PNG and upscale by 200% with 300 DPI\n\n\ndef convert_and_upscale_images(source_directory, destination_directory):\n    # Create the destination directory if it doesn't exist\n    os.makedirs(destination_directory, exist_ok=True)\n\n    for filename in os.listdir(source_directory):\n        if filename.endswith(\".png\"):\n            source_file = os.path.join(source_directory, filename)\n            filename_no_ext = os.path.splitext(filename)[0]\n            destination_file = os.path.join(destination_directory, f\"{filename_no_ext}.png\")\n\n            # Convert WebP to PNG and upscale by 200% with 300 DPI\n            im = Image.open(source_file)\n            width, height = im.size",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2161",
    "name": "good_commenting_strategy_and_new_qs_system.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/good_commenting_strategy_and_new_qs_system.py",
    "category": "02_media_processing",
    "type": "organization",
    "lines": 159,
    "size": 5153,
    "docstring": "This template is written by @the-unknown\n\nWhat does this quickstart script aim to do?\n- This is my template which includes the new QS system.\n  It includes a randomizer for my hashtags... with every run, it selects 10\n  random hashtags from the list.\n\nNOTES:\n- I am using the bot headless on my vServer and proxy into a Raspberry PI I\nhave at home, to always use my home IP to connect to Instagram.\n  In my comments, I always ask for feedback, use more than 4 words and\n  always have emojis.\n  My comments work very well, as I get a lot of feedback to my posts and\n  profile visits since I use this tactic.\n\n  As I target mainly active accounts, I use two unfollow methods.\n  The first will unfollow everyone who did not follow back within 12h.\n  The second one will unfollow the followers within 24h.",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "random",
      "instapy"
    ],
    "preview": "\"\"\"\nThis template is written by @the-unknown\n\nWhat does this quickstart script aim to do?\n- This is my template which includes the new QS system.\n  It includes a randomizer for my hashtags... with every run, it selects 10\n  random hashtags from the list.\n\nNOTES:\n- I am using the bot headless on my vServer and proxy into a Raspberry PI I\nhave at home, to always use my home IP to connect to Instagram.\n  In my comments, I always ask for feedback, use more than 4 words and\n  always have emojis.\n  My comments work very well, as I get a lot of feedback to my posts and\n  profile visits since I use this tactic.\n\n  As I target mainly active accounts, I use two unfollow methods.\n  The first will unfollow everyone who did not follow back within 12h.\n  The second one will unfollow the followers within 24h.\n\"\"\"",
    "last_modified": "2025-09-13T05:53:49.408731"
  },
  {
    "id": "2162",
    "name": "bulk_upload.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/bulk_upload.py",
    "category": "02_media_processing",
    "type": "youtube",
    "lines": 537,
    "size": 23244,
    "docstring": "",
    "keywords": [
      "analysis",
      "youtube",
      "web_tools"
    ],
    "functions": [
      "__init__",
      "find_input_files",
      "prompt_user_confirmation_or_raise_exception",
      "prompt_user_bool",
      "prompt_user_text",
      "validate_input_parameters",
      "authenticate_youtube",
      "get_channel_id",
      "check_if_video_title_exists_on_youtube_channel",
      "truncate_to_nearest_word"
    ],
    "classes": [
      "YouTubeBulkUpload"
    ],
    "imports": [
      "json",
      "logging",
      "os",
      "pickle",
      "re",
      "tempfile",
      "google.auth.transport.requests",
      "google_auth_oauthlib.flow",
      "googleapiclient.discovery",
      "googleapiclient.http"
    ],
    "preview": "import json\nimport logging\nimport os\nimport pickle\nimport re\nimport tempfile\n\nfrom google.auth.transport.requests import Request\nfrom google_auth_oauthlib.flow import InstalledAppFlow\nfrom googleapiclient.discovery import build\nfrom googleapiclient.http import MediaFileUpload\nfrom thefuzz import fuzz\n\nYOUTUBE_URL_PREFIX = \"https://www.youtube.com/watch?v=\"\n\n\nclass YouTubeBulkUpload:\n    def __init__(\n        self,\n        logger=None,",
    "last_modified": "2025-09-13T05:53:46.412163"
  },
  {
    "id": "2163",
    "name": "universe4.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/universe4.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 79,
    "size": 3118,
    "docstring": "",
    "keywords": [
      "web_tools"
    ],
    "functions": [],
    "classes": [],
    "imports": [
      "json",
      "os",
      "time",
      "requests"
    ],
    "preview": "import json\nimport os\nimport time\n\nimport requests\n\napi_key = \"3ca7a396-64c0-4e22-8cad-b00bc972d4b3\"\nauthorization = \"Bearer %s\" % api_key\n\nheaders = {\n    \"accept\": \"application/json\",\n    \"content-type\": \"application/json\",\n    \"authorization\": authorization,\n}\n\n# Directory containing images\ndirectory_path = \"/Users/steven/Pictures/CookiMonster/cookie-midnight\"\n\n# Styles to apply\n# Update the styles list according to your needs",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2164",
    "name": "sort copy.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/sort copy.py",
    "category": "02_media_processing",
    "type": "organization",
    "lines": 61,
    "size": 2397,
    "docstring": "",
    "keywords": [
      "organization"
    ],
    "functions": [
      "sort_images"
    ],
    "classes": [],
    "imports": [
      "os",
      "shutil",
      "datetime",
      "PIL"
    ],
    "preview": "import os\nimport shutil\nfrom datetime import datetime\n\nfrom PIL import Image\n\n\ndef sort_images(source_dir, target_dir):\n    # Create the target directory if it doesn't exist\n    os.makedirs(target_dir, exist_ok=True)\n\n    # Supported image formats\n    extensions = (\".png\", \".jpg\", \".jpeg\", \".tiff\")\n\n    # Walk through the directory\n    for root, dirs, files in os.walk(source_dir):\n        for file in files:\n            if file.lower().endswith(extensions):\n                try:\n                    # Construct the full file path",
    "last_modified": "2025-09-13T05:53:42.963445"
  },
  {
    "id": "2165",
    "name": "config_20241204083635.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/config_20241204083635.py",
    "category": "02_media_processing",
    "type": "setup",
    "lines": 3,
    "size": 56,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [],
    "preview": "# config.py\nSOURCE_DIRECTORY = \"/Users/steven/Pictures\"\n",
    "last_modified": "2024-12-13T00:56:52.178525"
  },
  {
    "id": "2166",
    "name": "pdf2img.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/pdf2img.py",
    "category": "02_media_processing",
    "type": "image_processing",
    "lines": 20,
    "size": 531,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "os",
      "pdf2image"
    ],
    "preview": "import os\n\nfrom pdf2image import convert_from_path\n\n# Path to your PDF file\npdf_path = \"/Users/steven/Documents/tesla/1-ocr.pdf\"\n\n# Output directory for images\noutput_dir = \"/Users/steven/Documents/tesla/output_images\"\nos.makedirs(output_dir, exist_ok=True)\n\n# Convert PDF to images\nimages = convert_from_path(pdf_path, dpi=300)\n\n# Save images to the output directory\nfor i, image in enumerate(images):\n    image_path = os.path.join(output_dir, f\"page_{i+1}.png\")\n    image.save(image_path, \"PNG\")\n    print(f\"Saved {image_path}\")\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2167",
    "name": "main_20221230233748.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/main_20221230233748.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 429,
    "size": 15514,
    "docstring": "",
    "keywords": [
      "transcription",
      "video_processing",
      "analysis",
      "youtube",
      "web_tools",
      "organization"
    ],
    "functions": [
      "downloadVideo",
      "scrapeVideos",
      "usedQuoteToDifferentFile",
      "getQuoteFromApi",
      "getQuoteFromTxtFile",
      "videoIntro",
      "createVideo",
      "audioClip",
      "randomBgMusic",
      "deleteTempFiles"
    ],
    "classes": [
      "bcolors"
    ],
    "imports": [
      "glob",
      "json",
      "os",
      "random",
      "subprocess",
      "gtts",
      "requests",
      "moviepy.editor",
      "moviepy.video.io.VideoFileClip",
      "mutagen.mp3"
    ],
    "preview": "import glob\nimport json\nimport os\nimport random\nimport subprocess\n\nimport gtts\nimport requests\nfrom moviepy.editor import *\nfrom moviepy.video.io.VideoFileClip import VideoFileClip\nfrom mutagen.mp3 import MP3\nfrom tqdm.auto import tqdm\n\n\n# download background video from pexels - https://www.pexels.com/api/documentation/#videos-search__parameters\ndef downloadVideo(id) -> str:\n    \"\"\"Downloads video from Pexels with the according video ID\"\"\"\n    url = \"https://www.pexels.com/video/\" + str(id) + \"/download.mp4\"\n    # Streaming, so we can iterate over the response.\n    response = requests.get(url, stream=True)",
    "last_modified": "2025-09-13T05:53:57.141459"
  },
  {
    "id": "2168",
    "name": "escprober.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/escprober.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 98,
    "size": 3985,
    "docstring": "",
    "keywords": [],
    "functions": [
      "__init__",
      "reset",
      "charset_name",
      "language",
      "get_confidence",
      "feed"
    ],
    "classes": [
      "EscCharSetProber"
    ],
    "imports": [
      "typing",
      "charsetprober",
      "codingstatemachine",
      "enums",
      "escsm"
    ],
    "preview": "######################## BEGIN LICENSE BLOCK ########################\n# The Original Code is mozilla.org code.\n#\n# The Initial Developer of the Original Code is\n# Netscape Communications Corporation.\n# Portions created by the Initial Developer are Copyright (C) 1998\n# the Initial Developer. All Rights Reserved.\n#\n# Contributor(s):\n#   Mark Pilgrim - port to Python\n#\n# This library is free software; you can redistribute it and/or\n# modify it under the terms of the GNU Lesser General Public\n# License as published by the Free Software Foundation; either\n# version 2.1 of the License, or (at your option) any later version.\n#\n# This library is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU\n# Lesser General Public License for more details.",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2169",
    "name": "test_gallery_init.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/test_gallery_init.py",
    "category": "02_media_processing",
    "type": "testing",
    "lines": 227,
    "size": 7778,
    "docstring": "",
    "keywords": [
      "testing"
    ],
    "functions": [
      "check_gallery_files",
      "test_nonexisting_gallery_path",
      "test_existing_gallery_no_force",
      "check_gallery_config",
      "test_new_gallery_created",
      "test_existing_gallery_override",
      "test_default_gallery_config",
      "test_new_onedrive_gallery_created",
      "test_new_google_gallery_created",
      "test_new_invalid_remote_gallery"
    ],
    "classes": [
      "SPGInitTestCase"
    ],
    "imports": [
      "json",
      "os",
      "sys",
      "unittest",
      "unittest",
      "simplegallery.gallery_init",
      "testfixtures"
    ],
    "preview": "import json\nimport os\nimport sys\nimport unittest\nfrom unittest import mock\n\nimport simplegallery.gallery_init as gallery_init\nfrom testfixtures import TempDirectory\n\n\ndef check_gallery_files(tempdir, files_photos, files_other):\n    tempdir.compare([\"templates\", \"public\", \"gallery.json\"] + files_other, recursive=False)\n    tempdir.compare(\n        [\"index_template.jinja\", \"gallery_macros.jinja\"],\n        path=\"templates\",\n        recursive=False,\n    )\n    tempdir.compare([\"css\", \"images\", \"js\"], path=\"public\", recursive=False)\n    tempdir.compare([\".empty\"] + files_photos, path=\"public/images/photos\")\n",
    "last_modified": "2025-09-13T05:53:53.046106"
  },
  {
    "id": "2170",
    "name": "move-date.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/move-date.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 29,
    "size": 992,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "os",
      "shutil",
      "datetime"
    ],
    "preview": "import os\nimport shutil\nfrom datetime import datetime\n\n# Set the source directory where your images are stored\nsource_dir = \"/path/to/your/images\"\n\n# Loop through each file in the source directory\nfor filename in os.listdir(source_dir):\n    if filename.lower().endswith((\".png\", \".jpg\", \".jpeg\", \".gif\", \".bmp\")):\n        # Get the full path to the file\n        file_path = os.path.join(source_dir, filename)\n\n        # Get the creation time and convert it to a year\n        creation_time = os.path.getctime(file_path)\n        year = datetime.fromtimestamp(creation_time).strftime(\"%Y\")\n\n        # Define the destination directory based on the year\n        dest_dir = os.path.join(source_dir, year)\n",
    "last_modified": "2025-03-28T18:37:08"
  },
  {
    "id": "2171",
    "name": "sort2.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/sort2.py",
    "category": "02_media_processing",
    "type": "organization",
    "lines": 87,
    "size": 3759,
    "docstring": "",
    "keywords": [
      "organization"
    ],
    "functions": [
      "sort_files"
    ],
    "classes": [],
    "imports": [
      "os",
      "shutil",
      "datetime",
      "mutagen.mp3",
      "mutagen.mp4",
      "PIL"
    ],
    "preview": "import os\nimport shutil\nfrom datetime import datetime\n\nfrom mutagen.mp3 import MP3\nfrom mutagen.mp4 import MP4\nfrom PIL import Image\n\n\ndef sort_files(source_dir, target_dir):\n    # Create the target directory if it doesn't exist\n    os.makedirs(target_dir, exist_ok=True)\n\n    # Supported image and media formats\n    image_extensions = (\".png\", \".jpg\", \".jpeg\", \".tiff\", \".webp\")\n    media_extensions = (\".mp3\", \".mp4\")\n\n    # Walk through the directory\n    for root, dirs, files in os.walk(source_dir):\n        for file in files:",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2172",
    "name": "YouTube Livestream Botter 3.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/YouTube Livestream Botter 3.py",
    "category": "02_media_processing",
    "type": "youtube",
    "lines": 171,
    "size": 7730,
    "docstring": "",
    "keywords": [
      "web_tools"
    ],
    "functions": [
      "bot",
      "__init__",
      "printservice",
      "update",
      "get_proxy",
      "FormatProxy",
      "__init__"
    ],
    "classes": [
      "main",
      "proxy"
    ],
    "imports": [
      "os",
      "platform",
      "random",
      "string",
      "threading",
      "time",
      "queue",
      "requests",
      "colorama"
    ],
    "preview": "import os\nimport platform\nimport random\nimport string\nimport threading\nimport time\nfrom queue import Queue\n\nimport requests\nfrom colorama import Fore, init\n\nintro = \"\"\"\n\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2588\u2557   \u2588\u2588\u2588\u2557      \u2588\u2588\u2588\u2588\u2588\u2588\u2557  \u2588\u2588\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2588\u2588\u2588\u2588\u2557 \n\u2588\u2588\u2554\u2550\u2550\u2550\u2550\u255d\u255a\u2550\u2550\u2588\u2588\u2554\u2550\u2550\u255d\u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2557\u2588\u2588\u2554\u2550\u2550\u2550\u2550\u255d\u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2557\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2588\u2588\u2551      \u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2557\u2588\u2588\u2554\u2550\u2550\u2550\u2588\u2588\u2557\u255a\u2550\u2550\u2588\u2588\u2554\u2550\u2550\u255d\u255a\u2550\u2550\u2588\u2588\u2554\u2550\u2550\u255d\u2588\u2588\u2554\u2550\u2550\u2550\u2550\u255d\u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2557\n\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557   \u2588\u2588\u2551   \u2588\u2588\u2588\u2588\u2588\u2588\u2554\u255d\u2588\u2588\u2588\u2588\u2588\u2557  \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2551\u2588\u2588\u2554\u2588\u2588\u2588\u2588\u2554\u2588\u2588\u2551\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2588\u2588\u2588\u2588\u2554\u255d\u2588\u2588\u2551   \u2588\u2588\u2551   \u2588\u2588\u2551      \u2588\u2588\u2551   \u2588\u2588\u2588\u2588\u2588\u2557  \u2588\u2588\u2588\u2588\u2588\u2588\u2554\u255d\n\u255a\u2550\u2550\u2550\u2550\u2588\u2588\u2551   \u2588\u2588\u2551   \u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2557\u2588\u2588\u2554\u2550\u2550\u255d  \u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2551\u2588\u2588\u2551\u255a\u2588\u2588\u2554\u255d\u2588\u2588\u2551\u255a\u2550\u2550\u2550\u2550\u255d\u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2557\u2588\u2588\u2551   \u2588\u2588\u2551   \u2588\u2588\u2551      \u2588\u2588\u2551   \u2588\u2588\u2554\u2550\u2550\u255d  \u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2557\n\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2551   \u2588\u2588\u2551   \u2588\u2588\u2551  \u2588\u2588\u2551\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2551  \u2588\u2588\u2551\u2588\u2588\u2551 \u255a\u2550\u255d \u2588\u2588\u2551      \u2588\u2588\u2588\u2588\u2588\u2588\u2554\u255d\u255a\u2588\u2588\u2588\u2588\u2588\u2588\u2554\u255d   \u2588\u2588\u2551      \u2588\u2588\u2551   \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2551  \u2588\u2588\u2551\n\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u255d   \u255a\u2550\u255d   \u255a\u2550\u255d  \u255a\u2550\u255d\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u255d\u255a\u2550\u255d  \u255a\u2550\u255d\u255a\u2550\u255d     \u255a\u2550\u255d      \u255a\u2550\u2550\u2550\u2550\u2550\u255d  \u255a\u2550\u2550\u2550\u2550\u2550\u255d    \u255a\u2550\u255d      \u255a\u2550\u255d   \u255a\u2550\u2550\u2550\u2550\u2550\u2550\u255d\u255a\u2550\u255d  \u255a\u2550\u255d\n\nhttps://github.com/KevinLage/YouTube-Livestream-Botter",
    "last_modified": "2025-08-06T14:19:39.936929"
  },
  {
    "id": "2173",
    "name": "removebg2.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/removebg2.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 30,
    "size": 854,
    "docstring": "",
    "keywords": [],
    "functions": [
      "remove_background"
    ],
    "classes": [],
    "imports": [
      "os",
      "rembg"
    ],
    "preview": "import os\n\nfrom rembg import remove\n\n# Suppress OpenMP warnings\nos.environ[\"OMP_NUM_THREADS\"] = \"1\"\nos.environ[\"KMP_WARNINGS\"] = \"0\"\n\n# Paths\ninput_path = \"/Users/steven/Pictures/etsy/Cookie-all/Tzip/t cut/14.png\"\noutput_path = \"/Users/steven/Pictures/etsy/Cookie-all/Tzip/t cut/14_no_bg.png\"\n\n\n# Function to remove background\ndef remove_background(input_file, output_file):\n    try:\n        with open(input_file, \"rb\") as file:\n            input_data = file.read()\n        output_data = remove(input_data)  # Process image with rembg\n        with open(output_file, \"wb\") as file:",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2174",
    "name": "resizer.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/resizer.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 88,
    "size": 2567,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "parse_args",
      "process_image",
      "main"
    ],
    "classes": [],
    "imports": [
      "argparse",
      "math",
      "os",
      "pathlib",
      "PIL"
    ],
    "preview": "#!/usr/bin/env python3\nimport argparse\nimport math\nimport os\nfrom pathlib import Path\n\nfrom PIL import Image\n\n\ndef parse_args():\n    p = argparse.ArgumentParser(\n        description=\"Resize images \u2265 threshold to 300 DPI and scale them so they dip below the byte\u2010limit.\"\n    )\n    p.add_argument(\"input_dir\", type=Path, help=\"Root folder to scan (will recurse into subdirs)\")\n    p.add_argument(\n        \"--threshold\",\n        type=int,\n        default=9 * 1024 * 1024,\n        help=\"Only process files at or above this size in bytes (default 9 MB)\",\n    )",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2175",
    "name": "translation.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/translation.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 108,
    "size": 7729,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [
      "Translation"
    ],
    "imports": [],
    "preview": "class Translation(object):\n    START_TEXT = \"\"\"Hi Bro, This Is Telegrams Yet Another Sassiest Downloader com Renamer Bot Hit /help to know how to use me\n\u00a9 @RedbullFed\"\"\"\n    RENAME_403_ERR = \"\ud83e\udd23\ud83e\udd23 Bye Bye... My Dev Restricted You From here\"\n    ABS_TEXT = \"Go Away Stupid \ud83e\udd26\u200d\u2640\ufe0f.\"\n    # UPGRADE_TEXT = \"\ud83d\ude05 Ok Bie\"\n    UPGRADE_TEXT = \"\"\"MwK MegaBot Paid Plans\n-------\nPlan: FREE\nFilesize limit: 1500 MB\nDaily limit: UNLIMITED\nPrice \ud83c\udf0e: \u20b9 0 / 30 Days\nFEATURES:\n\ud83d\udc49 All Supported Video Formats of https, except HLS videos!\n\ud83d\udc49 Get a Telegram sticker as a Telegram downloadable media.\n\ud83d\udc49 Upload as file from any HTTP link, with custom thumbnail support.\n\ud83d\udc49 Get Low Speed Direct Download Link of any Telegram file.\n\n---@redbullfed----\"\"\"\n",
    "last_modified": "2025-09-13T05:53:44.822602"
  },
  {
    "id": "2176",
    "name": "test_gallery_logic.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/test_gallery_logic.py",
    "category": "02_media_processing",
    "type": "testing",
    "lines": 61,
    "size": 2340,
    "docstring": "",
    "keywords": [
      "testing"
    ],
    "functions": [
      "test_get_gallery_logic",
      "test_get_gallery_type"
    ],
    "classes": [
      "GalleryLogicTestCase"
    ],
    "imports": [
      "unittest",
      "simplegallery.logic.gallery_logic",
      "simplegallery.logic.variants.files_gallery_logic",
      "simplegallery.logic.variants.google_gallery_logic",
      "simplegallery.logic.variants.onedrive_gallery_logic"
    ],
    "preview": "import unittest\n\nimport simplegallery.logic.gallery_logic as gallery_logic\nfrom simplegallery.logic.variants.files_gallery_logic import FilesGalleryLogic\nfrom simplegallery.logic.variants.google_gallery_logic import GoogleGalleryLogic\nfrom simplegallery.logic.variants.onedrive_gallery_logic import OnedriveGalleryLogic\n\n\nclass GalleryLogicTestCase(unittest.TestCase):\n    def test_get_gallery_logic(self):\n        config_logic_mapping = [\n            (dict(), FilesGalleryLogic),\n            (dict(remote_gallery_type=\"\"), FilesGalleryLogic),\n            (dict(remote_gallery_type=\"onedrive\"), OnedriveGalleryLogic),\n            (dict(remote_gallery_type=\"google\"), GoogleGalleryLogic),\n            (dict(remote_gallery_type=\"aaaaaaaa\"), FilesGalleryLogic),\n        ]\n\n        for config_logic in config_logic_mapping:\n            self.assertIs(",
    "last_modified": "2025-09-13T05:53:52.768974"
  },
  {
    "id": "2177",
    "name": "resize_oct25.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/resize_oct25.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 67,
    "size": 2382,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "resize_image_to_target_size",
      "process_images"
    ],
    "classes": [],
    "imports": [
      "os",
      "sys",
      "logging",
      "PIL"
    ],
    "preview": "import os\nimport sys\nimport logging\nfrom PIL import Image, UnidentifiedImageError\n\n# \u2728 Constants\nTARGET_DPI = 300\nSIZE_THRESHOLD_MB = 9\n\ndef resize_image_to_target_size(image, output_path):\n    \"\"\"Resize an image to fit within target file size and maintain aspect ratio.\"\"\"\n    width, height = image.size\n    aspect_ratio = width / height\n\n    # Determine new dimensions based on aspect ratio\n    if width / height > aspect_ratio:\n        new_width = width\n        new_height = int(width / aspect_ratio)\n    else:\n        new_width = int(height * aspect_ratio)",
    "last_modified": "2025-10-09T02:50:08.561675"
  },
  {
    "id": "2178",
    "name": "utf8prober.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/utf8prober.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 83,
    "size": 2812,
    "docstring": "",
    "keywords": [],
    "functions": [
      "__init__",
      "reset",
      "charset_name",
      "language",
      "feed",
      "get_confidence"
    ],
    "classes": [
      "UTF8Prober"
    ],
    "imports": [
      "typing",
      "charsetprober",
      "codingstatemachine",
      "enums",
      "mbcssm"
    ],
    "preview": "######################## BEGIN LICENSE BLOCK ########################\n# The Original Code is mozilla.org code.\n#\n# The Initial Developer of the Original Code is\n# Netscape Communications Corporation.\n# Portions created by the Initial Developer are Copyright (C) 1998\n# the Initial Developer. All Rights Reserved.\n#\n# Contributor(s):\n#   Mark Pilgrim - port to Python\n#\n# This library is free software; you can redistribute it and/or\n# modify it under the terms of the GNU Lesser General Public\n# License as published by the Free Software Foundation; either\n# version 2.1 of the License, or (at your option) any later version.\n#\n# This library is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU\n# Lesser General Public License for more details.",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2179",
    "name": "test_bot_unlike.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/test_bot_unlike.py",
    "category": "02_media_processing",
    "type": "testing",
    "lines": 184,
    "size": 6569,
    "docstring": "",
    "keywords": [
      "testing"
    ],
    "functions": [
      "test_unlike",
      "test_unlike_comment",
      "test_unlike_medias",
      "test_unlike_media_comments",
      "test_unlike_user"
    ],
    "classes": [
      "TestBotFilter"
    ],
    "imports": [
      "pytest",
      "responses",
      "instabot.api.config",
      "test_bot",
      "test_variables",
      "unittest.mock",
      "mock"
    ],
    "preview": "import pytest\nimport responses\nfrom instabot.api.config import API_URL\n\nfrom .test_bot import TestBot\nfrom .test_variables import (\n    TEST_CAPTION_ITEM,\n    TEST_COMMENT_ITEM,\n    TEST_PHOTO_ITEM,\n    TEST_USERNAME_INFO_ITEM,\n)\n\ntry:\n    from unittest.mock import patch\nexcept ImportError:\n    from mock import patch\n\n\nclass TestBotFilter(TestBot):\n    @responses.activate",
    "last_modified": "2025-09-13T05:55:00.117017"
  },
  {
    "id": "2180",
    "name": "organize_albums 4.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/organize_albums 4.py",
    "category": "02_media_processing",
    "type": "organization",
    "lines": 62,
    "size": 2473,
    "docstring": "",
    "keywords": [
      "organization"
    ],
    "functions": [
      "organize_files"
    ],
    "classes": [],
    "imports": [
      "os",
      "shutil"
    ],
    "preview": "import os\nimport shutil\n\n# Define the base directory\nbase_dir = \"/Users/steven/Music/nocTurneMeLoDieS/assests/all\"\n\n\n# Helper function to create folders and move files\ndef organize_files():\n    # List all files in the base directory\n    files = os.listdir(base_dir)\n\n    # Process each file\n    for file in files:\n        # Skip directories\n        if os.path.isdir(os.path.join(base_dir, file)):\n            continue\n\n        # Extract the base name (album name) from the file\n        if file.endswith(\".mp3\"):",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2181",
    "name": "organize_albums1.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/organize_albums1.py",
    "category": "02_media_processing",
    "type": "organization",
    "lines": 74,
    "size": 3100,
    "docstring": "",
    "keywords": [
      "organization"
    ],
    "functions": [
      "organize_files"
    ],
    "classes": [],
    "imports": [
      "os",
      "shutil"
    ],
    "preview": "import os\nimport shutil\n\n# Define the base directory\nbase_dir = \"/Users/steven/Music/nocTurneMeLoDieS/MP3\"\n\n\n# Helper function to create folders and move files\ndef organize_files():\n    # List all files in the base directory\n    files = os.listdir(base_dir)\n\n    # Process each file\n    for file in files:\n        # Skip directories\n        if os.path.isdir(os.path.join(base_dir, file)):\n            continue\n\n        # Extract the base name (album name) from the file\n        if file.endswith(\".mp3\"):",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2182",
    "name": "imgconvert.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/imgconvert.py",
    "category": "02_media_processing",
    "type": "image_processing",
    "lines": 53,
    "size": 1739,
    "docstring": "",
    "keywords": [],
    "functions": [
      "convert_and_upscale_images",
      "main"
    ],
    "classes": [],
    "imports": [
      "os",
      "PIL"
    ],
    "preview": "import os\n\nfrom PIL import Image\n\n# Function to convert WebP images to PNG and upscale by 200% with 300 DPI\n\n\ndef convert_and_upscale_images(source_directory, destination_directory):\n    # Create the destination directory if it doesn't exist\n    os.makedirs(destination_directory, exist_ok=True)\n\n    for filename in os.listdir(source_directory):\n        if filename.endswith(\".webp\"):\n            source_file = os.path.join(source_directory, filename)\n            filename_no_ext = os.path.splitext(filename)[0]\n            destination_file = os.path.join(destination_directory, f\"{filename_no_ext}.png\")\n\n            # Convert WebP to PNG and upscale by 200% with 300 DPI\n            im = Image.open(source_file)\n            width, height = im.size",
    "last_modified": "2025-09-13T05:55:28.421680"
  },
  {
    "id": "2183",
    "name": "move.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/move.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 34,
    "size": 916,
    "docstring": "",
    "keywords": [],
    "functions": [
      "move_files_with_rsync"
    ],
    "classes": [],
    "imports": [
      "subprocess"
    ],
    "preview": "import subprocess\n\n\ndef move_files_with_rsync(source_dir, destination_dir):\n    command = [\n        \"rsync\",\n        \"-avP\",\n        \"--remove-source-files\",\n        \"--include=*.jpg\",\n        \"--include=*.jpeg\",\n        \"--include=*.png\",\n        \"--include=*.bmp\",\n        \"--include=*.gif\",\n        \"--include=*.tiff\",\n        \"--include=*/\",\n        \"--exclude=*\",\n        source_dir,\n        destination_dir,\n    ]\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2184",
    "name": "imgsearch.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/imgsearch.py",
    "category": "02_media_processing",
    "type": "image_processing",
    "lines": 41,
    "size": 1349,
    "docstring": "",
    "keywords": [],
    "functions": [
      "is_excluded_path"
    ],
    "classes": [],
    "imports": [
      "datetime",
      "os"
    ],
    "preview": "import datetime\nimport os\n\n\ndef is_excluded_path(path):\n    if path.startswith('/Users/steven/') and '/.' in path:\n        return True\n    excluded_paths = ['/Users/steven/.' ,'/System', '/Library', '/usr', '/bin', '/sbin', '/var', '/private', '/etc', '/tmp']\n    return any(path.startswith(excluded_path) for excluded_path in excluded_paths)\n\ndirectory_to_search = input(\"Please enter the directory to search for .png and .jpg files: \")\n\nif is_excluded_path(directory_to_search):\n    print(\"Excluded directories are not allowed.\")\n    exit()\n\ncurrent_date = datetime.datetime.now().strftime('%Y%m%d')\ndirectory_name = os.path.basename(os.path.normpath(directory_to_search))\nfilename = f\"{directory_name}_Images_{current_date}.csv\"\noutput_file = os.path.join(directory_to_search, filename)",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2185",
    "name": "mbcsgroupprober.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/mbcsgroupprober.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 58,
    "size": 2131,
    "docstring": "",
    "keywords": [],
    "functions": [
      "__init__"
    ],
    "classes": [
      "MBCSGroupProber"
    ],
    "imports": [
      "big5prober",
      "charsetgroupprober",
      "cp949prober",
      "enums",
      "eucjpprober",
      "euckrprober",
      "euctwprober",
      "gb2312prober",
      "johabprober",
      "sjisprober"
    ],
    "preview": "######################## BEGIN LICENSE BLOCK ########################\n# The Original Code is Mozilla Universal charset detector code.\n#\n# The Initial Developer of the Original Code is\n# Netscape Communications Corporation.\n# Portions created by the Initial Developer are Copyright (C) 2001\n# the Initial Developer. All Rights Reserved.\n#\n# Contributor(s):\n#   Mark Pilgrim - port to Python\n#   Shy Shalom - original C code\n#   Proofpoint, Inc.\n#\n# This library is free software; you can redistribute it and/or\n# modify it under the terms of the GNU Lesser General Public\n# License as published by the Free Software Foundation; either\n# version 2.1 of the License, or (at your option) any later version.\n#\n# This library is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2186",
    "name": "imgg-test.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/imgg-test.py",
    "category": "02_media_processing",
    "type": "image_processing",
    "lines": 18,
    "size": 436,
    "docstring": "",
    "keywords": [
      "web_tools"
    ],
    "functions": [],
    "classes": [],
    "imports": [
      "os",
      "requests",
      "dotenv"
    ],
    "preview": "import os\n\nimport requests\nfrom dotenv import load_dotenv\n\n# Load .env file\nload_dotenv(\"~/.env\")\npat = os.getenv(\"CLARIFAI_PAT\")\n\nauth = (os.getenv(\"IMAGGA_API_KEY\"), os.getenv(\"IMAGGA_API_SECRET\"))\nurl = \"\"\n\nresponse = requests.get(f\"https://api.imagga.com/v2/tags?image_url={url}\", auth=auth)\n\nprint(\"\ud83d\udd16 Imagga Tags:\")\nfor tag in response.json()[\"result\"][\"tags\"][:10]:\n    print(f\"- {tag['tag']['en']} ({tag['confidence']:.2f})\")\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2187",
    "name": "settings.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/settings.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 51,
    "size": 1148,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "pathlib"
    ],
    "preview": "from pathlib import Path\n\nsubreddits = [\n    \"AmItheAsshole\",\n    \"antiwork\",\n    \"AskMen\",\n    \"ChoosingBeggars\",\n    \"hatemyjob\",\n    \"NoStupidQuestions\",\n    \"pettyrevenge\",\n    \"Showerthoughts\",\n    \"TooAfraidToAsk\",\n    \"TwoXChromosomes\",\n    \"unpopularopinion\",\n]\n\nmax_video_length = 600  # Seconds\ncomment_limit = 600\n\nassets_directory = \"assets\"",
    "last_modified": "2025-03-28T18:35:46.763402"
  },
  {
    "id": "2188",
    "name": "youtube_dl_echo 2.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/youtube_dl_echo 2.py",
    "category": "02_media_processing",
    "type": "youtube",
    "lines": 319,
    "size": 13139,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "logging",
      "asyncio",
      "json",
      "math",
      "os",
      "time",
      "pyrogram",
      "translation",
      "helper_funcs.chat_base",
      "helper_funcs.display_progress"
    ],
    "preview": "import logging\n\nlogging.basicConfig(\n    level=logging.DEBUG, format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\n)\nlogger = logging.getLogger(__name__)\n\nimport asyncio\nimport json\nimport math\nimport os\nimport time\n\n# the secret configuration specific things\nif bool(os.environ.get(\"WEBHOOK\", False)):\n    from sample_config import Config\nelse:\n    from config import Config\n\n# the Strings used for this \"thing\"",
    "last_modified": "2025-09-13T05:54:10.235026"
  },
  {
    "id": "2189",
    "name": "midj.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/midj.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 28,
    "size": 650,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "json",
      "re"
    ],
    "preview": "import json\nimport re\n\n# Load the JSONL file.\nwith open(\n    \"/Users/steven/Pictures/midjourneyDownload_2023-10-13_1697181545353/metadata.jsonl\",\n    \"r\",\n) as f:\n    jsonl_data = json.load(f)\n\n# Compile the regular expression.\nregex = re.compile(r\"https:\\/\\/([\\w-]+.){1,}\\.(png)\")\n\n# Find all URLs in the JSONL file that match the regular expression.\nmatching_urls = []\nfor jsonl_object in jsonl_data:\n    url = jsonl_object.get(\"url\")\n    if regex.match(url):\n        matching_urls.append(url)\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2190",
    "name": "cb_buttons.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/cb_buttons.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 138,
    "size": 4849,
    "docstring": "",
    "keywords": [
      "youtube"
    ],
    "functions": [],
    "classes": [],
    "imports": [
      "logging",
      "json",
      "math",
      "os",
      "shutil",
      "subprocess",
      "time",
      "pyrogram",
      "translation",
      "hachoir.metadata"
    ],
    "preview": "import logging\n\nlogging.basicConfig(\n    level=logging.DEBUG, format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\n)\nlogger = logging.getLogger(__name__)\n\nimport json\nimport math\nimport os\nimport shutil\nimport subprocess\nimport time\n\n# the secret configuration specific things\nif bool(os.environ.get(\"WEBHOOK\", False)):\n    from sample_config import Config\nelse:\n    from config import Config\n",
    "last_modified": "2025-09-13T05:53:43.645264"
  },
  {
    "id": "2191",
    "name": "main_20221230225837.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/main_20221230225837.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 429,
    "size": 15518,
    "docstring": "",
    "keywords": [
      "transcription",
      "video_processing",
      "analysis",
      "youtube",
      "web_tools",
      "organization"
    ],
    "functions": [
      "downloadVideo",
      "scrapeVideos",
      "usedQuoteToDifferentFile",
      "getQuoteFromApi",
      "getQuoteFromTxtFile",
      "videoIntro",
      "createVideo",
      "audioClip",
      "randomBgMusic",
      "deleteTempFiles"
    ],
    "classes": [
      "bcolors"
    ],
    "imports": [
      "glob",
      "json",
      "os",
      "random",
      "subprocess",
      "gtts",
      "requests",
      "moviepy.editor",
      "moviepy.video.io.VideoFileClip",
      "mutagen.mp3"
    ],
    "preview": "import glob\nimport json\nimport os\nimport random\nimport subprocess\n\nimport gtts\nimport requests\nfrom moviepy.editor import *\nfrom moviepy.video.io.VideoFileClip import VideoFileClip\nfrom mutagen.mp3 import MP3\nfrom tqdm.auto import tqdm\n\n\n# download background video from pexels - https://www.pexels.com/api/documentation/#videos-search__parameters\ndef downloadVideo(id) -> str:\n    \"\"\"Downloads video from Pexels with the according video ID\"\"\"\n    url = \"https://www.pexels.com/video/\" + str(id) + \"/download.mp4\"\n    # Streaming, so we can iterate over the response.\n    response = requests.get(url, stream=True)",
    "last_modified": "2025-09-13T05:53:56.734743"
  },
  {
    "id": "2192",
    "name": "botError.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/botError.py",
    "category": "02_media_processing",
    "type": "web_tools",
    "lines": 706,
    "size": 20655,
    "docstring": "Created in 10/2020\n@Author: Paulo https://github.com/alpdias",
    "keywords": [],
    "functions": [
      "artName",
      "functionLike",
      "functionComment",
      "functionStories",
      "functionDraw",
      "botlogin",
      "findhashtag",
      "like",
      "botlogin",
      "findhashtag"
    ],
    "classes": [],
    "imports": [
      "os",
      "platform",
      "pathlib",
      "time",
      "pyfiglet",
      "selenium",
      "selenium.webdriver.common.keys"
    ],
    "preview": "# -*- coding: utf-8 -*-\n\n\"\"\"\nCreated in 10/2020\n@Author: Paulo https://github.com/alpdias\n\"\"\"\n\n# imported libraries\nimport os\nimport platform\nfrom pathlib import Path\nfrom time import sleep\n\nfrom pyfiglet import Figlet\nfrom selenium import webdriver\nfrom selenium.webdriver.common.keys import Keys\n\n\ndef artName(timeSleep=0):\n    \"\"\"",
    "last_modified": "2025-09-13T05:54:06.932977"
  },
  {
    "id": "2193",
    "name": "scan.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/scan.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 32,
    "size": 837,
    "docstring": "",
    "keywords": [],
    "functions": [
      "scan_directory"
    ],
    "classes": [],
    "imports": [
      "os"
    ],
    "preview": "import os\n\n# List of drives to scan\ndrives = [\n    \"/Users/steven/CoH-Story\",\n    \"/Users/steven/CoMic\",\n    \"/Users/steven/Downloads\",\n    \"/Users/steven/Documents\",\n    \"/Users/steven/\",\n    \"/Users/steven/Music\",\n    \"/Users/steven/Pictures\",\n]\n# File types to look for\nfile_types = (\".jpg\", \".jpeg\", \".png\", \".gif\", \"webp\")\n\n\n# Function to scan a directory for image files\ndef scan_directory(directory):\n    for root, dirs, files in os.walk(directory):\n        for file in files:",
    "last_modified": "2025-05-04T22:47:11.913272"
  },
  {
    "id": "2194",
    "name": "converts.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/converts.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 62,
    "size": 2268,
    "docstring": "",
    "keywords": [],
    "functions": [
      "convert_and_upscale_images",
      "main"
    ],
    "classes": [],
    "imports": [
      "os",
      "PIL"
    ],
    "preview": "import os\n\nfrom PIL import Image\n\n\ndef convert_and_upscale_images(source_directory, destination_directory):\n    # Create the destination directory if it doesn't exist\n    os.makedirs(destination_directory, exist_ok=True)\n\n    for filename in os.listdir(source_directory):\n        if filename.endswith((\".tiff\", \".png\", \".jpg\", \".jpeg\")):\n            source_file = os.path.join(source_directory, filename)\n            filename_no_ext, file_ext = os.path.splitext(filename)\n            file_ext = file_ext.lower()\n\n            if file_ext == \".tiff\":\n                destination_file = os.path.join(destination_directory, f\"{filename_no_ext}.png\")\n            elif file_ext in [\".png\", \".jpg\", \".jpeg\"]:\n                destination_file = os.path.join(destination_directory, filename)\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2195",
    "name": "AskReddit.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/AskReddit.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 257,
    "size": 8138,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "random_title_msg",
      "write_to_log",
      "check_video_in_db",
      "prompt_create_submission_video",
      "create_submission_video",
      "gen_video_from_hot",
      "main"
    ],
    "classes": [],
    "imports": [
      "json",
      "praw",
      "clips",
      "tinydb",
      "yt_upload"
    ],
    "preview": "#!./venv/bin/python\nimport json\n\nimport praw\nfrom clips import *\nfrom tinydb import Query, TinyDB\nfrom yt_upload import upload_video\n\ndb = TinyDB(\"log/db.json\")\ncreated_vids_db = db.table(\"created_videos\")\nuploaded_vids_db = db.table(\"uploaded_vids\")\n\nFPS = 30\nBACKGROUND_TRACK_VOLUME = 0.12\nDURATION: int = 60 * 5\nDESCRIPTION = \"Yes I'm an actual robot. \\n\"\n\n\ndef random_title_msg():\n    return \"Subscribe or I'll end humanity.\"",
    "last_modified": "2025-09-13T05:53:51.391388"
  },
  {
    "id": "2196",
    "name": "100sort.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/100sort.py",
    "category": "02_media_processing",
    "type": "organization",
    "lines": 38,
    "size": 1254,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "os",
      "shutil"
    ],
    "preview": "import os\nimport shutil\n\n# Prompt the user for the source directory\nsource_dir = input(\"Enter the source directory path: \")\n\n# Verify that the source directory exists\nif not os.path.exists(source_dir):\n    print(\"Source directory does not exist.\")\n    exit()\n\n# Prompt the user for the destination directory\ndestination_dir = input(\"Enter the destination directory path: \")\n\n# Create the destination directory if it doesn't exist\nos.makedirs(destination_dir, exist_ok=True)\n\n# Initialize variables\nimages_per_folder = 250\ncurrent_folder = None",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2197",
    "name": "instagram.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/instagram.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 2554,
    "size": 114277,
    "docstring": "",
    "keywords": [
      "web_tools"
    ],
    "functions": [
      "__init__",
      "script",
      "menu",
      "islemSec",
      "secilenIslemiGoster",
      "profilSec",
      "ilkGonderiTikla",
      "gonderileriIndir",
      "gonderileriBegen",
      "topluTakiptenCik"
    ],
    "classes": [
      "Instagram"
    ],
    "imports": [
      "getpass",
      "json",
      "os",
      "threading",
      "urllib.request",
      "builtins",
      "datetime",
      "random",
      "time",
      "requests"
    ],
    "preview": "import getpass\nimport json\nimport os\nimport threading\nimport urllib.request\nfrom builtins import print\nfrom datetime import datetime\nfrom random import randint\nfrom time import sleep\n\nimport requests\nfrom colorama import init\nfrom selenium import webdriver\nfrom selenium.webdriver.common.keys import Keys\nfrom selenium.webdriver.firefox.options import Options\nfrom termcolor import colored\n\n\nclass Instagram:\n    def __init__(self):",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2198",
    "name": "generate_album_html-pages_fixed 1.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/generate_album_html-pages_fixed 1.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 157,
    "size": 4911,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "os",
      "pathlib"
    ],
    "preview": "import os\nfrom pathlib import Path\n\nalbums_dir = Path(\"/Users/steven/Music/nocTurneMeLoDieS/mp3\")\noutput_file = albums_dir / \"disco25.html\"\n\nhtml_header = \"\"\"<!DOCTYPE html>\n<html lang=\"en\">\n<head>\n    <meta charset=\"UTF-8\">\n    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n    <title>Discography with MP3</title>\n    <style>\n        body { font-family: Arial, sans-serif; background-color: #f4f4f4; margin: 0; padding: 0; }\n        h1 { text-align: center; margin-top: 20px; font-size: 32px; color: #333; }\n        .grid-container {\n            display: grid;\n            grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));\n            gap: 20px;\n            padding: 20px;",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2199",
    "name": "9mbs.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/9mbs.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 90,
    "size": 3154,
    "docstring": "",
    "keywords": [],
    "functions": [
      "is_large_image",
      "resize_image_to_max_size",
      "resize_images_in_directory",
      "main"
    ],
    "classes": [],
    "imports": [
      "os",
      "PIL"
    ],
    "preview": "import os\n\nfrom PIL import Image\n\n# Set a limit for maximum pixels to avoid decompression bomb error\nImage.MAX_IMAGE_PIXELS = (\n    178956970  # Default limit, can be adjusted or removed entirely using 'None'\n)\n\n\ndef is_large_image(image_path):\n    \"\"\"Check if the image is too large based on pixel dimensions.\"\"\"\n    with Image.open(image_path) as img:\n        width, height = img.size\n        return (width * height) > Image.MAX_IMAGE_PIXELS\n\n\ndef resize_image_to_max_size(image_path, max_size_mb=9, upscale=True):\n    max_size_bytes = max_size_mb * 1024 * 1024  # Convert MB to bytes\n    target_dpi = (300, 300)  # Set target DPI",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2200",
    "name": "img-to-pdf-folder.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/img-to-pdf-folder.py",
    "category": "02_media_processing",
    "type": "image_processing",
    "lines": 33,
    "size": 1035,
    "docstring": "",
    "keywords": [
      "image_processing"
    ],
    "functions": [
      "image_to_pdf",
      "convert_images_to_pdf"
    ],
    "classes": [],
    "imports": [
      "os",
      "PIL",
      "reportlab.lib.pagesizes",
      "reportlab.pdfgen"
    ],
    "preview": "import os\n\nfrom PIL import Image\nfrom reportlab.lib.pagesizes import letter\nfrom reportlab.pdfgen import canvas\n\n\n# Function to convert an image to PDF\ndef image_to_pdf(image_path, pdf_path):\n    c = canvas.Canvas(pdf_path, pagesize=letter)\n    img = Image.open(image_path)\n    width, height = img.size\n    c.setPageSize((width, height))\n    c.drawImage(image_path, 0, 0, width, height)\n    c.showPage()\n    c.save()\n\n\n# Function to traverse the directory and convert images to PDFs\ndef convert_images_to_pdf(root_dir):",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2201",
    "name": "audiometadata.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/audiometadata.py",
    "category": "02_media_processing",
    "type": "transcription",
    "lines": 88,
    "size": 2862,
    "docstring": "Apply audio metadata",
    "keywords": [],
    "functions": [
      "__init__",
      "__load__",
      "apply_album_art",
      "__resize__",
      "apply_track_info"
    ],
    "classes": [
      "AudioMetadata"
    ],
    "imports": [
      "logging",
      "mutagen.easyid3",
      "mutagen.id3",
      "mutagen.mp3",
      "PIL",
      "ytdl.customerrors",
      "ytdl.oshelper"
    ],
    "preview": "\"Apply audio metadata\"\n\nimport logging\n\nfrom mutagen.easyid3 import EasyID3\nfrom mutagen.id3 import APIC, ID3, error\nfrom mutagen.mp3 import MP3\nfrom PIL import Image\nfrom ytdl.customerrors import FileNotFoundError\nfrom ytdl.oshelper import DEFAULT_FILE_NAME, dirname, filename_no_extension, join_paths\n\n\nclass AudioMetadata(object):\n    \"Responsible for applying metadata to file\"\n\n    def __init__(self, track_file):\n        self.track_file = track_file\n        self.logger = logging.getLogger(__name__)\n\n    def __load__(self):",
    "last_modified": "2025-09-13T05:54:14.912666"
  },
  {
    "id": "2202",
    "name": "bookmarks.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/bookmarks.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 28,
    "size": 691,
    "docstring": "",
    "keywords": [],
    "functions": [
      "add_bookmarks"
    ],
    "classes": [],
    "imports": [
      "pypdf"
    ],
    "preview": "from pypdf import PdfReader, PdfWriter\n\n\ndef add_bookmarks(output_path, toc_items):\n    reader = PdfReader(output_path)\n    writer = PdfWriter()\n\n    for page in reader.pages:\n        writer.add_page(page)\n\n    for item in toc_items:\n        title = item[\"title\"]\n        page_num = item[\"page\"] - 1  # Page numbers are zero-indexed\n        writer.add_outline_item(title, page_num)\n\n    with open(output_path, \"wb\") as output_file:\n        writer.write(output_file)\n\n\ntoc_items = [",
    "last_modified": "2025-05-04T22:47:13.331578"
  },
  {
    "id": "2203",
    "name": "organize_music_library.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/organize_music_library.py",
    "category": "02_media_processing",
    "type": "audio_processing",
    "lines": 55,
    "size": 1679,
    "docstring": "",
    "keywords": [
      "organization"
    ],
    "functions": [
      "normalize_name",
      "move_file",
      "organize_music_library"
    ],
    "classes": [],
    "imports": [
      "os",
      "re",
      "shutil"
    ],
    "preview": "import os\nimport re\nimport shutil\n\nBASE_DIR = \"/Users/steven/Movies/project2025/Media\"\n\n\ndef normalize_name(name):\n    return re.sub(r\"[^\\w\\s-]\", \"\", name).strip().replace(\" \", \"_\")\n\n\ndef move_file(src, dest):\n    os.makedirs(os.path.dirname(dest), exist_ok=True)\n    if not os.path.exists(dest):\n        shutil.move(src, dest)\n        print(f\"\u2705 Moved: {src} -> {dest}\")\n    else:\n        print(f\"\u26a0\ufe0f File exists, skipped: {dest}\")\n\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2204",
    "name": "youtube_dl_echo.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/youtube_dl_echo.py",
    "category": "02_media_processing",
    "type": "youtube",
    "lines": 319,
    "size": 13139,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "logging",
      "asyncio",
      "json",
      "math",
      "os",
      "time",
      "pyrogram",
      "translation",
      "helper_funcs.chat_base",
      "helper_funcs.display_progress"
    ],
    "preview": "import logging\n\nlogging.basicConfig(\n    level=logging.DEBUG, format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\n)\nlogger = logging.getLogger(__name__)\n\nimport asyncio\nimport json\nimport math\nimport os\nimport time\n\n# the secret configuration specific things\nif bool(os.environ.get(\"WEBHOOK\", False)):\n    from sample_config import Config\nelse:\n    from config import Config\n\n# the Strings used for this \"thing\"",
    "last_modified": "2025-09-13T05:53:44.747546"
  },
  {
    "id": "2205",
    "name": "upscale 2 copy.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/upscale 2 copy.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 76,
    "size": 1850,
    "docstring": "",
    "keywords": [
      "web_tools"
    ],
    "functions": [],
    "classes": [],
    "imports": [
      "json",
      "os",
      "time",
      "requests"
    ],
    "preview": "import json\nimport os\nimport time\n\nimport requests\n\napi_key = os.getenv(\"API_KEY\")\nif not api_key:\n    raise ValueError(\n        \"API key is not set. Please ensure the API_KEY environment variable is configured correctly.\"\n    )\n\nauthorization = f\"Bearer {api_key}\"\n\nheaders = {\n    \"accept\": \"application/json\",\n    \"content-type\": \"application/json\",\n    \"authorization\": authorization,\n}\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2206",
    "name": "get_gifs.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/get_gifs.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 96,
    "size": 2071,
    "docstring": "",
    "keywords": [
      "youtube",
      "web_tools"
    ],
    "functions": [
      "download_gif",
      "get_giphy_results",
      "download_gifs_from_terms"
    ],
    "classes": [],
    "imports": [
      "json",
      "sys",
      "time",
      "pathlib",
      "requests"
    ],
    "preview": "# Overview\n# - download gifs from giphy\n\n# Dependencies\n\nimport json\nimport sys\nimport time\nfrom pathlib import Path\n\nimport requests\n\n# Funcs\n\n\ndef download_gif(giphy_id: str, output_path: str):\n    \"\"\"\n    download gif from giphy by giphy id\n\n    :param giphy_id: str, ID of gif from giphy",
    "last_modified": "2025-03-28T18:36:55.645232"
  },
  {
    "id": "2207",
    "name": "YouTubeBot 3.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/YouTubeBot 3.py",
    "category": "02_media_processing",
    "type": "youtube",
    "lines": 158,
    "size": 4910,
    "docstring": "",
    "keywords": [
      "web_tools"
    ],
    "functions": [
      "fetch",
      "filter",
      "duration_split",
      "start"
    ],
    "classes": [],
    "imports": [
      "time",
      "tkinter",
      "tkinter.ttk",
      "pyautogui",
      "PIL",
      "selenium",
      "requests"
    ],
    "preview": "import time\nimport tkinter as tk\nimport tkinter.ttk as ttk\n\nimport pyautogui\nfrom PIL import Image, ImageTk\nfrom selenium import webdriver\n\nheight = pyautogui.size()[1]\nwidth = pyautogui.size()[0]\nprint(\"resolution = \" + str(width) + \", \" + str(height))\nwindow = tk.Tk()\nwindow.title(\"YouTube Bot\")\n\nwindow.resizable(0, 0)\nwindow.configure(background=\"white\")\nwindow.rowconfigure([0], minsize=round(width / 96), weight=0)\nwindow.columnconfigure([0, 2], minsize=round(width / 24), weight=0)\nwindow.columnconfigure(1, minsize=round(width / 2.13), weight=0)\n",
    "last_modified": "2025-09-13T05:54:08.258372"
  },
  {
    "id": "2208",
    "name": "generate_album_html-pages_fixed.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/generate_album_html-pages_fixed.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 157,
    "size": 4911,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "os",
      "pathlib"
    ],
    "preview": "import os\nfrom pathlib import Path\n\nalbums_dir = Path(\"/Users/steven/Music/nocTurneMeLoDieS/Mp3\")\noutput_file = albums_dir / \"disco25.html\"\n\nhtml_header = \"\"\"<!DOCTYPE html>\n<html lang=\"en\">\n<head>\n    <meta charset=\"UTF-8\">\n    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n    <title>Discography with MP3</title>\n    <style>\n        body { font-family: Arial, sans-serif; background-color: #f4f4f4; margin: 0; padding: 0; }\n        h1 { text-align: center; margin-top: 20px; font-size: 32px; color: #333; }\n        .grid-container {\n            display: grid;\n            grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));\n            gap: 20px;\n            padding: 20px;",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2209",
    "name": "8mb (1).py",
    "path": "github_repo/scripts/02_media_processing/image_tools/8mb (1).py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 71,
    "size": 2482,
    "docstring": "",
    "keywords": [],
    "functions": [
      "resize_image_to_max_size",
      "resize_images_in_directory"
    ],
    "classes": [],
    "imports": [
      "os",
      "PIL"
    ],
    "preview": "import os\n\nfrom PIL import Image\n\n\ndef resize_image_to_max_size(image_path, max_size_mb=8):\n    \"\"\"\n    Resize a PNG image to ensure it doesn't exceed the specified max size (in MB).\n\n    :param image_path: Path to the input PNG image.\n    :param max_size_mb: Maximum allowed size for the image in megabytes (default is 8MB).\n    \"\"\"\n    max_size_bytes = max_size_mb * 1024 * 1024  # Convert MB to bytes\n\n    img = Image.open(image_path)\n    current_size = os.path.getsize(image_path)\n\n    if current_size <= max_size_bytes:\n        print(f\"{os.path.basename(image_path)} is already under {max_size_mb}MB.\")\n        return",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2210",
    "name": "motion-upload copy.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/motion-upload copy.py",
    "category": "02_media_processing",
    "type": "youtube",
    "lines": 58,
    "size": 1678,
    "docstring": "",
    "keywords": [
      "web_tools"
    ],
    "functions": [],
    "classes": [],
    "imports": [
      "json",
      "time",
      "requests"
    ],
    "preview": "import json\nimport time\n\nimport requests\n\napi_key = \"<YOUR_API_KEY>\"\nauthorization = \"Bearer %s\" % api_key\n\nheaders = {\n    \"accept\": \"application/json\",\n    \"content-type\": \"application/json\",\n    \"authorization\": authorization,\n}\n\n# Get a presigned URL for uploading an image\nurl = \"https://cloud.leonardo.ai/api/rest/v1/init-image\"\n\npayload = {\"extension\": \"jpg\"}\n\nresponse = requests.post(url, json=payload, headers=headers)",
    "last_modified": "2025-05-04T22:47:12.937073"
  },
  {
    "id": "2211",
    "name": "sort2 (1).py",
    "path": "github_repo/scripts/02_media_processing/image_tools/sort2 (1).py",
    "category": "02_media_processing",
    "type": "organization",
    "lines": 61,
    "size": 2414,
    "docstring": "",
    "keywords": [
      "organization"
    ],
    "functions": [
      "sort_images"
    ],
    "classes": [],
    "imports": [
      "os",
      "shutil",
      "datetime",
      "PIL"
    ],
    "preview": "import os\nimport shutil\nfrom datetime import datetime\n\nfrom PIL import Image\n\n\ndef sort_images(source_dir, target_dir):\n    # Create the target directory if it doesn't exist\n    os.makedirs(target_dir, exist_ok=True)\n\n    # Supported image formats\n    extensions = (\".png\", \".jpg\", \".jpeg\", \".tiff\", \".mp4\", \".webp\")\n\n    # Walk through the directory\n    for root, dirs, files in os.walk(source_dir):\n        for file in files:\n            if file.lower().endswith(extensions):\n                try:\n                    # Construct the full file path",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2212",
    "name": "playlist_20221230180525.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/playlist_20221230180525.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 168,
    "size": 6429,
    "docstring": "",
    "keywords": [
      "youtube",
      "analysis",
      "web_tools"
    ],
    "functions": [
      "generate_token",
      "get_youtube_url",
      "download_mp3",
      "last_fm_artist_info",
      "set_metadata",
      "write_tracks",
      "write_playlist",
      "split_spotify_uri",
      "get_os",
      "get_folder"
    ],
    "classes": [],
    "imports": [
      "argparse",
      "json",
      "os",
      "re",
      "subprocess",
      "urllib.parse",
      "urllib.request",
      "secret",
      "spotipy",
      "spotipy.oauth2"
    ],
    "preview": "import argparse\nimport json\nimport os\nimport re\nimport subprocess\nimport urllib.parse\nimport urllib.request\n\nimport secret\nimport spotipy\nimport spotipy.oauth2 as oauth2\nfrom requests.utils import quote\n\n\ndef generate_token():\n    credentials = oauth2.SpotifyClientCredentials(\n        client_id=secret.28b20556906f4b75874c4ae98320c81d,\n        client_secret=secret.c7033fd14e1247cfb9eef73874dd2365 \n)\n    token = credentials.get_access_token()",
    "last_modified": "2025-05-04T23:28:21.397414"
  },
  {
    "id": "2213",
    "name": "motion-upload 3.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/motion-upload 3.py",
    "category": "02_media_processing",
    "type": "youtube",
    "lines": 57,
    "size": 1679,
    "docstring": "",
    "keywords": [
      "web_tools"
    ],
    "functions": [],
    "classes": [],
    "imports": [
      "json",
      "time",
      "requests"
    ],
    "preview": "import json\nimport time\n\nimport requests\n\napi_key = \"<YOUR_API_KEY>\"\nauthorization = \"Bearer %s\" % api_key\n\nheaders = {\n    \"accept\": \"application/json\",\n    \"content-type\": \"application/json\",\n    \"authorization\": authorization,\n}\n\n# Get a presigned URL for uploading an image\nurl = \"https://cloud.leonardo.ai/api/rest/v1/init-image\"\n\npayload = {\"extension\": \"jpg\"}\n\nresponse = requests.post(url, json=payload, headers=headers)",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2214",
    "name": "vidGen.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/vidGen.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 241,
    "size": 8359,
    "docstring": "",
    "keywords": [
      "opencv"
    ],
    "functions": [
      "getFileNames",
      "deleteSkippedClips",
      "deleteAllFilesInPath",
      "renderThread",
      "renderVideo"
    ],
    "classes": [],
    "imports": [
      "datetime",
      "os",
      "pickle",
      "random",
      "re",
      "shutil",
      "subprocess",
      "time",
      "distutils.dir_util",
      "time"
    ],
    "preview": "import datetime\nimport os\nimport pickle\nimport random\nimport re\nimport shutil\nimport subprocess\nimport time\nfrom distutils.dir_util import copy_tree\nfrom time import sleep\n\nimport cv2\nimport settings\n\n# File Paths\n\n\n# Creating file paths that are needed\n\n",
    "last_modified": "2025-09-13T05:53:32.666070"
  },
  {
    "id": "2215",
    "name": "web-png-upscale copy.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/web-png-upscale copy.py",
    "category": "02_media_processing",
    "type": "web_tools",
    "lines": 53,
    "size": 1739,
    "docstring": "",
    "keywords": [],
    "functions": [
      "convert_and_upscale_images",
      "main"
    ],
    "classes": [],
    "imports": [
      "os",
      "PIL"
    ],
    "preview": "import os\n\nfrom PIL import Image\n\n# Function to convert WebP images to PNG and upscale by 200% with 300 DPI\n\n\ndef convert_and_upscale_images(source_directory, destination_directory):\n    # Create the destination directory if it doesn't exist\n    os.makedirs(destination_directory, exist_ok=True)\n\n    for filename in os.listdir(source_directory):\n        if filename.endswith(\".webp\"):\n            source_file = os.path.join(source_directory, filename)\n            filename_no_ext = os.path.splitext(filename)[0]\n            destination_file = os.path.join(destination_directory, f\"{filename_no_ext}.png\")\n\n            # Convert WebP to PNG and upscale by 200% with 300 DPI\n            im = Image.open(source_file)\n            width, height = im.size",
    "last_modified": "2025-09-13T05:55:28.619379"
  },
  {
    "id": "2216",
    "name": "organize_albums 10.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/organize_albums 10.py",
    "category": "02_media_processing",
    "type": "organization",
    "lines": 62,
    "size": 2466,
    "docstring": "",
    "keywords": [
      "organization"
    ],
    "functions": [
      "organize_files"
    ],
    "classes": [],
    "imports": [
      "os",
      "shutil"
    ],
    "preview": "import os\nimport shutil\n\n# Define the base directory\nbase_dir = \"/Users/steven/Music/NocTurnE-meLoDieS/mp3\"\n\n\n# Helper function to create folders and move files\ndef organize_files():\n    # List all files in the base directory\n    files = os.listdir(base_dir)\n\n    # Process each file\n    for file in files:\n        # Skip directories\n        if os.path.isdir(os.path.join(base_dir, file)):\n            continue\n\n        # Extract the base name (album name) from the file\n        if file.endswith(\".mp3\"):",
    "last_modified": "2025-05-04T22:47:13"
  },
  {
    "id": "2217",
    "name": "api_photo.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/api_photo.py",
    "category": "02_media_processing",
    "type": "image_processing",
    "lines": 471,
    "size": 17197,
    "docstring": "",
    "keywords": [
      "youtube"
    ],
    "functions": [
      "download_photo",
      "compatible_aspect_ratio",
      "configure_photo",
      "upload_photo",
      "upload_album",
      "get_image_size",
      "resize_image",
      "stories_shaper"
    ],
    "classes": [],
    "imports": [
      "__future__",
      "imghdr",
      "json",
      "os",
      "random",
      "shutil",
      "struct",
      "time",
      "uuid",
      "math"
    ],
    "preview": "from __future__ import unicode_literals\n\nimport imghdr\nimport json\nimport os\nimport random\nimport shutil\nimport struct\nimport time\nfrom uuid import uuid4\n\nfrom . import config\n\n\ndef download_photo(self, media_id, filename, media=False, folder=\"photos\"):\n    if not media:\n        self.media_info(media_id)\n        if not self.last_json.get(\"items\"):\n            return True\n        media = self.last_json[\"items\"][0]",
    "last_modified": "2025-09-13T05:54:56.577285"
  },
  {
    "id": "2218",
    "name": "upload_story_photo.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/upload_story_photo.py",
    "category": "02_media_processing",
    "type": "youtube",
    "lines": 20,
    "size": 596,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "argparse",
      "os",
      "sys",
      "instabot"
    ],
    "preview": "import argparse\nimport os\nimport sys\n\nsys.path.append(os.path.join(sys.path[0], \"../\"))\nfrom instabot import Bot  # noqa: E402\n\nparser = argparse.ArgumentParser(add_help=True)\nparser.add_argument(\"-u\", type=str, help=\"username\")\nparser.add_argument(\"-p\", type=str, help=\"password\")\nparser.add_argument(\"-proxy\", type=str, help=\"proxy\")\nparser.add_argument(\"-photo\", type=str, help=\"photo name like 'picture.jpg' \")\nargs = parser.parse_args()\n\nbot = Bot()\nbot.login(username=args.u, password=args.p, proxy=args.proxy)\n\n# Publish a new story with the given photo\nbot.upload_story_photo(args.photo)\n",
    "last_modified": "2025-03-28T18:35:47"
  },
  {
    "id": "2219",
    "name": "gemini_storybook_downloader.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/gemini_storybook_downloader.py",
    "category": "02_media_processing",
    "type": "transcription",
    "lines": 410,
    "size": 12456,
    "docstring": "Gemini Storybook Downloader (macOS-friendly)\n--------------------------------------------\n\nDownloads assets (images, videos, audio) and text/metadata from public\nGemini \"storybook\" share links, e.g.:\n  https://gemini.google.com/gem/storybook/<id>\n  https://g.co/gemini/share/<id>\n\nIt renders pages with Playwright (Chromium) so JS-driven content is captured.\n\nUsage:\n  # one-off\n  python gemini_storybook_downloader.py <url1> <url2> ...\n\n  # or from a file (one URL per line)\n  python gemini_storybook_downloader.py --file urls.txt\n\nOutput:\n  ./downloads/<sanitized-title-or-id>/\n      page.html\n      metadata.json\n      text.md\n      assets/\n        img_001.jpg ...\n        video_001.mp4 ...\n        audio_001.mp3 ...\n      manifest.csv\n\nInstall:\n  pip install -r requirements.txt\n  playwright install chromium\n\nNotes:\n- Only public share links are supported. If a link requires login, this script\n  will still attempt to render but may save a \"Sign in\" page instead.\n- This script avoids duplicate downloads via a content-hash manifest.",
    "keywords": [],
    "functions": [
      "slugify",
      "ensure_dir",
      "sha256_bytes",
      "sanitize_url_to_id",
      "ext_for"
    ],
    "classes": [
      "DownloadRecord"
    ],
    "imports": [
      "__future__",
      "argparse",
      "asyncio",
      "csv",
      "hashlib",
      "json",
      "os",
      "re",
      "sys",
      "time"
    ],
    "preview": "#!/usr/bin/env python3\n\"\"\"\nGemini Storybook Downloader (macOS-friendly)\n--------------------------------------------\n\nDownloads assets (images, videos, audio) and text/metadata from public\nGemini \"storybook\" share links, e.g.:\n  https://gemini.google.com/gem/storybook/<id>\n  https://g.co/gemini/share/<id>\n\nIt renders pages with Playwright (Chromium) so JS-driven content is captured.\n\nUsage:\n  # one-off\n  python gemini_storybook_downloader.py <url1> <url2> ...\n\n  # or from a file (one URL per line)\n  python gemini_storybook_downloader.py --file urls.txt\n\nOutput:",
    "last_modified": "2025-09-13T05:54:31.192118"
  },
  {
    "id": "2220",
    "name": "upscale2.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/upscale2.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 85,
    "size": 2862,
    "docstring": "",
    "keywords": [],
    "functions": [
      "upscale_and_save_image",
      "compress_image_to_size"
    ],
    "classes": [],
    "imports": [
      "os",
      "PIL"
    ],
    "preview": "import os\n\nfrom PIL import Image\n\n# Input directory (no output directory needed since we're replacing the original images)\ninput_dir = \"/Users/steven/Music/NocTurnE-meLoDieS\"\n\n# Max file size in bytes (9 MB = 9 * 1024 * 1024)\nmax_size = 9 * 1024 * 1024\n\n\ndef upscale_and_save_image(image_path, max_size=max_size, dpi=300):\n    \"\"\"\n    Upscale the image by 2x, set the DPI, and compress to ensure the file size is <= 9MB.\n    Args:\n        image_path (str): Path to the input image.\n        max_size (int): Maximum file size in bytes.\n        dpi (int): Target DPI (default is 300).\n    \"\"\"\n    # Open the image using PIL",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2221",
    "name": "simple_but_effective.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/simple_but_effective.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 74,
    "size": 1845,
    "docstring": "This template is written by @zackvega\n\nWhat does this quickstart script aim to do?\n- This is my simple but effective script.",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "instapy"
    ],
    "preview": "\"\"\"\nThis template is written by @zackvega\n\nWhat does this quickstart script aim to do?\n- This is my simple but effective script.\n\"\"\"\n\nfrom instapy import InstaPy, smart_run\n\ninsta_username = \"\"\ninsta_password = \"\"\n\n# get a session!\nsession = InstaPy(\n    username=insta_username,\n    password=insta_password,\n    headless_browser=True,\n    multi_logs=True,\n)\n",
    "last_modified": "2025-03-28T18:36:56"
  },
  {
    "id": "2222",
    "name": "leonardo_script copy.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/leonardo_script copy.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 129,
    "size": 4439,
    "docstring": "",
    "keywords": [
      "analysis",
      "youtube",
      "web_tools"
    ],
    "functions": [
      "convert_image_to_jpeg",
      "get_presigned_url",
      "upload_image",
      "upscale_image",
      "get_upscaled_image",
      "process_images"
    ],
    "classes": [],
    "imports": [
      "json",
      "os",
      "sys",
      "time",
      "requests",
      "PIL"
    ],
    "preview": "import json\nimport os\nimport sys\nimport time\n\nimport requests\nfrom PIL import Image\n\napi_key = \"de7c9cb8-022f-42f8-8bf7-a8f9caadfaee\"\nauthorization = f\"Bearer {api_key}\"\n\nheaders = {\n    \"accept\": \"application/json\",\n    \"content-type\": \"application/json\",\n    \"authorization\": authorization,\n}\n\n# Styles to apply\nstyles = [\"GENERAL\", \"CINEMATIC\", \"2D_ART_ILLUSTRATION\", \"PHOTOREALISTIC\"]\n",
    "last_modified": "2025-09-13T05:53:50.450811"
  },
  {
    "id": "2223",
    "name": "sort--.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/sort--.py",
    "category": "02_media_processing",
    "type": "organization",
    "lines": 33,
    "size": 1090,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "os",
      "shutil"
    ],
    "preview": "import os\nimport shutil\n\n# Source directory containing your images\nsource_dir = \"/Volumes/baKs/105-mids\"\n\n# Destination directory where sorted folders will be created\ndestination_dir = \"/Volumes/baKs/105-mids\"\n\n# Create the destination directory if it doesn't exist\nos.makedirs(destination_dir, exist_ok=True)\n\n# Initialize variables\nimages_per_folder = 100\ncurrent_folder = None\nfolder_count = 0\n\n# Iterate through the source directory\nfor root, _, files in os.walk(source_dir):\n    for filename in files:",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2224",
    "name": "8mb (2).py",
    "path": "github_repo/scripts/02_media_processing/image_tools/8mb (2).py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 58,
    "size": 1929,
    "docstring": "",
    "keywords": [],
    "functions": [
      "resize_image_to_max_size",
      "main"
    ],
    "classes": [],
    "imports": [
      "os",
      "PIL"
    ],
    "preview": "import os\n\nfrom PIL import Image\n\n\ndef resize_image_to_max_size(image_path, max_size_mb=8):\n    \"\"\"\n    Resize a PNG image to ensure it doesn't exceed the specified max size (in MB).\n\n    :param image_path: Path to the input PNG image.\n    :param max_size_mb: Maximum allowed size for the image in megabytes (default is 8MB).\n    \"\"\"\n    max_size_bytes = max_size_mb * 1024 * 1024  # Convert MB to bytes\n    target_dpi = (300, 300)  # Set target DPI\n\n    img = Image.open(image_path)\n    current_size = os.path.getsize(image_path)\n\n    # Reduce image size by lowering the resolution\n    width, height = img.size",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2225",
    "name": "img2img copy.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/img2img copy.py",
    "category": "02_media_processing",
    "type": "image_processing",
    "lines": 47,
    "size": 1743,
    "docstring": "",
    "keywords": [],
    "functions": [
      "convert_and_upscale_images",
      "main"
    ],
    "classes": [],
    "imports": [
      "os",
      "PIL"
    ],
    "preview": "import os\n\nfrom PIL import Image\n\n\ndef convert_and_upscale_images(source_directory, destination_directory):\n    os.makedirs(destination_directory, exist_ok=True)\n\n    for filename in os.listdir(source_directory):\n        if filename.endswith(\".png\"):\n            source_file = os.path.join(source_directory, filename)\n            filename_no_ext = os.path.splitext(filename)[0]\n            destination_file = os.path.join(destination_directory, f\"{filename_no_ext}.png\")\n\n            im = Image.open(source_file)\n            width, height = im.size\n            upscale_width = width * 2\n            upscale_height = height * 2\n            im_resized = im.resize((upscale_width, upscale_height))\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2226",
    "name": "leodown_20250102105149.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/leodown_20250102105149.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 52,
    "size": 1328,
    "docstring": "",
    "keywords": [
      "web_tools"
    ],
    "functions": [],
    "classes": [],
    "imports": [
      "json",
      "requests"
    ],
    "preview": "import json\n\nimport requests\n\n# Base URL for API\nbase_url = (\n    \"https://cloud.leonardo.ai/api/rest/v1/generations/user/f7bb8476-e3f0-4f1f-9a06-4600866cc49c\"\n)\nheaders = {\n    \"accept\": \"application/json\",\n    \"authorization\": \"Bearer 93043291-957d-4ec1-8c79-ee734abcb6e3\",  # Replace with your actual token\n}\n\n# Output file\noutput_file = \"/Users/steven/Pictures/leonardo_library.json\"\n\n# Pagination parameters\noffset = 0\nlimit = 10  # Maximum number of items per request\nall_generations = []",
    "last_modified": "2025-09-13T05:53:49.579263"
  },
  {
    "id": "2227",
    "name": "convert-loop2 2.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/convert-loop2 2.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 167,
    "size": 5642,
    "docstring": "",
    "keywords": [
      "youtube",
      "web_tools"
    ],
    "functions": [
      "convert_image_to_jpeg",
      "get_presigned_url",
      "upload_image",
      "upscale_image",
      "get_upscaled_image"
    ],
    "classes": [],
    "imports": [
      "csv",
      "json",
      "os",
      "time",
      "datetime",
      "requests",
      "PIL"
    ],
    "preview": "import csv\nimport json\nimport os\nimport time\nfrom datetime import datetime\n\nimport requests\nfrom PIL import Image\n\napi_key = \"de7c9cb8-022f-42f8-8bf7-a8f9caadfaee\"\nauthorization = f\"Bearer {api_key}\"\n\nheaders = {\n    \"accept\": \"application/json\",\n    \"content-type\": \"application/json\",\n    \"authorization\": authorization,\n}\n\n# Directory containing images\ndirectory_path = \"/Users/steven/Pictures/TrashCaT/trashy-heartbreak\"",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2228",
    "name": "codec.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/codec.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 117,
    "size": 3370,
    "docstring": "",
    "keywords": [],
    "functions": [
      "getregentry",
      "encode",
      "decode",
      "_buffer_encode",
      "_buffer_decode"
    ],
    "classes": [
      "Codec",
      "IncrementalEncoder",
      "IncrementalDecoder",
      "StreamWriter",
      "StreamReader"
    ],
    "imports": [
      "codecs",
      "re",
      "typing",
      "core"
    ],
    "preview": "import codecs\nimport re\nfrom typing import Optional, Tuple\n\nfrom .core import IDNAError, alabel, decode, encode, ulabel\n\n_unicode_dots_re = re.compile(\"[\\u002e\\u3002\\uff0e\\uff61]\")\n\n\nclass Codec(codecs.Codec):\n\n    def encode(self, data: str, errors: str = \"strict\") -> Tuple[bytes, int]:\n        if errors != \"strict\":\n            raise IDNAError('Unsupported error handling \"{}\"'.format(errors))\n\n        if not data:\n            return b\"\", 0\n\n        return encode(data), len(data)\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2229",
    "name": "sbcharsetprober.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/sbcharsetprober.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 163,
    "size": 6400,
    "docstring": "",
    "keywords": [],
    "functions": [
      "__init__",
      "reset",
      "charset_name",
      "language",
      "feed",
      "get_confidence"
    ],
    "classes": [
      "SingleByteCharSetModel",
      "SingleByteCharSetProber"
    ],
    "imports": [
      "typing",
      "charsetprober",
      "enums"
    ],
    "preview": "######################## BEGIN LICENSE BLOCK ########################\n# The Original Code is Mozilla Universal charset detector code.\n#\n# The Initial Developer of the Original Code is\n# Netscape Communications Corporation.\n# Portions created by the Initial Developer are Copyright (C) 2001\n# the Initial Developer. All Rights Reserved.\n#\n# Contributor(s):\n#   Mark Pilgrim - port to Python\n#   Shy Shalom - original C code\n#\n# This library is free software; you can redistribute it and/or\n# modify it under the terms of the GNU Lesser General Public\n# License as published by the Free Software Foundation; either\n# version 2.1 of the License, or (at your option) any later version.\n#\n# This library is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2230",
    "name": "index_repo.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/index_repo.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 190,
    "size": 5146,
    "docstring": "",
    "keywords": [],
    "functions": [
      "iter_files",
      "guess_project",
      "classify",
      "tag_for",
      "build_index",
      "write_csv",
      "write_json",
      "main"
    ],
    "classes": [
      "FileEntry"
    ],
    "imports": [
      "__future__",
      "csv",
      "json",
      "os",
      "dataclasses",
      "pathlib",
      "typing"
    ],
    "preview": "from __future__ import annotations\n\nimport csv\nimport json\nimport os\nfrom dataclasses import asdict, dataclass\nfrom pathlib import Path\nfrom typing import Iterable, List, Dict\n\nROOT = Path(__file__).resolve().parents[1]\n\nIGNORE_DIRS = {\n    \".git\",\n    \".venv\",\n    \"venv\",\n    \"__pycache__\",\n    \"node_modules\",\n    \"dist\",\n    \"build\",\n}",
    "last_modified": "2025-09-13T06:01:11.267457"
  },
  {
    "id": "2231",
    "name": "dl_button.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/dl_button.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 321,
    "size": 13013,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "logging",
      "asyncio",
      "json",
      "math",
      "os",
      "shutil",
      "time",
      "datetime",
      "aiohttp",
      "pyrogram"
    ],
    "preview": "import logging\n\nlogging.basicConfig(\n    level=logging.DEBUG, format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\n)\nlogger = logging.getLogger(__name__)\n\nimport asyncio\nimport json\nimport math\nimport os\nimport shutil\nimport time\nfrom datetime import datetime\n\nimport aiohttp\n\n# the secret configuration specific things\nif bool(os.environ.get(\"WEBHOOK\", False)):\n    from sample_config import Config",
    "last_modified": "2025-09-13T05:53:44.016329"
  },
  {
    "id": "2232",
    "name": "organize_albums 14.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/organize_albums 14.py",
    "category": "02_media_processing",
    "type": "organization",
    "lines": 62,
    "size": 2469,
    "docstring": "",
    "keywords": [
      "organization"
    ],
    "functions": [
      "organize_files"
    ],
    "classes": [],
    "imports": [
      "os",
      "shutil"
    ],
    "preview": "import os\nimport shutil\n\n# Define the base directory\nbase_dir = \"/Users/steven/Music/NocTurnE-meLoDieS/v4/mp3\"\n\n\n# Helper function to create folders and move files\ndef organize_files():\n    # List all files in the base directory\n    files = os.listdir(base_dir)\n\n    # Process each file\n    for file in files:\n        # Skip directories\n        if os.path.isdir(os.path.join(base_dir, file)):\n            continue\n\n        # Extract the base name (album name) from the file\n        if file.endswith(\".mp3\"):",
    "last_modified": "2025-08-02T18:25:58.480553"
  },
  {
    "id": "2233",
    "name": "Mp3toMp4ximg.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/Mp3toMp4ximg.py",
    "category": "02_media_processing",
    "type": "image_processing",
    "lines": 53,
    "size": 1737,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "get_cover_image",
      "convert_mp3_to_mp4",
      "process_directory"
    ],
    "classes": [],
    "imports": [
      "glob",
      "os",
      "moviepy.editor",
      "PIL",
      "sys"
    ],
    "preview": "import glob\nimport os\n\nfrom moviepy.editor import AudioFileClip, ImageClip\nfrom PIL import Image\n\n\ndef get_cover_image(file_name, cover_image_directory):\n    # Check for both JPG and PNG extensions\n    jpg_path = os.path.join(cover_image_directory, f\"{file_name}.jpg\")\n    png_path = os.path.join(cover_image_directory, f\"{file_name}.png\")\n\n    if os.path.exists(jpg_path):\n        return jpg_path\n    elif os.path.exists(png_path):\n        return png_path\n    else:\n        print(f\"Cover image not found for {file_name}. Please ensure the cover image exists.\")\n        return None\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2234",
    "name": "gui_utils.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/gui_utils.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 213,
    "size": 6455,
    "docstring": "",
    "keywords": [],
    "functions": [
      "get_checks",
      "get_config",
      "check",
      "modify_settings",
      "delete_background",
      "add_background",
      "unpack_checks",
      "modify_config"
    ],
    "classes": [],
    "imports": [
      "json",
      "re",
      "pathlib",
      "toml",
      "tomlkit",
      "flask"
    ],
    "preview": "import json\nimport re\nfrom pathlib import Path\n\nimport toml\nimport tomlkit\nfrom flask import flash\n\n\n# Get validation checks from template\ndef get_checks():\n    template = toml.load(\"utils/.config.template.toml\")\n    checks = {}\n\n    def unpack_checks(obj: dict):\n        for key in obj.keys():\n            if \"optional\" in obj[key].keys():\n                checks[key] = obj[key]\n            else:\n                unpack_checks(obj[key])",
    "last_modified": "2025-09-13T05:54:00.273216"
  },
  {
    "id": "2235",
    "name": "sagemaker_huggingface.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/sagemaker_huggingface.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 191,
    "size": 7653,
    "docstring": "",
    "keywords": [],
    "functions": [
      "__init__",
      "deploy",
      "__init__",
      "deploy",
      "prepare_and_deploy_model"
    ],
    "classes": [
      "SagemakerHuggingfaceStrategy",
      "DeploymentService"
    ],
    "imports": [
      "enum",
      "typing",
      "loguru",
      "llm_engineering.domain.inference",
      "llm_engineering.settings",
      "boto3",
      "sagemaker.enums",
      "sagemaker.huggingface"
    ],
    "preview": "import enum\nfrom typing import Optional\n\nfrom loguru import logger\n\ntry:\n    import boto3\n    from sagemaker.enums import EndpointType\n    from sagemaker.huggingface import HuggingFaceModel\nexcept ModuleNotFoundError:\n    logger.warning(\n        \"Couldn't load AWS or SageMaker imports. Run 'poetry install --with aws' to support AWS.\"\n    )\n\nfrom llm_engineering.domain.inference import DeploymentStrategy\nfrom llm_engineering.settings import settings\n\n\nclass SagemakerHuggingfaceStrategy(DeploymentStrategy):\n    def __init__(self, deployment_service) -> None:",
    "last_modified": "2025-09-13T05:53:42.048901"
  },
  {
    "id": "2236",
    "name": "info.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/info.py",
    "category": "02_media_processing",
    "type": "organization",
    "lines": 45,
    "size": 1807,
    "docstring": "PrNdOwN module that holds package information.\nNote:\n    All those info could be stored in the __init__ file\n    but we keep them here to keep the code clean.",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "__future__"
    ],
    "preview": "# -*- coding: utf-8 -*-\n\n\"\"\"PrNdOwN module that holds package information.\nNote:\n    All those info could be stored in the __init__ file\n    but we keep them here to keep the code clean.\n\"\"\"\n\nfrom __future__ import unicode_literals\n\n__author__ = \"Younes Ben-El\"\n__contact__ = \"ybenel@pm.me\"\n__projecturl__ = \"https://github.com/m1ndo/PrNdOwN/\"\n\n__appname__ = \"PrNdOwN\"\n__license__ = \"UNLICENSE\"\n\n__description__ = \"Multi Video Sharing Platform Downloader.\"\n\n__descriptionfull__ = (",
    "last_modified": "2025-09-13T05:53:58.946213"
  },
  {
    "id": "2237",
    "name": "best.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/best.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 76,
    "size": 2296,
    "docstring": "",
    "keywords": [
      "web_tools"
    ],
    "functions": [
      "get_bestseller_blueprints",
      "main"
    ],
    "classes": [],
    "imports": [
      "json",
      "os",
      "requests",
      "dotenv"
    ],
    "preview": "import json\nimport os\n\nimport requests\nfrom dotenv import load_dotenv\n\n# Load .env variables\nload_dotenv(dotenv_path=os.path.expanduser(\"~/.env\"))\n\n# Retrieve API Key & Shop Data\nAPI_TOKEN = os.getenv(\"PRINTIFY_API_KEY\")\nSHOP_DATA_RAW = os.getenv(\"PRINTIFY_SHOPS\")\n\n# Ensure environment variable is loaded\nif SHOP_DATA_RAW is None:\n    print(\"\u274c ERROR: PRINTIFY_SHOPS not found in environment!\")\n    exit(1)\n\n# Convert JSON string to a dictionary\nSHOP_DATA = json.loads(SHOP_DATA_RAW)",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2238",
    "name": "img-img-upscale.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/img-img-upscale.py",
    "category": "02_media_processing",
    "type": "image_processing",
    "lines": 36,
    "size": 1606,
    "docstring": "",
    "keywords": [],
    "functions": [
      "convert_and_upscale_images"
    ],
    "classes": [],
    "imports": [
      "os",
      "PIL"
    ],
    "preview": "import os\n\nfrom PIL import Image\n\n\ndef convert_and_upscale_images(directory):\n    # Walk through all directories and files within the specified directory\n    for root, _, files in os.walk(directory):\n        for filename in files:\n            if filename.lower().endswith(\".jpg\") or filename.lower().endswith(\".jpg\"):\n                # Construct full file path\n                file_path = os.path.join(root, filename)\n                # Open the .jpg image\n                with Image.open(file_path) as img:\n                    # Upscale the image by 2x\n                    img = img.resize((img.width * 2, img.height * 2), Image.LANCZOS)\n                    # Set DPI to 300\n                    img.info[\"dpi\"] = (300, 300)\n                    # Convert the image mode to RGB (if not already in that mode)\n                    if img.mode != \"RGB\":",
    "last_modified": "2025-05-04T22:47:13.378971"
  },
  {
    "id": "2239",
    "name": "bot_photo.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/bot_photo.py",
    "category": "02_media_processing",
    "type": "image_processing",
    "lines": 125,
    "size": 4025,
    "docstring": "",
    "keywords": [
      "youtube"
    ],
    "functions": [
      "upload_photo",
      "upload_album",
      "download_photo",
      "download_photos"
    ],
    "classes": [],
    "imports": [
      "os",
      "io",
      "tqdm"
    ],
    "preview": "import os\nfrom io import open\n\nfrom tqdm import tqdm\n\n\ndef upload_photo(\n    self,\n    photo,\n    caption=None,\n    upload_id=None,\n    from_video=False,\n    options={},\n    user_tags=None,\n    is_sidecar=False,\n):\n    \"\"\"Upload photo to Instagram\n\n    @param photo       Path to photo file (String)\n    @param caption     Media description (String)",
    "last_modified": "2025-09-13T05:54:57.950281"
  },
  {
    "id": "2240",
    "name": "9mb.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/9mb.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 101,
    "size": 3925,
    "docstring": "",
    "keywords": [],
    "functions": [
      "resize_image_to_max_size",
      "resize_images_in_directory",
      "main"
    ],
    "classes": [],
    "imports": [
      "os",
      "PIL"
    ],
    "preview": "import os\n\nfrom PIL import Image\n\n\ndef resize_image_to_max_size(image_path, max_size_mb=9, upscale=True):\n    \"\"\"\n    Resize a PNG image to ensure it doesn't exceed the specified max size (in MB) and optionally upscale.\n\n    :param image_path: Path to the input PNG image.\n    :param max_size_mb: Maximum allowed size for the image in megabytes (default is 9MB).\n    :param upscale: Whether to upscale the image if it's smaller than the max size (default is False).\n    \"\"\"\n    max_size_bytes = max_size_mb * 1024 * 1024  # Convert MB to bytes\n    target_dpi = (300, 300)  # Set target DPI\n\n    img = Image.open(image_path)\n    current_size = os.path.getsize(image_path)\n\n    # If the image is smaller than the limit and upscaling is allowed, upscale it",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2241",
    "name": "auto-image-gallery.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/auto-image-gallery.py",
    "category": "02_media_processing",
    "type": "image_processing",
    "lines": 81,
    "size": 2251,
    "docstring": "",
    "keywords": [],
    "functions": [
      "csv_to_html"
    ],
    "classes": [],
    "imports": [
      "csv"
    ],
    "preview": "import csv\n\n\ndef csv_to_html(csv_file, output_html):\n    # Start the HTML structure\n    html_content = \"\"\"\n    <!DOCTYPE html>\n    <html lang=\"en\">\n    <head>\n        <meta charset=\"UTF-8\">\n        <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n        <title>Image Gallery</title>\n        <style>\n            body {\n                font-family: Arial, sans-serif;\n                background-color: #f0f0f0;\n                padding: 20px;\n            }\n            h1 {\n                text-align: center;",
    "last_modified": "2025-05-04T22:47:13.344023"
  },
  {
    "id": "2242",
    "name": "test_gallery_upload.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/test_gallery_upload.py",
    "category": "02_media_processing",
    "type": "youtube",
    "lines": 171,
    "size": 5632,
    "docstring": "",
    "keywords": [
      "testing",
      "analysis"
    ],
    "functions": [
      "create_mock_image",
      "setup_gallery",
      "add_remote_location",
      "test_aws_without_location",
      "test_gallery_not_initialized",
      "test_gallery_not_built",
      "test_upload_aws",
      "test_upload_netlify"
    ],
    "classes": [
      "SPGUploadTestCase"
    ],
    "imports": [
      "json",
      "os",
      "subprocess",
      "sys",
      "unittest",
      "unittest",
      "simplegallery.gallery_build",
      "simplegallery.gallery_init",
      "simplegallery.gallery_upload",
      "PIL"
    ],
    "preview": "import json\nimport os\nimport subprocess\nimport sys\nimport unittest\nfrom unittest import mock\n\nimport simplegallery.gallery_build as gallery_build\nimport simplegallery.gallery_init as gallery_init\nimport simplegallery.gallery_upload as gallery_upload\nfrom PIL import Image\nfrom testfixtures import TempDirectory\n\n\ndef create_mock_image(path, width, height):\n    img = Image.new(\"RGB\", (width, height), color=\"red\")\n    img.save(path)\n    img.close()\n\n",
    "last_modified": "2025-09-13T05:53:53.163234"
  },
  {
    "id": "2243",
    "name": "download_photos_by_hashtag.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/download_photos_by_hashtag.py",
    "category": "02_media_processing",
    "type": "youtube",
    "lines": 28,
    "size": 693,
    "docstring": "instabot example\n\nWorkflow:\n    Download media photos with hashtag.",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "argparse",
      "os",
      "sys",
      "instabot"
    ],
    "preview": "\"\"\"\ninstabot example\n\nWorkflow:\n    Download media photos with hashtag.\n\"\"\"\n\nimport argparse\nimport os\nimport sys\n\nsys.path.append(os.path.join(sys.path[0], \"../\"))\nfrom instabot import Bot  # noqa: E402\n\nparser = argparse.ArgumentParser(add_help=True)\nparser.add_argument(\"-u\", type=str, help=\"username\")\nparser.add_argument(\"-p\", type=str, help=\"password\")\nparser.add_argument(\"-proxy\", type=str, help=\"proxy\")\nparser.add_argument(\"hashtags\", type=str, nargs=\"+\", help=\"hashtags\")\nargs = parser.parse_args()",
    "last_modified": "2025-03-28T18:35:47"
  },
  {
    "id": "2244",
    "name": "botDraw.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/botDraw.py",
    "category": "02_media_processing",
    "type": "web_tools",
    "lines": 220,
    "size": 6471,
    "docstring": "Created in 11/2020\n@Author: Paulo https://github.com/alpdias",
    "keywords": [],
    "functions": [
      "functionDraw",
      "botlogin",
      "findImg",
      "typephrase",
      "commentDraw"
    ],
    "classes": [],
    "imports": [
      "os",
      "random",
      "pathlib",
      "time",
      "art",
      "selenium",
      "selenium.webdriver.common.keys"
    ],
    "preview": "# -*- coding: utf-8 -*-\n\n\"\"\"\nCreated in 11/2020\n@Author: Paulo https://github.com/alpdias\n\"\"\"\n\n# imported libraries\nimport os\nimport random\nfrom pathlib import Path\nfrom time import sleep\n\nimport art\nfrom selenium import webdriver\nfrom selenium.webdriver.common.keys import Keys\n\n\ndef functionDraw(mySystem):\n    \"\"\"",
    "last_modified": "2025-05-04T23:28:20.980430"
  },
  {
    "id": "2245",
    "name": "8mb.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/8mb.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 58,
    "size": 1929,
    "docstring": "",
    "keywords": [],
    "functions": [
      "resize_image_to_max_size",
      "main"
    ],
    "classes": [],
    "imports": [
      "os",
      "PIL"
    ],
    "preview": "import os\n\nfrom PIL import Image\n\n\ndef resize_image_to_max_size(image_path, max_size_mb=8):\n    \"\"\"\n    Resize a PNG image to ensure it doesn't exceed the specified max size (in MB).\n\n    :param image_path: Path to the input PNG image.\n    :param max_size_mb: Maximum allowed size for the image in megabytes (default is 8MB).\n    \"\"\"\n    max_size_bytes = max_size_mb * 1024 * 1024  # Convert MB to bytes\n    target_dpi = (300, 300)  # Set target DPI\n\n    img = Image.open(image_path)\n    current_size = os.path.getsize(image_path)\n\n    # Reduce image size by lowering the resolution\n    width, height = img.size",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2246",
    "name": "macromanprober.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/macromanprober.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 161,
    "size": 6053,
    "docstring": "",
    "keywords": [],
    "functions": [
      "__init__",
      "reset",
      "charset_name",
      "language",
      "feed",
      "get_confidence"
    ],
    "classes": [
      "MacRomanProber"
    ],
    "imports": [
      "typing",
      "charsetprober",
      "enums"
    ],
    "preview": "######################## BEGIN LICENSE BLOCK ########################\n# This code was modified from latin1prober.py by Rob Speer <rob@lumino.so>.\n# The Original Code is Mozilla Universal charset detector code.\n#\n# The Initial Developer of the Original Code is\n# Netscape Communications Corporation.\n# Portions created by the Initial Developer are Copyright (C) 2001\n# the Initial Developer. All Rights Reserved.\n#\n# Contributor(s):\n#   Rob Speer - adapt to MacRoman encoding\n#   Mark Pilgrim - port to Python\n#   Shy Shalom - original C code\n#\n# This library is free software; you can redistribute it and/or\n# modify it under the terms of the GNU Lesser General Public\n# License as published by the Free Software Foundation; either\n# version 2.1 of the License, or (at your option) any later version.\n#\n# This library is distributed in the hope that it will be useful,",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2247",
    "name": "organize_media.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/organize_media.py",
    "category": "02_media_processing",
    "type": "analysis",
    "lines": 130,
    "size": 4232,
    "docstring": "Organize media and related text files by album base name.\n\n- Scans a single-level directory (no recursion) for files.\n- Derives an album_name from the filename by removing the extension and\n  stripping known suffixes like \"_analysis\" and \"_transcript\".\n- Creates a folder named after that album_name.\n- Moves any of these files into that folder, renaming them to a canonical pattern:\n    * {album_name}.mp3\n    * {album_name}.m4a\n    * {album_name}.mp4\n    * {album_name}_analysis.txt\n    * {album_name}_transcript.txt\n    * {album_name}.png    (cover image if present in the base directory)\n- Skips moving if the destination already exists (to avoid overwriting).\n\nUsage:\n    python organize_media.py [BASE_DIR]\n\nIf BASE_DIR is not provided, defaults to:\n    /Users/steven/Music/nocTurneMeLoDieS/MP3\n\nNotes:\n- This script is macOS-friendly and uses only the standard library.\n- File extensions are matched case-insensitively (.MP3 == .mp3).",
    "keywords": [
      "organization"
    ],
    "functions": [
      "normalized_album_name",
      "canonical_dest_name",
      "organize_files"
    ],
    "classes": [],
    "imports": [
      "os",
      "shutil",
      "sys",
      "pathlib"
    ],
    "preview": "#!/usr/bin/env python3\n\"\"\"\nOrganize media and related text files by album base name.\n\n- Scans a single-level directory (no recursion) for files.\n- Derives an album_name from the filename by removing the extension and\n  stripping known suffixes like \"_analysis\" and \"_transcript\".\n- Creates a folder named after that album_name.\n- Moves any of these files into that folder, renaming them to a canonical pattern:\n    * {album_name}.mp3\n    * {album_name}.m4a\n    * {album_name}.mp4\n    * {album_name}_analysis.txt\n    * {album_name}_transcript.txt\n    * {album_name}.png    (cover image if present in the base directory)\n- Skips moving if the destination already exists (to avoid overwriting).\n\nUsage:\n    python organize_media.py [BASE_DIR]\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2248",
    "name": "re_custom.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/re_custom.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 18,
    "size": 74552,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "re"
    ],
    "preview": "import re\n\nurls = ['https: // i.etsystatic.com / 39809135 / r / il / 54e062 / 5612464624 / il_fullxfull.5612464624_pe3m.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 952b6e / 5612464494 / il_fullxfull.5612464494_p1f2.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 3a4749 / 5660551873 / il_fullxfull.5660551873_c6gk.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 0c6bd6 / 5660551875 / il_fullxfull.5660551875_b6bs.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 7558b7 / 5612464116 / il_fullxfull.5612464116_6xq7.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / fd2792 / 5612463620 / il_fullxfull.5612463620_pjsl.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 762abb / 5660551663 / il_fullxfull.5660551663_lzmu.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 0ab9a7 / 5660551265 / il_fullxfull.5660551265_e2pp.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 62dacf / 5612464406 / il_fullxfull.5612464406_6of2.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 73a610 / 5612464432 / il_fullxfull.5612464432_pznr.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / e5cd54 / 5660551795 / il_fullxfull.5660551795_1hw6.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / b58018 / 5660551369 / il_fullxfull.5660551369_k7xo.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 5ced92 / 5660551203 / il_fullxfull.5660551203_cgmz.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 67556d / 5612464582 / il_fullxfull.5612464582_kxew.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 358db0 / 5660551839 / il_fullxfull.5660551839_l9iq.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 1532d6 / 5612464208 / il_fullxfull.5612464208_ibxx.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 3a6217 / 5612464286 / il_fullxfull.5612464286_hgvh.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 76f34c / 5612464152 / il_fullxfull.5612464152_pb3l.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / baf696 / 5612463472 / il_fullxfull.5612463472_emz8.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 532529 / 5660551691 / il_fullxfull.5660551691_n72r.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / c28b90 / 5660550567 / il_fullxfull.5660550567_fxcv.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 9f45bd / 5612463372 / il_fullxfull.5612463372_na2j.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / f060b3 / 5612463222 / il_fullxfull.5612463222_sm48.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / b20eab / 5612338004 / il_fullxfull.5612338004_3apk.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 644ff0 / 5612464586 / il_fullxfull.5612464586_cvey.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 1b3f8b / 5612464330 / il_fullxfull.5612464330_rhim.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 4511f0 / 5612464386 / il_fullxfull.5612464386_etp5.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / c64cbe / 5660551653 / il_fullxfull.5660551653_1ht3.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / ea20ee / 5612464540 / il_fullxfull.5612464540_6lbq.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / c71f9f / 5660551725 / il_fullxfull.5660551725_710g.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / e986c9 / 5660551415 / il_fullxfull.5660551415_dqlq.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / eaf4ec / 5612464388 / il_fullxfull.5612464388_frc6.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 61ed07 / 5612464234 / il_fullxfull.5612464234_ffmg.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 62a492 / 5660550945 / il_fullxfull.5660550945_sg9d.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 41aadb / 5660551267 / il_fullxfull.5660551267_570y.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 5f5ac8 / 5660550455 / il_fullxfull.5660550455_adji.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 768acc / 5660552075 / il_fullxfull.5660552075_cavd.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 78965b / 5612464410 / il_fullxfull.5612464410_h5yb.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 18399c / 5660551669 / il_fullxfull.5660551669_jeuq.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / ea38d7 / 5612464538 / il_fullxfull.5612464538_kkyy.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 998599 / 5612464414 / il_fullxfull.5612464414_ptp9.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 91354b / 5660551555 / il_fullxfull.5660551555_gh73.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 771197 / 5660426721 / il_fullxfull.5660426721_s34s.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / e5a551 / 5612338616 / il_fullxfull.5612338616_o9tq.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 7b67a9 / 5612338942 / il_fullxfull.5612338942_elej.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / aefa74 / 5612338768 / il_fullxfull.5612338768_ofb1.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / c2e59b / 5612338834 / il_fullxfull.5612338834_b7mh.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 2f7a79 / 5612338724 / il_fullxfull.5612338724_3b06.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / f00993 / 5660426273 / il_fullxfull.5660426273_duvq.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 00ebd7 / 5660426753 / il_fullxfull.5660426753_bvil.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / aba6d6 / 5612339172 / il_fullxfull.5612339172_hp5q.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 1a8e4f / 5660425925 / il_fullxfull.5660425925_4mao.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 690156 / 5660426679 / il_fullxfull.5660426679_8t2p.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 7b56d2 / 5660426179 / il_fullxfull.5660426179_f0fe.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / fcb522 / 5660426519 / il_fullxfull.5660426519_4etg.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 13fd98 / 5612338308 / il_fullxfull.5612338308_9053.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / e8af75 / 5660425973 / il_fullxfull.5660425973_8i2s.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 2904f8 / 5660426325 / il_fullxfull.5660426325_is5i.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 0b5b80 / 5660426871 / il_fullxfull.5660426871_gyk8.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 8414a8 / 5612338648 / il_fullxfull.5612338648_t3gf.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 624d10 / 5612339344 / il_fullxfull.5612339344_d7p8.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 52a291 / 5660426387 / il_fullxfull.5660426387_9tyw.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 86050b / 5660426639 / il_fullxfull.5660426639_et9f.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 7380fb / 5660426635 / il_fullxfull.5660426635_1tmu.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / f152ae / 5660426369 / il_fullxfull.5660426369_ocwc.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / a29f60 / 5612338800 / il_fullxfull.5612338800_4bts.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 0255c8 / 5612338968 / il_fullxfull.5612338968_b65a.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 747a25 / 5660425683 / il_fullxfull.5660425683_r1e6.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 6a1aa3 / 5660425987 / il_fullxfull.5660425987_43i5.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 16be65 / 5660426725 / il_fullxfull.5660426725_cfrf.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / c16fb0 / 5660426593 / il_fullxfull.5660426593_4015.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 8ee097 / 5612338722 / il_fullxfull.5612338722_ngnf.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 34463e / 5660425947 / il_fullxfull.5660425947_cfhn.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 5e08ff / 5660426239 / il_fullxfull.5660426239_hczx.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / d2b5e4 / 5612338904 / il_fullxfull.5612338904_e6md.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / f7484b / 5612338882 / il_fullxfull.5612338882_ipw5.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / da0dc5 / 5660426629 / il_fullxfull.5660426629_6pcq.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / e28058 / 5612338310 / il_fullxfull.5612338310_oep4.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 5c8d0e / 5660426139 / il_fullxfull.5660426139_extf.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 87709c / 5612339126 / il_fullxfull.5612339126_hnyd.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 39f6e8 / 5612338480 / il_fullxfull.5612338480_8tmh.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 010556 / 5660425917 / il_fullxfull.5660425917_tft9.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / b1aafc / 5348014299 / il_fullxfull.5348014299_acqz.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 54873b / 5548272527 / il_fullxfull.5548272527_qeo4.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 6822ca / 5587789252 / il_fullxfull.5587789252_42ax.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 231f48 / 5635868653 / il_fullxfull.5635868653_i1gx.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 230e06 / 5635891967 / il_fullxfull.5635891967_tsc6.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 0775af / 5348014153 / il_fullxfull.5348014153_9vgo.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 9d1074 / 5635869097 / il_fullxfull.5635869097_j9x8.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 633b45 / 5635867527 / il_fullxfull.5635867527_j20z.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / e9f196 / 5635851651 / il_fullxfull.5635851651_1a1t.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / abd359 / 5635891671 / il_fullxfull.5635891671_m33w.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / ee3f20 / 5635892385 / il_fullxfull.5635892385_cs6z.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 465f10 / 5587766032 / il_fullxfull.5587766032_l933.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 761cdd / 5587765822 / il_fullxfull.5587765822_b69l.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 3d4ba1 / 5587764566 / il_fullxfull.5587764566_a7tm.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / e2f5bd / 5635851051 / il_fullxfull.5635851051_mxhg.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 9c0afa / 5635890893 / il_fullxfull.5635890893_4gy5.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 3657cb / 5635868889 / il_fullxfull.5635868889_i4o8.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / af6bed / 5587765920 / il_fullxfull.5587765920_8y0i.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / d2ec6c / 5587748896 / il_fullxfull.5587748896_mjg0.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 6207e6 / 5587789102 / il_fullxfull.5587789102_bg36.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / e218b6 / 5587748288 / il_fullxfull.5587748288_j911.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / a1d5f7 / 5587748292 / il_fullxfull.5587748292_jv6c.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / bd008a / 5635890579 / il_fullxfull.5635890579_mbee.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 4f90d0 / 5587749046 / il_fullxfull.5587749046_bcz7.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 6dbf2c / 5635851873 / il_fullxfull.5635851873_1jj0.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 53aa1c / 5635851767 / il_fullxfull.5635851767_c46u.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 1c22c0 / 5635891869 / il_fullxfull.5635891869_onlb.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / fb627a / 5635891677 / il_fullxfull.5635891677_egof.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 7f6eae / 5587765814 / il_fullxfull.5587765814_fzou.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 461978 / 5635851189 / il_fullxfull.5635851189_9ooc.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 0e1653 / 5635851129 / il_fullxfull.5635851129_a713.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / bf765e / 5587788202 / il_fullxfull.5587788202_mq7g.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 9bc5ab / 5635868931 / il_fullxfull.5635868931_4dz3.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 5ff018 / 5587765866 / il_fullxfull.5587765866_4zmx.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 25c6c4 / 5635892323 / il_fullxfull.5635892323_504i.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 671fb3 / 5587787234 / il_fullxfull.5587787234_qlr3.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 6869d9 / 5587765752 / il_fullxfull.5587765752_762g.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 3919e0 / 5635892317 / il_fullxfull.5635892317_l6h6.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / d32ec5 / 5587788652 / il_fullxfull.5587788652_fsiz.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 99d91f / 5635868995 / il_fullxfull.5635868995_2j4k.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 38ce5c / 5587764886 / il_fullxfull.5587764886_c79a.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / efb2f7 / 5635851525 / il_fullxfull.5635851525_c60z.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 2b092e / 5587789586 / il_fullxfull.5587789586_4wsd.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 069b58 / 5635891201 / il_fullxfull.5635891201_7pcw.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 15ff01 / 5587765580 / il_fullxfull.5587765580_tqb3.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / c3a4ab / 5587787412 / il_fullxfull.5587787412_5dx4.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / ca6b9d / 5587766194 / il_fullxfull.5587766194_gnae.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / a22735 / 5587765878 / il_fullxfull.5587765878_grla.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 88ec7a / 5587765100 / il_fullxfull.5587765100_pi7b.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / eee24e / 5587748230 / il_fullxfull.5587748230_d5lt.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / d2bb30 / 5587748176 / il_fullxfull.5587748176_p3k6.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / e933c7 / 5635891565 / il_fullxfull.5635891565_sgny.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 7a8f3c / 5587787284 / il_fullxfull.5587787284_6mxy.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 2bb685 / 5587749130 / il_fullxfull.5587749130_3mne.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 7207cb / 5635852117 / il_fullxfull.5635852117_gdfn.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / aa96a9 / 5587788582 / il_fullxfull.5587788582_chva.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 8b0aa1 / 5587765998 / il_fullxfull.5587765998_stzf.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 7dcde3 / 5635868759 / il_fullxfull.5635868759_ifol.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / c0936e / 5635868403 / il_fullxfull.5635868403_jwkq.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / b74464 / 5587788708 / il_fullxfull.5587788708_7l1n.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 8f386a / 5635891667 / il_fullxfull.5635891667_4io5.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 02a42c / 5635890031 / il_fullxfull.5635890031_t1vb.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 9a02d4 / 5635868709 / il_fullxfull.5635868709_f948.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 22f43d / 5635851143 / il_fullxfull.5635851143_1a9n.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 561bd4 / 5635890777 / il_fullxfull.5635890777_6wil.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / fd4c5b / 5635868825 / il_fullxfull.5635868825_jbzr.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 2d5173 / 5635867273 / il_fullxfull.5635867273_20x1.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 0da93a / 5635852119 / il_fullxfull.5635852119_4496.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / bf709c / 5635869071 / il_fullxfull.5635869071_gimj.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 877739 / 5635867761 / il_fullxfull.5635867761_8z8t.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 815f14 / 5635867277 / il_fullxfull.5635867277_1crg.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / dbb7c1 / 5635867449 / il_fullxfull.5635867449_rnve.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / f71716 / 5635851123 / il_fullxfull.5635851123_h40k.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 33c828 / 5587788026 / il_fullxfull.5587788026_2y3s.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 0ef547 / 5587765036 / il_fullxfull.5587765036_f6m5.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 872209 / 5587748954 / il_fullxfull.5587748954_53li.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / dcef45 / 5587788392 / il_fullxfull.5587788392_8bnr.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / b55a04 / 5587765756 / il_fullxfull.5587765756_kj2w.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 174c25 / 5635867209 / il_fullxfull.5635867209_p2h8.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / d866ec / 5635851045 / il_fullxfull.5635851045_igcs.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / cb5291 / 5635892457 / il_fullxfull.5635892457_jlij.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / f41303 / 5635890945 / il_fullxfull.5635890945_74yu.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / c9c48f / 5635868251 / il_fullxfull.5635868251_gut4.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / ba8743 / 5635851867 / il_fullxfull.5635851867_tljl.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 3fcfa0 / 5635851933 / il_fullxfull.5635851933_m9h7.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / d32963 / 5635868489 / il_fullxfull.5635868489_iucc.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / ec8cd0 / 5587749202 / il_fullxfull.5587749202_nf67.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / ff142e / 5635850943 / il_fullxfull.5635850943_nf1y.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / c7a5a6 / 5635891795 / il_fullxfull.5635891795_7jsl.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / a7d067 / 5587765324 / il_fullxfull.5587765324_hjqu.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / d7e6d8 / 5587765046 / il_fullxfull.5587765046_ar5f.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 801d04 / 5635851773 / il_fullxfull.5635851773_scyb.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 812a83 / 5587748756 / il_fullxfull.5587748756_csou.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / afc543 / 5587748644 / il_fullxfull.5587748644_cdi9.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / e1d183 / 5587748586 / il_fullxfull.5587748586_nqz1.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / c9e4d9 / 5635892401 / il_fullxfull.5635892401_4pam.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / c83ba5 / 5635868545 / il_fullxfull.5635868545_62dk.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / a8c068 / 5587766106 / il_fullxfull.5587766106_tmox.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 531c1b / 5587747884 / il_fullxfull.5587747884_774s.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 024c0a / 5635892015 / il_fullxfull.5635892015_dnjk.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 52903d / 5587765386 / il_fullxfull.5587765386_7aea.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 2f5201 / 5587764696 / il_fullxfull.5587764696_4z1s.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 814356 / 5587748824 / il_fullxfull.5587748824_7pml.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / d49c62 / 5587789620 / il_fullxfull.5587789620_trhh.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / ed83a2 / 5587765636 / il_fullxfull.5587765636_3vme.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 23be88 / 5635851713 / il_fullxfull.5635851713_2sow.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 9c5d7f / 5587747818 / il_fullxfull.5587747818_dfn7.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / b57202 / 5635892409 / il_fullxfull.5635892409_nsg8.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / c8e72f / 5635851469 / il_fullxfull.5635851469_lj5d.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 39a8bb / 5587748828 / il_fullxfull.5587748828_1efs.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 89a341 / 5635851413 / il_fullxfull.5635851413_gxoc.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 08ebc4 / 5587789250 / il_fullxfull.5587789250_rwpr.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / e1f54d / 5587789186 / il_fullxfull.5587789186_f3zr.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 6d6300 / 5587765682 / il_fullxfull.5587765682_1ujw.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 35d1d1 / 5635850899 / il_fullxfull.5635850899_dhe4.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 876ae6 / 5635850847 / il_fullxfull.5635850847_hmu4.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 5e402f / 5635892523 / il_fullxfull.5635892523_ijzi.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 37d167 / 5587789314 / il_fullxfull.5587789314_tbtt.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 794461 / 5587788710 / il_fullxfull.5587788710_cgq6.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / e2b2b6 / 5635869013 / il_fullxfull.5635869013_2gwg.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / b861d4 / 5635851719 / il_fullxfull.5635851719_7ylx.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / ee412b / 5635851295 / il_fullxfull.5635851295_58pu.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 8df5ce / 5587788884 / il_fullxfull.5587788884_1n64.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / bea484 / 5635891725 / il_fullxfull.5635891725_3ux8.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 83ded5 / 5587749048 / il_fullxfull.5587749048_dc4k.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 5dccd2 / 5635852121 / il_fullxfull.5635852121_gvqm.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 9ab5f2 / 5587747722 / il_fullxfull.5587747722_749o.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / d3b5ba / 5587789310 / il_fullxfull.5587789310_mhhd.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / bb70a8 / 5587766078 / il_fullxfull.5587766078_rrc8.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / fac023 / 5635867653 / il_fullxfull.5635867653_imni.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 6f889b / 5635867763 / il_fullxfull.5635867763_avdu.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / b4f5e0 / 5635851673 / il_fullxfull.5635851673_rcgc.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 8ac530 / 5587789188 / il_fullxfull.5587789188_23v5.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / f730bb / 5635891411 / il_fullxfull.5635891411_6va6.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / c86f1b / 5635852115 / il_fullxfull.5635852115_9wvu.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / bf6ea3 / 5635850513 / il_fullxfull.5635850513_puel.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 3cf201 / 5584460266 / il_fullxfull.5584460266_of6u.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 1b1c5e / 5632572291 / il_fullxfull.5632572291_1054.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / a1dc06 / 5584460590 / il_fullxfull.5584460590_p9ll.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 234873 / 5632572727 / il_fullxfull.5632572727_8uo1.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 5ad017 / 5584460732 / il_fullxfull.5584460732_188a.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 3e5d24 / 5584460960 / il_fullxfull.5584460960_6xww.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / fb34d5 / 5584461104 / il_fullxfull.5584461104_hy5h.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 6a0cad / 5632573123 / il_fullxfull.5632573123_ei41.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 92dc41 / 5632573243 / il_fullxfull.5632573243_18em.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / e3b0f8 / 5584461322 / il_fullxfull.5584461322_bejn.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 941630 / 5632573351 / il_fullxfull.5632573351_7vlj.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 89ee52 / 5632573387 / il_fullxfull.5632573387_pak1.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / cc5652 / 5584461376 / il_fullxfull.5584461376_aygg.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / b64084 / 5584461488 / il_fullxfull.5584461488_9fbq.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / b9cc41 / 5632573485 / il_fullxfull.5632573485_hkiu.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 74f206 / 5584461470 / il_fullxfull.5584461470_6t7k.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 72315c / 5632573443 / il_fullxfull.5632573443_kboa.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 5762aa / 5632573607 / il_fullxfull.5632573607_25qz.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / bab30e / 5632573671 / il_fullxfull.5632573671_ayk5.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 1721e9 / 5632573663 / il_fullxfull.5632573663_h8wr.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / e750e4 / 5584461744 / il_fullxfull.5584461744_tuzl.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 1e081b / 5584461718 / il_fullxfull.5584461718_rblf.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 2a7e33 / 5584461570 / il_fullxfull.5584461570_7aoz.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 08fd92 / 5583100720 / il_fullxfull.5583100720_ec8t.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / b578fe / 5631207829 / il_fullxfull.5631207829_3jxp.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 92f782 / 5583085208 / il_fullxfull.5583085208_scrh.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 56b0cb / 5583085194 / il_fullxfull.5583085194_1pvv.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / aa4c70 / 5631192157 / il_fullxfull.5631192157_qo5g.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / d4f7e6 / 5583085170 / il_fullxfull.5583085170_pqqe.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / bafba6 / 5618898593 / il_fullxfull.5618898593_dpc5.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 86b5f4 / 5570793404 / il_fullxfull.5570793404_1usg.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 422e36 / 5618894133 / il_fullxfull.5618894133_pti4.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / b9722f / 5570793314 / il_fullxfull.5570793314_1e6z.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / c4d010 / 5570793418 / il_fullxfull.5570793418_cu0g.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 66a664 / 5618894259 / il_fullxfull.5618894259_mk5n.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 96d8de / 5618898595 / il_fullxfull.5618898595_dc02.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 001c5e / 5570797636 / il_fullxfull.5570797636_qe85.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 9699d9 / 5618898543 / il_fullxfull.5618898543_1ohg.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 56e1b0 / 5618894557 / il_fullxfull.5618894557_133i.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / cc414b / 5618894289 / il_fullxfull.5618894289_ihwc.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 3a5ce1 / 5618894185 / il_fullxfull.5618894185_lgmj.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / ecdd4c / 5570793462 / il_fullxfull.5570793462_fpnb.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 030069 / 5618894327 / il_fullxfull.5618894327_k05l.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / be2e88 / 5570797728 / il_fullxfull.5570797728_jyoi.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 0ae18b / 5618898601 / il_fullxfull.5618898601_mx0u.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 91f168 / 5570797628 / il_fullxfull.5570797628_ibgh.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / acb34b / 5618894535 / il_fullxfull.5618894535_6gjf.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 781d98 / 5618894085 / il_fullxfull.5618894085_5z3f.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 790a42 / 5570793180 / il_fullxfull.5570793180_f3z8.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 05363f / 5618894581 / il_fullxfull.5618894581_i1sq.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 6c894a / 5618894593 / il_fullxfull.5618894593_n4a7.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / ce39d2 / 5618894361 / il_fullxfull.5618894361_7zap.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / c04749 / 5618894087 / il_fullxfull.5618894087_gsmq.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 5f95e0 / 5618894421 / il_fullxfull.5618894421_j0c1.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 6b7f4d / 5618894413 / il_fullxfull.5618894413_f82l.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 079ddc / 5618893973 / il_fullxfull.5618893973_m9ht.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / f1a6e2 / 5570797638 / il_fullxfull.5570797638_4g9y.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 3d20de / 5570797724 / il_fullxfull.5570797724_cmdf.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 8e3aa2 / 5570797646 / il_fullxfull.5570797646_8o22.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 6a3c4d / 5570793734 / il_fullxfull.5570793734_cu8i.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 6b88ec / 5570793370 / il_fullxfull.5570793370_lvq4.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 72fd41 / 5618894187 / il_fullxfull.5618894187_9zaz.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 5a0d15 / 5570793176 / il_fullxfull.5570793176_qous.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 714f50 / 5570643774 / il_fullxfull.5570643774_s78j.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / a00904 / 5299837054 / il_fullxfull.5299837054_3g2f.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 691815 / 5563057752 / il_fullxfull.5563057752_spmb.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / d43ee4 / 5611157649 / il_fullxfull.5611157649_7f13.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / ba24d3 / 5563057676 / il_fullxfull.5563057676_apk1.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 62b1ad / 5611157647 / il_fullxfull.5611157647_pfbo.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / b33220 / 5611157469 / il_fullxfull.5611157469_h2md.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 042ee9 / 5611157823 / il_fullxfull.5611157823_nr3q.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / bcc600 / 5563057794 / il_fullxfull.5563057794_6kq9.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 600d32 / 5563057662 / il_fullxfull.5563057662_msdx.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 38240a / 5563057668 / il_fullxfull.5563057668_a5uq.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 5d5d53 / 5611157701 / il_fullxfull.5611157701_jg8n.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 9fdbe1 / 5611157757 / il_fullxfull.5611157757_ausj.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / d2a2ba / 5611157587 / il_fullxfull.5611157587_hv19.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 92ab6d / 5611157619 / il_fullxfull.5611157619_qevz.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 57eadb / 5611157667 / il_fullxfull.5611157667_nk1r.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 32cc4e / 5611157761 / il_fullxfull.5611157761_8x0v.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 5f197f / 5563057574 / il_fullxfull.5563057574_5c1q.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 04b47f / 5563057374 / il_fullxfull.5563057374_60yr.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / e6cb2d / 5611157733 / il_fullxfull.5611157733_5cje.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 21767b / 5611157871 / il_fullxfull.5611157871_j0b2.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 0082f6 / 5611157783 / il_fullxfull.5611157783_pm1a.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 871437 / 5608671961 / il_fullxfull.5608671961_cni4.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 6f2ebd / 5560570518 / il_fullxfull.5560570518_q0os.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / caffd4 / 5560570366 / il_fullxfull.5560570366_o8dc.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / ac7bb7 / 5560570832 / il_fullxfull.5560570832_il0d.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / e3cde7 / 5608672087 / il_fullxfull.5608672087_p5oe.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 5b9676 / 5608672505 / il_fullxfull.5608672505_dzkf.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 6f9645 / 5608672313 / il_fullxfull.5608672313_mywb.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 81bbb8 / 5608672449 / il_fullxfull.5608672449_luje.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / e27536 / 5560570244 / il_fullxfull.5560570244_jai6.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 112e00 / 5560570410 / il_fullxfull.5560570410_nx7q.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 811e21 / 5608671939 / il_fullxfull.5608671939_isrc.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 4f8107 / 5560570332 / il_fullxfull.5560570332_3pek.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 9f126c / 5608672139 / il_fullxfull.5608672139_ie8j.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 9fe3da / 5560571160 / il_fullxfull.5560571160_lsz8.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 8fa972 / 5608672057 / il_fullxfull.5608672057_1be5.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 6a7fc4 / 5608671899 / il_fullxfull.5608671899_js1i.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / c4a9ec / 5608672055 / il_fullxfull.5608672055_cpph.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 2b2cb8 / 5608671769 / il_fullxfull.5608671769_t4ea.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / b86d0d / 5608671859 / il_fullxfull.5608671859_pims.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 713f2e / 5560570068 / il_fullxfull.5560570068_nbvr.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / a10799 / 5608671345 / il_fullxfull.5608671345_qtf6.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 97a33f / 5560571104 / il_fullxfull.5560571104_e8iq.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / f72563 / 5608672315 / il_fullxfull.5608672315_68xa.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 6f1c1b / 5608672371 / il_fullxfull.5608672371_b88j.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / acf221 / 5608671833 / il_fullxfull.5608671833_lnrb.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / f99dfc / 5560571018 / il_fullxfull.5560571018_gsqo.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 007ce0 / 5560570386 / il_fullxfull.5560570386_o9tb.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 95d594 / 5560570184 / il_fullxfull.5560570184_mbf5.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 08437e / 5560570268 / il_fullxfull.5560570268_d4ym.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 8536b1 / 5560570502 / il_fullxfull.5560570502_mfb6.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / c0fbb3 / 5560569954 / il_fullxfull.5560569954_3n4y.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / b1b4a9 / 5608672375 / il_fullxfull.5608672375_a7lr.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 980bd3 / 5560571172 / il_fullxfull.5560571172_bb17.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 506577 / 5608672081 / il_fullxfull.5608672081_61qh.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / a51a96 / 5560570630 / il_fullxfull.5560570630_7x5q.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / f6ed31 / 5560570460 / il_fullxfull.5560570460_j3ns.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 6fcb9f / 5560570414 / il_fullxfull.5560570414_6cnb.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 3fd28c / 5608671787 / il_fullxfull.5608671787_tely.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 122fa5 / 5560570474 / il_fullxfull.5560570474_6fno.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / d61857 / 5608671455 / il_fullxfull.5608671455_qw8s.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 5ef07f / 5560570362 / il_fullxfull.5560570362_11sl.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / b730e6 / 5608671973 / il_fullxfull.5608671973_qggc.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / a78007 / 5560570096 / il_fullxfull.5560570096_rh5r.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / b641a2 / 5608672351 / il_fullxfull.5608672351_a7u8.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 748a36 / 5608671981 / il_fullxfull.5608671981_ptt3.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 5e3866 / 5608672229 / il_fullxfull.5608672229_t1ha.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 848531 / 5560570162 / il_fullxfull.5560570162_b5yg.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / b077fb / 5608671883 / il_fullxfull.5608671883_113c.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 3a28ab / 5560570420 / il_fullxfull.5560570420_ldxg.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 4c1e8f / 5560570590 / il_fullxfull.5560570590_drxy.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 552463 / 5608671139 / il_fullxfull.5608671139_rjvy.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 66be5b / 5607123301 / il_fullxfull.5607123301_g4hd.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / a32fb7 / 5546976064 / il_fullxfull.5546976064_3jm7.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / d792e2 / 5595091411 / il_fullxfull.5595091411_3hsw.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 86840f / 5548210248 / il_fullxfull.5548210248_8unk.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / b87b80 / 5548209960 / il_fullxfull.5548209960_sj00.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 0e1431 / 5596330047 / il_fullxfull.5596330047_qe0a.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 5ede9f / 5596329927 / il_fullxfull.5596329927_aruc.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 18257a / 5596329823 / il_fullxfull.5596329823_fpvj.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 5c2b84 / 5548209998 / il_fullxfull.5548209998_oqtk.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 4f89ab / 5596329745 / il_fullxfull.5596329745_hpp5.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 288db7 / 5596330157 / il_fullxfull.5596330157_l61h.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 41a2b3 / 5596330053 / il_fullxfull.5596330053_afzu.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 361464 / 5548209968 / il_fullxfull.5548209968_98wu.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 3c2ad8 / 5596330139 / il_fullxfull.5596330139_jdze.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 9ac5e2 / 5596329819 / il_fullxfull.5596329819_bhk4.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / c8bf02 / 5548210254 / il_fullxfull.5548210254_q205.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 0376d2 / 5596329991 / il_fullxfull.5596329991_cknj.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 9cb2d5 / 5548209902 / il_fullxfull.5548209902_ojoj.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / c691a3 / 5548209846 / il_fullxfull.5548209846_rdbi.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 890c93 / 5596329989 / il_fullxfull.5596329989_mzup.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 69a72c / 5548147576 / il_fullxfull.5548147576_smrc.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 0bc12c / 5596268149 / il_fullxfull.5596268149_1ta7.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 8c268c / 5548147428 / il_fullxfull.5548147428_inoz.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 3852d9 / 5596267993 / il_fullxfull.5596267993_6xxn.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / ad9062 / 5547063976 / il_fullxfull.5547063976_e1q7.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 65d411 / 5547063898 / il_fullxfull.5547063898_6c8x.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / b0da0a / 5547063554 / il_fullxfull.5547063554_lyc9.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / d874be / 5595179663 / il_fullxfull.5595179663_cgjn.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 50c79f / 5547064020 / il_fullxfull.5547064020_b28f.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 074e99 / 5595179919 / il_fullxfull.5595179919_rpl8.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 41d61c / 5547063932 / il_fullxfull.5547063932_a5tz.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 2a94cc / 5595180117 / il_fullxfull.5595180117_31w8.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 6ac9d2 / 5547063892 / il_fullxfull.5547063892_oxnv.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / ce0305 / 5547064080 / il_fullxfull.5547064080_gjl5.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 725627 / 5547064014 / il_fullxfull.5547064014_gb4n.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / cffd1b / 5547064024 / il_fullxfull.5547064024_1okl.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 60bf42 / 5547063700 / il_fullxfull.5547063700_e06e.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / c1cce8 / 5595180043 / il_fullxfull.5595180043_dso4.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 0100e1 / 5547063608 / il_fullxfull.5547063608_gqf7.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / ad0eaa / 5547063946 / il_fullxfull.5547063946_mq6r.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 79e0a7 / 5595179767 / il_fullxfull.5595179767_9e7g.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 60b038 / 5547063810 / il_fullxfull.5547063810_frio.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / ef8aa8 / 5595180081 / il_fullxfull.5595180081_rej6.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 665b4b / 5547064060 / il_fullxfull.5547064060_tw1r.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / f6f3a1 / 5547064094 / il_fullxfull.5547064094_ffo4.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 457559 / 5547063502 / il_fullxfull.5547063502_dnex.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 662b21 / 5595179507 / il_fullxfull.5595179507_2rc7.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 0b1772 / 5546997006 / il_fullxfull.5546997006_rd8z.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 7db867 / 5547013878 / il_fullxfull.5547013878_70vt.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 6b94ce / 5546996408 / il_fullxfull.5546996408_tict.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 3170dc / 5546997446 / il_fullxfull.5546997446_hrqt.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 8872f8 / 5546997404 / il_fullxfull.5546997404_ny8p.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 2752b8 / 5547013876 / il_fullxfull.5547013876_aay5.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 747129 / 5546996214 / il_fullxfull.5546996214_nwpx.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 6bf33f / 5595111187 / il_fullxfull.5595111187_577c.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / da48f5 / 5595111193 / il_fullxfull.5595111193_6b9t.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 3789bd / 5547013962 / il_fullxfull.5547013962_9zd2.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 01838b / 5547014012 / il_fullxfull.5547014012_rm62.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / b47477 / 5547013800 / il_fullxfull.5547013800_o4op.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 1f4b62 / 5546996334 / il_fullxfull.5546996334_gt0y.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 34d69e / 5546996884 / il_fullxfull.5546996884_6ejb.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / fd1203 / 5595111271 / il_fullxfull.5595111271_ofd5.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 9da5df / 5547013576 / il_fullxfull.5547013576_93le.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / f3c5ad / 5595129401 / il_fullxfull.5595129401_8xhj.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 05685f / 5547013794 / il_fullxfull.5547013794_swxn.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / d3fef0 / 5546997226 / il_fullxfull.5546997226_bwft.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 27503d / 5546997198 / il_fullxfull.5546997198_2de8.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / c8255e / 5595111429 / il_fullxfull.5595111429_o13y.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 6d8460 / 5546995848 / il_fullxfull.5546995848_qyga.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 4b9663 / 5595111227 / il_fullxfull.5595111227_pbd3.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 8d14aa / 5595129783 / il_fullxfull.5595129783_jz3v.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / b41986 / 5595112643 / il_fullxfull.5595112643_50y8.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 91dc59 / 5546995948 / il_fullxfull.5546995948_2jqa.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / eac666 / 5595129403 / il_fullxfull.5595129403_p013.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 884482 / 5546997396 / il_fullxfull.5546997396_zm36.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 96b91a / 5546997144 / il_fullxfull.5546997144_8poq.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 0a38bb / 5595111183 / il_fullxfull.5595111183_m0zk.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / e65cfd / 5547013928 / il_fullxfull.5547013928_dlmv.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / ad599f / 5595129649 / il_fullxfull.5595129649_ggvg.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / cf659f / 5546997322 / il_fullxfull.5546997322_mamh.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 0416bb / 5595111151 / il_fullxfull.5595111151_3dy9.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 2a92bd / 5547013964 / il_fullxfull.5547013964_l122.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 98b9aa / 5546997398 / il_fullxfull.5546997398_26p0.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / ef6c88 / 5546996846 / il_fullxfull.5546996846_7khd.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / a68d2b / 5546995794 / il_fullxfull.5546995794_t2zo.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 26d642 / 5595111297 / il_fullxfull.5595111297_ljfv.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 6d7319 / 5595129687 / il_fullxfull.5595129687_bv5r.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 568bca / 5547013826 / il_fullxfull.5547013826_p3w6.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / e6ccfb / 5595129525 / il_fullxfull.5595129525_5vv2.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 5c4664 / 5595111605 / il_fullxfull.5595111605_ksfq.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 9308fd / 5546997042 / il_fullxfull.5546997042_31ro.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 797652 / 5595112753 / il_fullxfull.5595112753_7egl.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / fe56c4 / 5595129641 / il_fullxfull.5595129641_mr3f.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / d9bd05 / 5547013616 / il_fullxfull.5547013616_7tef.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 6ca03e / 5595129533 / il_fullxfull.5595129533_pbpb.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / e8ad43 / 5546997282 / il_fullxfull.5546997282_6jk5.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 42e41c / 5595112821 / il_fullxfull.5595112821_gad0.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 282e3e / 5595111487 / il_fullxfull.5595111487_lcd8.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / e49ec7 / 5595111391 / il_fullxfull.5595111391_1z5s.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / ebbcdf / 5595112013 / il_fullxfull.5595112013_5ump.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / cd2bc4 / 5595129723 / il_fullxfull.5595129723_r1k1.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / b261d6 / 5547013832 / il_fullxfull.5547013832_oroa.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / a44bd6 / 5595111813 / il_fullxfull.5595111813_8qwk.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / d87152 / 5546997224 / il_fullxfull.5546997224_abxg.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / bf6d4b / 5546996274 / il_fullxfull.5546996274_rylv.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 66af71 / 5546997188 / il_fullxfull.5546997188_h4dx.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / bcd51d / 5595129521 / il_fullxfull.5595129521_hflm.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / db4a1f / 5547013802 / il_fullxfull.5547013802_98u0.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / ef4b93 / 5595112997 / il_fullxfull.5595112997_l669.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 623b66 / 5595112635 / il_fullxfull.5595112635_t3q5.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / fd2dfd / 5595112723 / il_fullxfull.5595112723_qmjo.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 08c5b9 / 5595112955 / il_fullxfull.5595112955_5hqy.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 81afb1 / 5595112171 / il_fullxfull.5595112171_50mz.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / b13c03 / 5546996956 / il_fullxfull.5546996956_bomv.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / fa5377 / 5546996184 / il_fullxfull.5546996184_rnto.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 91a80f / 5595091583 / il_fullxfull.5595091583_nnp1.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 58a277 / 5546976036 / il_fullxfull.5546976036_ahr8.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 5f402e / 5546862250 / il_fullxfull.5546862250_ktq2.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 669fb6 / 5546862212 / il_fullxfull.5546862212_q2k7.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 9ee009 / 5594977679 / il_fullxfull.5594977679_fq94.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / d5743a / 5594977691 / il_fullxfull.5594977691_eb27.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / fb9aa6 / 5546862210 / il_fullxfull.5546862210_4hz1.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 7edb28 / 5546862136 / il_fullxfull.5546862136_3lm5.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / f4ffa7 / 5546862150 / il_fullxfull.5546862150_36r9.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 1cee2e / 5546862170 / il_fullxfull.5546862170_fvxi.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 5ab591 / 5594952441 / il_fullxfull.5594952441_ho2b.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 74abbd / 5546836878 / il_fullxfull.5546836878_jjml.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 7b877e / 5546836976 / il_fullxfull.5546836976_i6r1.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 26f57e / 5546836550 / il_fullxfull.5546836550_4hfi.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 8381b4 / 5594952363 / il_fullxfull.5594952363_b5ss.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / f6dcec / 5546837654 / il_fullxfull.5546837654_gudk.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 2eff8c / 5594952899 / il_fullxfull.5594952899_bguk.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 23a8d9 / 5546837270 / il_fullxfull.5546837270_6lg8.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 9fea39 / 5594952493 / il_fullxfull.5594952493_4ibk.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / c1480e / 5594951151 / il_fullxfull.5594951151_k7jm.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / dddc9b / 5594953051 / il_fullxfull.5594953051_g3jp.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 338710 / 5546837774 / il_fullxfull.5546837774_fh83.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 5a5704 / 5546837452 / il_fullxfull.5546837452_r62j.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 15d742 / 5594952959 / il_fullxfull.5594952959_crsw.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 7d10fb / 5594952955 / il_fullxfull.5594952955_77k2.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / ee79a7 / 5546835916 / il_fullxfull.5546835916_lxzh.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 463aec / 5594952657 / il_fullxfull.5594952657_ojqo.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 292919 / 5594953255 / il_fullxfull.5594953255_ae91.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 9bb9ba / 5594953225 / il_fullxfull.5594953225_1vhd.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / d482aa / 5546837600 / il_fullxfull.5546837600_h9vr.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / b1802f / 5594951353 / il_fullxfull.5594951353_rifd.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / e64099 / 5546837846 / il_fullxfull.5546837846_s7jh.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / a3bd51 / 5594953163 / il_fullxfull.5594953163_9fu1.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / e70b3f / 5546837416 / il_fullxfull.5546837416_qfhc.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / f84306 / 5546837686 / il_fullxfull.5546837686_nm1e.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 095e2e / 5546837364 / il_fullxfull.5546837364_ia3d.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 15495a / 5546837148 / il_fullxfull.5546837148_mveq.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 364bcb / 5594953219 / il_fullxfull.5594953219_ryo2.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 2146a1 / 5594953103 / il_fullxfull.5594953103_c0mb.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 57ef36 / 5546838050 / il_fullxfull.5546838050_7wgr.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 9472a6 / 5546837844 / il_fullxfull.5546837844_hqdq.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 0b86d3 / 5594952533 / il_fullxfull.5594952533_lkhv.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 3503dc / 5594952821 / il_fullxfull.5594952821_afz9.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 59a964 / 5546837186 / il_fullxfull.5546837186_6wet.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / d86e5d / 5594952695 / il_fullxfull.5594952695_i0vg.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 956482 / 5594953193 / il_fullxfull.5594953193_qu9e.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 4fb6bb / 5546837680 / il_fullxfull.5546837680_la2v.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 2f1f27 / 5546837322 / il_fullxfull.5546837322_5bhu.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 476363 / 5546837874 / il_fullxfull.5546837874_fkjg.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / dcadf8 / 5594952853 / il_fullxfull.5594952853_4wjh.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 4511c4 / 5546837560 / il_fullxfull.5546837560_h5gc.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / e3d5c0 / 5546836234 / il_fullxfull.5546836234_mory.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / ae4bde / 5546837318 / il_fullxfull.5546837318_na2x.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 3727cf / 5546837646 / il_fullxfull.5546837646_kg6b.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 6479fc / 5594953131 / il_fullxfull.5594953131_qjq3.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / f7b33c / 5546730500 / il_fullxfull.5546730500_bkyf.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 3636e2 / 5594846235 / il_fullxfull.5594846235_9cc8.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 5b27a9 / 5546729780 / il_fullxfull.5546729780_3ms2.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 5b7116 / 5594846463 / il_fullxfull.5594846463_hboc.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 19aced / 5594846241 / il_fullxfull.5594846241_remk.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / b1e4ce / 5546729852 / il_fullxfull.5546729852_7evj.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 77b432 / 5546731094 / il_fullxfull.5546731094_d9oh.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / cb8168 / 5546731176 / il_fullxfull.5546731176_t9dd.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / bb0d8c / 5526167088 / il_fullxfull.5526167088_ccmf.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / b5fbdf / 5526163178 / il_fullxfull.5526163178_96ti.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / cd2ead / 5574276177 / il_fullxfull.5574276177_a116.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 93423e / 5526157570 / il_fullxfull.5526157570_jrb1.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / f0a3ff / 5526158626 / il_fullxfull.5526158626_1nkk.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 9b9db7 / 5574265251 / il_fullxfull.5574265251_toit.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 5e90cb / 5526146532 / il_fullxfull.5526146532_6qu9.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / b8dff9 / 5500166038 / il_fullxfull.5500166038_5r68.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 186ac5 / 5548272517 / il_fullxfull.5548272517_o3sx.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / b8c6b9 / 5500165760 / il_fullxfull.5500165760_lwzo.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 6a237c / 5548272471 / il_fullxfull.5548272471_thyq.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / ab6afe / 5548272747 / il_fullxfull.5548272747_d122.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / e45eb1 / 5500165810 / il_fullxfull.5500165810_by0c.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 20aa38 / 5548272707 / il_fullxfull.5548272707_igmr.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 37a835 / 5500165744 / il_fullxfull.5500165744_708o.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 576e2d / 5500166126 / il_fullxfull.5500166126_fbpa.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 97ecde / 5548272507 / il_fullxfull.5548272507_8cvg.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 8c3a50 / 5548272433 / il_fullxfull.5548272433_b6ak.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / cb9789 / 5546891081 / il_fullxfull.5546891081_58o1.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / de9b15 / 5546400945 / il_fullxfull.5546400945_5fs7.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / c4064b / 5530638307 / il_fullxfull.5530638307_q9sp.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / bf8229 / 5530597947 / il_fullxfull.5530597947_bolg.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / ce2085 / 5482430876 / il_fullxfull.5482430876_idud.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 70c174 / 5530539203 / il_fullxfull.5530539203_6l6i.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / e63379 / 5530323023 / il_fullxfull.5530323023_ocpl.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 08e1a1 / 5530315219 / il_fullxfull.5530315219_n970.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 0c2d59 / 5530247937 / il_fullxfull.5530247937_fhll.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 6957f9 / 5481866192 / il_fullxfull.5481866192_f9yu.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 711533 / 5529791743 / il_fullxfull.5529791743_s9cw.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 8f861f / 5414215299 / il_fullxfull.5414215299_2vmm.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 78fa9a / 5366049074 / il_fullxfull.5366049074_pm4b.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / a15b8f / 5414214957 / il_fullxfull.5414214957_5v18.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / dfbf1a / 5414214817 / il_fullxfull.5414214817_jxb1.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 2f9f49 / 5414162631 / il_fullxfull.5414162631_l8qi.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / df5809 / 5414162515 / il_fullxfull.5414162515_1nq8.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 0907c3 / 5414162493 / il_fullxfull.5414162493_8ro4.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 872097 / 5365996244 / il_fullxfull.5365996244_elme.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 6a2465 / 5414162777 / il_fullxfull.5414162777_o0pc.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 3cb083 / 5414162819 / il_fullxfull.5414162819_l34z.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / e1d256 / 5414162727 / il_fullxfull.5414162727_429e.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / e26549 / 5366048664 / il_fullxfull.5366048664_giwl.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 904d02 / 5366048692 / il_fullxfull.5366048692_kgad.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 8b4021 / 5365996266 / il_fullxfull.5365996266_anuk.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / fb0080 / 5365996052 / il_fullxfull.5365996052_stp8.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 62c4df / 5414215057 / il_fullxfull.5414215057_2jt6.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 3c53d0 / 5366049080 / il_fullxfull.5366049080_3zu4.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 8cdefb / 5414162721 / il_fullxfull.5414162721_m5cj.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 916a15 / 5414162577 / il_fullxfull.5414162577_jg24.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / f44eee / 5411820397 / il_fullxfull.5411820397_po0w.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 502467 / 5329912762 / il_fullxfull.5329912762_nwqe.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / cc90cd / 5377993355 / il_fullxfull.5377993355_bd5n.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / d2e8fb / 5377782179 / il_fullxfull.5377782179_sxdu.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 25a255 / 5329471914 / il_fullxfull.5329471914_o3jh.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / f963c2 / 5325332548 / il_fullxfull.5325332548_mqcr.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 252b1c / 5348013979 / il_fullxfull.5348013979_2bt8.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 5a7b1f / 5299836412 / il_fullxfull.5299836412_9r2r.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 44500f / 5348014053 / il_fullxfull.5348014053_r58i.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 36807f / 5348013947 / il_fullxfull.5348013947_2wf8.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / eed0dc / 5299836636 / il_fullxfull.5299836636_6e5g.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 101644 / 5299836622 / il_fullxfull.5299836622_4qsb.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 88c711 / 5299835936 / il_fullxfull.5299835936_hd85.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / b66632 / 5299836964 / il_fullxfull.5299836964_9z1s.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / def0df / 5348014321 / il_fullxfull.5348014321_mw08.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 14b18a / 5299836838 / il_fullxfull.5299836838_5sgu.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 639528 / 5299836914 / il_fullxfull.5299836914_gzjp.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 7f968d / 5348014377 / il_fullxfull.5348014377_rece.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 792b4a / 5299836726 / il_fullxfull.5299836726_fuqs.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / cf4d6e / 5348014285 / il_fullxfull.5348014285_iwk7.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 4bab85 / 5348014289 / il_fullxfull.5348014289_fh86.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 2c190e / 5348014081 / il_fullxfull.5348014081_88vg.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 217a53 / 5299836836 / il_fullxfull.5299836836_nddh.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 3b17dc / 5348014401 / il_fullxfull.5348014401_lvfs.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 3a4fb7 / 5348014375 / il_fullxfull.5348014375_3s4c.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 66b650 / 5299836866 / il_fullxfull.5299836866_6sz3.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 567875 / 5299836834 / il_fullxfull.5299836834_5y0e.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / a2ff8f / 5299836770 / il_fullxfull.5299836770_9z16.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / abec50 / 5299837254 / il_fullxfull.5299837254_nkjw.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 69a93e / 5299836860 / il_fullxfull.5299836860_6hbt.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / eb0722 / 5299836968 / il_fullxfull.5299836968_t82y.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / fae3a9 / 5299837048 / il_fullxfull.5299837048_88ot.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 8c9ae8 / 5348014183 / il_fullxfull.5348014183_4pzg.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 736e91 / 5348014311 / il_fullxfull.5348014311_opd9.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / b21d1b / 5348014403 / il_fullxfull.5348014403_yd9x.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 1cf383 / 5348014179 / il_fullxfull.5348014179_lh6t.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 13b439 / 5299836862 / il_fullxfull.5299836862_f5tb.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / e3958d / 5299837044 / il_fullxfull.5299837044_iwsk.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 8271fd / 5348014611 / il_fullxfull.5348014611_nowu.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / d492fc / 5348014161 / il_fullxfull.5348014161_4eap.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / ce97a7 / 5348014397 / il_fullxfull.5348014397_d0ld.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 4cce27 / 5348014287 / il_fullxfull.5348014287_1nx2.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / e36891 / 5299836940 / il_fullxfull.5299836940_40pq.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 405ddc / 5348014227 / il_fullxfull.5348014227_jps9.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 7a514c / 5299836912 / il_fullxfull.5299836912_60zx.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / c4cfd6 / 5299836832 / il_fullxfull.5299836832_69tu.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 6d488a / 5270374412 / il_fullxfull.5270374412_1xck.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / a1c91a / 5318563419 / il_fullxfull.5318563419_34ta.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / b8e736 / 5318563609 / il_fullxfull.5318563609_sg43.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / bbdd69 / 5318563707 / il_fullxfull.5318563707_4y0u.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 0147df / 5270374540 / il_fullxfull.5270374540_iavc.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 52a36d / 5318563691 / il_fullxfull.5318563691_e0ez.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 3dbdcc / 5270374486 / il_fullxfull.5270374486_ebgt.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 1591a9 / 5318563457 / il_fullxfull.5318563457_a8jz.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / c0cebe / 5318563661 / il_fullxfull.5318563661_174s.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 574d48 / 5318563721 / il_fullxfull.5318563721_1ctl.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / a8e27f / 5270374480 / il_fullxfull.5270374480_1h0z.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 4d9cbe / 5312745099 / il_fullxfull.5312745099_pd96.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 6c0f8a / 5294836353 / il_fullxfull.5294836353_jdv4.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / a9e4dc / 5294836291 / il_fullxfull.5294836291_4m0b.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / fdf286 / 5246648932 / il_fullxfull.5246648932_emis.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 81ac57 / 5246648846 / il_fullxfull.5246648846_9fho.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / cdf566 / 5294836255 / il_fullxfull.5294836255_30uz.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / c1e98f / 5294836249 / il_fullxfull.5294836249_nzf7.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / ec6638 / 5246648886 / il_fullxfull.5246648886_2obh.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 9f894f / 5294836361 / il_fullxfull.5294836361_nvmw.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / e6353b / 5294836367 / il_fullxfull.5294836367_mpof.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 013fc2 / 5246648854 / il_fullxfull.5246648854_p6rt.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 9dca20 / 5228617450 / il_fullxfull.5228617450_7042.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 4121a9 / 5276750835 / il_fullxfull.5276750835_fyko.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / ccf5c4 / 5276767997 / il_fullxfull.5276767997_p2qk.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / fd8eff / 5215043473 / il_fullxfull.5215043473_lstk.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 471b3b / 5214965187 / il_fullxfull.5214965187_nt4h.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 73f3e4 / 5214448171 / il_fullxfull.5214448171_o8w0.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / f16328 / 5090876676 / il_fullxfull.5090876676_edmc.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 7b8e6d / 5090866272 / il_fullxfull.5090866272_fory.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 47f6ea / 5090860042 / il_fullxfull.5090860042_3gh2.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 08bfaa / 5090855734 / il_fullxfull.5090855734_3atf.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / f7846b / 5139076229 / il_fullxfull.5139076229_l1d9.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / f41438 / 5082443774 / il_fullxfull.5082443774_czyo.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / e78c66 / 5166800768 / il_fullxfull.5166800768_ib50.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 0148d9 / 5082406136 / il_fullxfull.5082406136_a9jz.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / ce4d6b / 5082387368 / il_fullxfull.5082387368_9qsj.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 73d0e5 / 5130608965 / il_fullxfull.5130608965_fixu.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / c248dc / 5130589291 / il_fullxfull.5130589291_guhn.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / bc7bd0 / 5130573223 / il_fullxfull.5130573223_afv0.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 348f6d / 5082329378 / il_fullxfull.5082329378_6oj1.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / f01389 / 5082301910 / il_fullxfull.5082301910_l8jh.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 77a54b / 5130696949 / il_fullxfull.5130696949_s218.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / 241189 / 4737567890 / il_fullxfull.4737567890_saw5.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / e80f29 / 4785805099 / il_fullxfull.4785805099_23to.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / ffdc9e / 4779429393 / il_fullxfull.4779429393_fjmu.jpg, 'https: // i.etsystatic.com / 39809135 / r / il / 397892 / 4729964776 / il_fullxfull.4729964776_esj4.jpg,         'https: // i.etsystatic.com / 39809135 / r / il / aacc2b / 4727859616 / il_fullxfull.4727859616_l33j.jpg,\n        ]\n\n# Regex pattern to capture the variable part up to 'il_fullxfull.'\npattern = r''https // i\\.etsystatic\\.com /\\d + /r / il / [0 - 9a - f] + /\\d + /il_fullxfull\\.'\n\n# Replace the matched pattern with an empty string\nupdated_urls = [\n    re.sub(\n        pattern,\n        '/Users/steven/Pictures/etsy-bulk',\n        url) for url in urls]\n\n# Display the updated URLs\nprint(updated_urls)\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2249",
    "name": "leodown_20250102111110.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/leodown_20250102111110.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 51,
    "size": 1326,
    "docstring": "",
    "keywords": [
      "web_tools"
    ],
    "functions": [],
    "classes": [],
    "imports": [
      "json",
      "requests"
    ],
    "preview": "import json\n\nimport requests\n\n# Base URL for API\nbase_url = (\n    \"https://cloud.leonardo.ai/api/rest/v1/generations/user/f7bb8476-e3f0-4f1f-9a06-4600866cc49c\"\n)\nheaders = {\n    \"accept\": \"application/json\",\n    \"authorization\": \"Bearer 93043291-957d-4ec1-8c79-ee734abcb6e3\",  # Replace with your actual token\n}\n\n# Output file\noutput_file = \"/Users/steven/Pictures/leonardo_library.json\"\n\n# Pagination parameters\noffset = 0\nlimit = 10  # Maximum number of items per request\nall_generations = []",
    "last_modified": "2025-09-13T05:53:49.638943"
  },
  {
    "id": "2250",
    "name": "organize_albums 15.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/organize_albums 15.py",
    "category": "02_media_processing",
    "type": "organization",
    "lines": 62,
    "size": 2470,
    "docstring": "",
    "keywords": [
      "organization"
    ],
    "functions": [
      "organize_files"
    ],
    "classes": [],
    "imports": [
      "os",
      "shutil"
    ],
    "preview": "import os\nimport shutil\n\n# Define the base directory\nbase_dir = \"/Users/steven/Music/NocTurnE-meLoDieS/albums/\"\n\n\n# Helper function to create folders and move files\ndef organize_files():\n    # List all files in the base directory\n    files = os.listdir(base_dir)\n\n    # Process each file\n    for file in files:\n        # Skip directories\n        if os.path.isdir(os.path.join(base_dir, file)):\n            continue\n\n        # Extract the base name (album name) from the file\n        if file.endswith(\".mp3\"):",
    "last_modified": "2025-08-02T18:25:58.493471"
  },
  {
    "id": "2251",
    "name": "gui.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/gui.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 1041,
    "size": 45274,
    "docstring": "",
    "keywords": [
      "analysis",
      "youtube"
    ],
    "functions": [
      "main",
      "__init__",
      "show_welcome_popup",
      "open_link",
      "load_gui_config_options",
      "save_gui_config_options",
      "on_log_level_change",
      "create_gui_frames_widgets",
      "add_general_options_widgets",
      "add_youtube_title_widgets"
    ],
    "classes": [
      "YouTubeBulkUploaderGUI",
      "ReusableWidgetFrame",
      "Tooltip",
      "TextHandler",
      "DualLogger"
    ],
    "imports": [
      "json",
      "logging",
      "os",
      "sys",
      "threading",
      "tkinter",
      "pathlib",
      "tkinter",
      "pkg_resources",
      "youtube_bulk_upload"
    ],
    "preview": "import json\nimport logging\nimport os\nimport sys\nimport threading\nimport tkinter as tk\nfrom pathlib import Path\nfrom tkinter import filedialog, messagebox, scrolledtext, simpledialog\n\nimport pkg_resources\nfrom youtube_bulk_upload import YouTubeBulkUpload\n\n\nclass YouTubeBulkUploaderGUI:\n    def __init__(\n        self,\n        gui_root: tk.Tk,\n        logger: logging.Logger,\n        bundle_dir: Path,\n        running_in_pyinstaller: bool,",
    "last_modified": "2025-09-13T05:53:47.026555"
  },
  {
    "id": "2252",
    "name": "2leomotion 1.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/2leomotion 1.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 90,
    "size": 3097,
    "docstring": "",
    "keywords": [
      "web_tools"
    ],
    "functions": [],
    "classes": [],
    "imports": [
      "time",
      "requests"
    ],
    "preview": "import time\n\nimport requests\n\napi_key = \"b5b99021-8e7a-42ef-8df9-4eca2c6efd3c\"\nauthorization = \"Bearer %s\" % api_key\n\nheaders = {\n    \"accept\": \"application/json\",\n    \"content-type\": \"application/json\",\n    \"authorization\": authorization,\n}\n\n# Generate an image\nurl = \"https://cloud.leonardo.ai/api/rest/v1/generations\"\n\npayload = {\n    \"height\": 960,\n    \"modelId\": \"ac614f96-1082-45bf-be9d-757f2d31c174\",\n    \"prompt\": \"A detailed photograph of a serious cyberpunk Hacker Cyborg transhumanist the past looking directly at the camera, standing straight, hands relaxed, square jaws, masculine face, dark scruff and no wrinkles, slightly buff looking, wearing a dark graphic t-shirt, detailed clothing texture realistic skin texture, black background, sharp focus, front view, waist up shot, high contrast, strong backlighting, action film dark color lut, cinematic luts\",",
    "last_modified": "2025-05-04T22:47:13.035804"
  },
  {
    "id": "2253",
    "name": "charsetgroupprober.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/charsetgroupprober.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 105,
    "size": 3885,
    "docstring": "",
    "keywords": [],
    "functions": [
      "__init__",
      "reset",
      "charset_name",
      "language",
      "feed",
      "get_confidence"
    ],
    "classes": [
      "CharSetGroupProber"
    ],
    "imports": [
      "typing",
      "charsetprober",
      "enums"
    ],
    "preview": "######################## BEGIN LICENSE BLOCK ########################\n# The Original Code is Mozilla Communicator client code.\n#\n# The Initial Developer of the Original Code is\n# Netscape Communications Corporation.\n# Portions created by the Initial Developer are Copyright (C) 1998\n# the Initial Developer. All Rights Reserved.\n#\n# Contributor(s):\n#   Mark Pilgrim - port to Python\n#\n# This library is free software; you can redistribute it and/or\n# modify it under the terms of the GNU Lesser General Public\n# License as published by the Free Software Foundation; either\n# version 2.1 of the License, or (at your option) any later version.\n#\n# This library is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU\n# Lesser General Public License for more details.",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2254",
    "name": "sbcsgroupprober.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/sbcsgroupprober.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 89,
    "size": 4137,
    "docstring": "",
    "keywords": [],
    "functions": [
      "__init__"
    ],
    "classes": [
      "SBCSGroupProber"
    ],
    "imports": [
      "charsetgroupprober",
      "hebrewprober",
      "langbulgarianmodel",
      "langgreekmodel",
      "langhebrewmodel",
      "langrussianmodel",
      "langthaimodel",
      "langturkishmodel",
      "sbcharsetprober"
    ],
    "preview": "######################## BEGIN LICENSE BLOCK ########################\n# The Original Code is Mozilla Universal charset detector code.\n#\n# The Initial Developer of the Original Code is\n# Netscape Communications Corporation.\n# Portions created by the Initial Developer are Copyright (C) 2001\n# the Initial Developer. All Rights Reserved.\n#\n# Contributor(s):\n#   Mark Pilgrim - port to Python\n#   Shy Shalom - original C code\n#\n# This library is free software; you can redistribute it and/or\n# modify it under the terms of the GNU Lesser General Public\n# License as published by the Free Software Foundation; either\n# version 2.1 of the License, or (at your option) any later version.\n#\n# This library is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2255",
    "name": "good_usage_of_blacklist.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/good_usage_of_blacklist.py",
    "category": "02_media_processing",
    "type": "organization",
    "lines": 151,
    "size": 5054,
    "docstring": "This template is written by @jeremycjang\n\nWhat does this quickstart script aim to do?\n- Here's the configuration I use the most.\n\nNOTES:\n- Read the incredibly amazing advices & ideas from my experience at the end\nof this file :>",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "instapy"
    ],
    "preview": "\"\"\"\nThis template is written by @jeremycjang\n\nWhat does this quickstart script aim to do?\n- Here's the configuration I use the most.\n\nNOTES:\n- Read the incredibly amazing advices & ideas from my experience at the end\nof this file :>\n\"\"\"\n\nfrom instapy import InstaPy, smart_run\n\ninsta_username = \"username\"\ninsta_password = \"password\"\n\n# get a session!\nsession = InstaPy(\n    username=insta_username,\n    password=insta_password,",
    "last_modified": "2025-03-28T18:36:56"
  },
  {
    "id": "2256",
    "name": "leonardo2.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/leonardo2.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 129,
    "size": 4447,
    "docstring": "",
    "keywords": [
      "analysis",
      "youtube",
      "web_tools"
    ],
    "functions": [
      "convert_image_to_jpeg",
      "get_presigned_url",
      "upload_image",
      "upscale_image",
      "get_upscaled_image",
      "process_images"
    ],
    "classes": [],
    "imports": [
      "json",
      "os",
      "sys",
      "time",
      "requests",
      "PIL"
    ],
    "preview": "import json\nimport os\nimport sys\nimport time\n\nimport requests\nfrom PIL import Image\n\napi_key = \"de7c9cb8-022f-42f8-8bf7-a8f9caadfaee\"\nauthorization = f\"Bearer {api_key}\"\n\nheaders = {\n    \"accept\": \"application/json\",\n    \"content-type\": \"application/json\",\n    \"authorization\": authorization,\n}\n\n# Styles to apply\nstyles = [\"GENERAL\", \"CINEMATIC\", \"2D ART & ILLUSTRATION\", \"CG ART & GAME ASSETS\"]\n",
    "last_modified": "2025-09-13T05:53:50.276394"
  },
  {
    "id": "2257",
    "name": "virtualenv.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/virtualenv.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 105,
    "size": 3456,
    "docstring": "",
    "keywords": [],
    "functions": [
      "_running_under_venv",
      "_running_under_legacy_virtualenv",
      "running_under_virtualenv",
      "_get_pyvenv_cfg_lines",
      "_no_global_under_venv",
      "_no_global_under_legacy_virtualenv",
      "virtualenv_no_global"
    ],
    "classes": [],
    "imports": [
      "logging",
      "os",
      "re",
      "site",
      "sys",
      "typing"
    ],
    "preview": "import logging\nimport os\nimport re\nimport site\nimport sys\nfrom typing import List, Optional\n\nlogger = logging.getLogger(__name__)\n_INCLUDE_SYSTEM_SITE_PACKAGES_REGEX = re.compile(\n    r\"include-system-site-packages\\s*=\\s*(?P<value>true|false)\"\n)\n\n\ndef _running_under_venv() -> bool:\n    \"\"\"Checks if sys.base_prefix and sys.prefix match.\n\n    This handles PEP 405 compliant virtual environments.\n    \"\"\"\n    return sys.prefix != getattr(sys, \"base_prefix\", sys.prefix)\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2258",
    "name": "test_variables.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/test_variables.py",
    "category": "02_media_processing",
    "type": "testing",
    "lines": 708,
    "size": 26281,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [],
    "preview": "DEFAULT_RESPONSE = {\"status\": \"ok\"}\n\nTEST_CAPTION_ITEM = {\n    \"bit_flags\": 0,\n    \"content_type\": \"comment\",\n    \"created_at\": 1494733796,\n    \"created_at_utc\": 1494733796,\n    \"did_report_as_spam\": False,\n    \"pk\": 17856098620165444,\n    \"status\": \"Active\",\n    \"text\": \"Old Harry Rocks, Dorset, UK\\n\\n#oldharryrocks #dorset #uk \"\n    + \"#rocks #clouds #water #photoshoot #nature #amazing #beautifulsky #sky \"\n    + \"#landscape #nice #beautiful #awesome #landscapes #l4l #f4f\",\n    \"type\": 1,\n    \"user\": {\n        \"full_name\": \"The best earth places\",\n        \"is_private\": False,\n        \"is_verified\": False,\n        \"pk\": 182696006,\n        \"profile_pic_id\": \"1477989239094674784_182696006\",",
    "last_modified": "2025-03-28T18:35:47"
  },
  {
    "id": "2259",
    "name": "YouTubeBot 2.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/YouTubeBot 2.py",
    "category": "02_media_processing",
    "type": "youtube",
    "lines": 160,
    "size": 5082,
    "docstring": "",
    "keywords": [
      "web_tools"
    ],
    "functions": [
      "fetch",
      "filter",
      "duration_split",
      "start"
    ],
    "classes": [],
    "imports": [
      "time",
      "tkinter",
      "tkinter.ttk",
      "pyautogui",
      "PIL",
      "selenium",
      "requests"
    ],
    "preview": "import time\nimport tkinter as tk\nimport tkinter.ttk as ttk\n\nimport pyautogui\nfrom PIL import Image, ImageTk\nfrom selenium import webdriver\n\nheight = pyautogui.size()[1]\nwidth = pyautogui.size()[0]\nprint(\"resolution = \" + str(width) + \", \" + str(height))\nwindow = tk.Tk()\nwindow.title(\"YouTube Bot\")\nwindow.resizable(0, 0)\nwindow.configure(background=\"white\")\nwindow.rowconfigure([0], minsize=round(width / 96), weight=0)\nwindow.columnconfigure([0, 2], minsize=round(width / 24), weight=0)\nwindow.columnconfigure(1, minsize=round(width / 2.13), weight=0)\n\n",
    "last_modified": "2025-09-13T05:54:08.142856"
  },
  {
    "id": "2260",
    "name": "loop-upscale copy 2.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/loop-upscale copy 2.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 58,
    "size": 2241,
    "docstring": "",
    "keywords": [
      "web_tools"
    ],
    "functions": [],
    "classes": [],
    "imports": [
      "json",
      "os",
      "time",
      "requests"
    ],
    "preview": "import json\nimport os\nimport time\n\nimport requests\n\napi_key = \"3ca7a396-64c0-4e22-8cad-b00bc972d4b3\"\nauthorization = \"Bearer %s\" % api_key\n\nheaders = {\n    \"accept\": \"application/json\",\n    \"content-type\": \"application/json\",\n    \"authorization\": authorization,\n}\n\n# Directory containing images\ndirectory_path = \"/Users/steven/Pictures/CookiMonster/1\"\n\n# Loop through each file in the directory\nfor filename in os.listdir(directory_path):",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2261",
    "name": "motion-upload 2.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/motion-upload 2.py",
    "category": "02_media_processing",
    "type": "youtube",
    "lines": 58,
    "size": 1678,
    "docstring": "",
    "keywords": [
      "web_tools"
    ],
    "functions": [],
    "classes": [],
    "imports": [
      "json",
      "time",
      "requests"
    ],
    "preview": "import json\nimport time\n\nimport requests\n\napi_key = \"<YOUR_API_KEY>\"\nauthorization = \"Bearer %s\" % api_key\n\nheaders = {\n    \"accept\": \"application/json\",\n    \"content-type\": \"application/json\",\n    \"authorization\": authorization,\n}\n\n# Get a presigned URL for uploading an image\nurl = \"https://cloud.leonardo.ai/api/rest/v1/init-image\"\n\npayload = {\"extension\": \"jpg\"}\n\nresponse = requests.post(url, json=payload, headers=headers)",
    "last_modified": "2025-05-04T22:47:12.936867"
  },
  {
    "id": "2262",
    "name": "imgconvert_colab copy.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/imgconvert_colab copy.py",
    "category": "02_media_processing",
    "type": "image_processing",
    "lines": 53,
    "size": 1729,
    "docstring": "",
    "keywords": [],
    "functions": [
      "convert_and_upscale_images",
      "main"
    ],
    "classes": [],
    "imports": [
      "os",
      "PIL"
    ],
    "preview": "import os\n\nfrom PIL import Image\n\n# Function to convert WebP images to PNG and upscale by 200% with 300 DPI\n\n\ndef convert_and_upscale_images(source_directory, destination_directory):\n    # Create the destination directory if it doesn't exist\n    os.makedirs(destination_directory, exist_ok=True)\n\n    for filename in os.listdir(source_directory):\n        if filename.endswith(\".png\"):\n            source_file = os.path.join(source_directory, filename)\n            filename_no_ext = os.path.splitext(filename)[0]\n            destination_file = os.path.join(destination_directory, f\"{filename_no_ext}.png\")\n\n            # Convert WebP to PNG and upscale by 200% with 300 DPI\n            im = Image.open(source_file)\n            width, height = im.size",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2263",
    "name": "upscaled.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/upscaled.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 174,
    "size": 5876,
    "docstring": "",
    "keywords": [
      "youtube",
      "web_tools"
    ],
    "functions": [
      "convert_image_to_jpeg",
      "get_presigned_url",
      "upload_image",
      "upscale_image",
      "get_upscaled_image",
      "initialize_csv",
      "log_to_csv"
    ],
    "classes": [],
    "imports": [
      "csv",
      "json",
      "os",
      "time",
      "datetime",
      "requests",
      "PIL"
    ],
    "preview": "import csv\nimport json\nimport os\nimport time\nfrom datetime import datetime\n\nimport requests\nfrom PIL import Image\n\napi_key = \"de7c9cb8-022f-42f8-8bf7-a8f9caadfaee\"\nauthorization = f\"Bearer {api_key}\"\n\nheaders = {\n    \"accept\": \"application/json\",\n    \"content-type\": \"application/json\",\n    \"authorization\": authorization,\n}\n\n# Directory containing images\ndirectory_path = \"/Users/steven/Pictures/TrashCaT/trashy-heartbreak\"",
    "last_modified": "2025-09-13T05:53:56.051707"
  },
  {
    "id": "2264",
    "name": "csv_from_json.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/csv_from_json.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 29,
    "size": 913,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "csv",
      "json"
    ],
    "preview": "import csv\nimport json\n\n# Paths for the JSON file and CSV output\njson_file_path = \"/Users/steven/Pictures/etsy/bubble-html/Tshirt/ChatGPT-TrashCat_Junkyard_Singer_Inspires_Change.json\"\ncsv_file_path = \"/Users/steven/Documents/output.csv\"\n\n# Load JSON data\nwith open(json_file_path, \"r\") as json_file:\n    data = json.load(json_file)\n\n# Open CSV file for writing\nwith open(csv_file_path, \"w\", newline=\"\") as csv_file:\n    csv_writer = csv.writer(csv_file)\n\n    # Write header (keys from JSON)\n    csv_writer.writerow([\"Title\", \"Author\", \"Theme\", \"Story\", \"Date\"])\n\n    # Write data\n    title = data.get(\"title\", \"N/A\")",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2265",
    "name": "leo-download-csv.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/leo-download-csv.py",
    "category": "02_media_processing",
    "type": "youtube",
    "lines": 83,
    "size": 2454,
    "docstring": "",
    "keywords": [
      "web_tools"
    ],
    "functions": [
      "save_urls_to_csv",
      "fetch_and_save_all_urls"
    ],
    "classes": [],
    "imports": [
      "csv",
      "os",
      "requests",
      "tqdm"
    ],
    "preview": "import csv\nimport os\n\nimport requests\nfrom tqdm import tqdm\n\n# Configuration\nBASE_URL = (\n    \"https://cloud.leonardo.ai/api/rest/v1/generations/user/f7bb8476-e3f0-4f1f-9a06-4600866cc49c\"\n)\nAUTH_TOKEN = \"Bearer 93043291-957d-4ec1-8c79-ee734abcb6e3\"\nOUTPUT_DIR = \"/Users/steven/Pictures/leodowns\"\nCSV_FILE = os.path.join(OUTPUT_DIR, \"leonardo_urls.csv\")\nMAX_RECORDS_PER_BATCH = 50  # Adjust based on API constraints\n\nHEADERS = {\n    \"accept\": \"application/json\",\n    \"authorization\": AUTH_TOKEN,\n}\n",
    "last_modified": "2025-09-13T05:53:50.017720"
  },
  {
    "id": "2266",
    "name": "imgupscale copy.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/imgupscale copy.py",
    "category": "02_media_processing",
    "type": "image_processing",
    "lines": 69,
    "size": 2829,
    "docstring": "",
    "keywords": [],
    "functions": [
      "convert_and_upscale_images",
      "main"
    ],
    "classes": [],
    "imports": [
      "os",
      "PIL"
    ],
    "preview": "import os\n\nfrom PIL import Image, UnidentifiedImageError\n\n\n# Function to convert and upscale PNG and JPEG images by 200% with 300 DPI\ndef convert_and_upscale_images(source_directory, destination_directory, max_size_mb=8):\n    # Create the destination directory if it doesn't exist\n    os.makedirs(destination_directory, exist_ok=True)\n\n    for filename in os.listdir(source_directory):\n        if filename.lower().endswith((\".png\", \".jpeg\", \".jpg\")):\n            source_file = os.path.join(source_directory, filename)\n            filename_no_ext = os.path.splitext(filename)[0]\n            ext = filename.split(\".\")[-1].lower()\n            destination_file = os.path.join(destination_directory, f\"{filename_no_ext}.{ext}\")\n\n            try:\n                # Convert and upscale PNG or JPEG\n                with Image.open(source_file) as im:",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2267",
    "name": "auto_post.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/auto_post.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 60,
    "size": 1545,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "glob",
      "os",
      "sys",
      "time",
      "io",
      "instabot"
    ],
    "preview": "import glob\nimport os\nimport sys\nimport time\nfrom io import open\n\nsys.path.append(os.path.join(sys.path[0], \"../../\"))\nfrom instabot import Bot  # noqa: E402\n\nposted_pic_list = []\ntry:\n    with open(\"pics.txt\", \"r\", encoding=\"utf8\") as f:\n        posted_pic_list = f.read().splitlines()\nexcept Exception:\n    posted_pic_list = []\n\ntimeout = 24 * 60 * 60  # pics will be posted every 24 hours\n\nbot = Bot()\nbot.login()",
    "last_modified": "2025-03-28T18:35:47"
  },
  {
    "id": "2268",
    "name": "leonardo_script.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/leonardo_script.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 129,
    "size": 4439,
    "docstring": "",
    "keywords": [
      "analysis",
      "youtube",
      "web_tools"
    ],
    "functions": [
      "convert_image_to_jpeg",
      "get_presigned_url",
      "upload_image",
      "upscale_image",
      "get_upscaled_image",
      "process_images"
    ],
    "classes": [],
    "imports": [
      "json",
      "os",
      "sys",
      "time",
      "requests",
      "PIL"
    ],
    "preview": "import json\nimport os\nimport sys\nimport time\n\nimport requests\nfrom PIL import Image\n\napi_key = \"de7c9cb8-022f-42f8-8bf7-a8f9caadfaee\"\nauthorization = f\"Bearer {api_key}\"\n\nheaders = {\n    \"accept\": \"application/json\",\n    \"content-type\": \"application/json\",\n    \"authorization\": authorization,\n}\n\n# Styles to apply\nstyles = [\"GENERAL\", \"CINEMATIC\", \"2D_ART_ILLUSTRATION\", \"PHOTOREALISTIC\"]\n",
    "last_modified": "2025-09-13T05:53:50.510331"
  },
  {
    "id": "2269",
    "name": "dalle-html.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/dalle-html.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 110,
    "size": 3240,
    "docstring": "",
    "keywords": [],
    "functions": [
      "csv_to_html"
    ],
    "classes": [],
    "imports": [
      "csv"
    ],
    "preview": "import csv\n\n\ndef csv_to_html(csv_file, output_html):\n    # Start the HTML structure\n    html_content = \"\"\"\n    <!DOCTYPE html>\n    <html lang=\"en\">\n    <head>\n        <meta charset=\"UTF-8\">\n        <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n        <title>Image Gallery</title>\n        <style>\n            body {\n                font-family: Arial, sans-serif;\n                background-color: #1a1a1a;\n                color: #fff;\n                margin: 0;\n                padding: 0;\n            }",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2270",
    "name": "organize_albums 11.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/organize_albums 11.py",
    "category": "02_media_processing",
    "type": "organization",
    "lines": 62,
    "size": 2466,
    "docstring": "",
    "keywords": [
      "organization"
    ],
    "functions": [
      "organize_files"
    ],
    "classes": [],
    "imports": [
      "os",
      "shutil"
    ],
    "preview": "import os\nimport shutil\n\n# Define the base directory\nbase_dir = \"/Users/steven/Music/NocTurnE-meLoDieS/mp3\"\n\n\n# Helper function to create folders and move files\ndef organize_files():\n    # List all files in the base directory\n    files = os.listdir(base_dir)\n\n    # Process each file\n    for file in files:\n        # Skip directories\n        if os.path.isdir(os.path.join(base_dir, file)):\n            continue\n\n        # Extract the base name (album name) from the file\n        if file.endswith(\".mp3\"):",
    "last_modified": "2025-05-05T01:51:28"
  },
  {
    "id": "2271",
    "name": "downsize.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/downsize.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 56,
    "size": 2159,
    "docstring": "",
    "keywords": [],
    "functions": [
      "convert_and_downscale_images_in_subfolders",
      "main"
    ],
    "classes": [],
    "imports": [
      "os",
      "PIL"
    ],
    "preview": "import os\n\nfrom PIL import Image\n\n\ndef convert_and_downscale_images_in_subfolders(source_directory):\n    for root, dirs, files in os.walk(source_directory):\n        for filename in files:\n            if filename.endswith(\".png\"):\n                source_file = os.path.join(root, filename)\n                filename_no_ext = os.path.splitext(filename)[0]\n                temp_file = os.path.join(root, f\"{filename_no_ext}_temp.png\")\n\n                # Open the image and retrieve the original dimensions\n                im = Image.open(source_file)\n                width, height = im.size\n                print(f\"\ud83d\uddbc\ufe0f Processing {filename}: Original size: {width}x{height}\")\n\n                # Downscale the image by 50%\n                downscale_width = width // 2",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2272",
    "name": "save_urls_to_csv.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/save_urls_to_csv.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 75,
    "size": 2240,
    "docstring": "",
    "keywords": [
      "web_tools"
    ],
    "functions": [
      "save_urls_to_csv",
      "fetch_and_save_all_urls"
    ],
    "classes": [],
    "imports": [
      "csv",
      "os",
      "requests",
      "tqdm"
    ],
    "preview": "import csv\nimport os\n\nimport requests\nfrom tqdm import tqdm\n\n# Configuration\nBASE_URL = (\n    \"https://cloud.leonardo.ai/api/rest/v1/generations/user/f7bb8476-e3f0-4f1f-9a06-4600866cc49c\"\n)\nAUTH_TOKEN = \"Bearer 93043291-957d-4ec1-8c79-ee734abcb6e3\"\nOUTPUT_DIR = \"/Users/steven/Pictures/leodowns\"\nCSV_FILE = os.path.join(OUTPUT_DIR, \"leonardo_urls.csv\")\nMAX_RECORDS_PER_BATCH = 50  # Limit records per API request\n\nHEADERS = {\n    \"accept\": \"application/json\",\n    \"authorization\": AUTH_TOKEN,\n}\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2273",
    "name": "massive_follow_then_unfollow_works-non-stop.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/massive_follow_then_unfollow_works-non-stop.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 58,
    "size": 2007,
    "docstring": "This template is written by @loopypanda\n\nWhat does this quickstart script aim to do?\n- My settings is for running InstaPY 24/7 with approximately 1400\nfollows/day - 1400 unfollows/day running follow until reaches 7500 and than\nswitch to unfollow until reaches 0.",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "instapy"
    ],
    "preview": "\"\"\"\nThis template is written by @loopypanda\n\nWhat does this quickstart script aim to do?\n- My settings is for running InstaPY 24/7 with approximately 1400\nfollows/day - 1400 unfollows/day running follow until reaches 7500 and than\nswitch to unfollow until reaches 0.\n\"\"\"\n\nfrom instapy import InstaPy, smart_run\n\n# get a session!\nsession = InstaPy(username=\"\", password=\"\")\n\n# let's go! :>\nwith smart_run(session):\n    # general settings\n\n    # session.set_relationship_bounds(enabled=True,\n    # delimit_by_numbers=False, max_followers=12000, max_following=4500,",
    "last_modified": "2025-03-28T18:36:56"
  },
  {
    "id": "2274",
    "name": "upscale3.py",
    "path": "github_repo/scripts/02_media_processing/image_tools/upscale3.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 34,
    "size": 1127,
    "docstring": "",
    "keywords": [],
    "functions": [
      "upscale_image"
    ],
    "classes": [],
    "imports": [
      "os",
      "PIL"
    ],
    "preview": "import os\n\nfrom PIL import Image\n\n# Function to upscale and set DPI of an image\n\n\ndef upscale_image(input_path, output_path, scale_factor=2, dpi=(300, 300)):\n    with Image.open(input_path) as img:\n        # Upscale the image\n        new_size = (img.width * scale_factor, img.height * scale_factor)\n        img_resized = img.resize(new_size, Image.ANTIALIAS)\n        # Set the DPI\n        img_resized.save(output_path, dpi=dpi)\n\n\n# Directory containing images to upscale\ninput_dir = input(\"Enter the path to the directory containing the images: \")\n# Directory to save the upscaled images\noutput_dir = input(\"Enter the path to the directory to save the upscaled images: \")",
    "last_modified": "2025-05-04T22:47:13.383044"
  },
  {
    "id": "2275",
    "name": "captions_for_medias.py.py",
    "path": "github_repo/scripts/02_media_processing/captions_for_medias.py_consolidated/captions_for_medias.py.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 9,
    "size": 289,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [],
    "preview": "# flake8: noqa\n\nCAPTIONS = {\n    \"01.jpg\": \"I'm the caption of #horizontal #pic 01 that will be resized \"\n    + \"and cropped, with a mention to @maxdevblock\",\n    \"02.jpg\": \"I'm the caption of #vertical #pic 02 that will be resized and\"\n    + \" cropped, with a mention to @maxdevblock\",\n}\n",
    "last_modified": "2025-03-28T18:35:47"
  },
  {
    "id": "2276",
    "name": "captions_for_medias.py_02.py",
    "path": "github_repo/scripts/02_media_processing/captions_for_medias.py_consolidated/captions_for_medias.py_02.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 10,
    "size": 345,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [],
    "preview": "# flake8: noqa\n\nCAPTIONS = {\n    \"60s.mp4\": \"I'm the caption of #vertical #mp4 #video that will be \"\n    + \"cropped only and cut at 30 seconds from start, with a mention \"\n    + \"to @maxdevblock\",\n    \"02.mov\": \"I'm the caption of #horizontal #mov #video that will be\"\n    + \" resized and cropped but not cut, with a mention to @maxdevblock\",\n}\n",
    "last_modified": "2025-03-28T18:35:47"
  },
  {
    "id": "2277",
    "name": "main.py_03.py",
    "path": "github_repo/scripts/02_media_processing/main.py_consolidated/main.py_03.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 91,
    "size": 2778,
    "docstring": "",
    "keywords": [],
    "functions": [
      "setup",
      "start"
    ],
    "classes": [],
    "imports": [
      "json",
      "pathlib",
      "pprint",
      "googleapiclient.errors",
      "InquirerPy",
      "InquirerPy.utils",
      "auth",
      "constants",
      "presets",
      "upload"
    ],
    "preview": "import json\nfrom pathlib import Path\nfrom pprint import pprint\n\nfrom googleapiclient.errors import HttpError\nfrom InquirerPy import inquirer\nfrom InquirerPy.utils import color_print\n\nfrom .auth import get_authenticated_service\nfrom .constants import SETTINGS_FILE\nfrom .presets import PRESETS, Preset\nfrom .upload import initialize_upload\nfrom .utils import load_local_file, save_local_file\n\n\ndef setup() -> Preset:\n    settings = json.loads(load_local_file(SETTINGS_FILE, \"{}\"))\n    folder = settings.get(\"folder\", None)\n    video_filepath = None\n    while not video_filepath:",
    "last_modified": "2025-09-13T05:53:47.224353"
  },
  {
    "id": "2278",
    "name": "main.py.py",
    "path": "github_repo/scripts/02_media_processing/main.py_consolidated/main.py.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 218,
    "size": 6775,
    "docstring": "",
    "keywords": [
      "web_tools"
    ],
    "functions": [
      "listener",
      "speak",
      "q"
    ],
    "classes": [],
    "imports": [
      "re",
      "time",
      "mpv",
      "mpvListener",
      "pafy",
      "pyttsx3",
      "requests",
      "speech_recognition",
      "bs4",
      "selenium"
    ],
    "preview": "# This is the main file that will hold the majority\n# of the voice assistants functionality\n\n# TODO Eventually create a GUI interface akin to Suri\n\nimport re  # Regex library for manipulating strings\nimport time  # Library that allows us to manipulate time\n\n# PROTOTYPE\nimport mpv\nimport mpvListener\nimport pafy\nimport pyttsx3  # Library that allows for text to speech\nimport requests  # Library that allows us to send HTTP requests\nimport speech_recognition as sr  # Library that allows us to find\nfrom bs4 import BeautifulSoup  # Library that allows us to scrape elements from an HTML file\nfrom selenium import webdriver\nfrom selenium.webdriver.firefox.options import Options\n\n# TODO Find a way to change the driver to espeak",
    "last_modified": "2025-09-13T05:53:29.959028"
  },
  {
    "id": "2279",
    "name": "main.py_02.py",
    "path": "github_repo/scripts/02_media_processing/main.py_consolidated/main.py_02.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 96,
    "size": 2379,
    "docstring": "",
    "keywords": [],
    "functions": [
      "remove_old_files",
      "main"
    ],
    "classes": [],
    "imports": [
      "logging",
      "os",
      "threading",
      "modules.clipEditor",
      "modules.cmd_logs",
      "modules.configHandler",
      "modules.input_handler",
      "modules.twitchClips",
      "tqdm"
    ],
    "preview": "#!/usr/bin/env python\nimport logging\nimport os\nimport threading\n\nfrom modules.clipEditor import *\nfrom modules.cmd_logs import *\nfrom modules.configHandler import *\nfrom modules.input_handler import *\nfrom modules.twitchClips import *\nfrom tqdm import tqdm\n\n\ndef remove_old_files() -> None:\n    # Delete temporary file that may still exist if the program was\n    # interrupted during the editing of the clips\n    try:\n        if os.path.isfile(get_output_title() + \"TEMP_MPY_wvf_snd.mp3\"):\n            os.remove(get_output_title() + \"TEMP_MPY_wvf_snd.mp3\")\n        remove_all_clips()",
    "last_modified": "2025-09-13T05:53:44.863541"
  },
  {
    "id": "2280",
    "name": "config.py.py",
    "path": "github_repo/scripts/02_media_processing/config.py_consolidated/config.py.py",
    "category": "02_media_processing",
    "type": "setup",
    "lines": 73,
    "size": 2926,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "pathlib"
    ],
    "preview": "import pathlib\n\n# Note:\n# Changing FRAMES and or RESOLUTION will heavily impact load on CPU.\n# If you have a powerful enough computer you may set it to 1080p60\n\n# other\nPATH = str(pathlib.Path().absolute()).replace(\"\\\\\", \"/\")\nCLIP_PATH = PATH + \"/clips/{}/{}\"\nCHECK_VERSION = True  # see if you're running the latest versions\nDEBUG = True  # If additional/debug information should be printed (True/False)\n\nDATA = [\"c xQcOW\", \"c Trainwreckstv\", \"g Just Chatting\"]\nBLACKLIST = [\n    \"c ludwig\",\n    \"g Pools, Hot Tubs, and Beaches\",\n]  # channels/games you dont want to be included in the video\n\n# twitch\nCLIENT_ID = \"\"  # Twitch Client ID",
    "last_modified": "2025-09-13T05:53:56.213378"
  },
  {
    "id": "2281",
    "name": "config.py_02.py",
    "path": "github_repo/scripts/02_media_processing/config.py_consolidated/config.py_02.py",
    "category": "02_media_processing",
    "type": "setup",
    "lines": 33,
    "size": 780,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [],
    "preview": "subreddit = \"funny\"  # exclude 'r/'\n\nreddit_login = {  # more info to set up rovided by reddit api documentation\n    \"client_id\": \"\",\n    \"client_secret\": \"\",\n    \"password\": \"\",\n    \"user_agent\": \"\",\n    \"username\": \"\",\n}\n\n\nyoutube = {\n    \"title\": \"\",\n    \"description\": \"\",\n    \"tags\": \"\",\n    \"category\": 23,  # has to be an int, more about category below\n    \"status\": \"\",  # {public, private, unlisted}\n}\n\nvideo = {",
    "last_modified": "2025-05-06T04:35:15"
  },
  {
    "id": "2282",
    "name": "y--.py",
    "path": "github_repo/scripts/02_media_processing/audio_tools/y--.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 28,
    "size": 783,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "pytube"
    ],
    "preview": "from pytube import Playlist\n\n# Replace the playlist URL below with your desired playlist URL\nplaylist_url = (\n    \"https://www.youtube.com/playlist?list=PLfudK7D_bQIj3NLnflIYxIWHwpmz_Muyc\"\n)\n\n# Define the directory where you want to save the downloads\ndownload_directory = \"/Volumes/baKs/shorts-need-mp3\"\n\n# Create a Playlist object\nplaylist = Playlist(playlist_url)\n\n# Loop through the videos in the playlist\nfor video in playlist.videos:\n    # Download the video\n    video.streams.get_highest_resolution().download()\n\n    # Get video metadata\n    title = video.title",
    "last_modified": "2025-08-10T20:33:59.515623"
  },
  {
    "id": "2283",
    "name": "file-sort.py",
    "path": "github_repo/scripts/02_media_processing/audio_tools/file-sort.py",
    "category": "02_media_processing",
    "type": "organization",
    "lines": 28,
    "size": 1070,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "csv",
      "os",
      "collections"
    ],
    "preview": "import csv\nimport os\nfrom collections import defaultdict\n\n# Directory containing the files\ndirectory = \"/Users/steven/Music/NocTurnE-meLoDieS/v4/mp3\"\noutput_csv = \"/Users/steven/Music/NocTurnE-meLoDieS/v4/song-info.csv\"\n\n# Step 1: Group files by song title (ignoring version indicators)\nfile_groups = defaultdict(list)\n\nfor filename in os.listdir(directory):\n    if filename.endswith(\".txt\"):\n        # Group by the base title, ignoring version-specific suffixes\n        base_title = filename.split(\"_analysis\")[0].split(\"-analysis\")[0].split(\"(\")[0]\n        file_groups[base_title.strip()].append(os.path.join(directory, filename))\n\n# Step 2: Write grouped files to CSV\nwith open(output_csv, mode=\"w\", newline=\"\", encoding=\"utf-8\") as csv_file:\n    writer = csv.writer(csv_file)",
    "last_modified": "2025-05-04T22:47:13.346025"
  },
  {
    "id": "2284",
    "name": "analyze_migration.py",
    "path": "github_repo/scripts/02_media_processing/audio_tools/analyze_migration.py",
    "category": "02_media_processing",
    "type": "analysis",
    "lines": 211,
    "size": 7798,
    "docstring": "Analyze current structure and show what will be migrated where",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "analyze_current_structure",
      "show_migration_plan",
      "show_benefits",
      "show_risks_and_mitigation",
      "main"
    ],
    "classes": [],
    "imports": [
      "os",
      "pathlib",
      "collections"
    ],
    "preview": "#!/usr/bin/env python3\n\"\"\"\nAnalyze current structure and show what will be migrated where\n\"\"\"\n\nimport os\nfrom pathlib import Path\nfrom collections import defaultdict\n\ndef analyze_current_structure():\n    \"\"\"Analyze the current directory structure.\"\"\"\n    base_path = Path(\"/Users/steven/Documents/python\")\n    \n    # Categories for analysis\n    categories = {\n        \"analysis_scripts\": [],\n        \"youtube_projects\": [],\n        \"ai_creative\": [],\n        \"web_scraping\": [],\n        \"audio_video\": [],",
    "last_modified": "2025-10-09T05:26:14.631739"
  },
  {
    "id": "2285",
    "name": "split-2.py",
    "path": "github_repo/scripts/02_media_processing/audio_tools/split-2.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 44,
    "size": 1491,
    "docstring": "",
    "keywords": [],
    "functions": [
      "split_html_file"
    ],
    "classes": [],
    "imports": [
      "os"
    ],
    "preview": "import os\n\n\ndef split_html_file(file_path, lines_per_chunk=1000):\n    # Ensure the file exists\n    if not os.path.isfile(file_path):\n        print(f\"File '{file_path}' not found.\")\n        return\n\n    # Create output directory\n    output_dir = os.path.join(os.path.dirname(file_path), \"chunks\")\n    os.makedirs(output_dir, exist_ok=True)\n\n    # Initialize variables\n    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n        lines = file.readlines()\n        total_lines = len(lines)\n        chunk_count = 0\n\n        # Iterate over lines in chunks",
    "last_modified": "2025-05-04T22:47:13.351739"
  },
  {
    "id": "2286",
    "name": "tts_main.py",
    "path": "github_repo/scripts/02_media_processing/audio_tools/tts_main.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 27,
    "size": 1239,
    "docstring": "",
    "keywords": [],
    "functions": [
      "tts_start"
    ],
    "classes": [],
    "imports": [
      "time",
      "polly_main",
      "s3bucket"
    ],
    "preview": "import time\n\nfrom polly_main import polly, tts_task_resp\nfrom s3bucket import audio_down\n\n\ndef tts_start():\n    audio_book_topic = input(\"enter audiobook name: \")\n    audio_content = input(\" enter audiobook content: \\n\")\n    file_link = polly(audio_book_topic, audio_content)\n    filename_to_save_local = audio_book_topic + file_link[\"SynthesisTask\"][\"TaskId\"]\n    print(\"audio book generated: \" + audio_book_topic)\n    print(audio_book_topic, file_link[\"SynthesisTask\"][\"TaskId\"], filename_to_save_local)\n    file_to_download = file_link[\"SynthesisTask\"][\"TaskId\"] + \".mp3\"\n    filename_to_save_local_with_ext = filename_to_save_local + \".mp3\"\n    s3_name = \"qa-ai-bucket\"\n    print(file_to_download, filename_to_save_local_with_ext)\n    tts_task_status = tts_task_resp(file_link[\"SynthesisTask\"][\"TaskId\"])\n\n    while tts_task_status[\"SynthesisTask\"][\"TaskStatus\"] == \"scheduled\":",
    "last_modified": "2025-09-13T05:53:28.699715"
  },
  {
    "id": "2287",
    "name": "mp3_processor.py",
    "path": "github_repo/scripts/02_media_processing/audio_tools/mp3_processor.py",
    "category": "02_media_processing",
    "type": "audio_processing",
    "lines": 97,
    "size": 3371,
    "docstring": "",
    "keywords": [
      "organization"
    ],
    "functions": [
      "organize_files"
    ],
    "classes": [],
    "imports": [
      "os",
      "shutil"
    ],
    "preview": "import os\nimport shutil\n\n# Define the base directory\nbase_dir = \"/Users/steven/Music/nocTurneMeLoDieS/mp4\"\n\n\ndef organize_files():\n    # Check if the base directory exists\n    if not os.path.exists(base_dir):\n        print(f\"\u274c Error: The directory '{base_dir}' does not exist.\")\n        return\n\n    # List all files in the base directory\n    try:\n        files = os.listdir(base_dir)\n    except Exception as e:\n        print(f\"\u274c Error accessing directory '{base_dir}': {e}\")\n        return\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2288",
    "name": "gmupload.py",
    "path": "github_repo/scripts/02_media_processing/audio_tools/gmupload.py",
    "category": "02_media_processing",
    "type": "youtube",
    "lines": 103,
    "size": 3127,
    "docstring": "This is used for uploading downloaded track to google play music",
    "keywords": [
      "youtube"
    ],
    "functions": [
      "__init__",
      "login",
      "logout",
      "upload",
      "__upload_file__"
    ],
    "classes": [
      "GoolgeMusicUploader"
    ],
    "imports": [
      "logging",
      "gmusicapi",
      "ytdl.audiometadata",
      "ytdl.customerrors",
      "ytdl.models",
      "ytdl.oshelper"
    ],
    "preview": "\"This is used for uploading downloaded track to google play music\"\n\nimport logging\n\nfrom gmusicapi import Musicmanager, clients\nfrom ytdl.audiometadata import AudioMetadata\nfrom ytdl.customerrors import AuthError, DirectoryNotFoundError\nfrom ytdl.models import TrackInfo, UploadResult\nfrom ytdl.oshelper import (\n    DEFAULT_FILE_NAME,\n    absolute_files,\n    get_album_art_file,\n    get_track_file,\n    get_track_info_file,\n    isdir,\n    lock_file_exists,\n)\n\n\nclass GoolgeMusicUploader(object):",
    "last_modified": "2025-09-13T05:54:15.015688"
  },
  {
    "id": "2289",
    "name": "server.py",
    "path": "github_repo/scripts/02_media_processing/audio_tools/server.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 168,
    "size": 5399,
    "docstring": "",
    "keywords": [
      "testing",
      "youtube"
    ],
    "functions": [
      "testFTPConnection",
      "getFileNames",
      "uploadCompleteVideo",
      "sendThread",
      "startFTPServer",
      "startHTTPServer",
      "init",
      "_set_headers",
      "do_HEAD",
      "do_GET"
    ],
    "classes": [
      "HTTPHandler"
    ],
    "imports": [
      "cgi",
      "ftplib",
      "http.server",
      "json",
      "os",
      "pickle",
      "random",
      "socketserver",
      "sys",
      "traceback"
    ],
    "preview": "import cgi\nimport ftplib\nimport http.server\nimport json\nimport os\nimport pickle\nimport random\nimport socketserver\nimport sys\nimport traceback\nfrom threading import Thread\nfrom time import sleep\n\nimport scriptwrapper\nimport settings\nimport vidGen\nfrom pyftpdlib.authorizers import DummyAuthorizer\nfrom pyftpdlib.handlers import FTPHandler\nfrom pyftpdlib.servers import FTPServer\n",
    "last_modified": "2025-09-13T05:53:32.505733"
  },
  {
    "id": "2290",
    "name": "webm-to-mp3.py",
    "path": "github_repo/scripts/02_media_processing/audio_tools/webm-to-mp3.py",
    "category": "02_media_processing",
    "type": "audio_processing",
    "lines": 44,
    "size": 1450,
    "docstring": "",
    "keywords": [],
    "functions": [
      "convert_webm_to_mp3"
    ],
    "classes": [],
    "imports": [
      "os",
      "subprocess"
    ],
    "preview": "import os\nimport subprocess\n\n\ndef convert_webm_to_mp3(directory_path):\n    \"\"\"\n    Converts all .webm files in the specified directory to .mp3 format.\n\n    Args:\n    directory_path (str): The path to the directory containing .webm files.\n    \"\"\"\n    for filename in os.listdir(directory_path):\n        if filename.endswith(\".webm\"):\n            # Construct the full path to the source .webm file\n            webm_path = os.path.join(directory_path, filename)\n            # Construct the full path for the output .mp3 file\n            mp3_path = os.path.join(directory_path, os.path.splitext(filename)[0] + \".mp3\")\n\n            # Construct the ffmpeg command for converting .webm to .mp3\n            command = [",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2291",
    "name": "style_engine.py",
    "path": "github_repo/scripts/02_media_processing/audio_tools/style_engine.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 82,
    "size": 2247,
    "docstring": "",
    "keywords": [],
    "functions": [
      "guess_domain",
      "sentiment",
      "decide_style"
    ],
    "classes": [],
    "imports": [
      "__future__",
      "typing",
      "textblob"
    ],
    "preview": "from __future__ import annotations\n\nfrom typing import Any, Dict, Optional\n\ntry:\n    from textblob import TextBlob\nexcept Exception:\n    TextBlob = None\n\nDOMAIN_HINTS = {\n    \"tech\": {\n        \"style\": \"neonpunk\",\n        \"font\": \"JetBrains Mono\",\n        \"palette\": [\"#00FF9C\", \"#00E1FF\", \"#111111\"],\n    },\n    \"finance\": {\n        \"style\": \"minimal\",\n        \"font\": \"Helvetica Neue\",\n        \"palette\": [\"#0E1E3A\", \"#1F497D\", \"#FFFFFF\"],\n    },",
    "last_modified": "2025-09-13T05:55:10.297687"
  },
  {
    "id": "2292",
    "name": "config_20241213005737.py",
    "path": "github_repo/scripts/02_media_processing/audio_tools/config_20241213005737.py",
    "category": "02_media_processing",
    "type": "setup",
    "lines": 3,
    "size": 71,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [],
    "preview": "# config.py\nSOURCE_DIRECTORY = \"/Users/steven/Music/NocTurnE-meLoDieS\"\n",
    "last_modified": "2025-09-13T05:53:47.430511"
  },
  {
    "id": "2293",
    "name": "mp3.py",
    "path": "github_repo/scripts/02_media_processing/audio_tools/mp3.py",
    "category": "02_media_processing",
    "type": "audio_processing",
    "lines": 45,
    "size": 1453,
    "docstring": "",
    "keywords": [],
    "functions": [
      "convert_webm_to_mp3"
    ],
    "classes": [],
    "imports": [
      "os",
      "subprocess"
    ],
    "preview": "import os\nimport subprocess\n\n\ndef convert_webm_to_mp3(directory_path):\n    \"\"\"\n    Converts all .webm files in the specified directory to .mp3 format.\n\n    Args:\n    directory_path (str): The path to the directory containing .webm files.\n    \"\"\"\n    for filename in os.listdir(directory_path):\n        if filename.endswith(\".webm\"):\n            # Construct the full path to the source .webm file\n            webm_path = os.path.join(directory_path, filename)\n            # Construct the full path for the output .mp3 file\n            mp3_path = os.path.join(directory_path, os.path.splitext(filename)[0] + \".mp3\")\n\n            # Construct the ffmpeg command for converting .webm to .mp3\n            command = [",
    "last_modified": "2025-09-13T05:55:28.499128"
  },
  {
    "id": "2294",
    "name": "ffmpegdl.py",
    "path": "github_repo/scripts/02_media_processing/audio_tools/ffmpegdl.py",
    "category": "02_media_processing",
    "type": "video_processing",
    "lines": 87,
    "size": 2781,
    "docstring": "",
    "keywords": [
      "youtube"
    ],
    "functions": [
      "__init__",
      "check_if_file",
      "download",
      "_download",
      "_download_win",
      "_download_mac",
      "_download_linux",
      "_unzip",
      "_untar",
      "_cleanup"
    ],
    "classes": [
      "FFmpegDL"
    ],
    "imports": [
      "os",
      "pathlib",
      "shutil",
      "sys",
      "uuid",
      "urllib.request",
      "zipfile",
      "tarfile"
    ],
    "preview": "import os\nfrom pathlib import Path\nfrom shutil import move, rmtree\nfrom sys import platform\nfrom uuid import uuid1\n\nFFMPEG_STATIC_LINUX = \"https://johnvansickle.com/ffmpeg/builds/ffmpeg-git-amd64-static.tar.xz\"\nFFMPEG_STATIC_WIN = \"https://www.gyan.dev/ffmpeg/builds/ffmpeg-release-essentials.zip\"\nFFMPEG_STATIC_MAC = \"https://evermeet.cx/ffmpeg/getrelease/zip\"\n\n\nclass FFmpegDL:\n    def __init__(self, data: str) -> None:\n        self.data_path = Path(data) / \"ffmpeg\"\n\n        if platform == \"win32\":\n            self.temp = self.data_path / str(uuid1())\n            self.download_link = FFMPEG_STATIC_WIN\n            self.final_location = self.data_path / \"bin\" / \"ffmpeg.exe\"\n            self.platform_task = self._download_win",
    "last_modified": "2025-09-13T05:55:15.548316"
  },
  {
    "id": "2295",
    "name": "yt-playlist2.py",
    "path": "github_repo/scripts/02_media_processing/audio_tools/yt-playlist2.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 28,
    "size": 783,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "pytube"
    ],
    "preview": "from pytube import Playlist\n\n# Replace the playlist URL below with your desired playlist URL\nplaylist_url = (\n    \"https://www.youtube.com/playlist?list=PLfudK7D_bQIjRgAqVU_jkbnb1N5V_pNiG\"\n)\n\n# Define the directory where you want to save the downloads\ndownload_directory = \"/Volumes/baKs/shorts-need-mp3\"\n\n# Create a Playlist object\nplaylist = Playlist(playlist_url)\n\n# Loop through the videos in the playlist\nfor video in playlist.videos:\n    # Download the video\n    video.streams.get_highest_resolution().download()\n\n    # Get video metadata\n    title = video.title",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2296",
    "name": "downloader.py",
    "path": "github_repo/scripts/02_media_processing/audio_tools/downloader.py",
    "category": "02_media_processing",
    "type": "youtube",
    "lines": 1537,
    "size": 51724,
    "docstring": "Important Notes:\nExternal Downloader And its args do only work in specific video formats In platforms like (youtube)\nUnfortunately It doesn't work on PH And Other Sites.\nEdit: It Works Now :)",
    "keywords": [
      "youtube",
      "web_tools"
    ],
    "functions": [
      "find_config",
      "read_config",
      "__init__",
      "add_values",
      "clear",
      "convert_size",
      "check_url",
      "check_connection",
      "animation",
      "get_current_dir"
    ],
    "classes": [
      "config_reader",
      "parser_args",
      "download"
    ],
    "imports": [
      "__future__",
      "configparser",
      "itertools",
      "json",
      "optparse",
      "os",
      "shutil",
      "sys",
      "threading",
      "distutils"
    ],
    "preview": "#!/usr/bin/python3\n\n# Updated On 01/01/2022\n# Created By ybenel\n\"\"\"\nImportant Notes:\nExternal Downloader And its args do only work in specific video formats In platforms like (youtube)\nUnfortunately It doesn't work on PH And Other Sites.\nEdit: It Works Now :)\n\"\"\"\nfrom __future__ import unicode_literals\n\nimport configparser\nimport itertools\nimport json\nimport optparse\nimport os\nimport shutil\nimport sys\nimport threading",
    "last_modified": "2025-09-13T05:53:58.934360"
  },
  {
    "id": "2297",
    "name": "sound_test.py",
    "path": "github_repo/scripts/02_media_processing/audio_tools/sound_test.py",
    "category": "02_media_processing",
    "type": "audio_processing",
    "lines": 4,
    "size": 49,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "clips"
    ],
    "preview": "from clips import *\n\ngen_audio_clip(\"wooo hooo\")\n",
    "last_modified": "2025-03-28T18:35:48"
  },
  {
    "id": "2298",
    "name": "downloadupload.py",
    "path": "github_repo/scripts/02_media_processing/audio_tools/downloadupload.py",
    "category": "02_media_processing",
    "type": "youtube",
    "lines": 120,
    "size": 4162,
    "docstring": "Ytdl",
    "keywords": [
      "youtube"
    ],
    "functions": [
      "__init__",
      "__download_tracks__",
      "__upload_tracks__",
      "__successful_upload_tasks__",
      "__failed_upload_tasks__",
      "download_and_upload"
    ],
    "classes": [
      "Downloadupload"
    ],
    "imports": [
      "__future__",
      "json",
      "logging",
      "ytdl.audiodownload",
      "ytdl.awsqueue",
      "ytdl.customerrors",
      "ytdl.gmupload",
      "ytdl.models",
      "ytdl.notify",
      "ytdl.oshelper"
    ],
    "preview": "\"Ytdl\"\n\nfrom __future__ import unicode_literals\n\nimport json\nimport logging\n\nfrom ytdl.audiodownload import AudioDownload\nfrom ytdl.awsqueue import Awsqueue\nfrom ytdl.customerrors import AuthError, DirectoryNotFoundError\nfrom ytdl.gmupload import GoolgeMusicUploader\nfrom ytdl.models import Payload\nfrom ytdl.notify import Iftttnotify\nfrom ytdl.oshelper import absolute_dirs, copy, isdir, remove\n\n\nclass Downloadupload(object):\n    \"Youtube downloader\"\n\n    def __init__(self, ytdl_config):",
    "last_modified": "2025-09-13T05:54:14.975384"
  },
  {
    "id": "2299",
    "name": "y.py",
    "path": "github_repo/scripts/02_media_processing/audio_tools/y.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 28,
    "size": 783,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "pytube"
    ],
    "preview": "from pytube import Playlist\n\n# Replace the playlist URL below with your desired playlist URL\nplaylist_url = (\n    \"https://www.youtube.com/playlist?list=PLfudK7D_bQIjRgAqVU_jkbnb1N5V_pNiG\"\n)\n\n# Define the directory where you want to save the downloads\ndownload_directory = \"/Volumes/baKs/shorts-need-mp3\"\n\n# Create a Playlist object\nplaylist = Playlist(playlist_url)\n\n# Loop through the videos in the playlist\nfor video in playlist.videos:\n    # Download the video\n    video.streams.get_highest_resolution().download()\n\n    # Get video metadata\n    title = video.title",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2300",
    "name": "y copy.py",
    "path": "github_repo/scripts/02_media_processing/audio_tools/y copy.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 28,
    "size": 783,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "pytube"
    ],
    "preview": "from pytube import Playlist\n\n# Replace the playlist URL below with your desired playlist URL\nplaylist_url = (\n    \"https://www.youtube.com/playlist?list=PLfudK7D_bQIjRgAqVU_jkbnb1N5V_pNiG\"\n)\n\n# Define the directory where you want to save the downloads\ndownload_directory = \"/Volumes/baKs/shorts-need-mp3\"\n\n# Create a Playlist object\nplaylist = Playlist(playlist_url)\n\n# Loop through the videos in the playlist\nfor video in playlist.videos:\n    # Download the video\n    video.streams.get_highest_resolution().download()\n\n    # Get video metadata\n    title = video.title",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2301",
    "name": "mp3_colab.py",
    "path": "github_repo/scripts/02_media_processing/audio_tools/mp3_colab.py",
    "category": "02_media_processing",
    "type": "audio_processing",
    "lines": 45,
    "size": 1453,
    "docstring": "",
    "keywords": [],
    "functions": [
      "convert_webm_to_mp3"
    ],
    "classes": [],
    "imports": [
      "os",
      "subprocess"
    ],
    "preview": "import os\nimport subprocess\n\n\ndef convert_webm_to_mp3(directory_path):\n    \"\"\"\n    Converts all .webm files in the specified directory to .mp3 format.\n\n    Args:\n    directory_path (str): The path to the directory containing .webm files.\n    \"\"\"\n    for filename in os.listdir(directory_path):\n        if filename.endswith(\".webm\"):\n            # Construct the full path to the source .webm file\n            webm_path = os.path.join(directory_path, filename)\n            # Construct the full path for the output .mp3 file\n            mp3_path = os.path.join(directory_path, os.path.splitext(filename)[0] + \".mp3\")\n\n            # Construct the ffmpeg command for converting .webm to .mp3\n            command = [",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2302",
    "name": "speech--.py",
    "path": "github_repo/scripts/02_media_processing/audio_tools/speech--.py",
    "category": "02_media_processing",
    "type": "transcription",
    "lines": 52,
    "size": 1562,
    "docstring": "",
    "keywords": [],
    "functions": [
      "extract_text_from_audio"
    ],
    "classes": [],
    "imports": [
      "glob",
      "os",
      "moviepy.editor",
      "speech_recognition"
    ],
    "preview": "import glob\nimport os\n\nimport moviepy.editor as mp\nimport speech_recognition as sr\n\n# Prompt for the movie directory path\nmovie_directory = input(\"Enter the path to the directory containing your movies: \")\n\n# Prompt for the output .txt file name\noutput_file_name = input(\"Enter the name for the output .txt file (without extension): \")\n\n# Add the .txt file extension\noutput_txt_file = output_file_name + \".txt\"\n\n# Initialize the recognizer\nrecognizer = sr.Recognizer()\n\n\n# Function to extract text from audio",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2303",
    "name": "s3bucket.py",
    "path": "github_repo/scripts/02_media_processing/audio_tools/s3bucket.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 46,
    "size": 1584,
    "docstring": "",
    "keywords": [
      "transcription"
    ],
    "functions": [
      "audio_down",
      "get_audio_link",
      "set_public_access"
    ],
    "classes": [],
    "imports": [
      "boto3",
      "botocore.config",
      "utilities.const"
    ],
    "preview": "import boto3\nfrom botocore.config import Config\nfrom utilities.const import AWS_ACCESS_KEY, AWS_SEC_KEY, get_current_date\n\nmy_config = Config(\n    region_name=\"ap-south-1\",\n    signature_version=\"v4\",\n    retries={\"max_attempts\": 3, \"mode\": \"standard\"},\n)\ns3 = boto3.client(\n    \"s3\",\n    config=my_config,\n    aws_access_key_id=AWS_ACCESS_KEY,\n    aws_secret_access_key=AWS_SEC_KEY,\n)\n\n\ndef audio_down(s3_name, audio_book_topic, aws_filename, filename_to_save_local):\n    str_get_current_date = get_current_date()\n    s3.download_file(",
    "last_modified": "2025-09-13T05:53:28.676404"
  },
  {
    "id": "2304",
    "name": "background.py",
    "path": "github_repo/scripts/02_media_processing/audio_tools/background.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 49,
    "size": 1676,
    "docstring": "",
    "keywords": [
      "youtube"
    ],
    "functions": [
      "get_start_and_end_times",
      "download_background",
      "chop_background_video"
    ],
    "classes": [],
    "imports": [
      "pathlib",
      "random",
      "moviepy.editor",
      "moviepy.video.io.ffmpeg_tools",
      "pytube",
      "utils.console"
    ],
    "preview": "from pathlib import Path\nfrom random import randrange\n\nfrom moviepy.editor import VideoFileClip\nfrom moviepy.video.io.ffmpeg_tools import ffmpeg_extract_subclip\nfrom pytube import YouTube\n\nfrom utils.console import print_step, print_substep\n\n\ndef get_start_and_end_times(video_length, length_of_clip):\n\n    random_time = randrange(180, int(length_of_clip) - int(video_length))\n    return random_time, random_time + video_length\n\n\ndef download_background():\n    \"\"\"Downloads the background video from youtube.\n\n    Shoutout to: bbswitzer (https://www.youtube.com/watch?v=n_Dv4JMiwK8)",
    "last_modified": "2025-09-13T05:53:59.387864"
  },
  {
    "id": "2305",
    "name": "organize_albums 2.py",
    "path": "github_repo/scripts/02_media_processing/audio_tools/organize_albums 2.py",
    "category": "02_media_processing",
    "type": "organization",
    "lines": 106,
    "size": 4065,
    "docstring": "",
    "keywords": [
      "organization"
    ],
    "functions": [
      "organize_files"
    ],
    "classes": [],
    "imports": [
      "os",
      "shutil"
    ],
    "preview": "import os\nimport shutil\n\n# Define the base directory\nbase_dir = \"/Users/steven/Music/nocTurneMeLoDieS/mp3\"\n\n\ndef organize_files():\n    # Check if the base directory exists\n    if not os.path.exists(base_dir):\n        print(f\"\u274c Error: The directory '{base_dir}' does not exist.\")\n        return\n\n    # List all files in the base directory\n    try:\n        files = os.listdir(base_dir)\n    except Exception as e:\n        print(f\"\u274c Error accessing directory '{base_dir}': {e}\")\n        return\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2306",
    "name": "splt-1.py",
    "path": "github_repo/scripts/02_media_processing/audio_tools/splt-1.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 39,
    "size": 1315,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "os",
      "bs4"
    ],
    "preview": "import os\n\nfrom bs4 import BeautifulSoup\n\n# Path to the large HTML file\ninput_path = (\n    \"/Users/steven/Music/NocTurnE-meLoDieS/Song-origins-html/Raccoon Alley Album Art(83% copy).html\"\n)\noutput_dir = \"/Users/steven/Music/NocTurnE-meLoDieS/Song-origins-html/chunks\"\n\n# Create output directory if it doesn't exist\nos.makedirs(output_dir, exist_ok=True)\n\n# Load the HTML content\nwith open(input_path, \"r\", encoding=\"utf-8\") as file:\n    html_content = file.read()\n\n# Parse HTML using BeautifulSoup\nsoup = BeautifulSoup(html_content, \"html.parser\")\n",
    "last_modified": "2025-09-13T05:53:55.451673"
  },
  {
    "id": "2307",
    "name": "types.py",
    "path": "github_repo/scripts/02_media_processing/audio_tools/types.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 37,
    "size": 552,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [
      "Type",
      "Platform",
      "Format",
      "Quality"
    ],
    "imports": [],
    "preview": "__all__ = [\"Type\", \"Platform\", \"Format\", \"Quality\"]\n\n\nclass Type:\n    TRACK = \"track\"\n    ALBUM = \"album\"\n    PLAYLIST = \"playlist\"\n    EPISODE = \"episode\"\n    SHOW = \"show\"\n    ARTIST = \"artist\"\n\n\nclass Platform:\n    SPOTIFY = \"spotify\"\n    YOUTUBE = \"youtube\"\n\n\nclass Format:\n    MP3 = \"mp3\"\n    AAC = \"aac\"",
    "last_modified": "2025-05-04T23:27:53.630386"
  },
  {
    "id": "2308",
    "name": "aws_polly.py",
    "path": "github_repo/scripts/02_media_processing/audio_tools/aws_polly.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 78,
    "size": 2422,
    "docstring": "",
    "keywords": [],
    "functions": [
      "__init__",
      "run",
      "randomvoice"
    ],
    "classes": [
      "AWSPolly"
    ],
    "imports": [
      "random",
      "sys",
      "boto3",
      "botocore.exceptions",
      "utils"
    ],
    "preview": "import random\nimport sys\n\nfrom boto3 import Session\nfrom botocore.exceptions import BotoCoreError, ClientError, ProfileNotFound\n\nfrom utils import settings\n\nvoices = [\n    \"Brian\",\n    \"Emma\",\n    \"Russell\",\n    \"Joey\",\n    \"Matthew\",\n    \"Joanna\",\n    \"Kimberly\",\n    \"Amy\",\n    \"Geraint\",\n    \"Nicole\",\n    \"Justin\",",
    "last_modified": "2025-09-13T05:53:59.605179"
  },
  {
    "id": "2309",
    "name": "ffmpeg_install.py",
    "path": "github_repo/scripts/02_media_processing/audio_tools/ffmpeg_install.py",
    "category": "02_media_processing",
    "type": "video_processing",
    "lines": 143,
    "size": 5243,
    "docstring": "",
    "keywords": [
      "web_tools"
    ],
    "functions": [
      "ffmpeg_install_windows",
      "ffmpeg_install_linux",
      "ffmpeg_install_mac",
      "ffmpeg_install"
    ],
    "classes": [],
    "imports": [
      "os",
      "subprocess",
      "zipfile",
      "requests"
    ],
    "preview": "import os\nimport subprocess\nimport zipfile\n\nimport requests\n\n\ndef ffmpeg_install_windows():\n    try:\n        ffmpeg_url = (\n            \"https://github.com/GyanD/codexffmpeg/releases/download/6.0/ffmpeg-6.0-full_build.zip\"\n        )\n        ffmpeg_zip_filename = \"ffmpeg.zip\"\n        ffmpeg_extracted_folder = \"ffmpeg\"\n\n        # Check if ffmpeg.zip already exists\n        if os.path.exists(ffmpeg_zip_filename):\n            os.remove(ffmpeg_zip_filename)\n\n        # Download FFmpeg",
    "last_modified": "2025-09-13T05:54:00.180946"
  },
  {
    "id": "2310",
    "name": "play.py",
    "path": "github_repo/scripts/02_media_processing/audio_tools/play.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 2,
    "size": 44,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [],
    "preview": "# This file handles playing music or videos\n",
    "last_modified": "2025-03-28T18:35:47.781740"
  },
  {
    "id": "2311",
    "name": "text_to_speech.py",
    "path": "github_repo/scripts/02_media_processing/audio_tools/text_to_speech.py",
    "category": "02_media_processing",
    "type": "transcription",
    "lines": 27,
    "size": 983,
    "docstring": "",
    "keywords": [],
    "functions": [
      "main"
    ],
    "classes": [],
    "imports": [
      "gtts"
    ],
    "preview": "# This program will convert a text string into a text to speech audio file\n\nfrom gtts import gTTS\n\n\ndef main() -> int:\n    # A title, simulating scraping a title from a subreddit\n    title = \"What\u2019s a \u201cboring\u201d hobby that\u2019s not boring at all?\"\n\n    # A comment, simulating scraping a comment to the post\n    comment = \"I do counted cross stitch. I'm not creative at all, but give me a coded pattern that creates a map (I always do maps - but you can make way more intricate things than you think if you invest the time) and I'm all over that shit. It's how I quit smoking.\"\n\n    # Creating text to speech creation on those strings making mp3 data\n    tts_entry1 = gTTS(title, lang=\"en\", tld=\"ie\")\n    tts_entry2 = gTTS(comment, lang=\"en\", tld=\"ie\")\n\n    # Write the mp3 data to a file to create audio object\n    with open(\"reddit.mp3\", \"wb\") as f:\n        tts_entry1.write_to_fp(f)\n        tts_entry2.write_to_fp(f)",
    "last_modified": "2025-05-04T23:27:53.407788"
  },
  {
    "id": "2312",
    "name": "scriptwrapper.py",
    "path": "github_repo/scripts/02_media_processing/audio_tools/scriptwrapper.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 168,
    "size": 5207,
    "docstring": "",
    "keywords": [],
    "functions": [
      "__init__",
      "__init__",
      "__init__",
      "addClipAtStart",
      "addScriptWrapper",
      "moveDown",
      "moveUp",
      "setupScriptMap",
      "keep",
      "skip"
    ],
    "classes": [
      "TwitchVideo",
      "DownloadedTwitchClipWrapper",
      "ScriptWrapper"
    ],
    "imports": [
      "datetime",
      "math",
      "os",
      "subprocess"
    ],
    "preview": "import datetime\nimport math\nimport os\nimport subprocess\n\ncurrent_path = os.path.dirname(os.path.realpath(__file__))\n\n\nclass TwitchVideo:\n    def __init__(self, scriptwrapper):\n        self.scriptWrapper = scriptwrapper\n        self.final_clips = None\n\n\nclass DownloadedTwitchClipWrapper:\n    def __init__(\n        self,\n        id,\n        author_name,\n        clip_title,",
    "last_modified": "2025-09-13T05:53:31.550137"
  },
  {
    "id": "2313",
    "name": "voices.py",
    "path": "github_repo/scripts/02_media_processing/audio_tools/voices.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 37,
    "size": 1296,
    "docstring": "",
    "keywords": [],
    "functions": [
      "save_text_to_mp3"
    ],
    "classes": [],
    "imports": [
      "pathlib",
      "gtts",
      "mutagen.mp3",
      "rich.progress",
      "utils.console"
    ],
    "preview": "from pathlib import Path\n\nfrom gtts import gTTS\nfrom mutagen.mp3 import MP3\nfrom rich.progress import track\n\nfrom utils.console import print_step, print_substep\n\n\ndef save_text_to_mp3(reddit_obj):\n    \"\"\"Saves Text to MP3 files.\n\n    Args:\n        reddit_obj : The reddit object you received from the reddit API in the askreddit.py file.\n    \"\"\"\n    print_step(\"Saving Text to MP3 files \ud83c\udfb6\")\n    length = 0\n\n    # Create a folder for the mp3 files.\n    Path(\"assets/mp3\").mkdir(parents=True, exist_ok=True)",
    "last_modified": "2025-09-11T13:27:05.489415"
  },
  {
    "id": "2314",
    "name": "audio_test.py",
    "path": "github_repo/scripts/02_media_processing/audio_tools/audio_test.py",
    "category": "02_media_processing",
    "type": "transcription",
    "lines": 6,
    "size": 191,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "clips"
    ],
    "preview": "from clips import *\n\nvideoclip = VideoFileClip(\"media/askreddit_submission_test0.mp4\")\nvideoclip.audio = gen_background_audio_clip(videoclip.duration)\nvideoclip.to_videofile(\"temp/loop.mp4\")\n",
    "last_modified": "2025-05-04T23:27:53.305626"
  },
  {
    "id": "2315",
    "name": "visual_fx.py",
    "path": "github_repo/scripts/02_media_processing/audio_tools/visual_fx.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 15,
    "size": 539,
    "docstring": "",
    "keywords": [],
    "functions": [
      "lut_filter"
    ],
    "classes": [],
    "imports": [
      "__future__"
    ],
    "preview": "from __future__ import annotations\n\n\ndef lut_filter(style: str) -> str:\n    s = (style or \"\").lower()\n    if s == \"neonpunk\":\n        return \"eq=contrast=1.2:saturation=1.45:brightness=0.02,curves=blue='0/0 0.5/0.45 1/1'\"\n    if s == \"retrovhs\":\n        return \"curves=m='0/0 0.4/0.35 1/1',noise=alls=10:allf=t,format=yuv420p\"\n    if s == \"comicbook\":\n        return \"edgedetect=low=0.1:high=0.2,unsharp=7:7:1.0,eq=saturation=1.6\"\n    if s == \"dreamwave\":\n        return \"eq=contrast=1.05:saturation=1.3:brightness=0.03\"\n    return \"null\"\n",
    "last_modified": "2025-09-11T13:24:00.303408"
  },
  {
    "id": "2316",
    "name": "basics.py",
    "path": "github_repo/scripts/02_media_processing/audio_tools/basics.py",
    "category": "02_media_processing",
    "type": "organization",
    "lines": 275,
    "size": 8866,
    "docstring": "MIT License\n\nCopyright (c) 2021-2022 MShawon\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "create_proxy_folder",
      "get_driver",
      "play_video",
      "play_music",
      "type_keyword",
      "scroll_search",
      "search_video"
    ],
    "classes": [],
    "imports": [
      "os",
      "glob",
      "features"
    ],
    "preview": "\"\"\"\nMIT License\n\nCopyright (c) 2021-2022 MShawon\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,",
    "last_modified": "2025-09-13T05:54:11.568125"
  },
  {
    "id": "2317",
    "name": "tiktok.py",
    "path": "github_repo/scripts/02_media_processing/audio_tools/tiktok.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 168,
    "size": 5653,
    "docstring": "",
    "keywords": [
      "web_tools"
    ],
    "functions": [
      "__init__",
      "run",
      "get_voices",
      "random_voice",
      "__init__",
      "__str__"
    ],
    "classes": [
      "TikTok",
      "TikTokTTSException"
    ],
    "imports": [
      "base64",
      "random",
      "time",
      "typing",
      "requests",
      "utils"
    ],
    "preview": "# documentation for tiktok api: https://github.com/oscie57/tiktok-voice/wiki\nimport base64\nimport random\nimport time\nfrom typing import Final, Optional\n\nimport requests\n\nfrom utils import settings\n\n__all__ = [\"TikTok\", \"TikTokTTSException\"]\n\ndisney_voices: Final[tuple] = (\n    \"en_us_ghostface\",  # Ghost Face\n    \"en_us_chewbacca\",  # Chewbacca\n    \"en_us_c3po\",  # C3PO\n    \"en_us_stitch\",  # Stitch\n    \"en_us_stormtrooper\",  # Stormtrooper\n    \"en_us_rocket\",  # Rocket\n    \"en_female_madam_leota\",  # Madame Leota",
    "last_modified": "2025-09-13T05:53:59.570995"
  },
  {
    "id": "2318",
    "name": "audiodownload.py",
    "path": "github_repo/scripts/02_media_processing/audio_tools/audiodownload.py",
    "category": "02_media_processing",
    "type": "transcription",
    "lines": 64,
    "size": 2183,
    "docstring": "This is used for downloading a youtube video as mp3",
    "keywords": [
      "youtube"
    ],
    "functions": [
      "__init__",
      "__my_hook__",
      "download"
    ],
    "classes": [
      "AudioDownload"
    ],
    "imports": [
      "__future__",
      "logging",
      "youtube_dl",
      "ytdl.models",
      "ytdl.oshelper"
    ],
    "preview": "\"This is used for downloading a youtube video as mp3\"\n\nfrom __future__ import unicode_literals\n\nimport logging\n\nfrom youtube_dl import DownloadError, YoutubeDL\nfrom ytdl.models import DownloadResult\nfrom ytdl.oshelper import dirname, join_paths, try_create_lock_file, try_delete_lock_file\n\n\nclass AudioDownload(object):\n    \"This is used for downloading a youtube video as mp3\"\n\n    def __init__(self, config):\n        self.download_folder = config.download_folder\n        self.downloaded_to_folder = \"\"\n        self.logger = logging.getLogger(__name__)\n\n    def __my_hook__(self, hook):",
    "last_modified": "2025-09-13T05:54:14.869695"
  },
  {
    "id": "2319",
    "name": "webm-to-mp4.py",
    "path": "github_repo/scripts/02_media_processing/audio_tools/webm-to-mp4.py",
    "category": "02_media_processing",
    "type": "video_processing",
    "lines": 44,
    "size": 1443,
    "docstring": "",
    "keywords": [],
    "functions": [
      "convert_webm_to_mp4"
    ],
    "classes": [],
    "imports": [
      "os",
      "subprocess"
    ],
    "preview": "import os\nimport subprocess\n\n\ndef convert_webm_to_mp4(directory_path):\n    \"\"\"\n    Converts all .webm files in the specified directory to .mp4 format.\n\n    Args:\n    directory_path (str): The path to the directory containing .webm files.\n    \"\"\"\n    for filename in os.listdir(directory_path):\n        if filename.endswith(\".webm\"):\n            # Construct the full path to the source .webm file\n            webm_path = os.path.join(directory_path, filename)\n            # Construct the full path for the output .mp4 file\n            mp4_path = os.path.join(directory_path, filename[:-5] + \".mp4\")\n\n            # Construct the ffmpeg command for converting .webm to .mp4\n            command = [",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2320",
    "name": "speech.py",
    "path": "github_repo/scripts/02_media_processing/audio_tools/speech.py",
    "category": "02_media_processing",
    "type": "transcription",
    "lines": 52,
    "size": 1562,
    "docstring": "",
    "keywords": [],
    "functions": [
      "extract_text_from_audio"
    ],
    "classes": [],
    "imports": [
      "glob",
      "os",
      "moviepy.editor",
      "speech_recognition"
    ],
    "preview": "import glob\nimport os\n\nimport moviepy.editor as mp\nimport speech_recognition as sr\n\n# Prompt for the movie directory path\nmovie_directory = input(\"Enter the path to the directory containing your movies: \")\n\n# Prompt for the output .txt file name\noutput_file_name = input(\"Enter the name for the output .txt file (without extension): \")\n\n# Add the .txt file extension\noutput_txt_file = output_file_name + \".txt\"\n\n# Initialize the recognizer\nrecognizer = sr.Recognizer()\n\n\n# Function to extract text from audio",
    "last_modified": "2025-09-13T05:54:14.333067"
  },
  {
    "id": "2321",
    "name": "config_20241213005714.py",
    "path": "github_repo/scripts/02_media_processing/audio_tools/config_20241213005714.py",
    "category": "02_media_processing",
    "type": "setup",
    "lines": 3,
    "size": 71,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [],
    "preview": "# config.py\nSOURCE_DIRECTORY = \"/Users/steven/Music/NocTurnE-meLoDieS\"\n",
    "last_modified": "2024-12-13T00:57:14.700472"
  },
  {
    "id": "2322",
    "name": "yt_auto_main.py",
    "path": "github_repo/scripts/02_media_processing/audio_tools/yt_auto_main.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 188,
    "size": 7985,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "_news",
      "generate_video",
      "check_and_add_topic",
      "__init__",
      "get_logger"
    ],
    "classes": [
      "MultiLogger"
    ],
    "imports": [
      "json",
      "logging",
      "os",
      "re",
      "processing.extract_topic",
      "utilities.const",
      "utilities.create_directories",
      "video.create_vd",
      "video.subtitle"
    ],
    "preview": "import json\nimport logging\nimport os\nimport re\n\nfrom processing.extract_topic import ExtractNews\nfrom utilities.const import (\n    EXISTING_TOPICS,\n    LOG_PATH,\n    NEWS_API_KEY,\n    OUTPUT_FINAL_INFO,\n    OUTPUT_FINAL_VIDEO,\n    OUTPUT_TMP,\n    STOCK_VIDEO_FOLDER,\n    get_current_date,\n)\nfrom utilities.create_directories import create_directories\nfrom video.create_vd import VideoProcessor\nfrom video.subtitle import AddAudio, VideoTextOverlay\n",
    "last_modified": "2025-09-13T05:53:28.956420"
  },
  {
    "id": "2323",
    "name": "song-process2.py",
    "path": "github_repo/scripts/02_media_processing/audio_tools/song-process2.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 106,
    "size": 4054,
    "docstring": "",
    "keywords": [
      "organization"
    ],
    "functions": [
      "organize_files"
    ],
    "classes": [],
    "imports": [
      "os",
      "shutil"
    ],
    "preview": "import os\nimport shutil\n\n# Define the base directory\nbase_dir = \"/Users/steven/Movies/2025/mp4\"\n\n\ndef organize_files():\n    # Check if the base directory exists\n    if not os.path.exists(base_dir):\n        print(f\"\u274c Error: The directory '{base_dir}' does not exist.\")\n        return\n\n    # List all files in the base directory\n    try:\n        files = os.listdir(base_dir)\n    except Exception as e:\n        print(f\"\u274c Error accessing directory '{base_dir}': {e}\")\n        return\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2324",
    "name": "video.py",
    "path": "github_repo/scripts/02_media_processing/audio_tools/video.py",
    "category": "02_media_processing",
    "type": "youtube",
    "lines": 304,
    "size": 8225,
    "docstring": "",
    "keywords": [],
    "functions": [
      "make_video",
      "get_clip_paths",
      "add_clip",
      "render"
    ],
    "classes": [],
    "imports": [
      "os",
      "glob",
      "json",
      "pathlib",
      "moviepy.editor",
      "opplast",
      "opplast",
      "twitchtube",
      "clips",
      "config"
    ],
    "preview": "import os\nfrom glob import glob\nfrom json import dump\nfrom pathlib import Path\n\nfrom moviepy.editor import VideoFileClip, concatenate_videoclips\nfrom opplast import Upload\nfrom opplast import __version__ as opplast_version\n\nfrom twitchtube import __version__ as twitchtube_version\n\nfrom .clips import download_clips, get_clips\nfrom .config import *\nfrom .exceptions import *\nfrom .logging import Log as log\nfrom .utils import *\n\n\n# add language as param\ndef make_video(",
    "last_modified": "2025-09-13T05:53:56.433409"
  },
  {
    "id": "2325",
    "name": "streamlabs_polly.py",
    "path": "github_repo/scripts/02_media_processing/audio_tools/streamlabs_polly.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 65,
    "size": 1893,
    "docstring": "",
    "keywords": [
      "web_tools"
    ],
    "functions": [
      "__init__",
      "run",
      "randomvoice"
    ],
    "classes": [
      "StreamlabsPolly"
    ],
    "imports": [
      "random",
      "requests",
      "requests.exceptions",
      "utils",
      "utils.voice"
    ],
    "preview": "import random\n\nimport requests\nfrom requests.exceptions import JSONDecodeError\n\nfrom utils import settings\nfrom utils.voice import check_ratelimit\n\nvoices = [\n    \"Brian\",\n    \"Emma\",\n    \"Russell\",\n    \"Joey\",\n    \"Matthew\",\n    \"Joanna\",\n    \"Kimberly\",\n    \"Amy\",\n    \"Geraint\",\n    \"Nicole\",\n    \"Justin\",",
    "last_modified": "2025-09-13T05:53:59.777684"
  },
  {
    "id": "2326",
    "name": "up-down-mp4.py",
    "path": "github_repo/scripts/02_media_processing/audio_tools/up-down-mp4.py",
    "category": "02_media_processing",
    "type": "video_processing",
    "lines": 41,
    "size": 1275,
    "docstring": "",
    "keywords": [],
    "functions": [
      "downscale_video",
      "main"
    ],
    "classes": [],
    "imports": [
      "os",
      "moviepy.editor"
    ],
    "preview": "import os\n\nfrom moviepy.editor import VideoFileClip\n\n# Basic target size reduction (in MB)\nTARGET_SIZE_MB = 500  # Feel free to adjust based on how much you want to reduce\n\n\ndef downscale_video(input_path, output_path):\n    clip = VideoFileClip(input_path)\n\n    # Reduce resolution to 720p (if it's HD)\n    target_resolution = (1280, 720)\n    print(f\"Resizing to {target_resolution}...\")\n\n    try:\n        # Resize the video and set a moderate bitrate\n        clip_resized = clip.resize(height=target_resolution[1])\n        clip_resized.write_videofile(\n            output_path, codec=\"libx264\", bitrate=\"1500k\"",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2327",
    "name": "RedditScrape.py",
    "path": "github_repo/scripts/02_media_processing/audio_tools/RedditScrape.py",
    "category": "02_media_processing",
    "type": "web_tools",
    "lines": 86,
    "size": 3142,
    "docstring": "",
    "keywords": [
      "web_tools"
    ],
    "functions": [
      "__init__",
      "scrape_post"
    ],
    "classes": [
      "RedditScrape"
    ],
    "imports": [
      "sys",
      "gtts.tokenizer.symbols",
      "praw",
      "gtts.tokenizer",
      "config"
    ],
    "preview": "# RedditScrape.py\n# Last edited: June 28th 2021\n#\n# Called from run.py\n# Given a URL and an argument for number of posts to scrape, Class will connect to reddit API\n# and scrape the content from the post returning a title, authors, and replies\n#\n\n\nimport sys\n\nimport gtts.tokenizer.symbols\n\n# including the reddit api wrapper\nimport praw\n\n# Pre process can let us exchange words. ie: exchanging curse words for others\nfrom gtts.tokenizer import pre_processors\n\n# importing the config.py file to connect to PRAW",
    "last_modified": "2025-09-13T05:53:51.680878"
  },
  {
    "id": "2328",
    "name": "config_20241213005701.py",
    "path": "github_repo/scripts/02_media_processing/audio_tools/config_20241213005701.py",
    "category": "02_media_processing",
    "type": "setup",
    "lines": 3,
    "size": 72,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [],
    "preview": "# config.py\nSOURCE_DIRECTORY = \"/Users/steven/Music/NocTurnE-meLoDieSs\"\n",
    "last_modified": "2024-12-13T00:57:01.721926"
  },
  {
    "id": "2329",
    "name": "clean-album.py",
    "path": "github_repo/scripts/02_media_processing/audio_tools/clean-album.py",
    "category": "02_media_processing",
    "type": "organization",
    "lines": 104,
    "size": 3421,
    "docstring": "",
    "keywords": [
      "organization"
    ],
    "functions": [
      "hash_file",
      "flatten_directory",
      "remove_duplicates",
      "organize_by_type",
      "clean_directory"
    ],
    "classes": [],
    "imports": [
      "hashlib",
      "os",
      "shutil"
    ],
    "preview": "import hashlib\nimport os\nimport shutil\n\n# Define the base directory\nbase_dir = \"/Users/steven/Music/nocTurneMeLoDieS/mp4\"\n\n\ndef hash_file(file_path):\n    \"\"\"Generate a hash for a file to identify duplicates.\"\"\"\n    hasher = hashlib.md5()\n    with open(file_path, \"rb\") as f:\n        while chunk := f.read(8192):\n            hasher.update(chunk)\n    return hasher.hexdigest()\n\n\ndef flatten_directory(directory):\n    \"\"\"Move all files from nested folders to the base directory.\"\"\"\n    for root, _, files in os.walk(directory, topdown=False):",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2330",
    "name": "organize_albums.py_02.py",
    "path": "github_repo/scripts/02_media_processing/organize_albums.py_consolidated/organize_albums.py_02.py",
    "category": "02_media_processing",
    "type": "organization",
    "lines": 62,
    "size": 2355,
    "docstring": "",
    "keywords": [
      "organization"
    ],
    "functions": [
      "organize_files"
    ],
    "classes": [],
    "imports": [
      "os",
      "shutil"
    ],
    "preview": "import os\nimport shutil\n\n# Define the base directory\nbase_dir = \"/Users/steven/Movies/2025/mp4\"\n\n# Helper function to create folders and move files\ndef organize_files():\n    # List all files in the base directory\n    files = os.listdir(base_dir)\n\n    # Process each file\n    for file in files:\n        # Skip directories\n        if os.path.isdir(os.path.join(base_dir, file)):\n            continue\n\n        # Extract the base name (album name) from the file\n        if file.endswith(\".mp4\"):\n            album_name = file.replace(\".mp4\", \"\")",
    "last_modified": "2025-02-02T12:46:54.971548"
  },
  {
    "id": "2331",
    "name": "organize_albums.py.py",
    "path": "github_repo/scripts/02_media_processing/organize_albums.py_consolidated/organize_albums.py.py",
    "category": "02_media_processing",
    "type": "organization",
    "lines": 62,
    "size": 2470,
    "docstring": "",
    "keywords": [
      "organization"
    ],
    "functions": [
      "organize_files"
    ],
    "classes": [],
    "imports": [
      "os",
      "shutil"
    ],
    "preview": "import os\nimport shutil\n\n# Define the base directory\nbase_dir = \"/Users/steven/Music/NocTurnE-meLoDieS/albums/\"\n\n\n# Helper function to create folders and move files\ndef organize_files():\n    # List all files in the base directory\n    files = os.listdir(base_dir)\n\n    # Process each file\n    for file in files:\n        # Skip directories\n        if os.path.isdir(os.path.join(base_dir, file)):\n            continue\n\n        # Extract the base name (album name) from the file\n        if file.endswith(\".mp3\"):",
    "last_modified": "2025-05-04T22:47:11.429396"
  },
  {
    "id": "2332",
    "name": "cleanups.py",
    "path": "github_repo/scripts/02_media_processing/video_tools/cleanups.py",
    "category": "02_media_processing",
    "type": "organization",
    "lines": 56,
    "size": 1958,
    "docstring": "",
    "keywords": [],
    "functions": [
      "list_venv_directories",
      "main"
    ],
    "classes": [],
    "imports": [
      "os",
      "subprocess"
    ],
    "preview": "import os\nimport subprocess\n\n# Define the versions you want to keep\nrequired_versions = [\"3.10\", \"3.12.4\"]\n\n\ndef list_venv_directories(base_path):\n    \"\"\"List all virtual environment directories.\"\"\"\n    venv_dirs = []\n    for root, dirs, files in os.walk(base_path):\n        for dir in dirs:\n            if os.path.exists(os.path.join(root, dir, \"bin\", \"python\")):\n                venv_dirs.append(os.path.join(root, dir))\n    return venv_dirs\n\n\ndef main():\n    # Base path where your virtual environments are stored\n    venv_base_path = os.path.expanduser(\"~/venvs\")",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2333",
    "name": "sponsoredlinks.py",
    "path": "github_repo/scripts/02_media_processing/video_tools/sponsoredlinks.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 258,
    "size": 8181,
    "docstring": "",
    "keywords": [],
    "functions": [
      "__init__",
      "__str__",
      "html",
      "__init__",
      "__init__",
      "num_results",
      "_get_results_per_page",
      "_set_results_par_page",
      "get_results",
      "_get_all_results_sleep_fn"
    ],
    "classes": [
      "SLError(Exception)",
      "SLParseError(Exception)",
      "SponsoredLink(object)",
      "SponsoredLinks(object)"
    ],
    "imports": [
      "random",
      "re",
      "urllib",
      "BeautifulSoup",
      "browser",
      "htmlentitydefs"
    ],
    "preview": "#!/usr/bin/python\n#\n# Peteris Krumins (peter@catonmat.net)\n# http://www.catonmat.net  --  good coders code, great reuse\n#\n# http://www.catonmat.net/blog/python-library-for-google-sponsored-links-search/\n#\n# Code is licensed under MIT license.\n#\n\nimport random\nimport re\nimport urllib\n\nfrom BeautifulSoup import BeautifulSoup\nfrom browser import Browser, BrowserError\nfrom htmlentitydefs import name2codepoint\n\n#\n# TODO: join GoogleSearch and SponsoredLinks classes under a single base class",
    "last_modified": "2025-05-04T23:28:20.721929"
  },
  {
    "id": "2334",
    "name": "serialize.py",
    "path": "github_repo/scripts/02_media_processing/video_tools/serialize.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 203,
    "size": 7129,
    "docstring": "",
    "keywords": [
      "web_tools"
    ],
    "functions": [
      "dumps",
      "serialize",
      "loads",
      "prepare_response",
      "_loads_v0",
      "_loads_v1",
      "_loads_v2",
      "_loads_v3",
      "_loads_v4"
    ],
    "classes": [
      "Serializer"
    ],
    "imports": [
      "__future__",
      "io",
      "typing",
      "pip._vendor",
      "pip._vendor.requests.structures",
      "pip._vendor.urllib3",
      "pip._vendor.requests"
    ],
    "preview": "# SPDX-FileCopyrightText: 2015 Eric Larson\n#\n# SPDX-License-Identifier: Apache-2.0\nfrom __future__ import annotations\n\nimport io\nfrom typing import IO, TYPE_CHECKING, Any, Mapping, cast\n\nfrom pip._vendor import msgpack\nfrom pip._vendor.requests.structures import CaseInsensitiveDict\nfrom pip._vendor.urllib3 import HTTPResponse\n\nif TYPE_CHECKING:\n    from pip._vendor.requests import PreparedRequest\n\n\nclass Serializer:\n    serde_version = \"4\"\n\n    def dumps(",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2335",
    "name": "gback.py",
    "path": "github_repo/scripts/02_media_processing/video_tools/gback.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 32,
    "size": 985,
    "docstring": "",
    "keywords": [],
    "functions": [
      "move_files_back"
    ],
    "classes": [],
    "imports": [
      "json",
      "os",
      "shutil"
    ],
    "preview": "import json\nimport os\nimport shutil\n\n# Paths configuration\nbackup_file_path = \"/Volumes/iMac/15days/file_order_backup.json\"\n\n\ndef move_files_back(backup_path):\n    \"\"\"Move files back to their original locations.\"\"\"\n    # Load the backup file to get the file metadata\n    with open(backup_path, \"r\") as backup_file:\n        files_metadata = json.load(backup_file)\n\n    for file in files_metadata:\n        original_path = file[\"original_path\"]\n        destination_path = file[\"destination_path\"]\n\n        # Ensure the original directory exists\n        if not os.path.exists(os.path.dirname(original_path)):",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2336",
    "name": "resultdict.py",
    "path": "github_repo/scripts/02_media_processing/video_tools/resultdict.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 17,
    "size": 402,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [
      "ResultDict"
    ],
    "imports": [
      "typing",
      "typing"
    ],
    "preview": "from typing import TYPE_CHECKING, Optional\n\nif TYPE_CHECKING:\n    # TypedDict was introduced in Python 3.8.\n    #\n    # TODO: Remove the else block and TYPE_CHECKING check when dropping support\n    # for Python 3.7.\n    from typing import TypedDict\n\n    class ResultDict(TypedDict):\n        encoding: Optional[str]\n        confidence: float\n        language: Optional[str]\n\nelse:\n    ResultDict = dict\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2337",
    "name": "my_copy_utils.py",
    "path": "github_repo/scripts/02_media_processing/video_tools/my_copy_utils.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 39,
    "size": 1386,
    "docstring": "",
    "keywords": [],
    "functions": [
      "ensure_dir"
    ],
    "classes": [],
    "imports": [
      "csv",
      "os",
      "shutil"
    ],
    "preview": "import csv\nimport os\nimport shutil\n\n# Paths configuration\ncsv_file_path = \"/Users/steven/15days.csv\"\ndestination_root = \"/Volumes/iMac/15days\"\nbackup_root = os.path.join(destination_root, \"backup\")\n\n\ndef ensure_dir(directory):\n    \"\"\"Ensure the directory exists. If not, create it.\"\"\"\n    if not os.path.exists(directory):\n        os.makedirs(directory)\n\n\nwith open(csv_file_path, newline=\"\") as csvfile:\n    filereader = csv.reader(csvfile)\n    for row in filereader:\n        # Assuming each row has one column with the file path",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2338",
    "name": "static_test.py",
    "path": "github_repo/scripts/02_media_processing/video_tools/static_test.py",
    "category": "02_media_processing",
    "type": "testing",
    "lines": 9,
    "size": 158,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "clips"
    ],
    "preview": "from clips import *\n\nclips = []\n\nfor _ in range(0, 3):\n    clips.append(gen_transition_clip())\n\nconcatenate_videoclips(clips).to_videofile(\"temp/static.mp4\")\n",
    "last_modified": "2025-05-04T23:27:53.352828"
  },
  {
    "id": "2339",
    "name": "undo-move-csv.py",
    "path": "github_repo/scripts/02_media_processing/video_tools/undo-move-csv.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 32,
    "size": 985,
    "docstring": "",
    "keywords": [],
    "functions": [
      "move_files_back"
    ],
    "classes": [],
    "imports": [
      "json",
      "os",
      "shutil"
    ],
    "preview": "import json\nimport os\nimport shutil\n\n# Paths configuration\nbackup_file_path = \"/Volumes/iMac/15days/file_order_backup.json\"\n\n\ndef move_files_back(backup_path):\n    \"\"\"Move files back to their original locations.\"\"\"\n    # Load the backup file to get the file metadata\n    with open(backup_path, \"r\") as backup_file:\n        files_metadata = json.load(backup_file)\n\n    for file in files_metadata:\n        original_path = file[\"original_path\"]\n        destination_path = file[\"destination_path\"]\n\n        # Ensure the original directory exists\n        if not os.path.exists(os.path.dirname(original_path)):",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2340",
    "name": "config_20241213005652.py",
    "path": "github_repo/scripts/02_media_processing/video_tools/config_20241213005652.py",
    "category": "02_media_processing",
    "type": "setup",
    "lines": 4,
    "size": 99,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [],
    "preview": "# config.py\nSOURCE_DIRECTORY = \"        r\".*\\/Movies\\/CapCut\\/.*\",  # Specific capcut directory\ns\"\n",
    "last_modified": "2024-12-13T00:56:52.227762"
  },
  {
    "id": "2341",
    "name": "get_external_link.py",
    "path": "github_repo/scripts/02_media_processing/video_tools/get_external_link.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 123,
    "size": 4133,
    "docstring": "",
    "keywords": [
      "web_tools"
    ],
    "functions": [],
    "classes": [],
    "imports": [
      "logging",
      "os",
      "subprocess",
      "time",
      "datetime",
      "requests",
      "pyrogram",
      "translation",
      "helper_funcs.chat_base",
      "helper_funcs.display_progress"
    ],
    "preview": "import logging\n\nlogging.basicConfig(\n    level=logging.DEBUG, format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\n)\nlogger = logging.getLogger(__name__)\n\nimport os\nimport subprocess\nimport time\nfrom datetime import datetime\n\nimport requests\n\n# the secret configuration specific things\nif bool(os.environ.get(\"WEBHOOK\", False)):\n    from sample_config import Config\nelse:\n    from config import Config\n",
    "last_modified": "2025-09-13T05:53:44.214082"
  },
  {
    "id": "2342",
    "name": "NewUpload_20250607131239.py",
    "path": "github_repo/scripts/02_media_processing/video_tools/NewUpload_20250607131239.py",
    "category": "02_media_processing",
    "type": "youtube",
    "lines": 78,
    "size": 3077,
    "docstring": "",
    "keywords": [
      "youtube",
      "web_tools"
    ],
    "functions": [
      "get_authenticated_service",
      "upload_video"
    ],
    "classes": [],
    "imports": [
      "os",
      "google.auth.transport.requests",
      "google_auth_oauthlib.flow",
      "googleapiclient.discovery",
      "googleapiclient.errors"
    ],
    "preview": "import os\n\nimport google.auth.transport.requests\nimport google_auth_oauthlib.flow\nimport googleapiclient.discovery\nimport googleapiclient.errors\n\nSCOPES = [\"https://www.googleapis.com/auth/youtube.upload\"]\nCLIENT_SECRETS_FILE = \"/Users/steven/Documents/python/Youtube/client_secrets.json\"  # Replace with your client secrets file\nAPI_SERVICE_NAME = \"youtube\"\nAPI_VERSION = \"v3\"\n\n\ndef get_authenticated_service():\n    \"\"\"Authenticates and returns the YouTube Data API service.\"\"\"\n    flow = google_auth_oauthlib.flow.InstalledAppFlow.from_client_secrets_file(\n        CLIENT_SECRETS_FILE, SCOPES\n    )\n    credentials = flow.run_local_server(port=0)\n    return googleapiclient.discovery.build(API_SERVICE_NAME, API_VERSION, credentials=credentials)",
    "last_modified": "2025-09-13T05:54:12.178653"
  },
  {
    "id": "2343",
    "name": "help_uploadbot.py",
    "path": "github_repo/scripts/02_media_processing/video_tools/help_uploadbot.py",
    "category": "02_media_processing",
    "type": "youtube",
    "lines": 47,
    "size": 1533,
    "docstring": "",
    "keywords": [
      "web_tools"
    ],
    "functions": [
      "DetectFileSize",
      "DownLoadFile"
    ],
    "classes": [],
    "imports": [
      "logging",
      "os",
      "requests"
    ],
    "preview": "import logging\n\nlogging.basicConfig(\n    level=logging.DEBUG, format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\n)\nlogger = logging.getLogger(__name__)\n\nimport os\n\nimport requests\n\n\ndef DetectFileSize(url):\n    r = requests.get(url, allow_redirects=True, stream=True)\n    total_size = int(r.headers.get(\"content-length\", 0))\n    return total_size\n\n\ndef DownLoadFile(url, file_name, chunk_size, client, ud_type, message_id, chat_id):\n    if os.path.exists(file_name):",
    "last_modified": "2025-05-06T04:35:15.017768"
  },
  {
    "id": "2344",
    "name": "NewUpload_20250607125012.py",
    "path": "github_repo/scripts/02_media_processing/video_tools/NewUpload_20250607125012.py",
    "category": "02_media_processing",
    "type": "youtube",
    "lines": 78,
    "size": 2618,
    "docstring": "",
    "keywords": [
      "youtube",
      "web_tools"
    ],
    "functions": [
      "get_authenticated_service",
      "upload_video"
    ],
    "classes": [],
    "imports": [
      "os",
      "google.auth.transport.requests",
      "google_auth_oauthlib.flow",
      "googleapiclient.discovery",
      "googleapiclient.errors"
    ],
    "preview": "import os\n\nimport google.auth.transport.requests\nimport google_auth_oauthlib.flow\nimport googleapiclient.discovery\nimport googleapiclient.errors\n\nSCOPES = [\"https://www.googleapis.com/auth/youtube.upload\"]\nCLIENT_SECRETS_FILE = \"/Users/steven/Documents/python/Youtube/client_secrets.json\"  # Replace with your client secrets file\nAPI_SERVICE_NAME = \"youtube\"\nAPI_VERSION = \"v3\"\n\n\ndef get_authenticated_service():\n    \"\"\"Authenticates and returns the YouTube Data API service.\"\"\"\n    flow = google_auth_oauthlib.flow.InstalledAppFlow.from_client_secrets_file(\n        CLIENT_SECRETS_FILE, SCOPES\n    )\n    credentials = flow.run_local_server(port=0)\n    return googleapiclient.discovery.build(API_SERVICE_NAME, API_VERSION, credentials=credentials)",
    "last_modified": "2025-09-13T05:54:11.820695"
  },
  {
    "id": "2345",
    "name": "sample_config.py",
    "path": "github_repo/scripts/02_media_processing/video_tools/sample_config.py",
    "category": "02_media_processing",
    "type": "setup",
    "lines": 50,
    "size": 1954,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [
      "Config"
    ],
    "imports": [
      "os"
    ],
    "preview": "import os\n\n\nclass Config(object):\n    # get a token from https://chatbase.com\n    CHAT_BASE_TOKEN = os.environ.get(\"CHAT_BASE_TOKEN\", \"\")\n    # get a token from @BotFather\n    TG_BOT_TOKEN = os.environ.get(\"TG_BOT_TOKEN\", \"\")\n    # The Telegram API things\n    APP_ID = int(os.environ.get(\"APP_ID\", 12345))\n    API_HASH = os.environ.get(\"API_HASH\")\n    # Get these values from my.telegram.org\n    # Array to store users who are authorized to use the bot\n    AUTH_USERS = set(str(x) for x in os.environ.get(\"AUTH_USERS\", \"\").split())\n    # reg: Procedures\n    UTUBE_BOT_USERS = AUTH_USERS\n    SUPER_DLBOT_USERS = AUTH_USERS\n    SUPER3X_DLBOT_USERS = AUTH_USERS\n    SUPER7X_DLBOT_USERS = AUTH_USERS\n    BANNED_USERS = []",
    "last_modified": "2025-09-13T05:53:44.774704"
  },
  {
    "id": "2346",
    "name": "version.py",
    "path": "github_repo/scripts/02_media_processing/video_tools/version.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 22,
    "size": 827,
    "docstring": "",
    "keywords": [
      "web_tools"
    ],
    "functions": [
      "checkversion"
    ],
    "classes": [],
    "imports": [
      "requests",
      "utils.console"
    ],
    "preview": "import requests\n\nfrom utils.console import print_step\n\n\ndef checkversion(__VERSION__: str):\n    response = requests.get(\n        \"https://api.github.com/repos/elebumm/RedditVideoMakerBot/releases/latest\"\n    )\n    latestversion = response.json()[\"tag_name\"]\n    if __VERSION__ == latestversion:\n        print_step(f\"You are using the newest version ({__VERSION__}) of the bot\")\n        return True\n    elif __VERSION__ < latestversion:\n        print_step(\n            f\"You are using an older version ({__VERSION__}) of the bot. Download the newest version ({latestversion}) from https://github.com/elebumm/RedditVideoMakerBot/releases/latest\"\n        )\n    else:\n        print_step(\n            f\"Welcome to the test version ({__VERSION__}) of the bot. Thanks for testing and feel free to report any bugs you find.\"",
    "last_modified": "2025-09-11T13:27:01.773005"
  },
  {
    "id": "2347",
    "name": "youtube.py",
    "path": "github_repo/scripts/02_media_processing/video_tools/youtube.py",
    "category": "02_media_processing",
    "type": "youtube",
    "lines": 37,
    "size": 1066,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "googleapiclient.discovery",
      "csv"
    ],
    "preview": "from googleapiclient.discovery import build\n\n# Set up YouTube Data API\nyoutube = build(\"youtube\", \"v3\", developerKey=\"AIzaSyC08MXHwy-tkAwAhvW0TumdKJmSfOJYFqw\")\n\n# Retrieve channel's videos\nvideos = []\nnext_page_token = None\nwhile True:\n    request = youtube.search().list(\n        part=\"snippet\",\n        channelId=\"UCDl7VmS3gD2BQBVZUlL21-A\",\n        maxResults=50,  # Max allowed value\n        pageToken=next_page_token,\n    )\n    response = request.execute()\n    videos.extend(response[\"items\"])\n    next_page_token = response.get(\"nextPageToken\")\n    if not next_page_token:\n        break",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2348",
    "name": "NewUpload_20250607130948.py",
    "path": "github_repo/scripts/02_media_processing/video_tools/NewUpload_20250607130948.py",
    "category": "02_media_processing",
    "type": "youtube",
    "lines": 95,
    "size": 4012,
    "docstring": "",
    "keywords": [
      "video_processing",
      "youtube",
      "web_tools"
    ],
    "functions": [
      "get_authenticated_service",
      "upload_video"
    ],
    "classes": [],
    "imports": [
      "os",
      "google.auth.transport.requests",
      "google_auth_oauthlib.flow",
      "googleapiclient.discovery",
      "googleapiclient.errors"
    ],
    "preview": "import os\n\nimport google.auth.transport.requests\nimport google_auth_oauthlib.flow\nimport googleapiclient.discovery\nimport googleapiclient.errors\n\nSCOPES = [\"https://www.googleapis.com/auth/youtube.upload\"]\nCLIENT_SECRETS_FILE = \"/Users/steven/Documents/python/Youtube/client_secrets.json\"  # Replace with your client secrets file\nAPI_SERVICE_NAME = \"youtube\"\nAPI_VERSION = \"v3\"\n\ndef get_authenticated_service():\n    \"\"\"Authenticates and returns the YouTube Data API service.\"\"\"\n    flow = google_auth_oauthlib.flow.InstalledAppFlow.from_client_secrets_file(\n        CLIENT_SECRETS_FILE, SCOPES)\n    credentials = flow.run_local_server(port=0)\n    return googleapiclient.discovery.build(API_SERVICE_NAME, API_VERSION,\n                                            credentials=credentials)\n",
    "last_modified": "2025-09-06T12:24:11.130048"
  },
  {
    "id": "2349",
    "name": "autoscaling_sagemaker_endpoint.py",
    "path": "github_repo/scripts/02_media_processing/video_tools/autoscaling_sagemaker_endpoint.py",
    "category": "02_media_processing",
    "type": "setup",
    "lines": 176,
    "size": 7897,
    "docstring": "In Amazon SageMaker and other AWS services, Application Auto Scaling allows you to automatically scale resources in and out based on configurable policies. Within this context, registering a scalable target and creating a scalable policy are two critical steps that work together to enable this functionality. Here's a breakdown of each and how they differ:\nRegister Scalable Target\n\nWhen you register a scalable target with Application Auto Scaling, you are essentially telling AWS which resource you want to scale and defining the minimum and maximum capacity limits for that resource. This step does not define how the scaling should occur; rather, it sets up the parameters within which scaling can happen. In your example with SageMaker:\n\n    Resource ID: This is a unique identifier for the scalable target. For SageMaker inference components, it typically includes the inference component name.\n    Service Namespace: This indicates the AWS service where the resource resides, which is \"sagemaker\" in this case.\n    Scalable Dimension: This specifies the aspect of the resource you want to scale. For SageMaker inference components, this is often the desired number of copies (instances) of an inference component.\n    MinCapacity and MaxCapacity: These values define the minimum and maximum number of copies that the auto scaling can adjust to.\n\nBy registering a scalable target, you prepare your SageMaker inference component for scaling but do not specify when or how the scaling should occur.\nScalable Policy\n\nCreating a scalable policy is where you define the specific criteria and rules for scaling. This policy uses metrics and thresholds to automatically adjust the resource's capacity within the limits set by the registered scalable target. In your SageMaker example:\n\n    Policy Type: You've chosen \"TargetTrackingScaling,\" which adjusts the scalable target's capacity as required to maintain a target value for a specific metric.\n    Target Tracking Configuration: This includes the metric to track (e.g., SageMakerInferenceComponentInvocationsPerCopy), the target value for that metric, and cooldown periods for scaling in and out. The policy uses these parameters to decide when to scale the resources up or down.\n\nThe scalable policy is what actively manages the scaling process. It monitors the specified metric and, based on its value relative to the target value, triggers scaling actions to increase or decrease the number of copies of the inference component within the bounds set by the registered scalable target.",
    "keywords": [
      "organization"
    ],
    "functions": [
      "register_scalable_target",
      "put_scaling_policy",
      "describe_scalable_targets",
      "describe_scaling_policies",
      "delete_scaling_policy",
      "deregister_scalable_target",
      "apply_policy",
      "__init__",
      "apply_policy",
      "__init__"
    ],
    "classes": [
      "IAutoScalingClient",
      "ScalingPolicyStrategy",
      "TargetTrackingScalingPolicy",
      "ScalableTarget",
      "AutoscalingSagemakerEndpoint"
    ],
    "imports": [],
    "preview": "\"\"\"\nIn Amazon SageMaker and other AWS services, Application Auto Scaling allows you to automatically scale resources in and out based on configurable policies. Within this context, registering a scalable target and creating a scalable policy are two critical steps that work together to enable this functionality. Here's a breakdown of each and how they differ:\nRegister Scalable Target\n\nWhen you register a scalable target with Application Auto Scaling, you are essentially telling AWS which resource you want to scale and defining the minimum and maximum capacity limits for that resource. This step does not define how the scaling should occur; rather, it sets up the parameters within which scaling can happen. In your example with SageMaker:\n\n    Resource ID: This is a unique identifier for the scalable target. For SageMaker inference components, it typically includes the inference component name.\n    Service Namespace: This indicates the AWS service where the resource resides, which is \"sagemaker\" in this case.\n    Scalable Dimension: This specifies the aspect of the resource you want to scale. For SageMaker inference components, this is often the desired number of copies (instances) of an inference component.\n    MinCapacity and MaxCapacity: These values define the minimum and maximum number of copies that the auto scaling can adjust to.\n\nBy registering a scalable target, you prepare your SageMaker inference component for scaling but do not specify when or how the scaling should occur.\nScalable Policy\n\nCreating a scalable policy is where you define the specific criteria and rules for scaling. This policy uses metrics and thresholds to automatically adjust the resource's capacity within the limits set by the registered scalable target. In your SageMaker example:\n\n    Policy Type: You've chosen \"TargetTrackingScaling,\" which adjusts the scalable target's capacity as required to maintain a target value for a specific metric.\n    Target Tracking Configuration: This includes the metric to track (e.g., SageMakerInferenceComponentInvocationsPerCopy), the target value for that metric, and cooldown periods for scaling in and out. The policy uses these parameters to decide when to scale the resources up or down.\n\nThe scalable policy is what actively manages the scaling process. It monitors the specified metric and, based on its value relative to the target value, triggers scaling actions to increase or decrease the number of copies of the inference component within the bounds set by the registered scalable target.",
    "last_modified": "2025-05-06T04:35:14.984234"
  },
  {
    "id": "2350",
    "name": "copy_images.py",
    "path": "github_repo/scripts/02_media_processing/video_tools/copy_images.py",
    "category": "02_media_processing",
    "type": "image_processing",
    "lines": 39,
    "size": 1386,
    "docstring": "",
    "keywords": [],
    "functions": [
      "ensure_dir"
    ],
    "classes": [],
    "imports": [
      "csv",
      "os",
      "shutil"
    ],
    "preview": "import csv\nimport os\nimport shutil\n\n# Paths configuration\ncsv_file_path = \"/Users/steven/15days.csv\"\ndestination_root = \"/Volumes/iMac/15days\"\nbackup_root = os.path.join(destination_root, \"backup\")\n\n\ndef ensure_dir(directory):\n    \"\"\"Ensure the directory exists. If not, create it.\"\"\"\n    if not os.path.exists(directory):\n        os.makedirs(directory)\n\n\nwith open(csv_file_path, newline=\"\") as csvfile:\n    filereader = csv.reader(csvfile)\n    for row in filereader:\n        # Assuming each row has one column with the file path",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2351",
    "name": "index.py",
    "path": "github_repo/scripts/02_media_processing/video_tools/index.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 135,
    "size": 4698,
    "docstring": "",
    "keywords": [],
    "functions": [
      "add_options",
      "run",
      "_build_package_finder",
      "get_available_package_versions"
    ],
    "classes": [
      "IndexCommand"
    ],
    "imports": [
      "logging",
      "optparse",
      "typing",
      "pip._internal.cli",
      "pip._internal.cli.req_command",
      "pip._internal.cli.status_codes",
      "pip._internal.commands.search",
      "pip._internal.exceptions",
      "pip._internal.index.collector",
      "pip._internal.index.package_finder"
    ],
    "preview": "import logging\nfrom optparse import Values\nfrom typing import Any, Iterable, List, Optional, Union\n\nfrom pip._internal.cli import cmdoptions\nfrom pip._internal.cli.req_command import IndexGroupCommand\nfrom pip._internal.cli.status_codes import ERROR, SUCCESS\nfrom pip._internal.commands.search import print_dist_installation_info\nfrom pip._internal.exceptions import CommandError, DistributionNotFound, PipError\nfrom pip._internal.index.collector import LinkCollector\nfrom pip._internal.index.package_finder import PackageFinder\nfrom pip._internal.models.selection_prefs import SelectionPreferences\nfrom pip._internal.models.target_python import TargetPython\nfrom pip._internal.network.session import PipSession\nfrom pip._internal.utils.misc import write_output\nfrom pip._vendor.packaging.version import LegacyVersion, Version\n\nlogger = logging.getLogger(__name__)\n\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2352",
    "name": "ytcsv.py",
    "path": "github_repo/scripts/02_media_processing/video_tools/ytcsv.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 26,
    "size": 778,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "pytube"
    ],
    "preview": "from pytube import Playlist\n\n# Replace the playlist URL below with your desired playlist URL\nplaylist_url = \"https://www.youtube.com/playlist?list=PLfudK7D_bQIjRgAqVU_jkbnb1N5V_pNiG\"\n\n# Define the directory where you want to save the downloads\ndownload_directory = \"/Users/steven/Movies/ESO/qshorts\"\n\n# Create a Playlist object\nplaylist = Playlist(playlist_url)\n\n# Loop through the videos in the playlist\nfor video in playlist.videos:\n    # Download the video\n    video.streams.get_highest_resolution().download()\n\n    # Get video metadata\n    title = video.title\n    description = video.description\n    tags = video.keywords",
    "last_modified": "2025-09-13T05:53:55.665101"
  },
  {
    "id": "2353",
    "name": "completion.py",
    "path": "github_repo/scripts/02_media_processing/video_tools/completion.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 129,
    "size": 4257,
    "docstring": "",
    "keywords": [],
    "functions": [
      "add_options",
      "run"
    ],
    "classes": [
      "CompletionCommand"
    ],
    "imports": [
      "sys",
      "textwrap",
      "optparse",
      "typing",
      "pip._internal.cli.base_command",
      "pip._internal.cli.status_codes",
      "pip._internal.utils.misc"
    ],
    "preview": "import sys\nimport textwrap\nfrom optparse import Values\nfrom typing import List\n\nfrom pip._internal.cli.base_command import Command\nfrom pip._internal.cli.status_codes import SUCCESS\nfrom pip._internal.utils.misc import get_prog\n\nBASE_COMPLETION = \"\"\"\n# pip {shell} completion start{script}# pip {shell} completion end\n\"\"\"\n\nCOMPLETION_SCRIPTS = {\n    \"bash\": \"\"\"\n        _pip_completion()\n        {{\n            COMPREPLY=( $( COMP_WORDS=\"${{COMP_WORDS[*]}}\" \\\\\n                           COMP_CWORD=$COMP_CWORD \\\\\n                           PIP_AUTO_COMPLETE=1 $1 2>/dev/null ) )",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2354",
    "name": "youtube_analytics.py",
    "path": "github_repo/scripts/02_media_processing/video_tools/youtube_analytics.py",
    "category": "02_media_processing",
    "type": "youtube",
    "lines": 150,
    "size": 4751,
    "docstring": "",
    "keywords": [
      "youtube",
      "web_tools"
    ],
    "functions": [
      "authenticate",
      "build_youtube_service",
      "get_video_details",
      "get_channel_videos",
      "save_to_csv",
      "download_channel_videos_to_csv"
    ],
    "classes": [],
    "imports": [
      "csv",
      "os",
      "pickle",
      "google.auth.transport.requests",
      "google_auth_oauthlib.flow",
      "googleapiclient.discovery"
    ],
    "preview": "import csv\nimport os\nimport pickle\n\nfrom google.auth.transport.requests import Request\nfrom google_auth_oauthlib.flow import InstalledAppFlow\nfrom googleapiclient.discovery import build\n\n# Path to your client_secret.json file\nCLIENT_SECRETS_FILE = (\n    \"/Users/steven/Documents/client_secret.json\"  # Update with your file path\n)\n\n# Scopes for the YouTube Data API v3\nSCOPES = [\"https://www.googleapis.com/auth/youtube.readonly\"]\n\n\n# Authenticate the user using OAuth 2.0\ndef authenticate():\n    credentials = None",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2355",
    "name": "botStories.py",
    "path": "github_repo/scripts/02_media_processing/video_tools/botStories.py",
    "category": "02_media_processing",
    "type": "web_tools",
    "lines": 186,
    "size": 5037,
    "docstring": "Created in 12/2019\n@Author: Paulo https://github.com/alpdias",
    "keywords": [],
    "functions": [
      "functionStories",
      "botlogin",
      "stories"
    ],
    "classes": [],
    "imports": [
      "os",
      "random",
      "pathlib",
      "time",
      "art",
      "selenium",
      "selenium.webdriver.common.keys"
    ],
    "preview": "# -*- coding: utf-8 -*-\n\n\"\"\"\nCreated in 12/2019\n@Author: Paulo https://github.com/alpdias\n\"\"\"\n\n# imported libraries\nimport os\nimport random\nfrom pathlib import Path\nfrom time import sleep\n\nimport art\nfrom selenium import webdriver\nfrom selenium.webdriver.common.keys import Keys\n\n\ndef functionStories(mySystem):\n    \"\"\"",
    "last_modified": "2025-05-04T23:28:20.961006"
  },
  {
    "id": "2356",
    "name": "deprecation.py",
    "path": "github_repo/scripts/02_media_processing/video_tools/deprecation.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 124,
    "size": 3706,
    "docstring": "A module that implements tooling to enable easy warnings about deprecations.",
    "keywords": [],
    "functions": [
      "_showwarning",
      "install_warning_logger",
      "deprecated"
    ],
    "classes": [
      "PipDeprecationWarning"
    ],
    "imports": [
      "logging",
      "warnings",
      "typing",
      "pip",
      "pip._vendor.packaging.version"
    ],
    "preview": "\"\"\"\nA module that implements tooling to enable easy warnings about deprecations.\n\"\"\"\n\nimport logging\nimport warnings\nfrom typing import Any, Optional, TextIO, Type, Union\n\nfrom pip import __version__ as current_version  # NOTE: tests patch this name.\nfrom pip._vendor.packaging.version import parse\n\nDEPRECATION_MSG_PREFIX = \"DEPRECATION: \"\n\n\nclass PipDeprecationWarning(Warning):\n    pass\n\n\n_original_showwarning: Any = None\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2357",
    "name": "youtube_download 2.py",
    "path": "github_repo/scripts/02_media_processing/video_tools/youtube_download 2.py",
    "category": "02_media_processing",
    "type": "youtube",
    "lines": 53,
    "size": 1213,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "json",
      "os",
      "sys",
      "urllib",
      "datetime",
      "subprocess"
    ],
    "preview": "#!/usr/bin/python\n\"\"\"\nYouTube Channel Downloader\n\"\"\"\n \nimport json\nimport os\nimport sys\nimport urllib\nfrom datetime import date\nfrom subprocess import call\n\nprint \"--------------------------\"\nprint \"YouTube Channel Downloader\"\nprint \"--------------------------\"\nprint \"\"\nwhile True:\n    try:\n        author = raw_input(\"Enter username of YouTube channel: \")\n        break",
    "last_modified": "2025-08-06T14:24:26.086338"
  },
  {
    "id": "2358",
    "name": "yt-meta 2.py",
    "path": "github_repo/scripts/02_media_processing/video_tools/yt-meta 2.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 44,
    "size": 1223,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "csv",
      "os",
      "googleapiclient.discovery"
    ],
    "preview": "import csv\nimport os\n\nfrom googleapiclient.discovery import build\n\n# Securely load the API key\napi_key = os.getenv(\"YOUTUBE_API_KEY\")\n\n# Set up YouTube Data API\nyoutube = build(\"youtube\", \"v3\", developerKey=api_key)\n\n# Retrieve channel's videos\nvideos = []\nnext_page_token = None\ntry:\n    while True:\n        request = youtube.search().list(\n            part=\"snippet\",\n            channelId=\"UCDl7VmS3gD2BQBVZUlL21-A\",\n            maxResults=50,  # Max allowed value",
    "last_modified": "2025-08-06T13:51:17.086091"
  },
  {
    "id": "2359",
    "name": "NewUpload_20250607131031.py",
    "path": "github_repo/scripts/02_media_processing/video_tools/NewUpload_20250607131031.py",
    "category": "02_media_processing",
    "type": "youtube",
    "lines": 120,
    "size": 4753,
    "docstring": "",
    "keywords": [
      "video_processing",
      "youtube",
      "web_tools"
    ],
    "functions": [
      "get_authenticated_service",
      "upload_video"
    ],
    "classes": [],
    "imports": [
      "os",
      "google.auth.transport.requests",
      "google_auth_oauthlib.flow",
      "googleapiclient.discovery",
      "googleapiclient.errors"
    ],
    "preview": "import os\n\nimport google.auth.transport.requests\nimport google_auth_oauthlib.flow\nimport googleapiclient.discovery\nimport googleapiclient.errors\n\n# --- Analysis of the code structure and logic ---\n\n# 1. Imports:\n#    - The script imports necessary modules for Google API client, authentication, and OS operations.\n#    - It assumes the googleapiclient and google_auth_oauthlib libraries are installed.\n\n# 2. Constants:\n#    - SCOPES: Defines the OAuth scope for uploading to YouTube.\n#    - CLIENT_SECRETS_FILE: Path to the OAuth client secrets JSON file.\n#    - API_SERVICE_NAME and API_VERSION: Used to build the YouTube API client.\n\n# 3. Authentication Function:\ndef get_authenticated_service():",
    "last_modified": "2025-09-06T12:24:11.133340"
  },
  {
    "id": "2360",
    "name": "goapi.py",
    "path": "github_repo/scripts/02_media_processing/video_tools/goapi.py",
    "category": "02_media_processing",
    "type": "web_tools",
    "lines": 21,
    "size": 943,
    "docstring": "",
    "keywords": [
      "web_tools"
    ],
    "functions": [],
    "classes": [],
    "imports": [
      "requests"
    ],
    "preview": "import requests\n\nX - API - KEY = \"k-r4PvyLSTQ6122zbwdky3T3BlbkFJCCdmdHniFBJTDOi8cKjV\"\n\nendpoint = \"https://api.goapi.ai/mj/v2/imagine\"\n\nheaders = {\"X-API-KEY\": X - API - KEY}\n\ndata = {\n    \"prompt\": \"Wraith, a master of stealth and assassination, moves unseen through the Rogue Isles, his blades \ufb01nding marks unseen until it's too late. His tale is one of vengeance and shadow, as he cuts a silent path through his enemies, from the treacherous jungles of Mercy Island to the dark alleys of St. Martial. Wraith's journey explores the depths of the Stalker's path, where invisibility and the element of surprise are wielded with deadly precision, illustrating that the most formidable threats are those unseen.\",\n    \"aspect_ratio\": \"9:16\",\n    \"process_mode\": \"mixed\",\n    \"webhook_endpoint\": \"\",\n    \"webhook_secret\": \"\",\n}\n\nresponse = requests.post(endpoint, headers=headers, json=data)\n\nprint(response.status_code)\nprint(response.json())",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2361",
    "name": "NewUpload_20250607131227.py",
    "path": "github_repo/scripts/02_media_processing/video_tools/NewUpload_20250607131227.py",
    "category": "02_media_processing",
    "type": "youtube",
    "lines": 92,
    "size": 3847,
    "docstring": "",
    "keywords": [
      "video_processing",
      "youtube",
      "web_tools"
    ],
    "functions": [
      "get_authenticated_service",
      "upload_video"
    ],
    "classes": [],
    "imports": [
      "os",
      "google.auth.transport.requests",
      "google_auth_oauthlib.flow",
      "googleapiclient.discovery",
      "googleapiclient.errors"
    ],
    "preview": "import os\n\nimport google.auth.transport.requests\nimport google_auth_oauthlib.flow\nimport googleapiclient.discovery\nimport googleapiclient.errors\n\nSCOPES = [\"https://www.googleapis.com/auth/youtube.upload\"]\nCLIENT_SECRETS_FILE = \"/Users/steven/Documents/python/Youtube/client_secrets.json\"  # Replace with your client secrets file\nAPI_SERVICE_NAME = \"youtube\"\nAPI_VERSION = \"v3\"\n\ndef get_authenticated_service():\n    \"\"\"Authenticates and returns the YouTube Data API service.\"\"\"\n    flow = google_auth_oauthlib.flow.InstalledAppFlow.from_client_secrets_file(\n        CLIENT_SECRETS_FILE, SCOPES)\n    credentials = flow.run_local_server(port=0)\n    return googleapiclient.discovery.build(API_SERVICE_NAME, API_VERSION,\n                                            credentials=credentials)\n",
    "last_modified": "2025-09-06T12:24:11.136449"
  },
  {
    "id": "2362",
    "name": "file_cache.py",
    "path": "github_repo/scripts/02_media_processing/video_tools/file_cache.py",
    "category": "02_media_processing",
    "type": "organization",
    "lines": 180,
    "size": 5338,
    "docstring": "",
    "keywords": [],
    "functions": [
      "_secure_open_write",
      "url_to_file_path",
      "__init__",
      "encode",
      "_fn",
      "get",
      "set",
      "_write",
      "_delete",
      "delete"
    ],
    "classes": [
      "_FileCacheMixin",
      "FileCache",
      "SeparateBodyFileCache"
    ],
    "imports": [
      "__future__",
      "hashlib",
      "os",
      "textwrap",
      "typing",
      "pip._vendor.cachecontrol.cache",
      "pip._vendor.cachecontrol.controller",
      "datetime",
      "filelock",
      "filelock"
    ],
    "preview": "# SPDX-FileCopyrightText: 2015 Eric Larson\n#\n# SPDX-License-Identifier: Apache-2.0\nfrom __future__ import annotations\n\nimport hashlib\nimport os\nfrom textwrap import dedent\nfrom typing import IO, TYPE_CHECKING\n\nfrom pip._vendor.cachecontrol.cache import BaseCache, SeparateBodyBaseCache\nfrom pip._vendor.cachecontrol.controller import CacheController\n\nif TYPE_CHECKING:\n    from datetime import datetime\n\n    from filelock import BaseFileLock\n\n\ndef _secure_open_write(filename: str, fmode: int) -> IO[bytes]:",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2363",
    "name": "util.py",
    "path": "github_repo/scripts/02_media_processing/video_tools/util.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 279,
    "size": 8596,
    "docstring": "",
    "keywords": [],
    "functions": [
      "col",
      "lineno",
      "line",
      "_escape_regex_range_chars",
      "_collapse_string_to_ranges",
      "_flatten",
      "_make_synonym_function",
      "replaced_by_pep8",
      "_set",
      "__init__"
    ],
    "classes": [
      "__config_flags",
      "_UnboundedCache",
      "_FifoCache",
      "LRUMemo",
      "UnboundedMemo"
    ],
    "imports": [
      "collections",
      "inspect",
      "itertools",
      "types",
      "warnings",
      "functools",
      "typing"
    ],
    "preview": "# util.py\nimport collections\nimport inspect\nimport itertools\nimport types\nimport warnings\nfrom functools import lru_cache, wraps\nfrom typing import Callable, Iterable, List, TypeVar, Union, cast\n\n_bslash = chr(92)\nC = TypeVar(\"C\", bound=Callable)\n\n\nclass __config_flags:\n    \"\"\"Internal class for defining compatibility and debugging flags\"\"\"\n\n    _all_names: List[str] = []\n    _fixed_names: List[str] = []\n    _type_desc = \"configuration\"\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2364",
    "name": "unzip.py",
    "path": "github_repo/scripts/02_media_processing/video_tools/unzip.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 149,
    "size": 5530,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "logging",
      "os",
      "shutil",
      "subprocess",
      "time",
      "pyrogram",
      "translation",
      "helper_funcs.chat_base",
      "helper_funcs.display_progress",
      "sample_config"
    ],
    "preview": "import logging\n\nlogging.basicConfig(\n    level=logging.DEBUG, format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\n)\nlogger = logging.getLogger(__name__)\n\nimport os\nimport shutil\nimport subprocess\nimport time\n\n# the secret configuration specific things\nif bool(os.environ.get(\"WEBHOOK\", False)):\n    from sample_config import Config\nelse:\n    from config import Config\n\n# the Strings used for this \"thing\"\nimport pyrogram",
    "last_modified": "2025-09-13T05:53:44.359071"
  },
  {
    "id": "2365",
    "name": "generate-category.py",
    "path": "github_repo/scripts/02_media_processing/video_tools/generate-category.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 188,
    "size": 6306,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "read_html_file",
      "extract_title",
      "categorize_file",
      "scan_and_categorize",
      "generate_html",
      "save_html"
    ],
    "classes": [],
    "imports": [
      "os",
      "re"
    ],
    "preview": "import os\nimport re\n\n# Directory where your HTML files are located\nHTML_DIRECTORY = \"/Users/steven/Documents/HTML\"\n\n# Categories and keywords to search for in HTML files\nCATEGORIES = {\n    \"Art & Design\": [\n        \"art\",\n        \"design\",\n        \"creative\",\n        \"raccoon\",\n        \"fantasy\",\n        \"cosmic\",\n        \"whimsical\",\n        \"coverart\",\n        \"mystical\",\n    ],\n    \"Technology\": [",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2366",
    "name": "reset_following.py",
    "path": "github_repo/scripts/02_media_processing/video_tools/reset_following.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 91,
    "size": 2307,
    "docstring": "This is a account reset tool.\nUse this before you start boting.\nYou can then reset the users you follow to what you had before botting.",
    "keywords": [],
    "functions": [
      "start",
      "one",
      "two"
    ],
    "classes": [
      "Task"
    ],
    "imports": [
      "os",
      "sys",
      "instabot"
    ],
    "preview": "\"\"\"\n\nThis is a account reset tool.\nUse this before you start boting.\nYou can then reset the users you follow to what you had before botting.\n\n\"\"\"\n\nimport os\nimport sys\n\nsys.path.append(os.path.join(sys.path[0], \"../\"))\nfrom instabot import Bot  # noqa: E402\n\n\n# class of all the tasks\nclass Task(object):\n    # getting the user to pick what to do\n    @staticmethod\n    def start(bot):",
    "last_modified": "2025-09-13T05:54:55.789013"
  },
  {
    "id": "2367",
    "name": "my_copy.py",
    "path": "github_repo/scripts/02_media_processing/video_tools/my_copy.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 39,
    "size": 1386,
    "docstring": "",
    "keywords": [],
    "functions": [
      "ensure_dir"
    ],
    "classes": [],
    "imports": [
      "csv",
      "os",
      "shutil"
    ],
    "preview": "import csv\nimport os\nimport shutil\n\n# Paths configuration\ncsv_file_path = \"/Users/steven/15days.csv\"\ndestination_root = \"/Volumes/iMac/15days\"\nbackup_root = os.path.join(destination_root, \"backup\")\n\n\ndef ensure_dir(directory):\n    \"\"\"Ensure the directory exists. If not, create it.\"\"\"\n    if not os.path.exists(directory):\n        os.makedirs(directory)\n\n\nwith open(csv_file_path, newline=\"\") as csvfile:\n    filereader = csv.reader(csvfile)\n    for row in filereader:\n        # Assuming each row has one column with the file path",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2368",
    "name": "NewUpload.py",
    "path": "github_repo/scripts/02_media_processing/video_tools/NewUpload.py",
    "category": "02_media_processing",
    "type": "youtube",
    "lines": 78,
    "size": 3077,
    "docstring": "",
    "keywords": [
      "youtube",
      "web_tools"
    ],
    "functions": [
      "get_authenticated_service",
      "upload_video"
    ],
    "classes": [],
    "imports": [
      "os",
      "google.auth.transport.requests",
      "google_auth_oauthlib.flow",
      "googleapiclient.discovery",
      "googleapiclient.errors"
    ],
    "preview": "import os\n\nimport google.auth.transport.requests\nimport google_auth_oauthlib.flow\nimport googleapiclient.discovery\nimport googleapiclient.errors\n\nSCOPES = [\"https://www.googleapis.com/auth/youtube.upload\"]\nCLIENT_SECRETS_FILE = \"/Users/steven/Documents/python/Youtube/client_secrets.json\"  # Replace with your client secrets file\nAPI_SERVICE_NAME = \"youtube\"\nAPI_VERSION = \"v3\"\n\n\ndef get_authenticated_service():\n    \"\"\"Authenticates and returns the YouTube Data API service.\"\"\"\n    flow = google_auth_oauthlib.flow.InstalledAppFlow.from_client_secrets_file(\n        CLIENT_SECRETS_FILE, SCOPES\n    )\n    credentials = flow.run_local_server(port=0)\n    return googleapiclient.discovery.build(API_SERVICE_NAME, API_VERSION, credentials=credentials)",
    "last_modified": "2025-09-13T05:54:12.214304"
  },
  {
    "id": "2369",
    "name": "yt-meta.py",
    "path": "github_repo/scripts/02_media_processing/video_tools/yt-meta.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 44,
    "size": 1223,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "csv",
      "os",
      "googleapiclient.discovery"
    ],
    "preview": "import csv\nimport os\n\nfrom googleapiclient.discovery import build\n\n# Securely load the API key\napi_key = os.getenv(\"YOUTUBE_API_KEY\")\n\n# Set up YouTube Data API\nyoutube = build(\"youtube\", \"v3\", developerKey=api_key)\n\n# Retrieve channel's videos\nvideos = []\nnext_page_token = None\ntry:\n    while True:\n        request = youtube.search().list(\n            part=\"snippet\",\n            channelId=\"UCDl7VmS3gD2BQBVZUlL21-A\",\n            maxResults=50,  # Max allowed value",
    "last_modified": "2025-05-04T22:47:13.354999"
  },
  {
    "id": "2370",
    "name": "test_bot_support.py",
    "path": "github_repo/scripts/02_media_processing/video_tools/test_bot_support.py",
    "category": "02_media_processing",
    "type": "testing",
    "lines": 64,
    "size": 1920,
    "docstring": "",
    "keywords": [
      "testing"
    ],
    "functions": [
      "test_extract_urls",
      "test_check_if_file_exist",
      "test_check_if_file_exist_fail",
      "test_console_print"
    ],
    "classes": [
      "TestBotSupport"
    ],
    "imports": [
      "__future__",
      "os",
      "sys",
      "pytest",
      "test_bot",
      "io",
      "StringIO"
    ],
    "preview": "# -*- coding: utf-8 -*-\nfrom __future__ import unicode_literals\n\nimport os\nimport sys\n\nimport pytest\n\nfrom .test_bot import TestBot\n\n\nclass TestBotSupport(TestBot):\n    @pytest.mark.parametrize(\n        \"url,result\",\n        [\n            (\"https://google.com\", [\"https://google.com\"]),\n            (\"google.com\", [\"google.com\"]),\n            (\"google.com/search?q=instabot\", [\"google.com/search?q=instabot\"]),\n            (\n                \"https://google.com/search?q=instabot\",",
    "last_modified": "2025-09-13T05:55:00.005854"
  },
  {
    "id": "2371",
    "name": "youtube 2.py",
    "path": "github_repo/scripts/02_media_processing/video_tools/youtube 2.py",
    "category": "02_media_processing",
    "type": "youtube",
    "lines": 64,
    "size": 2227,
    "docstring": "",
    "keywords": [],
    "functions": [
      "add_video_to_playlist"
    ],
    "classes": [],
    "imports": [
      "csv",
      "google_auth_oauthlib.flow",
      "googleapiclient.discovery",
      "googleapiclient.http"
    ],
    "preview": "import csv\n\nfrom google_auth_oauthlib.flow import InstalledAppFlow\nfrom googleapiclient.discovery import build\nfrom googleapiclient.http import MediaFileUpload\n\n# Define the scopes\nSCOPES = [\n    \"https://www.googleapis.com/auth/youtube.upload\",\n    \"https://www.googleapis.com/auth/youtube\",\n]\n\n## Authenticate and build the YouTube API service\nflow = InstalledAppFlow.from_client_secrets_file(\n    \"/Users/steven/Movies/youtube-upload/client_secret.json\", SCOPES\n)\n\n# Use run_local_server instead of run_console\ncredentials = flow.run_local_server(port=0)\nyoutube = build(\"youtube\", \"v3\", credentials=credentials)",
    "last_modified": "2025-09-13T05:54:08.725834"
  },
  {
    "id": "2372",
    "name": "req_set.py",
    "path": "github_repo/scripts/02_media_processing/video_tools/req_set.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 116,
    "size": 4667,
    "docstring": "",
    "keywords": [],
    "functions": [
      "__init__",
      "__str__",
      "__repr__",
      "add_unnamed_requirement",
      "add_named_requirement",
      "has_requirement",
      "get_requirement",
      "all_requirements",
      "requirements_to_install",
      "warn_legacy_versions_and_specifiers"
    ],
    "classes": [
      "RequirementSet"
    ],
    "imports": [
      "logging",
      "collections",
      "typing",
      "pip._internal.req.req_install",
      "pip._internal.utils.deprecation",
      "pip._vendor.packaging.specifiers",
      "pip._vendor.packaging.utils",
      "pip._vendor.packaging.version"
    ],
    "preview": "import logging\nfrom collections import OrderedDict\nfrom typing import Dict, List\n\nfrom pip._internal.req.req_install import InstallRequirement\nfrom pip._internal.utils.deprecation import deprecated\nfrom pip._vendor.packaging.specifiers import LegacySpecifier\nfrom pip._vendor.packaging.utils import canonicalize_name\nfrom pip._vendor.packaging.version import LegacyVersion\n\nlogger = logging.getLogger(__name__)\n\n\nclass RequirementSet:\n    def __init__(self, check_supported_wheels: bool = True) -> None:\n        \"\"\"Create a RequirementSet.\"\"\"\n\n        self.requirements: Dict[str, InstallRequirement] = OrderedDict()\n        self.check_supported_wheels = check_supported_wheels\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2373",
    "name": "found_candidates.py",
    "path": "github_repo/scripts/02_media_processing/video_tools/found_candidates.py",
    "category": "02_media_processing",
    "type": "analysis",
    "lines": 156,
    "size": 5705,
    "docstring": "Utilities to lazily create and visit candidates found.\n\nCreating and visiting a candidate is a *very* costly operation. It involves\nfetching, extracting, potentially building modules from source, and verifying\ndistribution metadata. It is therefore crucial for performance to keep\neverything here lazy all the way down, so we only touch candidates that we\nabsolutely need, and not \"download the world\" when we only need one version of\nsomething.",
    "keywords": [],
    "functions": [
      "_iter_built",
      "_iter_built_with_prepended",
      "_iter_built_with_inserted",
      "__init__",
      "__getitem__",
      "__iter__",
      "__len__",
      "__bool__"
    ],
    "classes": [
      "FoundCandidates"
    ],
    "imports": [
      "functools",
      "collections.abc",
      "typing",
      "pip._vendor.packaging.version",
      "base"
    ],
    "preview": "\"\"\"Utilities to lazily create and visit candidates found.\n\nCreating and visiting a candidate is a *very* costly operation. It involves\nfetching, extracting, potentially building modules from source, and verifying\ndistribution metadata. It is therefore crucial for performance to keep\neverything here lazy all the way down, so we only touch candidates that we\nabsolutely need, and not \"download the world\" when we only need one version of\nsomething.\n\"\"\"\n\nimport functools\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING, Any, Callable, Iterator, Optional, Set, Tuple\n\nfrom pip._vendor.packaging.version import _BaseVersion\n\nfrom .base import Candidate\n\nIndexCandidateInfo = Tuple[_BaseVersion, Callable[[], Optional[Candidate]]]\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2374",
    "name": "YouTube_VIEWBOT.py",
    "path": "github_repo/scripts/02_media_processing/video_tools/YouTube_VIEWBOT.py",
    "category": "02_media_processing",
    "type": "youtube",
    "lines": 279,
    "size": 15983,
    "docstring": "",
    "keywords": [],
    "functions": [
      "open_autoplaying_window"
    ],
    "classes": [
      "Color"
    ],
    "imports": [
      "random",
      "threading",
      "time",
      "webbrowser"
    ],
    "preview": "##############################################################################################\n#                                                                                           #\n# INSTRUCTIONS TO RUN THE SCRIPT:                                                           #\n# STEP ZERO                                                                                 #\n# MUTE YOUR SPEAKERS, SAVE ALL OF YOUR WORK                                                 #\n# YOU CAN USE THIS ON A POOR LAPTOP IN THE CORNER                                           #\n# IT JUST USE MANY RAMS SO START WITH LONGER DURATION AND IT WILL BE BETTER                 #\n#       FULL COMMENTS BELOW                                                                 #\n#      BUT YOU CAN RUN THE SCRIPT RIGHT NOW IN CMD OR TERMINAL OR VSCODE (I RECOMMEND VSCODE)#\n#                                 VSCODE IF YOU WANT  https://code.visualstudio.com/download #\n# 1. Prerequisites:                                                                         #\n#    - Ensure you have Python installed on your system. You can download it from            #\n#      https://www.python.org/downloads/                                                    #\n#                                                                                           #\n# 2. Download the Script:                                                                   #\n#    - Download the script file and save it with a '.py' extension, e.g.,                   #\n#      'youtube_viewbot.py'.                                                                #\n#                                                                                           #\n# 3. Open a Terminal or Command Prompt:                                                     #\n#    - On Windows, open the Command Prompt. On macOS or Linux, open the Terminal.           #",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2375",
    "name": "youtube 3.py",
    "path": "github_repo/scripts/02_media_processing/video_tools/youtube 3.py",
    "category": "02_media_processing",
    "type": "youtube",
    "lines": 64,
    "size": 2227,
    "docstring": "",
    "keywords": [],
    "functions": [
      "add_video_to_playlist"
    ],
    "classes": [],
    "imports": [
      "csv",
      "google_auth_oauthlib.flow",
      "googleapiclient.discovery",
      "googleapiclient.http"
    ],
    "preview": "import csv\n\nfrom google_auth_oauthlib.flow import InstalledAppFlow\nfrom googleapiclient.discovery import build\nfrom googleapiclient.http import MediaFileUpload\n\n# Define the scopes\nSCOPES = [\n    \"https://www.googleapis.com/auth/youtube.upload\",\n    \"https://www.googleapis.com/auth/youtube\",\n]\n\n## Authenticate and build the YouTube API service\nflow = InstalledAppFlow.from_client_secrets_file(\n    \"/Users/steven/Movies/youtube-upload/client_secret.json\", SCOPES\n)\n\n# Use run_local_server instead of run_console\ncredentials = flow.run_local_server(port=0)\nyoutube = build(\"youtube\", \"v3\", credentials=credentials)",
    "last_modified": "2025-09-13T05:54:08.761972"
  },
  {
    "id": "2376",
    "name": "NewUpload_20250607130507.py",
    "path": "github_repo/scripts/02_media_processing/video_tools/NewUpload_20250607130507.py",
    "category": "02_media_processing",
    "type": "youtube",
    "lines": 78,
    "size": 2714,
    "docstring": "",
    "keywords": [
      "youtube",
      "web_tools"
    ],
    "functions": [
      "get_authenticated_service",
      "upload_video"
    ],
    "classes": [],
    "imports": [
      "os",
      "google.auth.transport.requests",
      "google_auth_oauthlib.flow",
      "googleapiclient.discovery",
      "googleapiclient.errors"
    ],
    "preview": "import os\n\nimport google.auth.transport.requests\nimport google_auth_oauthlib.flow\nimport googleapiclient.discovery\nimport googleapiclient.errors\n\nSCOPES = [\"https://www.googleapis.com/auth/youtube.upload\"]\nCLIENT_SECRETS_FILE = \"/Users/steven/Documents/python/Youtube/client_secrets.json\"  # Replace with your client secrets file\nAPI_SERVICE_NAME = \"youtube\"\nAPI_VERSION = \"v3\"\n\n\ndef get_authenticated_service():\n    \"\"\"Authenticates and returns the YouTube Data API service.\"\"\"\n    flow = google_auth_oauthlib.flow.InstalledAppFlow.from_client_secrets_file(\n        CLIENT_SECRETS_FILE, SCOPES\n    )\n    credentials = flow.run_local_server(port=0)\n    return googleapiclient.discovery.build(API_SERVICE_NAME, API_VERSION, credentials=credentials)",
    "last_modified": "2025-09-13T05:54:11.938776"
  },
  {
    "id": "2377",
    "name": "cache.py",
    "path": "github_repo/scripts/02_media_processing/video_tools/cache.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 224,
    "size": 7922,
    "docstring": "",
    "keywords": [],
    "functions": [
      "add_options",
      "run",
      "get_cache_dir",
      "get_cache_info",
      "list_cache_items",
      "format_for_human",
      "format_for_abspath",
      "remove_cache_items",
      "purge_cache",
      "_cache_dir"
    ],
    "classes": [
      "CacheCommand"
    ],
    "imports": [
      "os",
      "textwrap",
      "optparse",
      "typing",
      "pip._internal.cli.base_command",
      "pip._internal.cli.status_codes",
      "pip._internal.exceptions",
      "pip._internal.utils",
      "pip._internal.utils.logging"
    ],
    "preview": "import os\nimport textwrap\nfrom optparse import Values\nfrom typing import Any, List\n\nfrom pip._internal.cli.base_command import Command\nfrom pip._internal.cli.status_codes import ERROR, SUCCESS\nfrom pip._internal.exceptions import CommandError, PipError\nfrom pip._internal.utils import filesystem\nfrom pip._internal.utils.logging import getLogger\n\nlogger = getLogger(__name__)\n\n\nclass CacheCommand(Command):\n    \"\"\"\n    Inspect and manage pip's wheel cache.\n\n    Subcommands:\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2378",
    "name": "actions.py",
    "path": "github_repo/scripts/02_media_processing/video_tools/actions.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 218,
    "size": 6567,
    "docstring": "",
    "keywords": [],
    "functions": [
      "match_only_at_col",
      "replace_with",
      "remove_quotes",
      "with_attribute",
      "with_class",
      "replaceWith",
      "removeQuotes",
      "withAttribute",
      "withClass",
      "matchOnlyAtCol"
    ],
    "classes": [
      "OnlyOnce"
    ],
    "imports": [
      "exceptions",
      "util",
      "core"
    ],
    "preview": "# actions.py\n\nfrom .exceptions import ParseException\nfrom .util import col, replaced_by_pep8\n\n\nclass OnlyOnce:\n    \"\"\"\n    Wrapper for parse actions, to ensure they are only called once.\n    \"\"\"\n\n    def __init__(self, method_call):\n        from .core import _trim_arity\n\n        self.callable = _trim_arity(method_call)\n        self.called = False\n\n    def __call__(self, s, l, t):\n        if not self.called:\n            results = self.callable(s, l, t)",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2379",
    "name": "cleanupd.py",
    "path": "github_repo/scripts/02_media_processing/video_tools/cleanupd.py",
    "category": "02_media_processing",
    "type": "organization",
    "lines": 62,
    "size": 2217,
    "docstring": "",
    "keywords": [],
    "functions": [
      "list_venv_directories",
      "main"
    ],
    "classes": [],
    "imports": [
      "os",
      "subprocess"
    ],
    "preview": "import os\nimport subprocess\n\n# Define the versions you want to keep\nrequired_versions = [\"3.10\", \"3.12.4\"]\n\n\ndef list_venv_directories(base_path):\n    \"\"\"List all virtual environment directories.\"\"\"\n    venv_dirs = []\n    for root, dirs, files in os.walk(base_path):\n        for dir in dirs:\n            if os.path.exists(os.path.join(root, dir, \"bin\", \"python\")):\n                venv_dirs.append(os.path.join(root, dir))\n    return venv_dirs\n\n\ndef main():\n    # Base path where your virtual environments are stored\n    venv_base_path = os.path.expanduser(\"~/venvs\")",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2380",
    "name": "NewUpload_20250607131212.py",
    "path": "github_repo/scripts/02_media_processing/video_tools/NewUpload_20250607131212.py",
    "category": "02_media_processing",
    "type": "youtube",
    "lines": 79,
    "size": 3078,
    "docstring": "",
    "keywords": [
      "youtube",
      "web_tools"
    ],
    "functions": [
      "get_authenticated_service",
      "upload_video"
    ],
    "classes": [],
    "imports": [
      "os",
      "google.auth.transport.requests",
      "google_auth_oauthlib.flow",
      "googleapiclient.discovery",
      "googleapiclient.errors"
    ],
    "preview": "import os\n\nimport google.auth.transport.requests\nimport google_auth_oauthlib.flow\nimport googleapiclient.discovery\nimport googleapiclient.errors\n\nSCOPES = [\"https://www.googleapis.com/auth/youtube.upload\"]\nCLIENT_SECRETS_FILE = \"/Users/steven/Documents/python/Youtube/client_secrets.json\"  # Replace with your client secrets file\nAPI_SERVICE_NAME = \"youtube\"\nAPI_VERSION = \"v3\"\n\n\ndef get_authenticated_service():\n    \"\"\"Authenticates and returns the YouTube Data API service.\"\"\"\n    flow = google_auth_oauthlib.flow.InstalledAppFlow.from_client_secrets_file(\n        CLIENT_SECRETS_FILE, SCOPES\n    )\n    credentials = flow.run_local_server(port=0)\n    return googleapiclient.discovery.build(API_SERVICE_NAME, API_VERSION, credentials=credentials)",
    "last_modified": "2025-09-13T05:54:12.083841"
  },
  {
    "id": "2381",
    "name": "youtube_download 3.py",
    "path": "github_repo/scripts/02_media_processing/video_tools/youtube_download 3.py",
    "category": "02_media_processing",
    "type": "youtube",
    "lines": 53,
    "size": 1213,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "json",
      "os",
      "sys",
      "urllib",
      "datetime",
      "subprocess"
    ],
    "preview": "#!/usr/bin/python\n\"\"\"\nYouTube Channel Downloader\n\"\"\"\n \nimport json\nimport os\nimport sys\nimport urllib\nfrom datetime import date\nfrom subprocess import call\n\nprint \"--------------------------\"\nprint \"YouTube Channel Downloader\"\nprint \"--------------------------\"\nprint \"\"\nwhile True:\n    try:\n        author = raw_input(\"Enter username of YouTube channel: \")\n        break",
    "last_modified": "2025-08-06T13:43:33.302820"
  },
  {
    "id": "2382",
    "name": "playlistlistener.py",
    "path": "github_repo/scripts/02_media_processing/video_tools/playlistlistener.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 103,
    "size": 3495,
    "docstring": "Listens to playlist for new tracks",
    "keywords": [],
    "functions": [
      "__init__",
      "__init__",
      "listen_and_add_to_queue",
      "__get_last_upload_time__",
      "__save_last_upload_time__"
    ],
    "classes": [
      "Youtubeentity",
      "Playlistlistener"
    ],
    "imports": [
      "logging",
      "datetime",
      "dateutil.parser",
      "pytz",
      "googleapiclient.discovery",
      "ytdl.awsqueue",
      "ytdl.models",
      "ytdl.notify",
      "ytdl.oshelper"
    ],
    "preview": "\"Listens to playlist for new tracks\"\n\nimport logging\nfrom datetime import datetime\n\nimport dateutil.parser\nimport pytz\nfrom googleapiclient.discovery import build\nfrom ytdl.awsqueue import Awsqueue\nfrom ytdl.models import Payload\nfrom ytdl.notify import Iftttnotify\nfrom ytdl.oshelper import file_exists\n\n\nclass Youtubeentity(object):\n    \"Youtube entity object\"\n\n    def __init__(self, title, link, upload_time):\n        self.title = title\n        self.link = link",
    "last_modified": "2025-09-13T05:54:15.165599"
  },
  {
    "id": "2383",
    "name": "preset.py",
    "path": "github_repo/scripts/02_media_processing/video_tools/preset.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 82,
    "size": 2474,
    "docstring": "",
    "keywords": [],
    "functions": [
      "to_dict",
      "__init__",
      "construct",
      "confirm",
      "confirm_publish_at"
    ],
    "classes": [
      "PresetOptions",
      "Preset"
    ],
    "imports": [
      "abc",
      "dataclasses",
      "datetime",
      "pathlib",
      "typing",
      "InquirerPy",
      "constants"
    ],
    "preview": "from abc import ABC, abstractmethod\nfrom dataclasses import asdict, dataclass\nfrom datetime import datetime, timedelta\nfrom pathlib import Path\nfrom typing import List, Optional\n\nfrom InquirerPy import inquirer\n\nfrom ..constants import DESCRIPTION, PUBLISH_AT, TAGS, TITLE\n\n\n@dataclass\nclass PresetOptions:\n    file: str = \"\"\n    title: str = \"\"\n    description: str = \"\"\n    tags: Optional[List[str]] = None\n    category_id: Optional[int] = None\n    publish_at: Optional[datetime] = None\n    playlist_id: Optional[str] = None",
    "last_modified": "2025-09-13T05:53:47.295524"
  },
  {
    "id": "2384",
    "name": "logger.py",
    "path": "github_repo/scripts/02_media_processing/video_tools/logger.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 46,
    "size": 1515,
    "docstring": "",
    "keywords": [],
    "functions": [
      "__init__",
      "log_traceback",
      "debug",
      "warning",
      "error",
      "info"
    ],
    "classes": [
      "Logger"
    ],
    "imports": [
      "logging",
      "traceback",
      "datetime",
      "pathlib",
      "utils"
    ],
    "preview": "import logging\nimport traceback\nfrom datetime import datetime\nfrom pathlib import Path\n\nfrom .utils import create_dir\n\n\nclass Logger:\n    def __init__(self, log_location: str = \"\", log_level=logging.INFO) -> None:\n        self.logger = logging.getLogger(\"savify\")\n        self.logger.setLevel(logging.DEBUG)\n\n        time = (str(datetime.now()).replace(\" \", \"_\")).replace(\":\", \"_\")\n        log_location = f\"{log_location}/logs/{time}_savify.log\"\n        formatter = logging.Formatter(\"[%(levelname)s]\\t%(message)s\")\n\n        create_dir(Path(log_location).parent)\n\n        if log_level is not None:",
    "last_modified": "2025-05-04T23:28:25.606847"
  },
  {
    "id": "2385",
    "name": "createdb.py",
    "path": "github_repo/scripts/02_media_processing/video_tools/createdb.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 6,
    "size": 149,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "tinydb"
    ],
    "preview": "from tinydb import Query, TinyDB\n\ndb = TinyDB(\"log/db.json\")\ntable = db.table(\"created_videos\")\ntable.insert({\"url\": \"www.example.com\", \"id\": \"00\"})\n",
    "last_modified": "2025-05-04T23:28:22.816317"
  },
  {
    "id": "2386",
    "name": "uploadYT.py",
    "path": "github_repo/scripts/02_media_processing/video_tools/uploadYT.py",
    "category": "02_media_processing",
    "type": "youtube",
    "lines": 201,
    "size": 6868,
    "docstring": "",
    "keywords": [
      "youtube"
    ],
    "functions": [
      "get_authenticated_service",
      "initialize_upload",
      "resumable_upload",
      "upload2YT"
    ],
    "classes": [],
    "imports": [
      "os",
      "random",
      "sys",
      "time",
      "httplib2",
      "apiclient.discovery",
      "apiclient.errors",
      "apiclient.http",
      "oauth2client.client",
      "oauth2client.file"
    ],
    "preview": "#!/usr/bin/python\n\nimport os\nimport random\nimport sys\nimport time\n\nimport httplib2\nfrom apiclient.discovery import build\nfrom apiclient.errors import HttpError\nfrom apiclient.http import MediaFileUpload\nfrom oauth2client.client import flow_from_clientsecrets\nfrom oauth2client.file import Storage\nfrom oauth2client.tools import argparser, run_flow\n\n# Explicitly tell the underlying HTTP transport library not to retry, since\n# we are handling retry logic ourselves.\nhttplib2.RETRIES = 1\n\n# Maximum number of times to retry before giving up.",
    "last_modified": "2025-05-06T04:35:15"
  },
  {
    "id": "2387",
    "name": "NewUpload_20250607131143.py",
    "path": "github_repo/scripts/02_media_processing/video_tools/NewUpload_20250607131143.py",
    "category": "02_media_processing",
    "type": "youtube",
    "lines": 95,
    "size": 4012,
    "docstring": "",
    "keywords": [
      "video_processing",
      "youtube",
      "web_tools"
    ],
    "functions": [
      "get_authenticated_service",
      "upload_video"
    ],
    "classes": [],
    "imports": [
      "os",
      "google.auth.transport.requests",
      "google_auth_oauthlib.flow",
      "googleapiclient.discovery",
      "googleapiclient.errors"
    ],
    "preview": "import os\n\nimport google.auth.transport.requests\nimport google_auth_oauthlib.flow\nimport googleapiclient.discovery\nimport googleapiclient.errors\n\nSCOPES = [\"https://www.googleapis.com/auth/youtube.upload\"]\nCLIENT_SECRETS_FILE = \"/Users/steven/Documents/python/Youtube/client_secrets.json\"  # Replace with your client secrets file\nAPI_SERVICE_NAME = \"youtube\"\nAPI_VERSION = \"v3\"\n\ndef get_authenticated_service():\n    \"\"\"Authenticates and returns the YouTube Data API service.\"\"\"\n    flow = google_auth_oauthlib.flow.InstalledAppFlow.from_client_secrets_file(\n        CLIENT_SECRETS_FILE, SCOPES)\n    credentials = flow.run_local_server(port=0)\n    return googleapiclient.discovery.build(API_SERVICE_NAME, API_VERSION,\n                                            credentials=credentials)\n",
    "last_modified": "2025-09-06T12:24:11.145729"
  },
  {
    "id": "2388",
    "name": "DownloadVideos.py",
    "path": "github_repo/scripts/02_media_processing/video_tools/DownloadVideos.py",
    "category": "02_media_processing",
    "type": "youtube",
    "lines": 30,
    "size": 823,
    "docstring": "1. Download daily trending TikTok clips",
    "keywords": [
      "youtube"
    ],
    "functions": [
      "download_tiktoks"
    ],
    "classes": [],
    "imports": [
      "os",
      "random",
      "string",
      "TikTokApi"
    ],
    "preview": "\"\"\"\n1. Download daily trending TikTok clips\n\"\"\"\n\nimport os\nimport random\nimport string\n\nfrom TikTokApi import TikTokApi\n\nDAILY_TRENDING_DIR = r\"directory location for downloaded tiktok videos\"\nverifyFp = \"use s_v_web_id cookie from tiktok.com\"\ndid = \"\".join(random.choice(string.digits) for num in range(19))\n\napi = TikTokApi.get_instance(custom_verifyFp=verifyFp, use_test_endpoints=True)\n\ntiktoks = api.trending()\nvideo_bytes = api.get_Video_By_TikTok(tiktoks[0])\n\n",
    "last_modified": "2025-05-04T23:28:21"
  },
  {
    "id": "2389",
    "name": "NewUpload_20250607131221.py",
    "path": "github_repo/scripts/02_media_processing/video_tools/NewUpload_20250607131221.py",
    "category": "02_media_processing",
    "type": "youtube",
    "lines": 94,
    "size": 3858,
    "docstring": "",
    "keywords": [
      "video_processing",
      "youtube",
      "web_tools"
    ],
    "functions": [
      "get_authenticated_service",
      "upload_video"
    ],
    "classes": [],
    "imports": [
      "os",
      "google.auth.transport.requests",
      "google_auth_oauthlib.flow",
      "googleapiclient.discovery",
      "googleapiclient.errors"
    ],
    "preview": "import os\n\nimport google.auth.transport.requests\nimport google_auth_oauthlib.flow\nimport googleapiclient.discovery\nimport googleapiclient.errors\n\nSCOPES = [\"https://www.googleapis.com/auth/youtube.upload\"]\nCLIENT_SECRETS_FILE = \"/Users/steven/Documents/python/Youtube/client_secrets.json\"  # Replace with your client secrets file\nAPI_SERVICE_NAME = \"youtube\"\nAPI_VERSION = \"v3\"\n\ndef get_authenticated_service():\n    \"\"\"Authenticates and returns the YouTube Data API service.\"\"\"\n    flow = google_auth_oauthlib.flow.InstalledAppFlow.from_client_secrets_file(\n        CLIENT_SECRETS_FILE, SCOPES)\n    credentials = flow.run_local_server(port=0)\n    return googleapiclient.discovery.build(API_SERVICE_NAME, API_VERSION,\n                                            credentials=credentials)\n",
    "last_modified": "2025-09-06T12:24:11.148448"
  },
  {
    "id": "2390",
    "name": "NewUpload_20250607131215.py",
    "path": "github_repo/scripts/02_media_processing/video_tools/NewUpload_20250607131215.py",
    "category": "02_media_processing",
    "type": "youtube",
    "lines": 95,
    "size": 4012,
    "docstring": "",
    "keywords": [
      "video_processing",
      "youtube",
      "web_tools"
    ],
    "functions": [
      "get_authenticated_service",
      "upload_video"
    ],
    "classes": [],
    "imports": [
      "os",
      "google.auth.transport.requests",
      "google_auth_oauthlib.flow",
      "googleapiclient.discovery",
      "googleapiclient.errors"
    ],
    "preview": "import os\n\nimport google.auth.transport.requests\nimport google_auth_oauthlib.flow\nimport googleapiclient.discovery\nimport googleapiclient.errors\n\nSCOPES = [\"https://www.googleapis.com/auth/youtube.upload\"]\nCLIENT_SECRETS_FILE = \"/Users/steven/Documents/python/Youtube/client_secrets.json\"  # Replace with your client secrets file\nAPI_SERVICE_NAME = \"youtube\"\nAPI_VERSION = \"v3\"\n\ndef get_authenticated_service():\n    \"\"\"Authenticates and returns the YouTube Data API service.\"\"\"\n    flow = google_auth_oauthlib.flow.InstalledAppFlow.from_client_secrets_file(\n        CLIENT_SECRETS_FILE, SCOPES)\n    credentials = flow.run_local_server(port=0)\n    return googleapiclient.discovery.build(API_SERVICE_NAME, API_VERSION,\n                                            credentials=credentials)\n",
    "last_modified": "2025-09-06T12:24:11.151474"
  },
  {
    "id": "2391",
    "name": "bazaar.py",
    "path": "github_repo/scripts/02_media_processing/video_tools/bazaar.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 109,
    "size": 3475,
    "docstring": "",
    "keywords": [],
    "functions": [
      "get_base_rev_args",
      "fetch_new",
      "switch",
      "update",
      "get_url_rev_and_auth",
      "get_remote_url",
      "get_revision",
      "is_commit_id_equal"
    ],
    "classes": [
      "Bazaar"
    ],
    "imports": [
      "logging",
      "typing",
      "pip._internal.utils.misc",
      "pip._internal.utils.subprocess",
      "pip._internal.utils.urls",
      "pip._internal.vcs.versioncontrol"
    ],
    "preview": "import logging\nfrom typing import List, Optional, Tuple\n\nfrom pip._internal.utils.misc import HiddenText, display_path\nfrom pip._internal.utils.subprocess import make_command\nfrom pip._internal.utils.urls import path_to_url\nfrom pip._internal.vcs.versioncontrol import (\n    AuthInfo,\n    RemoteNotFoundError,\n    RevOptions,\n    VersionControl,\n    vcs,\n)\n\nlogger = logging.getLogger(__name__)\n\n\nclass Bazaar(VersionControl):\n    name = \"bzr\"\n    dirname = \".bzr\"",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2392",
    "name": "yt_upload.py",
    "path": "github_repo/scripts/02_media_processing/video_tools/yt_upload.py",
    "category": "02_media_processing",
    "type": "youtube",
    "lines": 209,
    "size": 7218,
    "docstring": "",
    "keywords": [
      "youtube"
    ],
    "functions": [
      "get_authenticated_service",
      "initialize_upload",
      "resumable_upload",
      "upload_video"
    ],
    "classes": [],
    "imports": [
      "http.client",
      "os",
      "random",
      "sys",
      "time",
      "types",
      "httplib2",
      "apiclient.discovery",
      "apiclient.errors",
      "apiclient.http"
    ],
    "preview": "import http.client as httplib\nimport os\nimport random\nimport sys\nimport time\nfrom types import SimpleNamespace\n\nimport httplib2\nfrom apiclient.discovery import build\nfrom apiclient.errors import HttpError\nfrom apiclient.http import MediaFileUpload\nfrom oauth2client.client import flow_from_clientsecrets\nfrom oauth2client.file import Storage\nfrom oauth2client.tools import argparser, run_flow\n\n# Explicitly tell the underlying HTTP transport library not to retry, since\n# we are handling retry logic ourselves.\nhttplib2.RETRIES = 1\n\n# Maximum number of times to retry before giving up.",
    "last_modified": "2025-09-13T05:53:51.618361"
  },
  {
    "id": "2393",
    "name": "voice.py",
    "path": "github_repo/scripts/02_media_processing/video_tools/voice.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 97,
    "size": 2904,
    "docstring": "",
    "keywords": [
      "web_tools"
    ],
    "functions": [
      "check_ratelimit",
      "sleep_until",
      "sanitize_text"
    ],
    "classes": [],
    "imports": [
      "re",
      "sys",
      "time",
      "datetime",
      "time",
      "cleantext",
      "requests",
      "utils",
      "datetime"
    ],
    "preview": "import re\nimport sys\nimport time as pytime\nfrom datetime import datetime\nfrom time import sleep\n\nfrom cleantext import clean\nfrom requests import Response\n\nfrom utils import settings\n\nif sys.version_info[0] >= 3:\n    from datetime import timezone\n\n\ndef check_ratelimit(response: Response) -> bool:\n    \"\"\"\n    Checks if the response is a ratelimit response.\n    If it is, it sleeps for the time specified in the response.\n    \"\"\"",
    "last_modified": "2025-09-13T05:54:00.571706"
  },
  {
    "id": "2394",
    "name": "api.py",
    "path": "github_repo/scripts/02_media_processing/video_tools/api.py",
    "category": "02_media_processing",
    "type": "web_tools",
    "lines": 39,
    "size": 1166,
    "docstring": "",
    "keywords": [
      "web_tools"
    ],
    "functions": [
      "request",
      "data",
      "helix",
      "top_clips",
      "get"
    ],
    "classes": [],
    "imports": [
      "requests"
    ],
    "preview": "import requests\n\nlocal = locals()\n\n\ndef request(endpoint: str, headers: dict, params: dict) -> requests.Response:\n    return requests.get(\"https://api.twitch.tv/\" + endpoint, headers=headers, params=params)\n\n\ndef data(slug: str, oauth_token: str, client_id: str) -> requests.Response:\n    return request(\n        \"helix/clips\",\n        {\"Authorization\": \"Bearer \" + oauth_token, \"Client-Id\": client_id},\n        {\"id\": slug},\n    )\n\n\ndef helix(category: str, data: list, oauth_token: str, client_id: str) -> requests.Response:\n    return request(\n        \"helix/\" + category,",
    "last_modified": "2025-09-13T05:53:56.104544"
  },
  {
    "id": "2395",
    "name": "subreddit.py",
    "path": "github_repo/scripts/02_media_processing/video_tools/subreddit.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 161,
    "size": 6898,
    "docstring": "",
    "keywords": [],
    "functions": [
      "get_subreddit_threads"
    ],
    "classes": [],
    "imports": [
      "re",
      "praw",
      "praw.models",
      "prawcore.exceptions",
      "utils",
      "utils.ai_methods",
      "utils.console",
      "utils.posttextparser",
      "utils.subreddit",
      "utils.videos"
    ],
    "preview": "import re\n\nimport praw\nfrom praw.models import MoreComments\nfrom prawcore.exceptions import ResponseException\n\nfrom utils import settings\nfrom utils.ai_methods import sort_by_similarity\nfrom utils.console import print_step, print_substep\nfrom utils.posttextparser import posttextparser\nfrom utils.subreddit import get_subreddit_undone\nfrom utils.videos import check_done\nfrom utils.voice import sanitize_text\n\n\ndef get_subreddit_threads(POST_ID: str):\n    \"\"\"\n    Returns a list of threads from the AskReddit subreddit.\n    \"\"\"\n",
    "last_modified": "2025-09-13T05:54:00.009385"
  },
  {
    "id": "2396",
    "name": "NewUpload_20250607131205.py",
    "path": "github_repo/scripts/02_media_processing/video_tools/NewUpload_20250607131205.py",
    "category": "02_media_processing",
    "type": "youtube",
    "lines": 95,
    "size": 4013,
    "docstring": "",
    "keywords": [
      "video_processing",
      "youtube",
      "web_tools"
    ],
    "functions": [
      "get_authenticated_service",
      "upload_video"
    ],
    "classes": [],
    "imports": [
      "os",
      "google.auth.transport.requests",
      "google_auth_oauthlib.flow",
      "googleapiclient.discovery",
      "googleapiclient.errors"
    ],
    "preview": "import os\n\nimport google.auth.transport.requests\nimport google_auth_oauthlib.flow\nimport googleapiclient.discovery\nimport googleapiclient.errors\n\nSCOPES = [\"https://www.googleapis.com/auth/youtube.upload\"]\nCLIENT_SECRETS_FILE = \"/Users/steven/Documents/python/Youtube/client_secrets.json\"  # Replace with your client secrets file\nAPI_SERVICE_NAME = \"youtube\"\nAPI_VERSION = \"v3\"\n\ndef get_authenticated_service():\n    \"\"\"Authenticates and returns the YouTube Data API service.\"\"\"\n    flow = google_auth_oauthlib.flow.InstalledAppFlow.from_client_secrets_file(\n        CLIENT_SECRETS_FILE, SCOPES)\n    credentials = flow.run_local_server(port=0)\n    return googleapiclient.discovery.build(API_SERVICE_NAME, API_VERSION,\n                                            credentials=credentials)\n",
    "last_modified": "2025-09-06T12:24:11.154325"
  },
  {
    "id": "2397",
    "name": "bot_video.py",
    "path": "github_repo/scripts/02_media_processing/video_tools/bot_video.py",
    "category": "02_media_processing",
    "type": "youtube",
    "lines": 43,
    "size": 1779,
    "docstring": "",
    "keywords": [
      "youtube"
    ],
    "functions": [
      "upload_video",
      "download_video"
    ],
    "classes": [],
    "imports": [
      "os"
    ],
    "preview": "import os\n\n\ndef upload_video(self, video, caption=\"\", thumbnail=None, options={}):\n    \"\"\"Upload video to Instagram\n\n    @param video      Path to video file (String)\n    @param caption    Media description (String)\n    @param thumbnail  Path to thumbnail for video (String). When None, then\n                      thumbnail is generate automatically\n    @param options    Object with difference options, e.g. configure_timeout,\n                      rename_thumbnail, rename (Dict)\n                      Designed to reduce the number of function arguments!\n\n    @return           Object with state of uploading to Instagram (or False)\n    \"\"\"\n    self.small_delay()\n    self.logger.info(\"Started uploading '{video}'\".format(video=video))\n    result = self.api.upload_video(video, caption=caption, thumbnail=thumbnail, options=options)\n    if not result:",
    "last_modified": "2025-09-13T05:54:58.124236"
  },
  {
    "id": "2398",
    "name": "dynamic.py",
    "path": "github_repo/scripts/02_media_processing/video_tools/dynamic.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 118,
    "size": 3475,
    "docstring": "",
    "keywords": [],
    "functions": [
      "run_command",
      "dynamic_progress_bar",
      "log",
      "update_pip3",
      "update_brew",
      "update_all"
    ],
    "classes": [],
    "imports": [
      "os",
      "shutil",
      "subprocess",
      "time",
      "datetime"
    ],
    "preview": "import os\nimport shutil\nimport subprocess\nimport time\nfrom datetime import datetime\n\n\n# Function to execute a system command and print its output\ndef run_command(command):\n    process = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=True)\n    stdout, stderr = process.communicate()\n\n    if stdout:\n        print(stdout.decode())\n    if stderr:\n        print(stderr.decode())\n\n\n# Define the log directory and create it if it does not exist\nlog_dir = \"/Users/Steven/Documents/updateLog\"",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2399",
    "name": "youtube 4.py",
    "path": "github_repo/scripts/02_media_processing/video_tools/youtube 4.py",
    "category": "02_media_processing",
    "type": "youtube",
    "lines": 70,
    "size": 2248,
    "docstring": "",
    "keywords": [],
    "functions": [
      "publish"
    ],
    "classes": [],
    "imports": [
      "argparse",
      "logging",
      "argparse",
      "datetime",
      "login",
      "selenium",
      "selenium.webdriver.common.desired_capabilities",
      "selenium.webdriver.firefox.options",
      "selenium.webdriver.remote.file_detector",
      "upload"
    ],
    "preview": "import argparse\nimport logging\nfrom argparse import ArgumentError\nfrom datetime import datetime\n\nfrom login import confirm_logged_in, login_using_cookie_file\nfrom selenium import webdriver\nfrom selenium.webdriver.common.desired_capabilities import DesiredCapabilities\nfrom selenium.webdriver.firefox.options import Options\nfrom selenium.webdriver.remote.file_detector import LocalFileDetector\n\nfrom upload import upload_file\n\nlogin_cookies = \"cookies.json\"\n\nlogging.basicConfig(\n    format=\"%(asctime)s %(levelname)-8s %(message)s\",\n    level=logging.INFO,\n    datefmt=\"%Y-%m-%d %H:%M:%S\",\n    handlers=[logging.FileHandler(\"debug.log\"), logging.StreamHandler()],",
    "last_modified": "2025-09-11T13:26:57.296323"
  },
  {
    "id": "2400",
    "name": "ytube.py",
    "path": "github_repo/scripts/02_media_processing/video_tools/ytube.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 63,
    "size": 2260,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "googleapiclient.discovery",
      "csv"
    ],
    "preview": "from googleapiclient.discovery import build\n\n# Set up YouTube Data API\nyoutube = build(\"youtube\", \"v3\", developerKey=\"AIzaSyC08MXHwy-tkAwAhvW0TumdKJmSfOJYFqw\")\n\n# Retrieve channel's videos\nvideos = []\nnext_page_token = None\nwhile True:\n    request = youtube.search().list(\n        part=\"snippet\",\n        channelId=\"UCDl7VmS3gD2BQBVZUlL21-A\",\n        maxResults=50,  # Max allowed value\n        pageToken=next_page_token,\n    )\n    response = request.execute()\n    videos.extend(response[\"items\"])\n    next_page_token = response.get(\"nextPageToken\")\n    if not next_page_token:\n        break",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2401",
    "name": "youtube_download.py",
    "path": "github_repo/scripts/02_media_processing/video_tools/youtube_download.py",
    "category": "02_media_processing",
    "type": "youtube",
    "lines": 53,
    "size": 1213,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "json",
      "os",
      "sys",
      "urllib",
      "datetime",
      "subprocess"
    ],
    "preview": "#!/usr/bin/python\n\"\"\"\nYouTube Channel Downloader\n\"\"\"\n \nimport json\nimport os\nimport sys\nimport urllib\nfrom datetime import date\nfrom subprocess import call\n\nprint \"--------------------------\"\nprint \"YouTube Channel Downloader\"\nprint \"--------------------------\"\nprint \"\"\nwhile True:\n    try:\n        author = raw_input(\"Enter username of YouTube channel: \")\n        break",
    "last_modified": "2025-05-04T23:28:20"
  },
  {
    "id": "2402",
    "name": "bot_follow.py",
    "path": "github_repo/scripts/02_media_processing/video_tools/bot_follow.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 182,
    "size": 7285,
    "docstring": "",
    "keywords": [],
    "functions": [
      "follow",
      "follow_users",
      "follow_followers",
      "follow_following",
      "approve_pending_follow_requests",
      "reject_pending_follow_requests"
    ],
    "classes": [],
    "imports": [
      "time",
      "tqdm"
    ],
    "preview": "import time\n\nfrom tqdm import tqdm\n\n\ndef follow(self, user_id, check_user):\n    user_id = self.convert_to_user_id(user_id)\n    if self.log_follow_unfollow:\n        msg = \"Going to follow `user_id` {}.\".format(user_id)\n        self.logger.info(msg)\n    else:\n        msg = \" ===> Going to follow `user_id`: {}.\".format(user_id)\n        self.console_print(msg)\n    if check_user and not self.check_user(user_id):\n        return False\n    if not self.reached_limit(\"follows\"):\n        if self.blocked_actions[\"follows\"]:\n            self.logger.warning(\"YOUR `FOLLOW` ACTION IS BLOCKED\")\n            if self.blocked_actions_protection:\n                self.logger.warning(",
    "last_modified": "2025-09-13T05:54:57.658879"
  },
  {
    "id": "2403",
    "name": "proxy_check.py",
    "path": "github_repo/scripts/02_media_processing/video_tools/proxy_check.py",
    "category": "02_media_processing",
    "type": "organization",
    "lines": 246,
    "size": 6734,
    "docstring": "MIT License\n\nCopyright (c) 2021-2022 MShawon\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.",
    "keywords": [
      "organization",
      "web_tools"
    ],
    "functions": [
      "backup",
      "clean_exe_temp",
      "load_proxy",
      "main_checker",
      "proxy_check",
      "main"
    ],
    "classes": [
      "bcolors"
    ],
    "imports": [
      "os",
      "shutil",
      "sys",
      "concurrent.futures",
      "glob",
      "time",
      "requests",
      "fake_headers"
    ],
    "preview": "\"\"\"\nMIT License\n\nCopyright (c) 2021-2022 MShawon\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,",
    "last_modified": "2025-09-13T05:54:12.640931"
  },
  {
    "id": "2404",
    "name": "NewUpload_20250607130440.py",
    "path": "github_repo/scripts/02_media_processing/video_tools/NewUpload_20250607130440.py",
    "category": "02_media_processing",
    "type": "youtube",
    "lines": 78,
    "size": 2697,
    "docstring": "",
    "keywords": [
      "youtube",
      "web_tools"
    ],
    "functions": [
      "get_authenticated_service",
      "upload_video"
    ],
    "classes": [],
    "imports": [
      "os",
      "google.auth.transport.requests",
      "google_auth_oauthlib.flow",
      "googleapiclient.discovery",
      "googleapiclient.errors"
    ],
    "preview": "import os\n\nimport google.auth.transport.requests\nimport google_auth_oauthlib.flow\nimport googleapiclient.discovery\nimport googleapiclient.errors\n\nSCOPES = [\"https://www.googleapis.com/auth/youtube.upload\"]\nCLIENT_SECRETS_FILE = \"/Users/steven/Documents/python/Youtube/client_secrets.json\"  # Replace with your client secrets file\nAPI_SERVICE_NAME = \"youtube\"\nAPI_VERSION = \"v3\"\n\n\ndef get_authenticated_service():\n    \"\"\"Authenticates and returns the YouTube Data API service.\"\"\"\n    flow = google_auth_oauthlib.flow.InstalledAppFlow.from_client_secrets_file(\n        CLIENT_SECRETS_FILE, SCOPES\n    )\n    credentials = flow.run_local_server(port=0)\n    return googleapiclient.discovery.build(API_SERVICE_NAME, API_VERSION, credentials=credentials)",
    "last_modified": "2025-09-13T05:54:11.893226"
  },
  {
    "id": "2405",
    "name": "yt.py",
    "path": "github_repo/scripts/02_media_processing/video_tools/yt.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 130,
    "size": 3726,
    "docstring": "",
    "keywords": [
      "web_tools"
    ],
    "functions": [
      "get_authenticated_service",
      "print_response",
      "build_resource",
      "remove_empty_kwargs",
      "comment_threads_insert",
      "scrape"
    ],
    "classes": [],
    "imports": [
      "os",
      "random",
      "google.oauth2.credentials",
      "google_auth_oauthlib.flow",
      "requests",
      "bs4",
      "google_auth_oauthlib.flow",
      "googleapiclient.discovery",
      "googleapiclient.errors"
    ],
    "preview": "import os\nimport random\n\nimport google.oauth2.credentials\nimport google_auth_oauthlib.flow\nimport requests\nfrom bs4 import BeautifulSoup\nfrom google_auth_oauthlib.flow import InstalledAppFlow\nfrom googleapiclient.discovery import build\nfrom googleapiclient.errors import HttpError\n\nCLIENT_SECRETS_FILE = \"client_secret.json\"\n\nSCOPES = [\"https://www.googleapis.com/auth/youtube.force-ssl\"]\nAPI_SERVICE_NAME = \"youtube\"\nAPI_VERSION = \"v3\"\n\n\ndef get_authenticated_service():\n    flow = InstalledAppFlow.from_client_secrets_file(CLIENT_SECRETS_FILE, SCOPES)",
    "last_modified": "2025-05-04T23:28:21.027324"
  },
  {
    "id": "2406",
    "name": "search_scope.py",
    "path": "github_repo/scripts/02_media_processing/video_tools/search_scope.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 127,
    "size": 4581,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "create",
      "__init__",
      "get_formatted_locations",
      "get_index_urls_locations",
      "mkurl_pypi_url"
    ],
    "classes": [
      "SearchScope"
    ],
    "imports": [
      "itertools",
      "logging",
      "os",
      "posixpath",
      "urllib.parse",
      "typing",
      "pip._internal.models.index",
      "pip._internal.utils.compat",
      "pip._internal.utils.misc",
      "pip._vendor.packaging.utils"
    ],
    "preview": "import itertools\nimport logging\nimport os\nimport posixpath\nimport urllib.parse\nfrom typing import List\n\nfrom pip._internal.models.index import PyPI\nfrom pip._internal.utils.compat import has_tls\nfrom pip._internal.utils.misc import normalize_path, redact_auth_from_url\nfrom pip._vendor.packaging.utils import canonicalize_name\n\nlogger = logging.getLogger(__name__)\n\n\nclass SearchScope:\n    \"\"\"\n    Encapsulates the locations that pip is configured to search.\n    \"\"\"\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2407",
    "name": "example.py",
    "path": "github_repo/scripts/02_media_processing/video_tools/example.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 11,
    "size": 316,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "twitchtube.video"
    ],
    "preview": "from twitchtube.video import make_video\n\nmake_video(\n    data=[\"c xQcOW\", \"game Just Chatting\"],\n    client_id=\"1hq8ektpki36w5kn37mluioungyqjo\",  # example client id (fake)\n    oauth_token=\"9f5einm9qtp0bj4m9l1ykevpwdn98o\",  # example token (fake)\n    video_length=10.5,\n    resolution=(1080, 1920),\n    frames=60,\n)\n",
    "last_modified": "2025-03-28T18:37:10.655899"
  },
  {
    "id": "2408",
    "name": "NewUpload_20250607131235.py",
    "path": "github_repo/scripts/02_media_processing/video_tools/NewUpload_20250607131235.py",
    "category": "02_media_processing",
    "type": "youtube",
    "lines": 78,
    "size": 3078,
    "docstring": "",
    "keywords": [
      "youtube",
      "web_tools"
    ],
    "functions": [
      "get_authenticated_service",
      "upload_video"
    ],
    "classes": [],
    "imports": [
      "os",
      "google.auth.transport.requests",
      "google_auth_oauthlib.flow",
      "googleapiclient.discovery",
      "googleapiclient.errors"
    ],
    "preview": "import os\n\nimport google.auth.transport.requests\nimport google_auth_oauthlib.flow\nimport googleapiclient.discovery\nimport googleapiclient.errors\n\nSCOPES = [\"https://www.googleapis.com/auth/youtube.upload\"]\nCLIENT_SECRETS_FILE = \"/Users/steven/Documents/python/Youtube/client_secrets.json\"  # Replace with your client secrets file\nAPI_SERVICE_NAME = \"youtube\"\nAPI_VERSION = \"v3\"\n\n\ndef get_authenticated_service():\n    \"\"\"Authenticates and returns the YouTube Data API service.\"\"\"\n    flow = google_auth_oauthlib.flow.InstalledAppFlow.from_client_secrets_file(\n        CLIENT_SECRETS_FILE, SCOPES\n    )\n    credentials = flow.run_local_server(port=0)\n    return googleapiclient.discovery.build(API_SERVICE_NAME, API_VERSION, credentials=credentials)",
    "last_modified": "2025-09-13T05:54:12.145314"
  },
  {
    "id": "2409",
    "name": "yt_studio.py",
    "path": "github_repo/scripts/02_media_processing/video_tools/yt_studio.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 90,
    "size": 3177,
    "docstring": "",
    "keywords": [
      "youtube"
    ],
    "functions": [
      "main",
      "__init__",
      "get_authenticated_service",
      "upload_video"
    ],
    "classes": [
      "YouTubeUploader"
    ],
    "imports": [
      "logging",
      "datetime",
      "google.oauth2.credentials",
      "googleapiclient.discovery",
      "googleapiclient.http",
      "utilities.const"
    ],
    "preview": "import logging\nfrom datetime import datetime, timedelta\n\nimport google.oauth2.credentials\nfrom googleapiclient.discovery import build\nfrom googleapiclient.http import MediaFileUpload\nfrom utilities.const import CHANNEL_ID, LOG_PATH, SCOPES, YT_SECRET_FILE\n\n# Configure logging\n\nlogging.basicConfig(level=logging.INFO, format=\"[%(levelname)s] %(message)s\", filename=LOG_PATH)\n\n\nclass YouTubeUploader:\n    def __init__(self, video_file, channel_id, _YT_SECRET_FILE, _SCOPES):\n        self.video_file = video_file\n        self.channel_id = channel_id\n        self.CLIENT_SECRET_FILE = _YT_SECRET_FILE\n        self.SCOPES = SCOPES\n",
    "last_modified": "2025-09-13T05:53:28.865830"
  },
  {
    "id": "2410",
    "name": "NewUpload_20250607131201.py",
    "path": "github_repo/scripts/02_media_processing/video_tools/NewUpload_20250607131201.py",
    "category": "02_media_processing",
    "type": "youtube",
    "lines": 95,
    "size": 4012,
    "docstring": "",
    "keywords": [
      "video_processing",
      "youtube",
      "web_tools"
    ],
    "functions": [
      "get_authenticated_service",
      "upload_video"
    ],
    "classes": [],
    "imports": [
      "os",
      "google.auth.transport.requests",
      "google_auth_oauthlib.flow",
      "googleapiclient.discovery",
      "googleapiclient.errors"
    ],
    "preview": "import os\n\nimport google.auth.transport.requests\nimport google_auth_oauthlib.flow\nimport googleapiclient.discovery\nimport googleapiclient.errors\n\nSCOPES = [\"https://www.googleapis.com/auth/youtube.upload\"]\nCLIENT_SECRETS_FILE = \"/Users/steven/Documents/python/Youtube/client_secrets.json\"  # Replace with your client secrets file\nAPI_SERVICE_NAME = \"youtube\"\nAPI_VERSION = \"v3\"\n\ndef get_authenticated_service():\n    \"\"\"Authenticates and returns the YouTube Data API service.\"\"\"\n    flow = google_auth_oauthlib.flow.InstalledAppFlow.from_client_secrets_file(\n        CLIENT_SECRETS_FILE, SCOPES)\n    credentials = flow.run_local_server(port=0)\n    return googleapiclient.discovery.build(API_SERVICE_NAME, API_VERSION,\n                                            credentials=credentials)\n",
    "last_modified": "2025-09-06T12:24:11.162545"
  },
  {
    "id": "2411",
    "name": "debug.py",
    "path": "github_repo/scripts/02_media_processing/video_tools/debug.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 195,
    "size": 6708,
    "docstring": "",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "show_value",
      "show_sys_implementation",
      "create_vendor_txt_map",
      "get_module_from_module_name",
      "get_vendor_version_from_module",
      "show_actual_vendor_versions",
      "show_vendor_versions",
      "show_tags",
      "ca_bundle_info",
      "add_options"
    ],
    "classes": [
      "DebugCommand"
    ],
    "imports": [
      "importlib.resources",
      "locale",
      "logging",
      "os",
      "sys",
      "optparse",
      "types",
      "typing",
      "pip._vendor",
      "pip._internal.cli"
    ],
    "preview": "import importlib.resources\nimport locale\nimport logging\nimport os\nimport sys\nfrom optparse import Values\nfrom types import ModuleType\nfrom typing import Any, Dict, List, Optional\n\nimport pip._vendor\nfrom pip._internal.cli import cmdoptions\nfrom pip._internal.cli.base_command import Command\nfrom pip._internal.cli.cmdoptions import make_target_python\nfrom pip._internal.cli.status_codes import SUCCESS\nfrom pip._internal.configuration import Configuration\nfrom pip._internal.metadata import get_environment\nfrom pip._internal.utils.logging import indent_log\nfrom pip._internal.utils.misc import get_pip_version\nfrom pip._vendor.certifi import where\nfrom pip._vendor.packaging.version import parse as parse_version",
    "last_modified": "2025-09-13T05:54:25.400175"
  },
  {
    "id": "2412",
    "name": "entrypoints.py",
    "path": "github_repo/scripts/02_media_processing/video_tools/entrypoints.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 84,
    "size": 3056,
    "docstring": "",
    "keywords": [],
    "functions": [
      "_wrapper",
      "get_best_invocation_for_this_pip",
      "get_best_invocation_for_this_python"
    ],
    "classes": [],
    "imports": [
      "itertools",
      "os",
      "shutil",
      "sys",
      "typing",
      "pip._internal.cli.main",
      "pip._internal.utils.compat"
    ],
    "preview": "import itertools\nimport os\nimport shutil\nimport sys\nfrom typing import List, Optional\n\nfrom pip._internal.cli.main import main\nfrom pip._internal.utils.compat import WINDOWS\n\n_EXECUTABLE_NAMES = [\n    \"pip\",\n    f\"pip{sys.version_info.major}\",\n    f\"pip{sys.version_info.major}.{sys.version_info.minor}\",\n]\nif WINDOWS:\n    _allowed_extensions = {\"\", \".exe\"}\n    _EXECUTABLE_NAMES = [\n        \"\".join(parts) for parts in itertools.product(_EXECUTABLE_NAMES, _allowed_extensions)\n    ]\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2413",
    "name": "ytdlconfiguration 2.py",
    "path": "github_repo/scripts/02_media_processing/video_tools/ytdlconfiguration 2.py",
    "category": "02_media_processing",
    "type": "setup",
    "lines": 83,
    "size": 3383,
    "docstring": "Config",
    "keywords": [],
    "functions": [
      "__init__",
      "load",
      "is_valid"
    ],
    "classes": [
      "Ytdlconfiguration"
    ],
    "imports": [
      "configparser",
      "os.path",
      "ytdl.oshelper"
    ],
    "preview": "\"Config\"\n\nimport configparser\nfrom os.path import exists, expanduser\n\nfrom ytdl.oshelper import join_paths, mkdir\n\n\nclass Ytdlconfiguration(object):\n    \"Ytdl Configuration\"\n\n    def __init__(self):\n        self.__home_path__ = expanduser(\"~\")\n        self._ytdl_home_path_ = join_paths(self.__home_path__, \".ytdl\")\n        self.listener_time_file_path = join_paths(self._ytdl_home_path_, \"listener-timestamp.txt\")\n        self.config_file_path = join_paths(self._ytdl_home_path_, \"config.ini\")\n        self.download_folder = join_paths(self._ytdl_home_path_, \"downloads\")\n        self.log_folder = join_paths(self._ytdl_home_path_, \"logs\")\n\n        self.googleplay_credential_file = \"\"",
    "last_modified": "2025-09-13T05:54:11.201560"
  },
  {
    "id": "2414",
    "name": "upload_video.py",
    "path": "github_repo/scripts/02_media_processing/video_tools/upload_video.py",
    "category": "02_media_processing",
    "type": "youtube",
    "lines": 208,
    "size": 7655,
    "docstring": "",
    "keywords": [],
    "functions": [
      "get_authenticated_service",
      "initialize_upload",
      "resumable_upload",
      "simple_upload"
    ],
    "classes": [],
    "imports": [
      "os",
      "random",
      "sys",
      "time",
      "httplib2",
      "apiclient.discovery",
      "apiclient.errors",
      "apiclient.http",
      "oauth2client.client",
      "oauth2client.file"
    ],
    "preview": "#!/usr/bin/python\n\nimport os\nimport random\nimport sys\nimport time\n\nimport httplib2\nfrom apiclient.discovery import build\nfrom apiclient.errors import HttpError\nfrom apiclient.http import MediaFileUpload\nfrom oauth2client.client import flow_from_clientsecrets\nfrom oauth2client.file import Storage\nfrom oauth2client.tools import argparser, run_flow\n\n# Explicitly tell the underlying HTTP transport library not to retry, since\n# we are handling retry logic ourselves.\nhttplib2.RETRIES = 1\n\n# Maximum number of times to retry before giving up.",
    "last_modified": "2025-09-13T05:53:42.902193"
  },
  {
    "id": "2415",
    "name": "bbcode.py",
    "path": "github_repo/scripts/02_media_processing/video_tools/bbcode.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 109,
    "size": 3294,
    "docstring": "pygments.formatters.bbcode\n~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nBBcode formatter.\n\n:copyright: Copyright 2006-2023 by the Pygments team, see AUTHORS.\n:license: BSD, see LICENSE for details.",
    "keywords": [],
    "functions": [
      "__init__",
      "_make_styles",
      "format_unencoded"
    ],
    "classes": [
      "BBCodeFormatter"
    ],
    "imports": [
      "pip._vendor.pygments.formatter",
      "pip._vendor.pygments.util"
    ],
    "preview": "\"\"\"\npygments.formatters.bbcode\n~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nBBcode formatter.\n\n:copyright: Copyright 2006-2023 by the Pygments team, see AUTHORS.\n:license: BSD, see LICENSE for details.\n\"\"\"\n\nfrom pip._vendor.pygments.formatter import Formatter\nfrom pip._vendor.pygments.util import get_bool_opt\n\n__all__ = [\"BBCodeFormatter\"]\n\n\nclass BBCodeFormatter(Formatter):\n    \"\"\"\n    Format tokens with BBcodes. These formatting codes are used by many\n    bulletin boards, so you can highlight your sourcecode with pygments before",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2416",
    "name": "askreddit.py",
    "path": "github_repo/scripts/02_media_processing/video_tools/askreddit.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 49,
    "size": 1430,
    "docstring": "",
    "keywords": [],
    "functions": [
      "get_askreddit_threads"
    ],
    "classes": [],
    "imports": [
      "os",
      "random",
      "praw",
      "dotenv",
      "utils.console"
    ],
    "preview": "import os\nimport random\n\nimport praw\nfrom dotenv import load_dotenv\n\nfrom utils.console import print_markdown, print_step, print_substep\n\n\ndef get_askreddit_threads():\n    \"\"\"\n    Returns a list of threads from the AskReddit subreddit.\n    \"\"\"\n\n    print_step(\"Getting AskReddit threads...\")\n\n    content = {}\n    load_dotenv()\n    reddit = praw.Reddit(\n        client_id=os.getenv(\"REDDIT_CLIENT_ID\"),",
    "last_modified": "2025-09-11T13:27:05.507412"
  },
  {
    "id": "2417",
    "name": "main_parser.py",
    "path": "github_repo/scripts/02_media_processing/video_tools/main_parser.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 131,
    "size": 4299,
    "docstring": "A single place for constructing and exposing the main parser",
    "keywords": [
      "analysis"
    ],
    "functions": [
      "create_main_parser",
      "identify_python_interpreter",
      "parse_command"
    ],
    "classes": [],
    "imports": [
      "os",
      "subprocess",
      "sys",
      "typing",
      "pip._internal.build_env",
      "pip._internal.cli",
      "pip._internal.cli.parser",
      "pip._internal.commands",
      "pip._internal.exceptions",
      "pip._internal.utils.misc"
    ],
    "preview": "\"\"\"A single place for constructing and exposing the main parser\"\"\"\n\nimport os\nimport subprocess\nimport sys\nfrom typing import List, Optional, Tuple\n\nfrom pip._internal.build_env import get_runnable_pip\nfrom pip._internal.cli import cmdoptions\nfrom pip._internal.cli.parser import ConfigOptionParser, UpdatingDefaultsHelpFormatter\nfrom pip._internal.commands import commands_dict, get_similar_commands\nfrom pip._internal.exceptions import CommandError\nfrom pip._internal.utils.misc import get_pip_version, get_prog\n\n__all__ = [\"create_main_parser\", \"parse_command\"]\n\n\ndef create_main_parser() -> ConfigOptionParser:\n    \"\"\"Creates and returns the main parser for pip's CLI\"\"\"\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2418",
    "name": "download_user_videos.py",
    "path": "github_repo/scripts/02_media_processing/video_tools/download_user_videos.py",
    "category": "02_media_processing",
    "type": "youtube",
    "lines": 49,
    "size": 1269,
    "docstring": "Download a user videos:\nThis script could be very useful to download a users videos.\n\nDependencies:\n    pip install -U instabot\nRun:\n  python download_user_videos.py -u username -p password -user user\n\nNotes:\n    You can change file and add there your comments.\n\nDeveloped by:\n    Steffan Jensen\n    http://www.instabotai.com",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "argparse",
      "os",
      "sys",
      "instabot"
    ],
    "preview": "\"\"\"\nDownload a user videos:\nThis script could be very useful to download a users videos.\n\nDependencies:\n    pip install -U instabot\nRun:\n  python download_user_videos.py -u username -p password -user user\n\nNotes:\n    You can change file and add there your comments.\n\nDeveloped by:\n    Steffan Jensen\n    http://www.instabotai.com\n\"\"\"\n\nimport argparse\nimport os\nimport sys",
    "last_modified": "2025-05-04T23:28:20"
  },
  {
    "id": "2419",
    "name": "youtube2.py",
    "path": "github_repo/scripts/02_media_processing/video_tools/youtube2.py",
    "category": "02_media_processing",
    "type": "youtube",
    "lines": 88,
    "size": 2683,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "googleapiclient.discovery",
      "csv"
    ],
    "preview": "from googleapiclient.discovery import build\n\n# Set up YouTube Data API\nyoutube = build(\"youtube\", \"v3\", developerKey=\"AIzaSyC08MXHwy-tkAwAhvW0TumdKJmSfOJYFqw\")\n\n# Retrieve channel's videos\nvideos = []\nnext_page_token = None\nwhile True:\n    request = youtube.search().list(\n        part=\"snippet\",\n        channelId=\"UCDl7VmS3gD2BQBVZUlL21-A\",\n        maxResults=550,  # Max allowed value\n        pageToken=next_page_token,\n    )\n    response = request.execute()\n    videos.extend(response[\"items\"])\n    next_page_token = response.get(\"nextPageToken\")\n    if not next_page_token:\n        break",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2420",
    "name": "video-downloader.py",
    "path": "github_repo/scripts/02_media_processing/video_tools/video-downloader.py",
    "category": "02_media_processing",
    "type": "youtube",
    "lines": 26,
    "size": 722,
    "docstring": "This script downloads videos from pexels.com. Uses this to get videos background\nin case you need more.",
    "keywords": [
      "web_tools"
    ],
    "functions": [
      "getVideo"
    ],
    "classes": [],
    "imports": [
      "requests",
      "pexelsPy",
      "config"
    ],
    "preview": "\"\"\"\nThis script downloads videos from pexels.com. Uses this to get videos background\nin case you need more.\n\"\"\"\n\nimport requests\nfrom pexelsPy import API\n\nimport config\n\nPEXEL_API_KEY = config.pexelKey\n\napi = API(PEXEL_API_KEY)\n\n\ndef getVideo(num):\n    api.search_videos(\"drone shot of the sea\", orientation=\"portrait\", page=1, results_per_page=num)\n    videos = api.get_videos()\n\n    for video in videos:",
    "last_modified": "2025-09-13T05:53:29.707716"
  },
  {
    "id": "2421",
    "name": "dedupe_python.py",
    "path": "github_repo/scripts/02_media_processing/video_tools/dedupe_python.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 42,
    "size": 1702,
    "docstring": "",
    "keywords": [],
    "functions": [
      "similarity",
      "find_similar_files",
      "keep_latest_file"
    ],
    "classes": [],
    "imports": [
      "os",
      "difflib",
      "pathlib"
    ],
    "preview": "import os\nfrom difflib import SequenceMatcher\nfrom pathlib import Path\n\ndef similarity(a, b):\n    return SequenceMatcher(None, a, b).ratio()\n\ndef find_similar_files(directory, name_threshold=0.9, content_threshold=0.8):\n    py_files = [f for f in Path(directory).rglob(\"*.py\")]\n    similar_files = []\n    for i, file1 in enumerate(py_files):\n        for file2 in py_files[i + 1:]:\n            name_sim = similarity(file1.stem.lower(), file2.stem.lower())\n            if name_sim > name_threshold:\n                with open(file1, 'r', encoding='utf-8') as f1, open(file2, 'r', encoding='utf-8') as f2:\n                    content1 = f1.read()\n                    content2 = f2.read()\n                    content_sim = similarity(content1, content2)\n                    if content_sim > content_threshold:\n                        similar_files.append((str(file1), str(file2), name_sim, content_sim))",
    "last_modified": "2025-10-08T06:38:21"
  },
  {
    "id": "2422",
    "name": "NewUpload_20250607131149.py",
    "path": "github_repo/scripts/02_media_processing/video_tools/NewUpload_20250607131149.py",
    "category": "02_media_processing",
    "type": "youtube",
    "lines": 113,
    "size": 5445,
    "docstring": "",
    "keywords": [
      "youtube",
      "web_tools"
    ],
    "functions": [
      "get_authenticated_service",
      "upload_video"
    ],
    "classes": [],
    "imports": [
      "os",
      "google.auth.transport.requests",
      "google_auth_oauthlib.flow",
      "googleapiclient.discovery",
      "googleapiclient.errors"
    ],
    "preview": "import os\n\nimport google.auth.transport.requests\nimport google_auth_oauthlib.flow\nimport googleapiclient.discovery\nimport googleapiclient.errors\n\n# --- Analysis of the code structure and logic ---\n\n# 1. Imports:\n#    - The script imports necessary modules for Google API client, authentication, and OS operations.\n#    - It assumes the googleapiclient and google_auth_oauthlib libraries are installed.\n\n# 2. Constants:\n#    - SCOPES: Defines the OAuth scope for uploading to YouTube.\n#    - CLIENT_SECRETS_FILE: Path to the OAuth client secrets JSON file.\n#    - API_SERVICE_NAME and API_VERSION: Used to build the YouTube API client.\n\n\n# 3. Authentication Function:",
    "last_modified": "2025-09-13T05:54:12.042617"
  },
  {
    "id": "2423",
    "name": "download_driver.py",
    "path": "github_repo/scripts/02_media_processing/video_tools/download_driver.py",
    "category": "02_media_processing",
    "type": "youtube",
    "lines": 155,
    "size": 5109,
    "docstring": "MIT License\n\nCopyright (c) 2021-2022 MShawon\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.",
    "keywords": [
      "youtube"
    ],
    "functions": [
      "download_driver",
      "copy_drivers"
    ],
    "classes": [],
    "imports": [
      "platform",
      "shutil",
      "subprocess",
      "sys",
      "undetected_chromedriver._compat",
      "colors"
    ],
    "preview": "\"\"\"\nMIT License\n\nCopyright (c) 2021-2022 MShawon\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,",
    "last_modified": "2025-09-13T05:54:11.690616"
  },
  {
    "id": "2424",
    "name": "login.py",
    "path": "github_repo/scripts/02_media_processing/video_tools/login.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 51,
    "size": 1926,
    "docstring": "",
    "keywords": [],
    "functions": [
      "domain_to_url",
      "login_using_cookie_file",
      "confirm_logged_in"
    ],
    "classes": [],
    "imports": [
      "json",
      "typing",
      "selenium.webdriver.common.by",
      "selenium.webdriver.remote.webdriver",
      "selenium.webdriver.support",
      "selenium.webdriver.support.ui"
    ],
    "preview": "import json\nfrom typing import Dict, List\n\nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.remote.webdriver import WebDriver\nfrom selenium.webdriver.support import expected_conditions as EC\nfrom selenium.webdriver.support.ui import WebDriverWait\n\n\"\"\" Login module \"\"\"\n\n\ndef domain_to_url(domain: str) -> str:\n    \"\"\"Converts a (partial) domain to valid URL\"\"\"\n    if domain.startswith(\".\"):\n        domain = \"www\" + domain\n    return \"http://\" + domain\n\n\ndef login_using_cookie_file(driver: WebDriver, cookie_file: str):\n    \"\"\"Restore auth cookies from a file. Does not guarantee that the user is logged in afterwards.",
    "last_modified": "2025-09-13T05:53:29.286205"
  },
  {
    "id": "2425",
    "name": "SendNotification.py",
    "path": "github_repo/scripts/02_media_processing/video_tools/SendNotification.py",
    "category": "02_media_processing",
    "type": "youtube",
    "lines": 30,
    "size": 684,
    "docstring": "4. Send tweet when video is published",
    "keywords": [],
    "functions": [
      "send_tweet",
      "OAuth"
    ],
    "classes": [],
    "imports": [
      "tweepy"
    ],
    "preview": "\"\"\"\n4. Send tweet when video is published\n\"\"\"\n\nimport tweepy\n\nconsumer_key = \"consumer key\"\nconsumer_secret = \"consumer secret key\"\naccess_token = \"access token\"\naccess_token_secret = \"access token secret\"\n\n\ndef send_tweet(video_title, video_desc):\n    def OAuth():\n        try:\n            auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n            auth.set_access_token(access_token, access_token_secret)\n        except Exception as e:\n            return None\n",
    "last_modified": "2025-03-28T18:35:49"
  },
  {
    "id": "2426",
    "name": "NewUpload_20250607124913.py",
    "path": "github_repo/scripts/02_media_processing/video_tools/NewUpload_20250607124913.py",
    "category": "02_media_processing",
    "type": "youtube",
    "lines": 78,
    "size": 2579,
    "docstring": "",
    "keywords": [
      "youtube",
      "web_tools"
    ],
    "functions": [
      "get_authenticated_service",
      "upload_video"
    ],
    "classes": [],
    "imports": [
      "os",
      "google.auth.transport.requests",
      "google_auth_oauthlib.flow",
      "googleapiclient.discovery",
      "googleapiclient.errors"
    ],
    "preview": "import os\n\nimport google.auth.transport.requests\nimport google_auth_oauthlib.flow\nimport googleapiclient.discovery\nimport googleapiclient.errors\n\nSCOPES = [\"https://www.googleapis.com/auth/youtube.upload\"]\nCLIENT_SECRETS_FILE = \"client_secrets.json\"  # Replace with your client secrets file\nAPI_SERVICE_NAME = \"youtube\"\nAPI_VERSION = \"v3\"\n\n\ndef get_authenticated_service():\n    \"\"\"Authenticates and returns the YouTube Data API service.\"\"\"\n    flow = google_auth_oauthlib.flow.InstalledAppFlow.from_client_secrets_file(\n        CLIENT_SECRETS_FILE, SCOPES\n    )\n    credentials = flow.run_local_server(port=0)\n    return googleapiclient.discovery.build(API_SERVICE_NAME, API_VERSION, credentials=credentials)",
    "last_modified": "2025-09-13T05:54:11.787231"
  },
  {
    "id": "2427",
    "name": "bot_unfollow.py",
    "path": "github_repo/scripts/02_media_processing/video_tools/bot_unfollow.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 120,
    "size": 4880,
    "docstring": "",
    "keywords": [],
    "functions": [
      "unfollow",
      "unfollow_users",
      "unfollow_non_followers",
      "unfollow_everyone"
    ],
    "classes": [],
    "imports": [
      "time",
      "tqdm"
    ],
    "preview": "import time\n\nfrom tqdm import tqdm\n\n\ndef unfollow(self, user_id):\n    user_id = self.convert_to_user_id(user_id)\n    user_info = self.get_user_info(user_id)\n\n    if not user_info:\n        self.logger.info(\"Can't get user_id=%s info\" % str(user_id))\n        return False  # No user_info\n\n    username = user_info.get(\"username\")\n\n    if self.log_follow_unfollow:\n        msg = \"Going to unfollow `user_id` {} with username {}.\".format(user_id, username)\n        self.logger.info(msg)\n    else:\n        self.console_print(",
    "last_modified": "2025-09-13T05:54:58.068242"
  },
  {
    "id": "2428",
    "name": "scrape-youtube-channel-videos-url.py",
    "path": "github_repo/scripts/02_media_processing/video_tools/scrape-youtube-channel-videos-url.py",
    "category": "02_media_processing",
    "type": "youtube",
    "lines": 54,
    "size": 1611,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "datetime",
      "sys",
      "time",
      "unittest",
      "urllib.error",
      "urllib.parse",
      "urllib.request",
      "selenium",
      "selenium.common.exceptions",
      "selenium.webdriver.common.by"
    ],
    "preview": "# scrape-youtube-channel-videos-url.py\n# _*_coding: utf-8_*_\n\nimport datetime\nimport sys\nimport time\nimport unittest\nimport urllib.error\nimport urllib.parse\nimport urllib.request\n\nfrom selenium import webdriver\nfrom selenium.common.exceptions import InvalidArgumentException\nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.support import expected_conditions as EC\nfrom selenium.webdriver.support.ui import WebDriverWait\n\nurl = sys.argv[1]\nchannelid = url.split(\"/\")[4]\n# driver = webdriver.Firefox()",
    "last_modified": "2025-05-04T23:28:25.507626"
  },
  {
    "id": "2429",
    "name": "yt_upload 2.py",
    "path": "github_repo/scripts/02_media_processing/video_tools/yt_upload 2.py",
    "category": "02_media_processing",
    "type": "youtube",
    "lines": 209,
    "size": 7218,
    "docstring": "",
    "keywords": [
      "youtube"
    ],
    "functions": [
      "get_authenticated_service",
      "initialize_upload",
      "resumable_upload",
      "upload_video"
    ],
    "classes": [],
    "imports": [
      "http.client",
      "os",
      "random",
      "sys",
      "time",
      "types",
      "httplib2",
      "apiclient.discovery",
      "apiclient.errors",
      "apiclient.http"
    ],
    "preview": "import http.client as httplib\nimport os\nimport random\nimport sys\nimport time\nfrom types import SimpleNamespace\n\nimport httplib2\nfrom apiclient.discovery import build\nfrom apiclient.errors import HttpError\nfrom apiclient.http import MediaFileUpload\nfrom oauth2client.client import flow_from_clientsecrets\nfrom oauth2client.file import Storage\nfrom oauth2client.tools import argparser, run_flow\n\n# Explicitly tell the underlying HTTP transport library not to retry, since\n# we are handling retry logic ourselves.\nhttplib2.RETRIES = 1\n\n# Maximum number of times to retry before giving up.",
    "last_modified": "2025-09-13T05:54:10.988541"
  },
  {
    "id": "2430",
    "name": "user_agents.py",
    "path": "github_repo/scripts/02_media_processing/video_tools/user_agents.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 912,
    "size": 101590,
    "docstring": "",
    "keywords": [],
    "functions": [
      "get_user_agent"
    ],
    "classes": [],
    "imports": [
      "random"
    ],
    "preview": "# coding=utf-8\n#!/usr/bin/env python3\n\nimport random\n\n\ndef get_user_agent():\n    return random.choice(user_agents)\n\n\nuser_agents = [\n    \"Mozilla/1.22 (compatible; MSIE 10.0; Windows 3.1)\",\n    \"Mozilla/4.0 (compatible; MSIE 10.0; Windows NT 6.1; Trident/5.0)\",\n    \"Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 6.0; tr) Opera 10.10\",\n    \"Mozilla/4.0 (compatible; MSIE 6.0; X11; Linux i686; de) Opera 10.10\",\n    \"Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 6.1; Trident/5.0; SLCC2; .NET CLR 2.0.50727; .NET CLR 3.5.30729; .NET CLR 3.0.30729; Media Center PC 6.0; FDM; .NET CLR 1.1.4322; .NET4.0C; .NET4.0E; Tablet PC 2.0)\",\n    \"Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 6.1; WOW64; Trident/5.0; SLCC2; .NET CLR 2.0.50727; .NET CLR 3.5.30729; .NET CLR 3.0.30729; InfoPath.2; .NET4.0C; .NET4.0E)\",\n    \"Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 6.1; WOW64; Trident/5.0; SLCC2; .NET CLR 2.0.50727; .NET CLR 3.5.30729; .NET CLR 3.0.30729; Media Center PC 6.0; .NET4.0C; .NET4.0E; AskTB5.5)\",\n    \"Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 6.1; Win64; x64; Trident/5.0; .NET CLR 2.0.50727; SLCC2; .NET CLR 3.5.30729; .NET CLR 3.0.30729; Media Center PC 6.0; InfoPath.3; .NET4.0C)\",\n    \"Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 7.1; Trident/5.0; .NET CLR 2.0.50727; SLCC2; .NET CLR 3.5.30729; .NET CLR 3.0.30729; Media Center PC 6.0; InfoPath.3; .NET4.0C)\",",
    "last_modified": "2025-03-28T18:35:46"
  },
  {
    "id": "2431",
    "name": "AskRedditBot.py",
    "path": "github_repo/scripts/02_media_processing/video_tools/AskRedditBot.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 10,
    "size": 136,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "time",
      "AskReddit"
    ],
    "preview": "import time\n\nfrom AskReddit import gen_video_from_hot\n\ndelay = 60 * 60 * 12\n\nwhile True:\n    gen_video_from_hot()\n    time.sleep(delay)\n",
    "last_modified": "2025-05-04T23:28:22.830880"
  },
  {
    "id": "2432",
    "name": "NewUpload_20250607131039.py",
    "path": "github_repo/scripts/02_media_processing/video_tools/NewUpload_20250607131039.py",
    "category": "02_media_processing",
    "type": "youtube",
    "lines": 113,
    "size": 5445,
    "docstring": "",
    "keywords": [
      "youtube",
      "web_tools"
    ],
    "functions": [
      "get_authenticated_service",
      "upload_video"
    ],
    "classes": [],
    "imports": [
      "os",
      "google.auth.transport.requests",
      "google_auth_oauthlib.flow",
      "googleapiclient.discovery",
      "googleapiclient.errors"
    ],
    "preview": "import os\n\nimport google.auth.transport.requests\nimport google_auth_oauthlib.flow\nimport googleapiclient.discovery\nimport googleapiclient.errors\n\n# --- Analysis of the code structure and logic ---\n\n# 1. Imports:\n#    - The script imports necessary modules for Google API client, authentication, and OS operations.\n#    - It assumes the googleapiclient and google_auth_oauthlib libraries are installed.\n\n# 2. Constants:\n#    - SCOPES: Defines the OAuth scope for uploading to YouTube.\n#    - CLIENT_SECRETS_FILE: Path to the OAuth client secrets JSON file.\n#    - API_SERVICE_NAME and API_VERSION: Used to build the YouTube API client.\n\n\n# 3. Authentication Function:",
    "last_modified": "2025-09-13T05:54:11.995510"
  },
  {
    "id": "2433",
    "name": "Clip.py",
    "path": "github_repo/scripts/02_media_processing/video_tools/Clip.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 23,
    "size": 397,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [
      "Clip"
    ],
    "imports": [
      "dataclasses"
    ],
    "preview": "from dataclasses import dataclass\n\n\n@dataclass\nclass Clip:\n    id: str\n    url: str\n    embed_url: str\n    broadcaster_id: str\n    broadcaster_name: str\n    creator_id: str\n    creator_name: str\n    video_id: str\n    game_id: str\n    language: str\n    title: str\n    view_count: int\n    created_at: str\n    thumbnail_url: str\n    duration: float = 0.0",
    "last_modified": "2025-03-28T18:37:12.017791"
  },
  {
    "id": "2434",
    "name": "codingstatemachinedict.py",
    "path": "github_repo/scripts/02_media_processing/video_tools/codingstatemachinedict.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 20,
    "size": 542,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [
      "CodingStateMachineDict"
    ],
    "imports": [
      "typing",
      "typing"
    ],
    "preview": "from typing import TYPE_CHECKING, Tuple\n\nif TYPE_CHECKING:\n    # TypedDict was introduced in Python 3.8.\n    #\n    # TODO: Remove the else block and TYPE_CHECKING check when dropping support\n    # for Python 3.7.\n    from typing import TypedDict\n\n    class CodingStateMachineDict(TypedDict, total=False):\n        class_table: Tuple[int, ...]\n        class_factor: int\n        state_table: Tuple[int, ...]\n        char_len_table: Tuple[int, ...]\n        name: str\n        language: str  # Optional key\n\nelse:\n    CodingStateMachineDict = dict\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2435",
    "name": "bot_checkpoint.py",
    "path": "github_repo/scripts/02_media_processing/video_tools/bot_checkpoint.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 68,
    "size": 2024,
    "docstring": "Instabot Checkpoint methods.",
    "keywords": [],
    "functions": [
      "save_checkpoint",
      "load_checkpoint",
      "__init__",
      "fill_following",
      "fill_followers",
      "dump"
    ],
    "classes": [
      "Checkpoint"
    ],
    "imports": [
      "os",
      "pickle",
      "datetime"
    ],
    "preview": "\"\"\"\nInstabot Checkpoint methods.\n\"\"\"\n\nimport os\nimport pickle\nfrom datetime import datetime\n\nCHECKPOINT_PATH = \"{fname}.checkpoint\"\n\n\nclass Checkpoint(object):\n    \"\"\"\n    Checkpoint for instabot.Bot class which can store:\n        .total[<name>] - all Bot's counters\n        .blocked_actions[<name>] - Bot's blocked actions\n        .following (list of user_ids)\n        .followers (list of user_ids)\n        .date (of checkpoint creation)\n    \"\"\"",
    "last_modified": "2025-03-28T18:35:47"
  },
  {
    "id": "2436",
    "name": "help.py",
    "path": "github_repo/scripts/02_media_processing/video_tools/help.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 79,
    "size": 2863,
    "docstring": "",
    "keywords": [],
    "functions": [
      "help_msg"
    ],
    "classes": [],
    "imports": [
      "os",
      "webbrowser",
      "colorama",
      "libs.logo",
      "libs.utils"
    ],
    "preview": "import os\nimport webbrowser\n\nfrom colorama import Back, Fore, Style\nfrom libs.logo import print_logo\nfrom libs.utils import clearConsole\n\n\ndef help_msg():\n    print(Style.RESET_ALL)\n    que = print(\n        \"\"\"\n${Fore.GREEN}============================== HELP  ==============================\n\n    [1] Connection Error\n    [2] Not banning account\n    [3] More help\n    [4] Exit\n    \"\"\"\n    )",
    "last_modified": "2025-05-04T23:28:20"
  },
  {
    "id": "2437",
    "name": "listen.py",
    "path": "github_repo/scripts/02_media_processing/video_tools/listen.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 85,
    "size": 2246,
    "docstring": "listen",
    "keywords": [],
    "functions": [
      "configure_loggers",
      "handle_exception",
      "remove_old_log_files",
      "listen"
    ],
    "classes": [],
    "imports": [
      "datetime",
      "logging",
      "os",
      "sys",
      "time",
      "ytdl.oshelper",
      "ytdl.playlistlistener",
      "ytdl.ytdlconfiguration"
    ],
    "preview": "\"listen\"\n\nimport datetime\nimport logging\nimport os\nimport sys\nimport time\n\nfrom ytdl.oshelper import absolute_files, mkdir, remove\nfrom ytdl.playlistlistener import Playlistlistener\nfrom ytdl.ytdlconfiguration import Ytdlconfiguration\n\n\ndef configure_loggers(config):\n    \"Configure logger\"\n\n    logging.getLogger(\"\").handlers = []\n\n    mkdir(config.log_folder)\n    logs_file_name = os.path.join(config.log_folder, str(datetime.date.today()) + \"listen.log\")",
    "last_modified": "2025-09-13T05:54:15.047189"
  },
  {
    "id": "2438",
    "name": "yt 2.py",
    "path": "github_repo/scripts/02_media_processing/video_tools/yt 2.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 130,
    "size": 3726,
    "docstring": "",
    "keywords": [
      "web_tools"
    ],
    "functions": [
      "get_authenticated_service",
      "print_response",
      "build_resource",
      "remove_empty_kwargs",
      "comment_threads_insert",
      "scrape"
    ],
    "classes": [],
    "imports": [
      "os",
      "random",
      "google.oauth2.credentials",
      "google_auth_oauthlib.flow",
      "requests",
      "bs4",
      "google_auth_oauthlib.flow",
      "googleapiclient.discovery",
      "googleapiclient.errors"
    ],
    "preview": "import os\nimport random\n\nimport google.oauth2.credentials\nimport google_auth_oauthlib.flow\nimport requests\nfrom bs4 import BeautifulSoup\nfrom google_auth_oauthlib.flow import InstalledAppFlow\nfrom googleapiclient.discovery import build\nfrom googleapiclient.errors import HttpError\n\nCLIENT_SECRETS_FILE = \"client_secret.json\"\n\nSCOPES = [\"https://www.googleapis.com/auth/youtube.force-ssl\"]\nAPI_SERVICE_NAME = \"youtube\"\nAPI_VERSION = \"v3\"\n\n\ndef get_authenticated_service():\n    flow = InstalledAppFlow.from_client_secrets_file(CLIENT_SECRETS_FILE, SCOPES)",
    "last_modified": "2025-08-06T13:42:30.011436"
  },
  {
    "id": "2439",
    "name": "googleapi-upload.py",
    "path": "github_repo/scripts/02_media_processing/video_tools/googleapi-upload.py",
    "category": "02_media_processing",
    "type": "youtube",
    "lines": 135,
    "size": 4888,
    "docstring": "",
    "keywords": [
      "youtube"
    ],
    "functions": [
      "get_authenticated_service",
      "upload_video",
      "main"
    ],
    "classes": [],
    "imports": [
      "os",
      "google.oauth2.credentials",
      "google_auth_oauthlib.flow",
      "googleapiclient.discovery",
      "googleapiclient.errors",
      "googleapiclient.http"
    ],
    "preview": "import os\n\nimport google.oauth2.credentials\nimport google_auth_oauthlib.flow\nimport googleapiclient.discovery\nimport googleapiclient.errors\nfrom googleapiclient.http import MediaFileUpload\n\n# --- Configuration ---\nCLIENT_SECRETS_FILE = \"/Users/steven/Movies/PROJECt2025-DoMinIon/mp4/client_secret.json\"  # Path to your downloaded credentials.json\nSCOPES = [\"https://www.googleapis.com/auth/youtube.upload\"]  # YouTube Upload Scope\nAPI_SERVICE_NAME = \"youtube\"\nAPI_VERSION = \"v3\"\n\n\ndef get_authenticated_service():\n    \"\"\"\n    Authenticates and authorizes the user. Returns the YouTube Data API service object.\n    \"\"\"\n    # 1. Load existing credentials, if they exist",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2440",
    "name": "test_savify.py",
    "path": "github_repo/scripts/02_media_processing/video_tools/test_savify.py",
    "category": "02_media_processing",
    "type": "video_processing",
    "lines": 18,
    "size": 401,
    "docstring": "Tests for `savify` package.",
    "keywords": [
      "testing"
    ],
    "functions": [
      "test_command_line_interface"
    ],
    "classes": [],
    "imports": [
      "pytest",
      "click.testing",
      "savify.cli",
      "savify"
    ],
    "preview": "#!/usr/bin/env python\n\n\"\"\"Tests for `savify` package.\"\"\"\n\nimport pytest\nfrom click.testing import CliRunner\n\nimport savify.cli as cli\nfrom savify import savify\n\n\ndef test_command_line_interface():\n    \"\"\"Test the CLI.\"\"\"\n    runner = CliRunner()\n    help_result = runner.invoke(cli.main, [\"--help\"])\n    assert help_result.exit_code == 0\n    assert \"Show this message and exit.\" in help_result.output\n",
    "last_modified": "2025-09-11T13:27:06.393552"
  },
  {
    "id": "2441",
    "name": "ytdlconfiguration.py",
    "path": "github_repo/scripts/02_media_processing/video_tools/ytdlconfiguration.py",
    "category": "02_media_processing",
    "type": "setup",
    "lines": 83,
    "size": 3383,
    "docstring": "Config",
    "keywords": [],
    "functions": [
      "__init__",
      "load",
      "is_valid"
    ],
    "classes": [
      "Ytdlconfiguration"
    ],
    "imports": [
      "configparser",
      "os.path",
      "ytdl.oshelper"
    ],
    "preview": "\"Config\"\n\nimport configparser\nfrom os.path import exists, expanduser\n\nfrom ytdl.oshelper import join_paths, mkdir\n\n\nclass Ytdlconfiguration(object):\n    \"Ytdl Configuration\"\n\n    def __init__(self):\n        self.__home_path__ = expanduser(\"~\")\n        self._ytdl_home_path_ = join_paths(self.__home_path__, \".ytdl\")\n        self.listener_time_file_path = join_paths(self._ytdl_home_path_, \"listener-timestamp.txt\")\n        self.config_file_path = join_paths(self._ytdl_home_path_, \"config.ini\")\n        self.download_folder = join_paths(self._ytdl_home_path_, \"downloads\")\n        self.log_folder = join_paths(self._ytdl_home_path_, \"logs\")\n\n        self.googleplay_credential_file = \"\"",
    "last_modified": "2025-09-13T05:54:15.256480"
  },
  {
    "id": "2442",
    "name": "GUI.py",
    "path": "github_repo/scripts/02_media_processing/video_tools/GUI.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 110,
    "size": 2994,
    "docstring": "",
    "keywords": [
      "video_processing"
    ],
    "functions": [
      "after_request",
      "index",
      "backgrounds",
      "background_add",
      "background_delete",
      "settings",
      "videos_json",
      "backgrounds_json",
      "results",
      "voices"
    ],
    "classes": [],
    "imports": [
      "webbrowser",
      "pathlib",
      "tomlkit",
      "flask",
      "utils.gui_utils"
    ],
    "preview": "import webbrowser\nfrom pathlib import Path\n\n# Used \"tomlkit\" instead of \"toml\" because it doesn't change formatting on \"dump\"\nimport tomlkit\nfrom flask import Flask, redirect, render_template, request, send_from_directory, url_for\n\nimport utils.gui_utils as gui\n\n# Set the hostname\nHOST = \"localhost\"\n# Set the port number\nPORT = 4000\n\n# Configure application\napp = Flask(__name__, template_folder=\"GUI\")\n\n# Configure secret key only to use 'flash'\napp.secret_key = b'_5#y2L\"F4Q8z\\n\\xec]/'\n",
    "last_modified": "2025-09-13T05:53:59.497943"
  },
  {
    "id": "2443",
    "name": "__main__.py",
    "path": "github_repo/scripts/02_media_processing/video_tools/__main__.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 13,
    "size": 265,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "sys",
      "savify",
      "os.path"
    ],
    "preview": "import sys\n\nif __package__ is None and not hasattr(sys, \"frozen\"):\n    import os.path\n\n    path = os.path.realpath(os.path.abspath(__file__))\n    sys.path.insert(0, os.path.dirname(os.path.dirname(path)))\n\nimport savify\n\nif __name__ == \"__main__\":\n    savify.cli()\n",
    "last_modified": "2025-05-04T23:27:53.534935"
  },
  {
    "id": "2444",
    "name": "NewUpload_20250607131028.py",
    "path": "github_repo/scripts/02_media_processing/video_tools/NewUpload_20250607131028.py",
    "category": "02_media_processing",
    "type": "youtube",
    "lines": 112,
    "size": 4689,
    "docstring": "",
    "keywords": [
      "video_processing",
      "youtube",
      "web_tools"
    ],
    "functions": [
      "get_authenticated_service",
      "upload_video"
    ],
    "classes": [],
    "imports": [
      "os",
      "google.auth.transport.requests",
      "google_auth_oauthlib.flow",
      "googleapiclient.discovery",
      "googleapiclient.errors"
    ],
    "preview": "import os\n\nimport google.auth.transport.requests\nimport google_auth_oauthlib.flow\nimport googleapiclient.discovery\nimport googleapiclient.errors\n\n# --- Analysis of the code structure and logic ---\n\n# 1. Imports:\n#    - The script imports necessary modules for Google API client, authentication, and OS operations.\n#    - It assumes the googleapiclient and google_auth_oauthlib libraries are installed.\n\n# 2. Constants:\n#    - SCOPES: Defines the OAuth scope for uploading to YouTube.\n#    - CLIENT_SECRETS_FILE: Path to the OAuth client secrets JSON file.\n#    - API_SERVICE_NAME and API_VERSION: Used to build the YouTube API client.\n\n# 3. Authentication Function:\ndef get_authenticated_service():",
    "last_modified": "2025-09-06T12:24:11.176493"
  },
  {
    "id": "2445",
    "name": "dupes.py",
    "path": "github_repo/scripts/02_media_processing/video_tools/dupes.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 38,
    "size": 1220,
    "docstring": "",
    "keywords": [],
    "functions": [
      "parse_duplicates_file",
      "keep_latest_file"
    ],
    "classes": [],
    "imports": [
      "os",
      "pathlib"
    ],
    "preview": "import os\nfrom pathlib import Path\n\ndef parse_duplicates_file(file_path):\n    groups = []\n    current_group = []\n    with open(file_path, 'r') as f:\n        for line in f:\n            line = line.strip()\n            if line.endswith(\".md\"):\n                current_group.append(line)\n            elif line == \"\" and current_group:\n                groups.append(current_group)\n                current_group = []\n        if current_group:\n            groups.append(current_group)\n    return groups\n\ndef keep_latest_file(group):\n    # Keep the file with the most recent timestamp or simplest name",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2446",
    "name": "yt 3.py",
    "path": "github_repo/scripts/02_media_processing/video_tools/yt 3.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 130,
    "size": 3726,
    "docstring": "",
    "keywords": [
      "web_tools"
    ],
    "functions": [
      "get_authenticated_service",
      "print_response",
      "build_resource",
      "remove_empty_kwargs",
      "comment_threads_insert",
      "scrape"
    ],
    "classes": [],
    "imports": [
      "os",
      "random",
      "google.oauth2.credentials",
      "google_auth_oauthlib.flow",
      "requests",
      "bs4",
      "google_auth_oauthlib.flow",
      "googleapiclient.discovery",
      "googleapiclient.errors"
    ],
    "preview": "import os\nimport random\n\nimport google.oauth2.credentials\nimport google_auth_oauthlib.flow\nimport requests\nfrom bs4 import BeautifulSoup\nfrom google_auth_oauthlib.flow import InstalledAppFlow\nfrom googleapiclient.discovery import build\nfrom googleapiclient.errors import HttpError\n\nCLIENT_SECRETS_FILE = \"client_secret.json\"\n\nSCOPES = [\"https://www.googleapis.com/auth/youtube.force-ssl\"]\nAPI_SERVICE_NAME = \"youtube\"\nAPI_VERSION = \"v3\"\n\n\ndef get_authenticated_service():\n    flow = InstalledAppFlow.from_client_secrets_file(CLIENT_SECRETS_FILE, SCOPES)",
    "last_modified": "2025-08-06T14:25:20.073684"
  },
  {
    "id": "2447",
    "name": "prepare.py",
    "path": "github_repo/scripts/02_media_processing/video_tools/prepare.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 103,
    "size": 3509,
    "docstring": "",
    "keywords": [],
    "functions": [
      "get_credential_file",
      "add_credentials",
      "get_credentials",
      "check_secret",
      "delete_credentials"
    ],
    "classes": [],
    "imports": [
      "getpass",
      "os",
      "sys"
    ],
    "preview": "#!/usr/bin/env python\n\nimport getpass\nimport os\nimport sys\n\nDEFAULT_SECRET_DIR = os.path.abspath(os.getcwd())\n\n\ndef get_credential_file(base_path=DEFAULT_SECRET_DIR):\n    return base_path + \"/config/secret.txt\"\n\n\ndef add_credentials(base_path):\n    SECRET_FILE = get_credential_file(base_path)\n    with open(SECRET_FILE, \"a\") as f:\n        print(\"Enter your login: \")\n        f.write(str(sys.stdin.readline().strip()) + \":\")\n        print(\n            \"Enter your password: (it will not be shown due to security \"",
    "last_modified": "2025-03-28T18:35:47"
  },
  {
    "id": "2448",
    "name": "scrape.py",
    "path": "github_repo/scripts/02_media_processing/video_tools/scrape.py",
    "category": "02_media_processing",
    "type": "web_tools",
    "lines": 66,
    "size": 2157,
    "docstring": "",
    "keywords": [
      "web_tools",
      "organization"
    ],
    "functions": [
      "clean_string",
      "read_subreddit_list",
      "scrape_reddit_text"
    ],
    "classes": [],
    "imports": [
      "re",
      "time",
      "praw",
      "readability",
      "config"
    ],
    "preview": "import re\nfrom time import sleep, time\n\nimport praw\nfrom readability import parse_url\n\nfrom config import parse_config\n\nconfig = parse_config(\"local\")\n\n\ndef clean_string(string_to_clean):\n    \"\"\"Method to remove punctuation and numbers from a string\"\"\"\n    return re.sub(r\"[^\\sa-zA-Z0-9]\", \"\", string_to_clean).lower().strip()\n\n\ndef read_subreddit_list():\n    subreddit_list_path = config[\"subreddit_list_path\"]\n    subreddit_limit_list = []\n    with open(subreddit_list_path) as f:",
    "last_modified": "2025-09-13T05:53:51.245013"
  },
  {
    "id": "2449",
    "name": "ReportBot.py",
    "path": "github_repo/scripts/02_media_processing/video_tools/ReportBot.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 298,
    "size": 14481,
    "docstring": "",
    "keywords": [
      "video_processing",
      "web_tools"
    ],
    "functions": [
      "chunks",
      "profile_attack_process",
      "video_attack_process",
      "video_attack",
      "profile_attack",
      "unlock",
      "database",
      "main",
      "report"
    ],
    "classes": [],
    "imports": [
      "os",
      "time",
      "webbrowser",
      "multiprocessing",
      "os",
      "os",
      "sys",
      "firebase_admin",
      "requests",
      "about"
    ],
    "preview": "#!/usr/bin/env python3\nimport os  # line:26\nimport time  # line:22\nimport webbrowser  # line:27\nfrom multiprocessing import Process  # line:4\nfrom os import _exit  # line:25\nfrom os import path  # line:20\nfrom sys import exit  # line:24\n\nimport firebase_admin  # line:29\nimport requests  # line:21\nfrom about import about_msg  # line:5\nfrom colorama import Back, Fore, Style  # line:3\nfrom dotenv import load_dotenv  # line:33\nfrom firebase_admin import credentials  # line:30\nfrom firebase_admin import db  # line:31\nfrom firebase_admin import firestore  # line:32\nfrom libs.animation import animation_bar  # line:10\nfrom libs.animation import colorText  # line:7\nfrom libs.animation import load_animation  # line:9",
    "last_modified": "2025-09-13T05:53:30.454456"
  },
  {
    "id": "2450",
    "name": "self_outdated_check.py",
    "path": "github_repo/scripts/02_media_processing/video_tools/self_outdated_check.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 248,
    "size": 8377,
    "docstring": "",
    "keywords": [],
    "functions": [
      "_get_statefile_name",
      "_convert_date",
      "was_installed_by_pip",
      "_get_current_remote_pip_version",
      "_self_version_check_logic",
      "pip_self_version_check",
      "__init__",
      "key",
      "get",
      "set"
    ],
    "classes": [
      "SelfCheckState",
      "UpgradePrompt"
    ],
    "imports": [
      "datetime",
      "functools",
      "hashlib",
      "json",
      "logging",
      "optparse",
      "os.path",
      "sys",
      "dataclasses",
      "typing"
    ],
    "preview": "import datetime\nimport functools\nimport hashlib\nimport json\nimport logging\nimport optparse\nimport os.path\nimport sys\nfrom dataclasses import dataclass\nfrom typing import Any, Callable, Dict, Optional\n\nfrom pip._internal.index.collector import LinkCollector\nfrom pip._internal.index.package_finder import PackageFinder\nfrom pip._internal.metadata import get_default_environment\nfrom pip._internal.metadata.base import DistributionVersion\nfrom pip._internal.models.selection_prefs import SelectionPreferences\nfrom pip._internal.network.session import PipSession\nfrom pip._internal.utils.compat import WINDOWS\nfrom pip._internal.utils.entrypoints import (\n    get_best_invocation_for_this_pip,",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2451",
    "name": "videos.py",
    "path": "github_repo/scripts/02_media_processing/video_tools/videos.py",
    "category": "02_media_processing",
    "type": "youtube",
    "lines": 61,
    "size": 2162,
    "docstring": "",
    "keywords": [],
    "functions": [
      "check_done",
      "save_data"
    ],
    "classes": [],
    "imports": [
      "json",
      "time",
      "praw.models",
      "utils",
      "utils.console"
    ],
    "preview": "import json\nimport time\n\nfrom praw.models import Submission\n\nfrom utils import settings\nfrom utils.console import print_step\n\n\ndef check_done(\n    redditobj: Submission,\n) -> Submission:\n    # don't set this to be run anyplace that isn't subreddit.py bc of inspect stack\n    \"\"\"Checks if the chosen post has already been generated\n\n    Args:\n        redditobj (Submission): Reddit object gotten from reddit/subreddit.py\n\n    Returns:\n        Submission|None: Reddit object in args",
    "last_modified": "2025-09-13T05:54:00.533460"
  },
  {
    "id": "2452",
    "name": "yt_upload 3.py",
    "path": "github_repo/scripts/02_media_processing/video_tools/yt_upload 3.py",
    "category": "02_media_processing",
    "type": "youtube",
    "lines": 209,
    "size": 7218,
    "docstring": "",
    "keywords": [
      "youtube"
    ],
    "functions": [
      "get_authenticated_service",
      "initialize_upload",
      "resumable_upload",
      "upload_video"
    ],
    "classes": [],
    "imports": [
      "http.client",
      "os",
      "random",
      "sys",
      "time",
      "types",
      "httplib2",
      "apiclient.discovery",
      "apiclient.errors",
      "apiclient.http"
    ],
    "preview": "import http.client as httplib\nimport os\nimport random\nimport sys\nimport time\nfrom types import SimpleNamespace\n\nimport httplib2\nfrom apiclient.discovery import build\nfrom apiclient.errors import HttpError\nfrom apiclient.http import MediaFileUpload\nfrom oauth2client.client import flow_from_clientsecrets\nfrom oauth2client.file import Storage\nfrom oauth2client.tools import argparser, run_flow\n\n# Explicitly tell the underlying HTTP transport library not to retry, since\n# we are handling retry logic ourselves.\nhttplib2.RETRIES = 1\n\n# Maximum number of times to retry before giving up.",
    "last_modified": "2025-09-13T05:54:11.059864"
  },
  {
    "id": "2453",
    "name": "NewUpload_20250607125040.py",
    "path": "github_repo/scripts/02_media_processing/video_tools/NewUpload_20250607125040.py",
    "category": "02_media_processing",
    "type": "youtube",
    "lines": 78,
    "size": 2705,
    "docstring": "",
    "keywords": [
      "youtube",
      "web_tools"
    ],
    "functions": [
      "get_authenticated_service",
      "upload_video"
    ],
    "classes": [],
    "imports": [
      "os",
      "google.auth.transport.requests",
      "google_auth_oauthlib.flow",
      "googleapiclient.discovery",
      "googleapiclient.errors"
    ],
    "preview": "import os\n\nimport google.auth.transport.requests\nimport google_auth_oauthlib.flow\nimport googleapiclient.discovery\nimport googleapiclient.errors\n\nSCOPES = [\"https://www.googleapis.com/auth/youtube.upload\"]\nCLIENT_SECRETS_FILE = \"/Users/steven/Documents/python/Youtube/client_secrets.json\"  # Replace with your client secrets file\nAPI_SERVICE_NAME = \"youtube\"\nAPI_VERSION = \"v3\"\n\n\ndef get_authenticated_service():\n    \"\"\"Authenticates and returns the YouTube Data API service.\"\"\"\n    flow = google_auth_oauthlib.flow.InstalledAppFlow.from_client_secrets_file(\n        CLIENT_SECRETS_FILE, SCOPES\n    )\n    credentials = flow.run_local_server(port=0)\n    return googleapiclient.discovery.build(API_SERVICE_NAME, API_VERSION, credentials=credentials)",
    "last_modified": "2025-09-13T05:54:11.860951"
  },
  {
    "id": "2454",
    "name": "setup.py.py",
    "path": "github_repo/scripts/02_media_processing/setup.py_consolidated/setup.py.py",
    "category": "02_media_processing",
    "type": "setup",
    "lines": 25,
    "size": 556,
    "docstring": "Setup class",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "distutils.core"
    ],
    "preview": "\"Setup class\"\n\nfrom distutils.core import setup\n\nsetup(\n    name=\"ytdl\",\n    version=\"1.0.30\",\n    packages=[\"ytdl\"],\n    url=\"https://github.com/develohpanda/ytdl\",\n    description=\"YT download and upload to GMusic\",\n    install_requires=[\n        \"youtube-dl\",\n        \"gmusicapi==10.1.2.rc1\",\n        \"boto3\",\n        \"mutagen\",\n        \"configparser\",\n        \"image\",\n        \"google-api-python-client\",\n        \"pytz\",\n    ],",
    "last_modified": "2025-05-04T23:27:55"
  },
  {
    "id": "2455",
    "name": "setup.py_02.py",
    "path": "github_repo/scripts/02_media_processing/setup.py_consolidated/setup.py_02.py",
    "category": "02_media_processing",
    "type": "setup",
    "lines": 62,
    "size": 1617,
    "docstring": "The setup script.",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "setuptools"
    ],
    "preview": "#!/usr/bin/env python\n\n\"\"\"The setup script.\"\"\"\n\nfrom setuptools import Extension, find_packages, setup\n\nwith open(\"README.rst\") as readme_file:\n    readme = readme_file.read()\n\nrequirements = [\n    \"ffmpy>=0.3.0\",\n    \"spotipy>=2.16.1\",\n    \"tldextract>=3.1.0\",\n    \"validators>=0.18.2\",\n    \"youtube-dl>=2021.6.6\",\n    \"requests>=2.25.1\",\n    \"click>=7.1.2\",\n]\n\nsetup_requirements = [",
    "last_modified": "2025-05-04T23:28:25.577200"
  },
  {
    "id": "2456",
    "name": "setup.py_03.py",
    "path": "github_repo/scripts/02_media_processing/setup.py_consolidated/setup.py_03.py",
    "category": "02_media_processing",
    "type": "youtube",
    "lines": 33,
    "size": 1150,
    "docstring": "Upload videos to Youtube.",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "distutils.core"
    ],
    "preview": "#!/usr/bin/python\n\"\"\"Upload videos to Youtube.\"\"\"\nfrom distutils.core import setup\n\nsetup_kwargs = {\n    \"name\": \"youtube-upload\",\n    \"version\": \"0.8.0\",\n    \"description\": \"Upload videos to Youtube\",\n    \"author\": \"Arnau Sanchez\",\n    \"author_email\": \"pyarnau@gmail.com\",\n    \"url\": \"https://github.com/tokland/youtube-upload\",\n    \"packages\": [\"youtube_upload/\", \"youtube_upload/auth\"],\n    \"scripts\": [\"bin/youtube-upload\"],\n    \"license\": \"GNU Public License v3.0\",\n    \"long_description\": \" \".join(__doc__.strip().splitlines()),\n    \"classifiers\": [\n        \"Development Status :: 4 - Beta\",\n        \"Intended Audience :: End Users/Desktop\",\n        \"License :: OSI Approved :: GNU General Public License (GPL)\",\n        \"Natural Language :: English\",",
    "last_modified": "2025-03-28T18:37:08"
  },
  {
    "id": "2457",
    "name": "utils.py_02.py",
    "path": "github_repo/scripts/02_media_processing/utils.py_consolidated/utils.py_02.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 103,
    "size": 2778,
    "docstring": "",
    "keywords": [
      "youtube",
      "analysis",
      "organization"
    ],
    "functions": [
      "clean",
      "create_dir",
      "check_ffmpeg",
      "check_env",
      "check_file",
      "safe_path_string",
      "__init__",
      "get_download_dir",
      "get_temp_dir",
      "download_file"
    ],
    "classes": [
      "PathHolder"
    ],
    "imports": [
      "os",
      "re",
      "pathlib",
      "shutil",
      "sys",
      "urllib.request",
      "uuid",
      "shutil",
      "os"
    ],
    "preview": "import os\nimport re\nfrom pathlib import Path\nfrom shutil import rmtree\nfrom sys import platform\nfrom urllib.request import urlretrieve\nfrom uuid import uuid1\n\n__all__ = [\"PathHolder\"]\n\n\ndef clean(path) -> None:\n    for file in os.listdir(path):\n        file_path = os.path.join(path, file)\n        try:\n            if os.path.isfile(file_path) or os.path.islink(file_path):\n                os.unlink(file_path)\n            elif os.path.isdir(file_path):\n                rmtree(file_path)\n",
    "last_modified": "2025-05-04T23:28:25.609507"
  },
  {
    "id": "2458",
    "name": "utils.py_03.py",
    "path": "github_repo/scripts/02_media_processing/utils.py_consolidated/utils.py_03.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 131,
    "size": 3734,
    "docstring": "",
    "keywords": [
      "analysis",
      "web_tools"
    ],
    "functions": [
      "get_date",
      "get_path",
      "get_description",
      "get_current_version",
      "create_video_config",
      "get_category",
      "get_category_and_name",
      "name_to_ids",
      "remove_blacklisted",
      "format_blacklist"
    ],
    "classes": [],
    "imports": [
      "datetime",
      "random",
      "string",
      "requests",
      "api",
      "config",
      "exceptions"
    ],
    "preview": "from datetime import date\nfrom random import choice\nfrom string import ascii_lowercase, digits\n\nimport requests\n\nfrom .api import get\nfrom .config import CLIP_PATH\nfrom .exceptions import InvalidCategory\n\n\ndef get_date() -> str:\n    \"\"\"\n    Gets the current date and returns the date as a string.\n    \"\"\"\n    return date.today().strftime(\"%b-%d-%Y\")\n\n\ndef get_path() -> str:\n    return CLIP_PATH.format(",
    "last_modified": "2025-09-13T05:53:56.311605"
  },
  {
    "id": "2459",
    "name": "utils.py.py",
    "path": "github_repo/scripts/02_media_processing/utils.py_consolidated/utils.py.py",
    "category": "02_media_processing",
    "type": "utility",
    "lines": 147,
    "size": 4838,
    "docstring": "",
    "keywords": [
      "organization"
    ],
    "functions": [
      "get_date_string",
      "time_plus",
      "get_start_end_time",
      "get_valid_file_name",
      "get_game_path",
      "get_previous_path",
      "make_dirs",
      "clean_directory",
      "load_txt_file",
      "load_json_file"
    ],
    "classes": [],
    "imports": [
      "json",
      "logging",
      "os",
      "re",
      "shutil",
      "datetime",
      "typing",
      "src.APIHandler",
      "config"
    ],
    "preview": "import json\nimport logging\nimport os\nimport re\nimport shutil\nfrom datetime import datetime, time, timedelta, timezone\nfrom typing import Union\n\nfrom src.APIHandler import APIHandler\n\nimport config\n\nFOLDER_TIMESTAMP = datetime.now(timezone.utc).astimezone().strftime(\"%Y_%m_%d_%H_%M_%S\")\n\n\ndef get_date_string(date: datetime) -> str:\n    return date.strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n\n\n# Adds a specific amount of seconds ontop of the given time under consideration of all time/date rules",
    "last_modified": "2025-09-13T05:53:45.738170"
  },
  {
    "id": "2460",
    "name": "logging.py",
    "path": "github_repo/scripts/06_development_tools/testing_framework/logging.py",
    "category": "06_development_tools",
    "type": "utility",
    "lines": 53,
    "size": 1195,
    "docstring": "",
    "keywords": [],
    "functions": [
      "write_to_logs",
      "log",
      "info",
      "error",
      "warn",
      "clip",
      "debug"
    ],
    "classes": [
      "Log"
    ],
    "imports": [
      "datetime",
      "colorama",
      "colorama",
      "config",
      "utils"
    ],
    "preview": "from datetime import datetime\n\nfrom colorama import Fore as f\nfrom colorama import init\n\nfrom .config import DEBUG\nfrom .utils import get_date\n\ninit()\n\n\ndef write_to_logs(text: str) -> None:\n    with open(\"twitchtube/files/logs.txt\", \"a\") as f:\n        f.write(\"\\n\" + text)\n\n\ndef log(color: int, sort: str, text: str) -> None:\n    \"\"\"\n    Used for colored printing, does not return anything.\n    \"\"\"",
    "last_modified": "2025-09-13T05:53:56.248094"
  },
  {
    "id": "2461",
    "name": "isatty_test.py",
    "path": "github_repo/scripts/06_development_tools/testing_framework/isatty_test.py",
    "category": "06_development_tools",
    "type": "testing",
    "lines": 59,
    "size": 1867,
    "docstring": "",
    "keywords": [
      "testing"
    ],
    "functions": [
      "is_a_tty",
      "test_TTY",
      "test_nonTTY",
      "test_withPycharm",
      "test_withPycharmTTYOverride",
      "test_withPycharmNonTTYOverride",
      "test_withPycharmNoneOverride",
      "test_withPycharmStreamWrapped"
    ],
    "classes": [
      "IsattyTest"
    ],
    "imports": [
      "sys",
      "unittest",
      "ansitowin32",
      "utils"
    ],
    "preview": "# Copyright Jonathan Hartley 2013. BSD 3-Clause license, see LICENSE file.\nimport sys\nfrom unittest import TestCase, main\n\nfrom ..ansitowin32 import AnsiToWin32, StreamWrapper\nfrom .utils import StreamNonTTY, StreamTTY, pycharm, replace_by, replace_original_by\n\n\ndef is_a_tty(stream):\n    return StreamWrapper(stream, None).isatty()\n\n\nclass IsattyTest(TestCase):\n\n    def test_TTY(self):\n        tty = StreamTTY()\n        self.assertTrue(is_a_tty(tty))\n        with pycharm():\n            self.assertTrue(is_a_tty(tty))\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2462",
    "name": "utf1632prober.py",
    "path": "github_repo/scripts/06_development_tools/testing_framework/utf1632prober.py",
    "category": "06_development_tools",
    "type": "utility",
    "lines": 222,
    "size": 8457,
    "docstring": "",
    "keywords": [],
    "functions": [
      "__init__",
      "reset",
      "charset_name",
      "language",
      "approx_32bit_chars",
      "approx_16bit_chars",
      "is_likely_utf32be",
      "is_likely_utf32le",
      "is_likely_utf16be",
      "is_likely_utf16le"
    ],
    "classes": [
      "UTF1632Prober"
    ],
    "imports": [
      "typing",
      "charsetprober",
      "enums"
    ],
    "preview": "######################## BEGIN LICENSE BLOCK ########################\n#\n# Contributor(s):\n#   Jason Zavaglia\n#\n# This library is free software; you can redistribute it and/or\n# modify it under the terms of the GNU Lesser General Public\n# License as published by the Free Software Foundation; either\n# version 2.1 of the License, or (at your option) any later version.\n#\n# This library is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU\n# Lesser General Public License for more details.\n#\n# You should have received a copy of the GNU Lesser General Public\n# License along with this library; if not, write to the Free Software\n# Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA\n# 02110-1301  USA\n######################### END LICENSE BLOCK #########################",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2463",
    "name": "initialise_test.py",
    "path": "github_repo/scripts/06_development_tools/testing_framework/initialise_test.py",
    "category": "06_development_tools",
    "type": "testing",
    "lines": 183,
    "size": 6625,
    "docstring": "",
    "keywords": [
      "testing"
    ],
    "functions": [
      "setUp",
      "tearDown",
      "assertWrapped",
      "assertNotWrapped",
      "testInitWrapsOnWindows",
      "testInitDoesntWrapOnEmulatedWindows",
      "testInitDoesntWrapOnNonWindows",
      "testInitDoesntWrapIfNone",
      "testInitAutoresetOnWrapsOnAllPlatforms",
      "testInitWrapOffDoesntWrapOnWindows"
    ],
    "classes": [
      "InitTest",
      "JustFixWindowsConsoleTest"
    ],
    "imports": [
      "sys",
      "unittest",
      "ansitowin32",
      "initialise",
      "utils",
      "unittest.mock",
      "mock"
    ],
    "preview": "# Copyright Jonathan Hartley 2013. BSD 3-Clause license, see LICENSE file.\nimport sys\nfrom unittest import TestCase, main, skipUnless\n\ntry:\n    from unittest.mock import Mock, patch\nexcept ImportError:\n    from mock import patch, Mock\n\nfrom ..ansitowin32 import StreamWrapper\nfrom ..initialise import _wipe_internal_state_for_tests, init, just_fix_windows_console\nfrom .utils import osname, replace_by\n\norig_stdout = sys.stdout\norig_stderr = sys.stderr\n\n\nclass InitTest(TestCase):\n\n    @skipUnless(sys.stdout.isatty(), \"sys.stdout is not a tty\")",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2464",
    "name": "test_quantum_chaos.py",
    "path": "github_repo/scripts/06_development_tools/testing_framework/test_quantum_chaos.py",
    "category": "06_development_tools",
    "type": "testing",
    "lines": 9,
    "size": 233,
    "docstring": "",
    "keywords": [
      "testing"
    ],
    "functions": [
      "test_chaos_failure"
    ],
    "classes": [],
    "imports": [
      "pytest",
      "src.chaos_scheduler"
    ],
    "preview": "import pytest\nfrom src.chaos_scheduler import ChaosScheduler\n\n\ndef test_chaos_failure():\n    scheduler = ChaosScheduler(seed=42)\n    with pytest.raises(RuntimeError):\n        scheduler.schedule_operation(lambda: None, criticality=5)\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2465",
    "name": "test_uploader_factory.py",
    "path": "github_repo/scripts/06_development_tools/testing_framework/test_uploader_factory.py",
    "category": "06_development_tools",
    "type": "youtube",
    "lines": 20,
    "size": 646,
    "docstring": "",
    "keywords": [
      "testing"
    ],
    "functions": [
      "test_get_uploader"
    ],
    "classes": [
      "UploaderFactoryTestCase"
    ],
    "imports": [
      "unittest",
      "simplegallery.common",
      "simplegallery.upload.uploader_factory",
      "simplegallery.upload.variants.aws_uploader",
      "simplegallery.upload.variants.netlify_uploader"
    ],
    "preview": "import unittest\n\nimport simplegallery.common as spg_common\nfrom simplegallery.upload.uploader_factory import get_uploader\nfrom simplegallery.upload.variants.aws_uploader import AWSUploader\nfrom simplegallery.upload.variants.netlify_uploader import NetlifyUploader\n\n\nclass UploaderFactoryTestCase(unittest.TestCase):\n    def test_get_uploader(self):\n        self.assertIs(AWSUploader, get_uploader(\"aws\").__class__)\n        self.assertIs(NetlifyUploader, get_uploader(\"netlify\").__class__)\n\n        with self.assertRaises(spg_common.SPGException):\n            get_uploader(\"non_existing_uploader\")\n\n\nif __name__ == \"__main__\":\n    unittest.main()\n",
    "last_modified": "2025-05-04T23:28:22"
  },
  {
    "id": "2466",
    "name": "reporter.py",
    "path": "github_repo/scripts/06_development_tools/testing_framework/reporter.py",
    "category": "06_development_tools",
    "type": "utility",
    "lines": 81,
    "size": 3100,
    "docstring": "",
    "keywords": [],
    "functions": [
      "__init__",
      "rejecting_candidate",
      "starting",
      "starting_round",
      "ending_round",
      "ending",
      "adding_requirement",
      "rejecting_candidate",
      "pinning"
    ],
    "classes": [
      "PipReporter",
      "PipDebuggingReporter"
    ],
    "imports": [
      "collections",
      "logging",
      "typing",
      "pip._vendor.resolvelib.reporters",
      "base"
    ],
    "preview": "from collections import defaultdict\nfrom logging import getLogger\nfrom typing import Any, DefaultDict\n\nfrom pip._vendor.resolvelib.reporters import BaseReporter\n\nfrom .base import Candidate, Requirement\n\nlogger = getLogger(__name__)\n\n\nclass PipReporter(BaseReporter):\n    def __init__(self) -> None:\n        self.reject_count_by_package: DefaultDict[str, int] = defaultdict(int)\n\n        self._messages_at_reject_count = {\n            1: (\n                \"pip is looking at multiple versions of {package_name} to \"\n                \"determine which version is compatible with other \"\n                \"requirements. This could take a while.\"",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2467",
    "name": "ansi_test.py",
    "path": "github_repo/scripts/06_development_tools/testing_framework/ansi_test.py",
    "category": "06_development_tools",
    "type": "testing",
    "lines": 74,
    "size": 2836,
    "docstring": "",
    "keywords": [
      "testing"
    ],
    "functions": [
      "setUp",
      "tearDown",
      "testForeAttributes",
      "testBackAttributes",
      "testStyleAttributes"
    ],
    "classes": [
      "AnsiTest"
    ],
    "imports": [
      "sys",
      "unittest",
      "ansi",
      "ansitowin32"
    ],
    "preview": "# Copyright Jonathan Hartley 2013. BSD 3-Clause license, see LICENSE file.\nimport sys\nfrom unittest import TestCase, main\n\nfrom ..ansi import Back, Fore, Style\nfrom ..ansitowin32 import AnsiToWin32\n\nstdout_orig = sys.stdout\nstderr_orig = sys.stderr\n\n\nclass AnsiTest(TestCase):\n\n    def setUp(self):\n        # sanity check: stdout should be a file or StringIO object.\n        # It will only be AnsiToWin32 if init() has previously wrapped it\n        self.assertNotEqual(type(sys.stdout), AnsiToWin32)\n        self.assertNotEqual(type(sys.stderr), AnsiToWin32)\n\n    def tearDown(self):",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2468",
    "name": "winterm_test.py",
    "path": "github_repo/scripts/06_development_tools/testing_framework/winterm_test.py",
    "category": "06_development_tools",
    "type": "testing",
    "lines": 130,
    "size": 3683,
    "docstring": "",
    "keywords": [
      "testing"
    ],
    "functions": [
      "testInit",
      "testGetAttrs",
      "testResetAll",
      "testFore",
      "testBack",
      "testStyle",
      "testSetConsole",
      "testSetConsoleOnStderr"
    ],
    "classes": [
      "WinTermTest"
    ],
    "imports": [
      "sys",
      "unittest",
      "winterm",
      "unittest.mock",
      "mock"
    ],
    "preview": "# Copyright Jonathan Hartley 2013. BSD 3-Clause license, see LICENSE file.\nimport sys\nfrom unittest import TestCase, main, skipUnless\n\ntry:\n    from unittest.mock import Mock, patch\nexcept ImportError:\n    from mock import Mock, patch\n\nfrom ..winterm import WinColor, WinStyle, WinTerm\n\n\nclass WinTermTest(TestCase):\n\n    @patch(\"colorama.winterm.win32\")\n    def testInit(self, mockWin32):\n        mockAttr = Mock()\n        mockAttr.wAttributes = 7 + 6 * 16 + 8\n        mockWin32.GetConsoleScreenBufferInfo.return_value = mockAttr\n        term = WinTerm()",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2469",
    "name": "integration_example_test.py",
    "path": "github_repo/scripts/06_development_tools/testing_framework/integration_example_test.py",
    "category": "06_development_tools",
    "type": "testing",
    "lines": 5,
    "size": 129,
    "docstring": "",
    "keywords": [
      "testing"
    ],
    "functions": [
      "test_integration_example"
    ],
    "classes": [],
    "imports": [],
    "preview": "def test_integration_example() -> None:\n    string = \"integration_test_example\"\n\n    assert string == \"integration_test_example\"\n",
    "last_modified": "2025-05-04T22:47:11.574892"
  },
  {
    "id": "2470",
    "name": "unit_example_test.py",
    "path": "github_repo/scripts/06_development_tools/testing_framework/unit_example_test.py",
    "category": "06_development_tools",
    "type": "testing",
    "lines": 5,
    "size": 108,
    "docstring": "",
    "keywords": [
      "testing"
    ],
    "functions": [
      "test_unit_example"
    ],
    "classes": [],
    "imports": [],
    "preview": "def test_unit_example() -> None:\n    string = \"unit_test_example\"\n\n    assert string == \"unit_test_example\"\n",
    "last_modified": "2025-05-04T22:47:11.575669"
  },
  {
    "id": "2471",
    "name": "test_bot_filter.py",
    "path": "github_repo/scripts/06_development_tools/testing_framework/test_bot_filter.py",
    "category": "06_development_tools",
    "type": "testing",
    "lines": 55,
    "size": 1604,
    "docstring": "",
    "keywords": [
      "testing"
    ],
    "functions": [
      "test_check_user"
    ],
    "classes": [
      "TestBotFilter"
    ],
    "imports": [
      "pytest",
      "responses",
      "instabot.api.config",
      "test_bot",
      "test_variables",
      "unittest.mock",
      "mock"
    ],
    "preview": "import pytest\nimport responses\nfrom instabot.api.config import API_URL\n\nfrom .test_bot import TestBot\nfrom .test_variables import TEST_USERNAME_INFO_ITEM\n\ntry:\n    from unittest.mock import patch\nexcept ImportError:\n    from mock import patch\n\n\nclass TestBotFilter(TestBot):\n    @pytest.mark.parametrize(\n        \"filter_users,filter_business_accounts,\" + \"filter_verified_accounts,expected\",\n        [\n            (False, False, False, True),\n            (True, False, False, True),\n            (True, True, False, False),",
    "last_modified": "2025-05-04T23:28:20"
  },
  {
    "id": "2472",
    "name": "tests.py",
    "path": "github_repo/scripts/06_development_tools/testing_framework/tests.py",
    "category": "06_development_tools",
    "type": "web_tools",
    "lines": 169,
    "size": 6599,
    "docstring": "webencodings.tests\n~~~~~~~~~~~~~~~~~~\n\nA basic test suite for Encoding.\n\n:copyright: Copyright 2012 by Simon Sapin\n:license: BSD, see LICENSE for details.",
    "keywords": [
      "testing"
    ],
    "functions": [
      "assert_raises",
      "test_labels",
      "test_all_labels",
      "test_invalid_label",
      "test_decode",
      "test_encode",
      "test_iter_decode",
      "test_iter_encode",
      "test_x_user_defined",
      "iter_decode_to_string"
    ],
    "classes": [],
    "imports": [
      "__future__"
    ],
    "preview": "# coding: utf-8\n\"\"\"\n\nwebencodings.tests\n~~~~~~~~~~~~~~~~~~\n\nA basic test suite for Encoding.\n\n:copyright: Copyright 2012 by Simon Sapin\n:license: BSD, see LICENSE for details.\n\n\"\"\"\n\nfrom __future__ import unicode_literals\n\nfrom . import (\n    LABELS,\n    UTF8,\n    IncrementalDecoder,\n    IncrementalEncoder,",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2473",
    "name": "main.py.py",
    "path": "github_repo/scripts/06_development_tools/main.py_consolidated/main.py.py",
    "category": "06_development_tools",
    "type": "utility",
    "lines": 31,
    "size": 810,
    "docstring": "",
    "keywords": [],
    "functions": [
      "Test"
    ],
    "classes": [],
    "imports": [
      "os",
      "re",
      "google_url_scrapper",
      "rank_provider"
    ],
    "preview": "import os\nimport re\n\nfrom google_url_scrapper import google_url_scrapper\nfrom rank_provider import AlexaTrafficRank, GooglePageRank, RankProvider\n\n\ndef Test():\n    keyword = raw_input(\"Prompt :\")\n    G = google_url_scrapper()\n    urls = G.scrape(keyword)\n    for purl in urls:\n        url = purl[0]\n        Title = purl[1]\n        results = G.MajesticSEO_API(url)\n        print(\"Traffic stats for: %s\" % (url))\n        print(\"Title: %s\" % (Title))\n        print(\"ACRank: %s\" % (results[\"ACRank\"]))\n        print(\"ExtBackLinks: %s\" % (results[\"ExtBackLinks\"]))\n        providers = (",
    "last_modified": "2025-05-04T23:28:20.717386"
  },
  {
    "id": "2474",
    "name": "main.py_02.py",
    "path": "github_repo/scripts/06_development_tools/main.py_consolidated/main.py_02.py",
    "category": "06_development_tools",
    "type": "utility",
    "lines": 9,
    "size": 104,
    "docstring": "",
    "keywords": [],
    "functions": [
      "h"
    ],
    "classes": [],
    "imports": [
      "fastapi"
    ],
    "preview": "from fastapi import FastAPI\n\napp = FastAPI()\n\n\n@app.get(\"/api/health\")\ndef h():\n    return {\"ok\": True}\n",
    "last_modified": "2025-09-11T13:24:01.259289"
  },
  {
    "id": "2475",
    "name": "deepseek_python_20250608130224.py",
    "path": "github_repo/scripts/06_development_tools/development_utilities/deepseek_python_20250608130224.py",
    "category": "06_development_tools",
    "type": "utility",
    "lines": 2,
    "size": 7,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [],
    "preview": "format\n",
    "last_modified": "2025-09-13T05:53:27.386086"
  },
  {
    "id": "2476",
    "name": "worker.py",
    "path": "github_repo/scripts/06_development_tools/development_utilities/worker.py",
    "category": "06_development_tools",
    "type": "utility",
    "lines": 2,
    "size": 16,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [],
    "preview": "print(\"worker\")\n",
    "last_modified": "2025-09-11T13:24:01.247951"
  },
  {
    "id": "2477",
    "name": "config_20250430201612.py",
    "path": "github_repo/scripts/06_development_tools/development_utilities/config_20250430201612.py",
    "category": "06_development_tools",
    "type": "setup",
    "lines": 3,
    "size": 48,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [],
    "preview": "# config.py\nSOURCE_DIRECTORY = \"/Users/steven/\"\n",
    "last_modified": "2025-04-30T20:16:12.706453"
  },
  {
    "id": "2478",
    "name": "ptt.py",
    "path": "github_repo/scripts/06_development_tools/development_utilities/ptt.py",
    "category": "06_development_tools",
    "type": "utility",
    "lines": 11,
    "size": 241,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "pyttsx3"
    ],
    "preview": "import pyttsx3\n\nengine = pyttsx3.init()\nvoices = engine.getProperty(\"voices\")\nfor voice in voices:\n    print(voice, voice.id)\n    engine.setProperty(\"voice\", voice.id)\n    engine.say(\"Hello World!\")\n    engine.runAndWait()\n    engine.stop()\n",
    "last_modified": "2025-05-04T22:47:11.885485"
  },
  {
    "id": "2479",
    "name": "selection_prefs.py",
    "path": "github_repo/scripts/06_development_tools/development_utilities/selection_prefs.py",
    "category": "06_development_tools",
    "type": "utility",
    "lines": 52,
    "size": 1907,
    "docstring": "",
    "keywords": [],
    "functions": [
      "__init__"
    ],
    "classes": [
      "SelectionPreferences"
    ],
    "imports": [
      "typing",
      "pip._internal.models.format_control"
    ],
    "preview": "from typing import Optional\n\nfrom pip._internal.models.format_control import FormatControl\n\n\nclass SelectionPreferences:\n    \"\"\"\n    Encapsulates the candidate selection preferences for downloading\n    and installing files.\n    \"\"\"\n\n    __slots__ = [\n        \"allow_yanked\",\n        \"allow_all_prereleases\",\n        \"format_control\",\n        \"prefer_binary\",\n        \"ignore_requires_python\",\n    ]\n\n    # Don't include an allow_yanked default value to make sure each call",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2480",
    "name": "before.py",
    "path": "github_repo/scripts/06_development_tools/development_utilities/before.py",
    "category": "06_development_tools",
    "type": "utility",
    "lines": 49,
    "size": 1568,
    "docstring": "",
    "keywords": [],
    "functions": [
      "before_nothing",
      "before_log",
      "log_it"
    ],
    "classes": [],
    "imports": [
      "typing",
      "pip._vendor.tenacity",
      "logging",
      "pip._vendor.tenacity"
    ],
    "preview": "# Copyright 2016 Julien Danjou\n# Copyright 2016 Joshua Harlow\n# Copyright 2013-2014 Ray Holder\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport typing\n\nfrom pip._vendor.tenacity import _utils\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2481",
    "name": "console.py",
    "path": "github_repo/scripts/06_development_tools/development_utilities/console.py",
    "category": "06_development_tools",
    "type": "utility",
    "lines": 78,
    "size": 1681,
    "docstring": "pygments.console\n~~~~~~~~~~~~~~~~\n\nFormat colored console output.\n\n:copyright: Copyright 2006-2023 by the Pygments team, see AUTHORS.\n:license: BSD, see LICENSE for details.",
    "keywords": [],
    "functions": [
      "reset_color",
      "colorize",
      "ansiformat"
    ],
    "classes": [],
    "imports": [],
    "preview": "\"\"\"\npygments.console\n~~~~~~~~~~~~~~~~\n\nFormat colored console output.\n\n:copyright: Copyright 2006-2023 by the Pygments team, see AUTHORS.\n:license: BSD, see LICENSE for details.\n\"\"\"\n\nesc = \"\\x1b[\"\n\ncodes = {}\ncodes[\"\"] = \"\"\ncodes[\"reset\"] = esc + \"39;49;00m\"\n\ncodes[\"bold\"] = esc + \"01m\"\ncodes[\"faint\"] = esc + \"02m\"\ncodes[\"standout\"] = esc + \"03m\"\ncodes[\"underline\"] = esc + \"04m\"",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2482",
    "name": "vid.py",
    "path": "github_repo/scripts/06_development_tools/development_utilities/vid.py",
    "category": "06_development_tools",
    "type": "utility",
    "lines": 17,
    "size": 237,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [],
    "preview": "(\n    yt\n    - dlp\n    - a / Users / steven / Downloads / Misc / Thumbnails / vid.txt\n    - -write\n    - annotations\n    - -write\n    - description\n    - -write\n    - info\n    - json\n    - -write\n    - sub\n    - -write\n    - thumbnail\n)\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2483",
    "name": "ollama-run.py",
    "path": "github_repo/scripts/06_development_tools/development_utilities/ollama-run.py",
    "category": "06_development_tools",
    "type": "utility",
    "lines": 40,
    "size": 1009,
    "docstring": "",
    "keywords": [],
    "functions": [
      "execute_command"
    ],
    "classes": [],
    "imports": [
      "os",
      "tkinter",
      "subprocess",
      "tkinter"
    ],
    "preview": "import os\nimport tkinter as tk\nfrom subprocess import PIPE, STDOUT, Popen\nfrom tkinter import scrolledtext\n\n\ndef execute_command():\n    # Get the command entered by the user\n    command = command_entry.get()\n\n    # Execute the command using subprocess\n    process = Popen(command, shell=True, stdout=PIPE, stderr=STDOUT)\n    output, _ = process.communicate()\n\n    # Display the output in the text area\n    output_text.delete(1.0, tk.END)\n    output_text.insert(tk.END, output.decode(\"utf-8\"))\n\n\n# GUI setup",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2484",
    "name": "before_sleep.py",
    "path": "github_repo/scripts/06_development_tools/development_utilities/before_sleep.py",
    "category": "06_development_tools",
    "type": "utility",
    "lines": 73,
    "size": 2384,
    "docstring": "",
    "keywords": [],
    "functions": [
      "before_sleep_nothing",
      "before_sleep_log",
      "log_it"
    ],
    "classes": [],
    "imports": [
      "typing",
      "pip._vendor.tenacity",
      "logging",
      "pip._vendor.tenacity"
    ],
    "preview": "# Copyright 2016 Julien Danjou\n# Copyright 2016 Joshua Harlow\n# Copyright 2013-2014 Ray Holder\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport typing\n\nfrom pip._vendor.tenacity import _utils\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2485",
    "name": "mpvListener.py",
    "path": "github_repo/scripts/06_development_tools/development_utilities/mpvListener.py",
    "category": "06_development_tools",
    "type": "utility",
    "lines": 5,
    "size": 45,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "mpv"
    ],
    "preview": "import mpv\n\nplayer = mpv.MPV()\nplayer.stop()\n",
    "last_modified": "2025-03-28T18:35:47.779566"
  },
  {
    "id": "2486",
    "name": "enums.py",
    "path": "github_repo/scripts/06_development_tools/development_utilities/enums.py",
    "category": "06_development_tools",
    "type": "utility",
    "lines": 86,
    "size": 1683,
    "docstring": "All of the Enums that are used throughout the chardet package.\n\n:author: Dan Blanchard (dan.blanchard@gmail.com)",
    "keywords": [],
    "functions": [
      "get_num_categories"
    ],
    "classes": [
      "InputState",
      "LanguageFilter",
      "ProbingState",
      "MachineState",
      "SequenceLikelihood",
      "CharacterCategory"
    ],
    "imports": [
      "enum"
    ],
    "preview": "\"\"\"\nAll of the Enums that are used throughout the chardet package.\n\n:author: Dan Blanchard (dan.blanchard@gmail.com)\n\"\"\"\n\nfrom enum import Enum, Flag\n\n\nclass InputState:\n    \"\"\"\n    This enum represents the different states a universal detector can be in.\n    \"\"\"\n\n    PURE_ASCII = 0\n    ESC_ASCII = 1\n    HIGH_BYTE = 2\n\n\nclass LanguageFilter(Flag):",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2487",
    "name": "ml_service.py",
    "path": "github_repo/scripts/06_development_tools/development_utilities/ml_service.py",
    "category": "06_development_tools",
    "type": "utility",
    "lines": 7,
    "size": 206,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "llm_engineering.infrastructure.inference_pipeline_api",
      "uvicorn"
    ],
    "preview": "from llm_engineering.infrastructure.inference_pipeline_api import app  # noqa\n\nif __name__ == \"__main__\":\n    import uvicorn\n\n    uvicorn.run(\"tools.ml_service:app\", host=\"0.0.0.0\", port=8000, reload=True)\n",
    "last_modified": "2025-05-04T22:47:11.576891"
  },
  {
    "id": "2488",
    "name": "package_data.py",
    "path": "github_repo/scripts/06_development_tools/development_utilities/package_data.py",
    "category": "06_development_tools",
    "type": "analysis",
    "lines": 2,
    "size": 20,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [],
    "preview": "__version__ = \"3.4\"\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2489",
    "name": "playwright.py",
    "path": "github_repo/scripts/06_development_tools/development_utilities/playwright.py",
    "category": "06_development_tools",
    "type": "utility",
    "lines": 6,
    "size": 253,
    "docstring": "",
    "keywords": [],
    "functions": [
      "clear_cookie_by_name"
    ],
    "classes": [],
    "imports": [],
    "preview": "def clear_cookie_by_name(context, cookie_cleared_name):\n    cookies = context.cookies()\n    filtered_cookies = [cookie for cookie in cookies if cookie[\"name\"] != cookie_cleared_name]\n    context.clear_cookies()\n    context.add_cookies(filtered_cookies)\n",
    "last_modified": "2025-09-13T05:54:00.328852"
  },
  {
    "id": "2490",
    "name": "config.py",
    "path": "github_repo/scripts/06_development_tools/development_utilities/config.py",
    "category": "06_development_tools",
    "type": "setup",
    "lines": 3,
    "size": 47,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [],
    "preview": "# config.py\nSOURCE_DIRECTORY = \"/Volumes/Pics\"\n",
    "last_modified": "2025-05-04T22:47:12"
  },
  {
    "id": "2491",
    "name": "version.py",
    "path": "github_repo/scripts/06_development_tools/development_utilities/version.py",
    "category": "06_development_tools",
    "type": "setup",
    "lines": 10,
    "size": 244,
    "docstring": "This module exists only to simplify retrieving the version number of chardet\nfrom within setuptools and from chardet subpackages.\n\n:author: Dan Blanchard (dan.blanchard@gmail.com)",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [],
    "preview": "\"\"\"\nThis module exists only to simplify retrieving the version number of chardet\nfrom within setuptools and from chardet subpackages.\n\n:author: Dan Blanchard (dan.blanchard@gmail.com)\n\"\"\"\n\n__version__ = \"5.1.0\"\nVERSION = __version__.split(\".\")\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2492",
    "name": "NewUpload_20250607124910.py",
    "path": "github_repo/scripts/06_development_tools/development_utilities/NewUpload_20250607124910.py",
    "category": "06_development_tools",
    "type": "youtube",
    "lines": 1,
    "size": 0,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [],
    "preview": "",
    "last_modified": "2025-06-07T12:49:13.216444"
  },
  {
    "id": "2493",
    "name": "models.py",
    "path": "github_repo/scripts/06_development_tools/development_utilities/models.py",
    "category": "06_development_tools",
    "type": "utility",
    "lines": 39,
    "size": 1192,
    "docstring": "Utilities for defining models",
    "keywords": [],
    "functions": [
      "__init__",
      "__hash__",
      "__lt__",
      "__le__",
      "__gt__",
      "__ge__",
      "__eq__",
      "_compare"
    ],
    "classes": [
      "KeyBasedCompareMixin"
    ],
    "imports": [
      "operator",
      "typing"
    ],
    "preview": "\"\"\"Utilities for defining models\"\"\"\n\nimport operator\nfrom typing import Any, Callable, Type\n\n\nclass KeyBasedCompareMixin:\n    \"\"\"Provides comparison capabilities that is based on a key\"\"\"\n\n    __slots__ = [\"_compare_key\", \"_defining_class\"]\n\n    def __init__(self, key: Any, defining_class: Type[\"KeyBasedCompareMixin\"]) -> None:\n        self._compare_key = key\n        self._defining_class = defining_class\n\n    def __hash__(self) -> int:\n        return hash(self._compare_key)\n\n    def __lt__(self, other: Any) -> bool:\n        return self._compare(other, operator.__lt__)",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2494",
    "name": "terminal.py",
    "path": "github_repo/scripts/06_development_tools/development_utilities/terminal.py",
    "category": "06_development_tools",
    "type": "utility",
    "lines": 134,
    "size": 4357,
    "docstring": "pygments.formatters.terminal\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nFormatter for terminal output with ANSI sequences.\n\n:copyright: Copyright 2006-2023 by the Pygments team, see AUTHORS.\n:license: BSD, see LICENSE for details.",
    "keywords": [],
    "functions": [
      "__init__",
      "format",
      "_write_lineno",
      "_get_color",
      "format_unencoded"
    ],
    "classes": [
      "TerminalFormatter"
    ],
    "imports": [
      "pip._vendor.pygments.console",
      "pip._vendor.pygments.formatter",
      "pip._vendor.pygments.token",
      "pip._vendor.pygments.util"
    ],
    "preview": "\"\"\"\npygments.formatters.terminal\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nFormatter for terminal output with ANSI sequences.\n\n:copyright: Copyright 2006-2023 by the Pygments team, see AUTHORS.\n:license: BSD, see LICENSE for details.\n\"\"\"\n\nfrom pip._vendor.pygments.console import ansiformat\nfrom pip._vendor.pygments.formatter import Formatter\nfrom pip._vendor.pygments.token import (\n    Comment,\n    Error,\n    Generic,\n    Keyword,\n    Name,\n    Number,\n    Operator,",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2495",
    "name": "credentials.py",
    "path": "github_repo/scripts/06_development_tools/development_utilities/credentials.py",
    "category": "06_development_tools",
    "type": "utility",
    "lines": 3,
    "size": 42,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [],
    "preview": "username = \"xxxxxxx\"\npassword = \"xxxxxxx\"\n",
    "last_modified": "2025-09-13T05:53:38.642643"
  },
  {
    "id": "2496",
    "name": "config_20250430201608.py",
    "path": "github_repo/scripts/06_development_tools/development_utilities/config_20250430201608.py",
    "category": "06_development_tools",
    "type": "setup",
    "lines": 3,
    "size": 48,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [],
    "preview": "# config.py\nSOURCE_DIRECTORY = \"/Users/steven/\"\n",
    "last_modified": "2025-04-30T20:16:08.984808"
  },
  {
    "id": "2497",
    "name": "diag_20250530223526.py",
    "path": "github_repo/scripts/06_development_tools/development_utilities/diag_20250530223526.py",
    "category": "06_development_tools",
    "type": "utility",
    "lines": 1,
    "size": 0,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [],
    "preview": "",
    "last_modified": "2025-05-30T22:35:32.974215"
  },
  {
    "id": "2498",
    "name": "base_uploader.py",
    "path": "github_repo/scripts/06_development_tools/development_utilities/base_uploader.py",
    "category": "06_development_tools",
    "type": "youtube",
    "lines": 21,
    "size": 713,
    "docstring": "",
    "keywords": [
      "youtube"
    ],
    "functions": [
      "check_location",
      "upload_gallery"
    ],
    "classes": [
      "BaseUploader"
    ],
    "imports": [],
    "preview": "class BaseUploader:\n    \"\"\"\n    Base class defining the interface to a remote uploader.\n    \"\"\"\n\n    def check_location(self, location):\n        \"\"\"\n        Checks if the provided location for the upload is valid or not\n        :param location: location where the gallery will be uploaded (uploader specific)\n        :return: True if the location is valid, False otherwise\n        \"\"\"\n        pass\n\n    def upload_gallery(self, location, gallery_path):\n        \"\"\"\n        Upload the gallery to the specified location\n        :param location: location where the gallery will be uploaded (uploader specific)\n        :param gallery_path: path to the root of the public files of the gallery\n        \"\"\"\n        pass",
    "last_modified": "2025-05-04T22:47:13"
  },
  {
    "id": "2499",
    "name": "config_20250329125101.py",
    "path": "github_repo/scripts/06_development_tools/development_utilities/config_20250329125101.py",
    "category": "06_development_tools",
    "type": "setup",
    "lines": 3,
    "size": 56,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [],
    "preview": "# config.py\nSOURCE_DIRECTORY = \"/Volumes/oG-bAk/steven\"\n",
    "last_modified": "2025-04-30T20:16:01.517106"
  },
  {
    "id": "2500",
    "name": "config 3.py",
    "path": "github_repo/scripts/06_development_tools/development_utilities/config 3.py",
    "category": "06_development_tools",
    "type": "setup",
    "lines": 4,
    "size": 89,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [],
    "preview": "assemblyai = \"ASSEMBLY_API_KEY\"\npexelKey = \"PEXELS_KEY\"\nelevenLabsKey = \"ELEVENLABS_KEY\"\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2501",
    "name": "txt-csv (1).py",
    "path": "github_repo/scripts/06_development_tools/development_utilities/txt-csv (1).py",
    "category": "06_development_tools",
    "type": "utility",
    "lines": 34,
    "size": 1062,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "csv"
    ],
    "preview": "import csv\n\n# Input and output file paths\ninput_file = \"/Users/steven/dalle/trashy.txt\"\noutput_file = \"/Users/steven/dalle/trashy_output.txt\"\n\n# Open the input file for reading\nwith open(input_file, \"r\") as file:\n    lines = file.readlines()\n\n# Open the output file for writing\nwith open(output_file, \"w\", newline=\"\") as csvfile:\n    writer = csv.writer(csvfile)\n    writer.writerow([\"Url\", \"Prompt\"])  # Write the CSV header\n\n    url = None\n    prompt = \"\"\n\n    for line in lines:\n        line = line.strip()",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2502",
    "name": "api_env_setup.py",
    "path": "github_repo/scripts/06_development_tools/development_utilities/api_env_setup.py",
    "category": "06_development_tools",
    "type": "web_tools",
    "lines": 1,
    "size": 0,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [],
    "preview": "",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2503",
    "name": "stop.py",
    "path": "github_repo/scripts/06_development_tools/development_utilities/stop.py",
    "category": "06_development_tools",
    "type": "utility",
    "lines": 104,
    "size": 3086,
    "docstring": "",
    "keywords": [],
    "functions": [
      "__call__",
      "__and__",
      "__or__",
      "__init__",
      "__call__",
      "__init__",
      "__call__",
      "__call__",
      "__init__",
      "__call__"
    ],
    "classes": [
      "stop_base",
      "stop_any",
      "stop_all",
      "_stop_never",
      "stop_when_event_set",
      "stop_after_attempt",
      "stop_after_delay"
    ],
    "imports": [
      "abc",
      "typing",
      "pip._vendor.tenacity",
      "threading",
      "pip._vendor.tenacity"
    ],
    "preview": "# Copyright 2016\u20132021 Julien Danjou\n# Copyright 2016 Joshua Harlow\n# Copyright 2013-2014 Ray Holder\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nimport abc\nimport typing\n\nfrom pip._vendor.tenacity import _utils\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2504",
    "name": "irc.py",
    "path": "github_repo/scripts/06_development_tools/development_utilities/irc.py",
    "category": "06_development_tools",
    "type": "utility",
    "lines": 164,
    "size": 4712,
    "docstring": "pygments.formatters.irc\n~~~~~~~~~~~~~~~~~~~~~~~\n\nFormatter for IRC output\n\n:copyright: Copyright 2006-2023 by the Pygments team, see AUTHORS.\n:license: BSD, see LICENSE for details.",
    "keywords": [],
    "functions": [
      "ircformat",
      "__init__",
      "_write_lineno",
      "format_unencoded"
    ],
    "classes": [
      "IRCFormatter"
    ],
    "imports": [
      "pip._vendor.pygments.formatter",
      "pip._vendor.pygments.token",
      "pip._vendor.pygments.util"
    ],
    "preview": "\"\"\"\npygments.formatters.irc\n~~~~~~~~~~~~~~~~~~~~~~~\n\nFormatter for IRC output\n\n:copyright: Copyright 2006-2023 by the Pygments team, see AUTHORS.\n:license: BSD, see LICENSE for details.\n\"\"\"\n\nfrom pip._vendor.pygments.formatter import Formatter\nfrom pip._vendor.pygments.token import (\n    Comment,\n    Error,\n    Generic,\n    Keyword,\n    Name,\n    Number,\n    Operator,\n    String,",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2505",
    "name": "leodown_20250102105144.py",
    "path": "github_repo/scripts/06_development_tools/development_utilities/leodown_20250102105144.py",
    "category": "06_development_tools",
    "type": "utility",
    "lines": 1,
    "size": 0,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [],
    "preview": "",
    "last_modified": "2025-03-28T18:37:01.034000"
  },
  {
    "id": "2506",
    "name": "x_user_defined.py",
    "path": "github_repo/scripts/06_development_tools/development_utilities/x_user_defined.py",
    "category": "06_development_tools",
    "type": "web_tools",
    "lines": 326,
    "size": 4287,
    "docstring": "webencodings.x_user_defined\n~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nAn implementation of the x-user-defined encoding.\n\n:copyright: Copyright 2012 by Simon Sapin\n:license: BSD, see LICENSE for details.",
    "keywords": [],
    "functions": [
      "encode",
      "decode",
      "encode",
      "decode"
    ],
    "classes": [
      "Codec",
      "IncrementalEncoder",
      "IncrementalDecoder",
      "StreamWriter",
      "StreamReader"
    ],
    "imports": [
      "__future__",
      "codecs"
    ],
    "preview": "# coding: utf-8\n\"\"\"\n\nwebencodings.x_user_defined\n~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nAn implementation of the x-user-defined encoding.\n\n:copyright: Copyright 2012 by Simon Sapin\n:license: BSD, see LICENSE for details.\n\n\"\"\"\n\nfrom __future__ import unicode_literals\n\nimport codecs\n\n### Codec APIs\n\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2507",
    "name": "datetime_local.py",
    "path": "github_repo/scripts/06_development_tools/development_utilities/datetime_local.py",
    "category": "06_development_tools",
    "type": "utility",
    "lines": 11,
    "size": 241,
    "docstring": "For when pip wants to check the date or time.",
    "keywords": [],
    "functions": [
      "today_is_later_than"
    ],
    "classes": [],
    "imports": [
      "datetime"
    ],
    "preview": "\"\"\"For when pip wants to check the date or time.\"\"\"\n\nimport datetime\n\n\ndef today_is_later_than(year: int, month: int, day: int) -> bool:\n    today = datetime.date.today()\n    given = datetime.date(year, month, day)\n\n    return today > given\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2508",
    "name": "customerrors.py",
    "path": "github_repo/scripts/06_development_tools/development_utilities/customerrors.py",
    "category": "06_development_tools",
    "type": "utility",
    "lines": 55,
    "size": 1260,
    "docstring": "Errors used in this application",
    "keywords": [],
    "functions": [
      "__init__",
      "__init__",
      "__init__",
      "__init__",
      "__init__"
    ],
    "classes": [
      "Error",
      "AuthError",
      "DirectoryNotFoundError",
      "FileNotFoundError",
      "InvalidConfig"
    ],
    "imports": [],
    "preview": "\"\"\"Errors used in this application\"\"\"\n\n\nclass Error(Exception):\n    \"\"\"Base class for exceptions in this module\"\"\"\n\n    def __init__(self, message):\n        super(Error, self).__init__()\n        self.message = message\n\n\nclass AuthError(Error):\n    \"\"\"Exception raised for authentication errors\n\n    Attributes:\n        message -- explanation of the error\n    \"\"\"\n\n    def __init__(self, message):\n        super(AuthError, self).__init__(message)",
    "last_modified": "2025-05-04T23:27:55"
  },
  {
    "id": "2509",
    "name": "__init__.py",
    "path": "github_repo/scripts/06_development_tools/development_utilities/__init__.py",
    "category": "06_development_tools",
    "type": "setup",
    "lines": 2,
    "size": 22,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [],
    "preview": "__version__ = \"0.1.0\"\n",
    "last_modified": "2025-09-11T13:24:00.164291"
  },
  {
    "id": "2510",
    "name": "notify.py",
    "path": "github_repo/scripts/06_development_tools/development_utilities/notify.py",
    "category": "06_development_tools",
    "type": "utility",
    "lines": 38,
    "size": 1022,
    "docstring": "Responsible for sending a notification",
    "keywords": [],
    "functions": [
      "__init__",
      "send"
    ],
    "classes": [
      "Iftttnotify"
    ],
    "imports": [
      "http.client",
      "json",
      "logging"
    ],
    "preview": "\"Responsible for sending a notification\"\n\nimport http.client\nimport json\nimport logging\n\n\nclass Iftttnotify(object):\n    \"Sends notification through IFTTT\"\n\n    def __init__(self, ytdl_config):\n        self.logger = logging.getLogger(__name__)\n        self.ytdl_config = ytdl_config\n\n    def send(self, value1=\"\", value2=\"\", value3=\"\"):\n        \"Sends value1, value2 and value3\"\n\n        try:\n            conn = http.client.HTTPSConnection(\"maker.ifttt.com\")\n",
    "last_modified": "2025-05-04T23:28:25"
  },
  {
    "id": "2511",
    "name": "status_codes.py",
    "path": "github_repo/scripts/06_development_tools/development_utilities/status_codes.py",
    "category": "06_development_tools",
    "type": "utility",
    "lines": 7,
    "size": 116,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [],
    "preview": "SUCCESS = 0\nERROR = 1\nUNKNOWN_ERROR = 2\nVIRTUALENV_NOT_FOUND = 3\nPREVIOUS_BUILD_DIR_ERROR = 4\nNO_MATCHES_FOUND = 23\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2512",
    "name": "pdfcsv.py",
    "path": "github_repo/scripts/06_development_tools/development_utilities/pdfcsv.py",
    "category": "06_development_tools",
    "type": "utility",
    "lines": 7,
    "size": 156,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "tabula"
    ],
    "preview": "import tabula\n\nfilename = input(\"Enter File Path: \")\ndf = tabula.read_pdf(filename, encoding=\"utf-8\", spreadsheet=True, pages=\"1\")\n\ndf.to_csv(\"output.csv\")\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2513",
    "name": "expand_prompts_by_style_20250530223226.py",
    "path": "github_repo/scripts/06_development_tools/development_utilities/expand_prompts_by_style_20250530223226.py",
    "category": "06_development_tools",
    "type": "utility",
    "lines": 1,
    "size": 0,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [],
    "preview": "",
    "last_modified": "2025-05-30T22:32:33.583994"
  },
  {
    "id": "2514",
    "name": "logo.py",
    "path": "github_repo/scripts/06_development_tools/development_utilities/logo.py",
    "category": "06_development_tools",
    "type": "utility",
    "lines": 25,
    "size": 3459,
    "docstring": "",
    "keywords": [],
    "functions": [
      "print_logo"
    ],
    "classes": [],
    "imports": [
      "libs.animation"
    ],
    "preview": "# coding=utf-8\n#!/usr/bin/env python3\n\nfrom libs.animation import colorText\n\nlogo = \"\"\"\n\n[[black-bright-background]][[yellow]] \u2588\u2588\u2593 [[yellow]]\u2588\u2588\u2588\u2584    \u2588  [[yellow]] \u2588\u2588\u2588\u2588\u2588\u2588 [[yellow]]\u2584\u2584\u2584\u2588\u2588\u2588\u2588\u2588\u2593 [[yellow]]\u2584\u2584\u2584         [[yellow]] \u2588\u2588\u2580\u2588\u2588\u2588  [[yellow]]\u2593\u2588\u2588\u2588\u2588\u2588 [[yellow]] \u2588\u2588\u2593\u2588\u2588\u2588   [[yellow]]\u2592\u2588\u2588\u2588\u2588\u2588   [[yellow]]\u2588\u2588\u2580\u2588\u2588\u2588  [[yellow]]\u2584\u2584\u2584\u2588\u2588\u2588\u2588\u2588\u2593[[reset]]\n[[black-bright-background]][[yellow]]\u2593\u2588\u2588\u2592 [[yellow]]\u2588\u2588 \u2580\u2588   \u2588 \u2592[[yellow]]\u2588\u2588    \u2592 [[yellow]]\u2593  \u2588\u2588\u2592 \u2593\u2592\u2592[[yellow]]\u2588\u2588\u2588\u2588\u2584       [[yellow]]\u2593\u2588\u2588 \u2592 \u2588\u2588\u2592[[yellow]]\u2593\u2588   \u2580 [[yellow]]\u2593\u2588\u2588\u2591  \u2588\u2588\u2592\u2592[[yellow]]\u2588\u2588\u2592  \u2588\u2588\u2592\u2593[[yellow]]\u2588\u2588 \u2592 \u2588\u2588\u2592[[yellow]]\u2593  \u2588\u2588\u2592 \u2593\u2592[[reset]]\n[[black-bright-background]][[yellow]]\u2592\u2588\u2588\u2592\u2593[[yellow]]\u2588\u2588  \u2580\u2588 \u2588\u2588\u2592\u2591[[yellow]] \u2593\u2588\u2588\u2584   [[yellow]]\u2592 \u2593\u2588\u2588\u2591 \u2592\u2591\u2592[[yellow]]\u2588\u2588  \u2580\u2588\u2584     [[yellow]]\u2593\u2588\u2588 \u2591\u2584\u2588 \u2592[[yellow]]\u2592\u2588\u2588\u2588   [[yellow]]\u2593\u2588\u2588\u2591 \u2588\u2588\u2593\u2592\u2592[[yellow]]\u2588\u2588\u2591  \u2588\u2588\u2592\u2593[[yellow]]\u2588\u2588 \u2591\u2584\u2588 \u2592[[yellow]]\u2592 \u2593\u2588\u2588\u2591 \u2592\u2591[[reset]]\n[[black-bright-background]][[yellow]]\u2591\u2588\u2588\u2591\u2593[[yellow]]\u2588\u2588\u2592  \u2590\u258c\u2588\u2588\u2592 [[yellow]] \u2592   \u2588\u2588\u2592[[yellow]]\u2591 \u2593\u2588\u2588\u2593 \u2591 \u2591[[yellow]]\u2588\u2588\u2584\u2584\u2584\u2584\u2588\u2588    [[yellow]]\u2592\u2588\u2588\u2580\u2580\u2588\u2584  [[yellow]]\u2592\u2593\u2588  \u2584 [[yellow]]\u2592\u2588\u2588\u2584\u2588\u2593\u2592 \u2592\u2592[[yellow]]\u2588\u2588   \u2588\u2588\u2591\u2592[[yellow]]\u2588\u2588\u2580\u2580\u2588\u2584  [[yellow]]\u2591 \u2593\u2588\u2588\u2593 \u2591 [[reset]]\n[[black-bright-background]][[yellow]]\u2591\u2588\u2588\u2591\u2592[[yellow]]\u2588\u2588\u2591   \u2593\u2588\u2588\u2591\u2592[[yellow]]\u2588\u2588\u2588\u2588\u2588\u2588\u2592\u2592[[yellow]]  \u2592\u2588\u2588\u2592 \u2591  [[yellow]]\u2593\u2588   \u2593\u2588\u2588\u2592   [[yellow]]\u2591\u2588\u2588\u2593 \u2592\u2588\u2588\u2592[[yellow]]\u2591\u2592\u2588\u2588\u2588\u2588\u2592[[yellow]]\u2592\u2588\u2588\u2592 \u2591  \u2591\u2591[[yellow]] \u2588\u2588\u2588\u2588\u2593\u2592\u2591\u2591[[yellow]]\u2588\u2588\u2593 \u2592\u2588\u2588\u2592[[yellow]]  \u2592\u2588\u2588\u2592 \u2591 [[reset]]\n[[black-bright-background]][[yellow]]\u2591\u2593  \u2591[[yellow]] \u2592\u2591   \u2592 \u2592 \u2592[[yellow]] \u2592\u2593\u2592 \u2592 \u2591[[yellow]]  \u2592 \u2591\u2591    [[yellow]]\u2592\u2592   \u2593\u2592\u2588\u2591   [[yellow]]\u2591 \u2592\u2593 \u2591\u2592\u2593\u2591[[yellow]]\u2591\u2591 \u2592\u2591 \u2591[[yellow]]\u2592\u2593\u2592\u2591 \u2591  \u2591\u2591[[yellow]] \u2592\u2591\u2592\u2591\u2592\u2591 \u2591[[yellow]] \u2592\u2593 \u2591\u2592\u2593\u2591[[yellow]]  \u2592 \u2591\u2591   [[reset]]\n[[black-bright-background]][[yellow]] \u2592 \u2591\u2591[[yellow]] \u2591\u2591   \u2591 \u2592\u2591\u2591[[yellow]] \u2591\u2592  \u2591 \u2591[[yellow]]    \u2591     [[yellow]] \u2592   \u2592\u2592 \u2591   [[yellow]]  \u2591\u2592 \u2591 \u2592\u2591[[yellow]] \u2591 \u2591  \u2591[[yellow]]\u2591\u2592 \u2591      [[yellow]] \u2591 \u2592 \u2592\u2591  [[yellow]] \u2591\u2592 \u2591 \u2592\u2591[[yellow]]    \u2591    [[reset]]\n[[black-bright-background]][[yellow]] \u2592 \u2591 [[yellow]]  \u2591   \u2591 \u2591 \u2591[[yellow]]  \u2591  \u2591  [[yellow]]  \u2591       [[yellow]] \u2591   \u2592      [[yellow]]  \u2591\u2591   \u2591 [[yellow]]   \u2591   [[yellow]]\u2591\u2591       \u2591[[yellow]] \u2591 \u2591 \u2592   [[yellow]] \u2591\u2591   \u2591 [[yellow]]  \u2591      [[reset]]\n[[black-bright-background]][[yellow]] \u2591   [[yellow]]        \u2591  [[yellow]]     \u2591  [[yellow]]          [[yellow]]     \u2591  \u2591   [[yellow]]   \u2591     [[yellow]]   \u2591  \u2591[[yellow]]          [[yellow]]   \u2591 \u2591   [[yellow]]  \u2591     [[yellow]]         [[reset]]\n                                                                                                  \n\n                                           \n\"\"\"",
    "last_modified": "2025-03-28T18:35:46"
  },
  {
    "id": "2515",
    "name": "config (2).py",
    "path": "github_repo/scripts/06_development_tools/development_utilities/config (2).py",
    "category": "06_development_tools",
    "type": "setup",
    "lines": 3,
    "size": 47,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [],
    "preview": "# config.py\nSOURCE_DIRECTORY = \"/Users/steven\"\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2516",
    "name": "hash.py",
    "path": "github_repo/scripts/06_development_tools/development_utilities/hash.py",
    "category": "06_development_tools",
    "type": "utility",
    "lines": 56,
    "size": 1643,
    "docstring": "",
    "keywords": [],
    "functions": [
      "_hash_of_file",
      "add_options",
      "run"
    ],
    "classes": [
      "HashCommand"
    ],
    "imports": [
      "hashlib",
      "logging",
      "sys",
      "optparse",
      "typing",
      "pip._internal.cli.base_command",
      "pip._internal.cli.status_codes",
      "pip._internal.utils.hashes",
      "pip._internal.utils.misc"
    ],
    "preview": "import hashlib\nimport logging\nimport sys\nfrom optparse import Values\nfrom typing import List\n\nfrom pip._internal.cli.base_command import Command\nfrom pip._internal.cli.status_codes import ERROR, SUCCESS\nfrom pip._internal.utils.hashes import FAVORITE_HASH, STRONG_HASHES\nfrom pip._internal.utils.misc import read_chunks, write_output\n\nlogger = logging.getLogger(__name__)\n\n\nclass HashCommand(Command):\n    \"\"\"\n    Compute a hash of a local package archive.\n\n    These can be used with --hash in a requirements file to do repeatable\n    installs.",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2517",
    "name": "pyttsx.py",
    "path": "github_repo/scripts/06_development_tools/development_utilities/pyttsx.py",
    "category": "06_development_tools",
    "type": "utility",
    "lines": 43,
    "size": 1201,
    "docstring": "",
    "keywords": [],
    "functions": [
      "__init__",
      "run",
      "randomvoice"
    ],
    "classes": [
      "pyttsx"
    ],
    "imports": [
      "random",
      "pyttsx3",
      "utils"
    ],
    "preview": "import random\n\nimport pyttsx3\n\nfrom utils import settings\n\n\nclass pyttsx:\n    def __init__(self):\n        self.max_chars = 5000\n        self.voices = []\n\n    def run(\n        self,\n        text: str,\n        filepath: str,\n        random_voice=False,\n    ):\n        voice_id = settings.config[\"settings\"][\"tts\"][\"python_voice\"]\n        voice_num = settings.config[\"settings\"][\"tts\"][\"py_voice_num\"]",
    "last_modified": "2025-09-13T05:53:59.748700"
  },
  {
    "id": "2518",
    "name": "config 5.py",
    "path": "github_repo/scripts/06_development_tools/development_utilities/config 5.py",
    "category": "06_development_tools",
    "type": "setup",
    "lines": 3,
    "size": 56,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [],
    "preview": "# config.py\nSOURCE_DIRECTORY = \"/Volumes/oG-bAk/steven\"\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2519",
    "name": "load_files.py",
    "path": "github_repo/scripts/06_development_tools/development_utilities/load_files.py",
    "category": "06_development_tools",
    "type": "organization",
    "lines": 66,
    "size": 2093,
    "docstring": "MIT License\n\nCopyright (c) 2021-2022 MShawon\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.",
    "keywords": [],
    "functions": [
      "load_url",
      "load_search",
      "get_hash"
    ],
    "classes": [],
    "imports": [
      "hashlib",
      "random",
      "colors"
    ],
    "preview": "\"\"\"\nMIT License\n\nCopyright (c) 2021-2022 MShawon\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,",
    "last_modified": "2025-09-13T05:54:11.711565"
  },
  {
    "id": "2520",
    "name": "fdupes.py",
    "path": "github_repo/scripts/06_development_tools/development_utilities/fdupes.py",
    "category": "06_development_tools",
    "type": "utility",
    "lines": 18,
    "size": 542,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "csv"
    ],
    "preview": "import csv\n\n# Read the output from the fdupes results\nwith open(\"duplicates.txt\", \"r\") as infile, open(\"duplicates.csv\", \"w\", newline=\"\") as outfile:\n    writer = csv.writer(outfile)\n    writer.writerow([\"File1\", \"File2\"])  # Write the CSV header\n\n    files = []\n    for line in infile:\n        line = line.strip()\n        if line:\n            files.append(line)\n        else:\n            # Write pairs of duplicate files\n            for i in range(1, len(files)):\n                writer.writerow([files[0], files[i]])\n            files = []\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2521",
    "name": "config (1).py",
    "path": "github_repo/scripts/06_development_tools/development_utilities/config (1).py",
    "category": "06_development_tools",
    "type": "setup",
    "lines": 3,
    "size": 47,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [],
    "preview": "# config.py\nSOURCE_DIRECTORY = \"/Users/steven\"\n",
    "last_modified": "2025-05-04T22:47:12.597946"
  },
  {
    "id": "2522",
    "name": "launch.py",
    "path": "github_repo/scripts/06_development_tools/development_utilities/launch.py",
    "category": "06_development_tools",
    "type": "utility",
    "lines": 5,
    "size": 67,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "src.main"
    ],
    "preview": "from src.main import start\n\nif __name__ == \"__main__\":\n    start()\n",
    "last_modified": "2025-05-04T22:47:12"
  },
  {
    "id": "2523",
    "name": "bot.py",
    "path": "github_repo/scripts/06_development_tools/development_utilities/bot.py",
    "category": "06_development_tools",
    "type": "web_tools",
    "lines": 129,
    "size": 3693,
    "docstring": "Before changing the program and publishing it somewhere, please\nPlease note that this program is under GPLv3 license.\nMore information:\nhttps://tr.wikipedia.org/wiki/gnu_genel_kamu_lisans%c4%b1\nhttps://www.gnu.org/licenses/quick-guide-gplv3.html",
    "keywords": [],
    "functions": [
      "MultiThread",
      "NoMultiThread"
    ],
    "classes": [],
    "imports": [
      "multiprocessing",
      "random",
      "time",
      "libs.instaclient",
      "libs.utils"
    ],
    "preview": "# coding=utf-8\n#!/usr/bin/env python3\n\n\"\"\"\nBefore changing the program and publishing it somewhere, please\nPlease note that this program is under GPLv3 license.\nMore information:\nhttps://tr.wikipedia.org/wiki/gnu_genel_kamu_lisans%c4%b1\nhttps://www.gnu.org/licenses/quick-guide-gplv3.html\n\"\"\"\n\n__author__ = \"Marwan 007 : @mrwn.007\"\n__license__ = \"GPLv3\"\n__version__ = \"0.1\"\n__status__ = \"being developed\"\n\n\nfrom multiprocessing import Process\nfrom random import choice\nfrom time import sleep, time",
    "last_modified": "2025-09-13T05:53:27.928570"
  },
  {
    "id": "2524",
    "name": "merge-pdfs.py",
    "path": "github_repo/scripts/06_development_tools/development_utilities/merge-pdfs.py",
    "category": "06_development_tools",
    "type": "utility",
    "lines": 22,
    "size": 528,
    "docstring": "",
    "keywords": [],
    "functions": [
      "merge_pdfs"
    ],
    "classes": [],
    "imports": [
      "PyPDF2"
    ],
    "preview": "from PyPDF2 import PdfReader, PdfWriter\n\n\ndef merge_pdfs(toc_path, main_pdf_path, output_path):\n    toc_reader = PdfReader(toc_path)\n    main_reader = PdfReader(main_pdf_path)\n    writer = PdfWriter()\n\n    # Add TOC pages\n    for page in toc_reader.pages:\n        writer.add_page(page)\n\n    # Add main PDF pages\n    for page in main_reader.pages:\n        writer.add_page(page)\n\n    with open(output_path, \"wb\") as output_file:\n        writer.write(output_file)\n\n",
    "last_modified": "2025-05-04T22:47:13.331849"
  },
  {
    "id": "2525",
    "name": "uploader_factory.py",
    "path": "github_repo/scripts/06_development_tools/development_utilities/uploader_factory.py",
    "category": "06_development_tools",
    "type": "youtube",
    "lines": 22,
    "size": 777,
    "docstring": "",
    "keywords": [],
    "functions": [
      "get_uploader"
    ],
    "classes": [],
    "imports": [
      "simplegallery.common",
      "simplegallery.upload.variants.aws_uploader",
      "simplegallery.upload.variants.netlify_uploader"
    ],
    "preview": "import simplegallery.common as spg_common\nfrom simplegallery.upload.variants.aws_uploader import AWSUploader\nfrom simplegallery.upload.variants.netlify_uploader import NetlifyUploader\n\n\ndef get_uploader(hosting_type):\n    \"\"\"\n    Factory function that returns an object of a class derived from BaseUploader based on the provided hosting type.\n    Supported uploaders:\n    - AWSUploader - uploader for AWS S3\n    - Netlify - uploader for Netlify\n\n    :param hosting_type: name of the hosting provider (aws or netlify)\n    :return: uploader object\n    \"\"\"\n    if hosting_type == \"aws\":\n        return AWSUploader()\n    elif hosting_type == \"netlify\":\n        return NetlifyUploader()\n    else:",
    "last_modified": "2025-05-04T22:47:13"
  },
  {
    "id": "2526",
    "name": "bot_archive.py",
    "path": "github_repo/scripts/06_development_tools/development_utilities/bot_archive.py",
    "category": "06_development_tools",
    "type": "utility",
    "lines": 44,
    "size": 1469,
    "docstring": "",
    "keywords": [],
    "functions": [
      "archive",
      "archive_medias",
      "unarchive_medias"
    ],
    "classes": [],
    "imports": [
      "tqdm"
    ],
    "preview": "from tqdm import tqdm\n\n\ndef archive(self, media_id, undo=False):\n    self.small_delay()\n    media = self.get_media_info(media_id)\n    media = media[0] if isinstance(media, list) else media\n    if self.api.archive_media(media, undo):\n        self.total[\"archived\"] += int(not undo)\n        self.total[\"unarchived\"] += int(undo)\n        return True\n    self.logger.info(\"Media id %s is not %s.\", media_id, \"unarchived\" if undo else \"archived\")\n    return False\n\n\ndef archive_medias(self, medias):\n    broken_items = []\n    if not medias:\n        self.logger.info(\"Nothing to archive.\")\n        return broken_items",
    "last_modified": "2025-09-13T05:54:57.086581"
  },
  {
    "id": "2527",
    "name": "inference_pipeline_api.py",
    "path": "github_repo/scripts/06_development_tools/development_utilities/inference_pipeline_api.py",
    "category": "06_development_tools",
    "type": "web_tools",
    "lines": 67,
    "size": 1921,
    "docstring": "",
    "keywords": [],
    "functions": [
      "call_llm_service",
      "rag"
    ],
    "classes": [
      "QueryRequest",
      "QueryResponse"
    ],
    "imports": [
      "opik",
      "fastapi",
      "llm_engineering",
      "llm_engineering.application.rag.retriever",
      "llm_engineering.application.utils",
      "llm_engineering.domain.embedded_chunks",
      "llm_engineering.infrastructure.opik_utils",
      "llm_engineering.model.inference",
      "opik",
      "pydantic"
    ],
    "preview": "import opik\nfrom fastapi import FastAPI, HTTPException\nfrom llm_engineering import settings\nfrom llm_engineering.application.rag.retriever import ContextRetriever\nfrom llm_engineering.application.utils import misc\nfrom llm_engineering.domain.embedded_chunks import EmbeddedChunk\nfrom llm_engineering.infrastructure.opik_utils import configure_opik\nfrom llm_engineering.model.inference import InferenceExecutor, LLMInferenceSagemakerEndpoint\nfrom opik import opik_context\nfrom pydantic import BaseModel\n\nconfigure_opik()\n\napp = FastAPI()\n\n\nclass QueryRequest(BaseModel):\n    query: str\n\n",
    "last_modified": "2025-09-13T05:53:42.104751"
  },
  {
    "id": "2528",
    "name": "analyze_all_images_20250530220426.py",
    "path": "github_repo/scripts/06_development_tools/development_utilities/analyze_all_images_20250530220426.py",
    "category": "06_development_tools",
    "type": "analysis",
    "lines": 1,
    "size": 0,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [],
    "preview": "",
    "last_modified": "2025-05-30T22:04:35.765492"
  },
  {
    "id": "2529",
    "name": "bot_delete.py",
    "path": "github_repo/scripts/06_development_tools/development_utilities/bot_delete.py",
    "category": "06_development_tools",
    "type": "utility",
    "lines": 35,
    "size": 1116,
    "docstring": "",
    "keywords": [],
    "functions": [
      "delete_media",
      "delete_medias",
      "delete_comment"
    ],
    "classes": [],
    "imports": [
      "tqdm"
    ],
    "preview": "from tqdm import tqdm\n\n\ndef delete_media(self, media_id):\n    self.small_delay()\n    media = self.get_media_info(media_id)\n    media = media[0] if isinstance(media, list) else media\n    if self.api.delete_media(media):\n        return True\n    self.logger.info(\"Media with {} is not {}.\".format(media.get(\"id\"), \"deleted\"))\n    return False\n\n\ndef delete_medias(self, medias):\n    broken_items = []\n    if not medias:\n        self.logger.info(\"Nothing to delete.\")\n        return broken_items\n    self.logger.info(\"Going to delete %d medias.\" % (len(medias)))\n    for media in tqdm(medias):",
    "last_modified": "2025-09-13T05:54:57.225078"
  },
  {
    "id": "2530",
    "name": "uninstall.py",
    "path": "github_repo/scripts/06_development_tools/development_utilities/uninstall.py",
    "category": "06_development_tools",
    "type": "setup",
    "lines": 109,
    "size": 3833,
    "docstring": "",
    "keywords": [],
    "functions": [
      "add_options",
      "run"
    ],
    "classes": [
      "UninstallCommand"
    ],
    "imports": [
      "logging",
      "optparse",
      "typing",
      "pip._internal.cli",
      "pip._internal.cli.base_command",
      "pip._internal.cli.req_command",
      "pip._internal.cli.status_codes",
      "pip._internal.exceptions",
      "pip._internal.req",
      "pip._internal.req.constructors"
    ],
    "preview": "import logging\nfrom optparse import Values\nfrom typing import List\n\nfrom pip._internal.cli import cmdoptions\nfrom pip._internal.cli.base_command import Command\nfrom pip._internal.cli.req_command import SessionCommandMixin, warn_if_run_as_root\nfrom pip._internal.cli.status_codes import SUCCESS\nfrom pip._internal.exceptions import InstallationError\nfrom pip._internal.req import parse_requirements\nfrom pip._internal.req.constructors import (\n    install_req_from_line,\n    install_req_from_parsed_requirement,\n)\nfrom pip._internal.utils.misc import (\n    check_externally_managed,\n    protect_pip_from_modification_on_windows,\n)\nfrom pip._vendor.packaging.utils import canonicalize_name\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2531",
    "name": "aws_uploader.py",
    "path": "github_repo/scripts/06_development_tools/development_utilities/aws_uploader.py",
    "category": "06_development_tools",
    "type": "youtube",
    "lines": 53,
    "size": 1727,
    "docstring": "",
    "keywords": [
      "youtube"
    ],
    "functions": [
      "check_location",
      "upload_gallery"
    ],
    "classes": [
      "AWSUploader"
    ],
    "imports": [
      "subprocess",
      "simplegallery.common",
      "simplegallery.upload.base_uploader"
    ],
    "preview": "import subprocess\n\nimport simplegallery.common as spg_common\nfrom simplegallery.upload.base_uploader import BaseUploader\n\n\nclass AWSUploader(BaseUploader):\n    def check_location(self, location):\n        \"\"\"\n        Checks if the location is empty or not\n        :param location: S3 bucket where the gallery should be uploaded\n        :return: True if the location is not empty, False otherwise\n        \"\"\"\n        if not location:\n            spg_common.log(\"Location cannot be empty when uploading to AWS\")\n\n        return bool(location)\n\n    def upload_gallery(self, location, gallery_path):\n        \"\"\"",
    "last_modified": "2025-09-13T05:53:53.263033"
  },
  {
    "id": "2532",
    "name": "autofill_20250530225744.py",
    "path": "github_repo/scripts/06_development_tools/development_utilities/autofill_20250530225744.py",
    "category": "06_development_tools",
    "type": "utility",
    "lines": 1,
    "size": 0,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [],
    "preview": "",
    "last_modified": "2025-05-30T22:57:52.525018"
  },
  {
    "id": "2533",
    "name": "chardetect.py",
    "path": "github_repo/scripts/06_development_tools/development_utilities/chardetect.py",
    "category": "06_development_tools",
    "type": "organization",
    "lines": 104,
    "size": 3153,
    "docstring": "Script which takes one or more file paths and reports on their detected\nencodings\n\nExample::\n\n    % chardetect somefile someotherfile\n    somefile: windows-1252 with confidence 0.5\n    someotherfile: ascii with confidence 1.0\n\nIf no paths are provided, it takes its input from stdin.",
    "keywords": [],
    "functions": [
      "description_of",
      "main"
    ],
    "classes": [],
    "imports": [
      "argparse",
      "sys",
      "typing",
      "universaldetector"
    ],
    "preview": "\"\"\"\nScript which takes one or more file paths and reports on their detected\nencodings\n\nExample::\n\n    % chardetect somefile someotherfile\n    somefile: windows-1252 with confidence 0.5\n    someotherfile: ascii with confidence 1.0\n\nIf no paths are provided, it takes its input from stdin.\n\n\"\"\"\n\nimport argparse\nimport sys\nfrom typing import Iterable, List, Optional\n\nfrom .. import __version__\nfrom ..universaldetector import UniversalDetector",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2534",
    "name": "singleton.py",
    "path": "github_repo/scripts/06_development_tools/development_utilities/singleton.py",
    "category": "06_development_tools",
    "type": "utility",
    "lines": 9,
    "size": 240,
    "docstring": "",
    "keywords": [],
    "functions": [
      "__call__"
    ],
    "classes": [
      "Singleton"
    ],
    "imports": [],
    "preview": "class Singleton(type):\n\n    _instances = {}\n\n    def __call__(cls, *args, **kwargs):\n        if cls not in cls._instances:\n            cls._instances[cls] = super(Singleton, cls).__call__(*args, **kwargs)\n        return cls._instances[cls]\n",
    "last_modified": "2025-03-28T18:35:47"
  },
  {
    "id": "2535",
    "name": "plugin.py",
    "path": "github_repo/scripts/06_development_tools/development_utilities/plugin.py",
    "category": "06_development_tools",
    "type": "analysis",
    "lines": 89,
    "size": 2491,
    "docstring": "pygments.plugin\n~~~~~~~~~~~~~~~\n\nPygments plugin interface. By default, this tries to use\n``importlib.metadata``, which is in the Python standard\nlibrary since Python 3.8, or its ``importlib_metadata``\nbackport for earlier versions of Python. It falls back on\n``pkg_resources`` if not found. Finally, if ``pkg_resources``\nis not found either, no plugins are loaded at all.\n\nlexer plugins::\n\n    [pygments.lexers]\n    yourlexer = yourmodule:YourLexer\n\nformatter plugins::\n\n    [pygments.formatters]\n    yourformatter = yourformatter:YourFormatter\n    /.ext = yourformatter:YourFormatter\n\nAs you can see, you can define extensions for the formatter\nwith a leading slash.\n\nsyntax plugins::\n\n    [pygments.styles]\n    yourstyle = yourstyle:YourStyle\n\nfilter plugin::\n\n    [pygments.filter]\n    yourfilter = yourfilter:YourFilter\n\n\n:copyright: Copyright 2006-2023 by the Pygments team, see AUTHORS.\n:license: BSD, see LICENSE for details.",
    "keywords": [],
    "functions": [
      "iter_entry_points",
      "find_plugin_lexers",
      "find_plugin_formatters",
      "find_plugin_styles",
      "find_plugin_filters"
    ],
    "classes": [],
    "imports": [
      "importlib.metadata",
      "importlib_metadata",
      "pip._vendor.pkg_resources"
    ],
    "preview": "\"\"\"\npygments.plugin\n~~~~~~~~~~~~~~~\n\nPygments plugin interface. By default, this tries to use\n``importlib.metadata``, which is in the Python standard\nlibrary since Python 3.8, or its ``importlib_metadata``\nbackport for earlier versions of Python. It falls back on\n``pkg_resources`` if not found. Finally, if ``pkg_resources``\nis not found either, no plugins are loaded at all.\n\nlexer plugins::\n\n    [pygments.lexers]\n    yourlexer = yourmodule:YourLexer\n\nformatter plugins::\n\n    [pygments.formatters]\n    yourformatter = yourformatter:YourFormatter",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2536",
    "name": "tts_test.py",
    "path": "github_repo/scripts/06_development_tools/development_utilities/tts_test.py",
    "category": "06_development_tools",
    "type": "testing",
    "lines": 3,
    "size": 22,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "*",
      "clips"
    ],
    "preview": "import *\nimport clips\n",
    "last_modified": "2025-05-04T23:28:22.826071"
  },
  {
    "id": "2537",
    "name": "filetypes.py",
    "path": "github_repo/scripts/06_development_tools/development_utilities/filetypes.py",
    "category": "06_development_tools",
    "type": "organization",
    "lines": 27,
    "size": 715,
    "docstring": "Filetype information.",
    "keywords": [],
    "functions": [
      "is_archive_file"
    ],
    "classes": [],
    "imports": [
      "typing",
      "pip._internal.utils.misc"
    ],
    "preview": "\"\"\"Filetype information.\"\"\"\n\nfrom typing import Tuple\n\nfrom pip._internal.utils.misc import splitext\n\nWHEEL_EXTENSION = \".whl\"\nBZ2_EXTENSIONS: Tuple[str, ...] = (\".tar.bz2\", \".tbz\")\nXZ_EXTENSIONS: Tuple[str, ...] = (\n    \".tar.xz\",\n    \".txz\",\n    \".tlz\",\n    \".tar.lz\",\n    \".tar.lzma\",\n)\nZIP_EXTENSIONS: Tuple[str, ...] = (\".zip\", WHEEL_EXTENSION)\nTAR_EXTENSIONS: Tuple[str, ...] = (\".tar.gz\", \".tgz\", \".tar\")\nARCHIVE_EXTENSIONS = ZIP_EXTENSIONS + BZ2_EXTENSIONS + TAR_EXTENSIONS + XZ_EXTENSIONS\n\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2538",
    "name": "exceptions.py",
    "path": "github_repo/scripts/06_development_tools/development_utilities/exceptions.py",
    "category": "06_development_tools",
    "type": "utility",
    "lines": 49,
    "size": 1081,
    "docstring": "",
    "keywords": [],
    "functions": [
      "__init__",
      "__str__"
    ],
    "classes": [
      "UnpackException",
      "BufferFull",
      "OutOfData",
      "FormatError",
      "StackError",
      "ExtraData"
    ],
    "imports": [],
    "preview": "class UnpackException(Exception):\n    \"\"\"Base class for some exceptions raised while unpacking.\n\n    NOTE: unpack may raise exception other than subclass of\n    UnpackException.  If you want to catch all error, catch\n    Exception instead.\n    \"\"\"\n\n\nclass BufferFull(UnpackException):\n    pass\n\n\nclass OutOfData(UnpackException):\n    pass\n\n\nclass FormatError(ValueError, UnpackException):\n    \"\"\"Invalid msgpack format\"\"\"\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2539",
    "name": "config_20250430201604.py",
    "path": "github_repo/scripts/06_development_tools/development_utilities/config_20250430201604.py",
    "category": "06_development_tools",
    "type": "setup",
    "lines": 3,
    "size": 40,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [],
    "preview": "# config.py\nSOURCE_DIRECTORY = \"/Users\"\n",
    "last_modified": "2025-04-30T20:16:04.178949"
  },
  {
    "id": "2540",
    "name": "process_leonardo_20250102104737.py",
    "path": "github_repo/scripts/06_development_tools/development_utilities/process_leonardo_20250102104737.py",
    "category": "06_development_tools",
    "type": "utility",
    "lines": 1,
    "size": 0,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [],
    "preview": "",
    "last_modified": "2025-03-28T18:37:01.038000"
  },
  {
    "id": "2541",
    "name": "candidate.py",
    "path": "github_repo/scripts/06_development_tools/development_utilities/candidate.py",
    "category": "06_development_tools",
    "type": "utility",
    "lines": 30,
    "size": 930,
    "docstring": "",
    "keywords": [],
    "functions": [
      "__init__",
      "__repr__",
      "__str__"
    ],
    "classes": [
      "InstallationCandidate"
    ],
    "imports": [
      "pip._internal.models.link",
      "pip._internal.utils.models",
      "pip._vendor.packaging.version"
    ],
    "preview": "from pip._internal.models.link import Link\nfrom pip._internal.utils.models import KeyBasedCompareMixin\nfrom pip._vendor.packaging.version import parse as parse_version\n\n\nclass InstallationCandidate(KeyBasedCompareMixin):\n    \"\"\"Represents a potential \"candidate\" for installation.\"\"\"\n\n    __slots__ = [\"name\", \"version\", \"link\"]\n\n    def __init__(self, name: str, version: str, link: Link) -> None:\n        self.name = name\n        self.version = parse_version(version)\n        self.link = link\n\n        super().__init__(\n            key=(self.name, self.version, self.link),\n            defining_class=InstallationCandidate,\n        )\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2542",
    "name": "pangomarkup.py",
    "path": "github_repo/scripts/06_development_tools/development_utilities/pangomarkup.py",
    "category": "06_development_tools",
    "type": "utility",
    "lines": 83,
    "size": 2191,
    "docstring": "pygments.formatters.pangomarkup\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nFormatter for Pango markup output.\n\n:copyright: Copyright 2006-2023 by the Pygments team, see AUTHORS.\n:license: BSD, see LICENSE for details.",
    "keywords": [],
    "functions": [
      "escape_special_chars",
      "__init__",
      "format_unencoded"
    ],
    "classes": [
      "PangoMarkupFormatter"
    ],
    "imports": [
      "pip._vendor.pygments.formatter"
    ],
    "preview": "\"\"\"\npygments.formatters.pangomarkup\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nFormatter for Pango markup output.\n\n:copyright: Copyright 2006-2023 by the Pygments team, see AUTHORS.\n:license: BSD, see LICENSE for details.\n\"\"\"\n\nfrom pip._vendor.pygments.formatter import Formatter\n\n__all__ = [\"PangoMarkupFormatter\"]\n\n\n_escape_table = {\n    ord(\"&\"): \"&amp;\",\n    ord(\"<\"): \"&lt;\",\n}\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2543",
    "name": "format_control.py",
    "path": "github_repo/scripts/06_development_tools/development_utilities/format_control.py",
    "category": "06_development_tools",
    "type": "utility",
    "lines": 76,
    "size": 2455,
    "docstring": "",
    "keywords": [],
    "functions": [
      "__init__",
      "__eq__",
      "__repr__",
      "handle_mutual_excludes",
      "get_allowed_formats",
      "disallow_binaries"
    ],
    "classes": [
      "FormatControl"
    ],
    "imports": [
      "typing",
      "pip._internal.exceptions",
      "pip._vendor.packaging.utils"
    ],
    "preview": "from typing import FrozenSet, Optional, Set\n\nfrom pip._internal.exceptions import CommandError\nfrom pip._vendor.packaging.utils import canonicalize_name\n\n\nclass FormatControl:\n    \"\"\"Helper for managing formats from which a package can be installed.\"\"\"\n\n    __slots__ = [\"no_binary\", \"only_binary\"]\n\n    def __init__(\n        self,\n        no_binary: Optional[Set[str]] = None,\n        only_binary: Optional[Set[str]] = None,\n    ) -> None:\n        if no_binary is None:\n            no_binary = set()\n        if only_binary is None:\n            only_binary = set()",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2544",
    "name": "datetime.py",
    "path": "github_repo/scripts/06_development_tools/development_utilities/datetime.py",
    "category": "06_development_tools",
    "type": "utility",
    "lines": 11,
    "size": 241,
    "docstring": "For when pip wants to check the date or time.",
    "keywords": [],
    "functions": [
      "today_is_later_than"
    ],
    "classes": [],
    "imports": [
      "datetime"
    ],
    "preview": "\"\"\"For when pip wants to check the date or time.\"\"\"\n\nimport datetime\n\n\ndef today_is_later_than(year: int, month: int, day: int) -> bool:\n    today = datetime.date.today()\n    given = datetime.date(year, month, day)\n\n    return today > given\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2545",
    "name": "help.py",
    "path": "github_repo/scripts/06_development_tools/development_utilities/help.py",
    "category": "06_development_tools",
    "type": "utility",
    "lines": 38,
    "size": 1083,
    "docstring": "",
    "keywords": [],
    "functions": [
      "run"
    ],
    "classes": [
      "HelpCommand"
    ],
    "imports": [
      "optparse",
      "typing",
      "pip._internal.cli.base_command",
      "pip._internal.cli.status_codes",
      "pip._internal.exceptions",
      "pip._internal.commands"
    ],
    "preview": "from optparse import Values\nfrom typing import List\n\nfrom pip._internal.cli.base_command import Command\nfrom pip._internal.cli.status_codes import SUCCESS\nfrom pip._internal.exceptions import CommandError\n\n\nclass HelpCommand(Command):\n    \"\"\"Show help for commands\"\"\"\n\n    usage = \"\"\"\n      %prog <command>\"\"\"\n    ignore_require_venv = True\n\n    def run(self, options: Values, args: List[str]) -> int:\n        from pip._internal.commands import commands_dict, create_command, get_similar_commands\n\n        try:\n            # 'pip help' with no args is handled by pip.__init__.parseopt()",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2546",
    "name": "installed.py",
    "path": "github_repo/scripts/06_development_tools/development_utilities/installed.py",
    "category": "06_development_tools",
    "type": "setup",
    "lines": 30,
    "size": 842,
    "docstring": "",
    "keywords": [],
    "functions": [
      "build_tracker_id",
      "get_metadata_distribution",
      "prepare_distribution_metadata"
    ],
    "classes": [
      "InstalledDistribution"
    ],
    "imports": [
      "typing",
      "pip._internal.distributions.base",
      "pip._internal.index.package_finder",
      "pip._internal.metadata"
    ],
    "preview": "from typing import Optional\n\nfrom pip._internal.distributions.base import AbstractDistribution\nfrom pip._internal.index.package_finder import PackageFinder\nfrom pip._internal.metadata import BaseDistribution\n\n\nclass InstalledDistribution(AbstractDistribution):\n    \"\"\"Represents an installed package.\n\n    This does not need any preparation as the required information has already\n    been computed.\n    \"\"\"\n\n    @property\n    def build_tracker_id(self) -> Optional[str]:\n        return None\n\n    def get_metadata_distribution(self) -> BaseDistribution:\n        assert self.req.satisfied_by is not None, \"not actually installed\"",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2547",
    "name": "spotify.py",
    "path": "github_repo/scripts/06_development_tools/development_utilities/spotify.py",
    "category": "06_development_tools",
    "type": "utility",
    "lines": 158,
    "size": 5122,
    "docstring": "",
    "keywords": [],
    "functions": [
      "_pack_album",
      "_pack_show",
      "_pack_playlist",
      "__init__",
      "search",
      "link",
      "_get_playlist_tracks",
      "_get_show_episodes",
      "_get_artist_albums",
      "_get_artist_top"
    ],
    "classes": [
      "Spotify"
    ],
    "imports": [
      "spotipy",
      "spotipy.oauth2",
      "track",
      "types"
    ],
    "preview": "import spotipy\nfrom spotipy.oauth2 import SpotifyClientCredentials\n\nfrom .track import Track\nfrom .types import Type\n\n\nclass Spotify:\n    def __init__(self, api_credentials=None) -> None:\n        if api_credentials is None:\n            self.sp = spotipy.Spotify(client_credentials_manager=SpotifyClientCredentials())\n        else:\n            client_id, client_secret = api_credentials\n            self.sp = spotipy.Spotify(\n                client_credentials_manager=SpotifyClientCredentials(\n                    client_id=client_id, client_secret=client_secret\n                )\n            )\n\n    def search(self, query, query_type=Type.TRACK, artist_albums: bool = False) -> list:",
    "last_modified": "2025-09-13T05:55:15.849584"
  },
  {
    "id": "2548",
    "name": "clean_test.py",
    "path": "github_repo/scripts/06_development_tools/development_utilities/clean_test.py",
    "category": "06_development_tools",
    "type": "testing",
    "lines": 4,
    "size": 43,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "clips"
    ],
    "preview": "from clips import clean_temp\n\nclean_temp()\n",
    "last_modified": "2025-05-04T23:27:53.300828"
  },
  {
    "id": "2549",
    "name": "emport_prompts_20250530225118.py",
    "path": "github_repo/scripts/06_development_tools/development_utilities/emport_prompts_20250530225118.py",
    "category": "06_development_tools",
    "type": "utility",
    "lines": 1,
    "size": 0,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [],
    "preview": "",
    "last_modified": "2025-05-30T22:51:21.768054"
  },
  {
    "id": "2550",
    "name": "update-file-organization-in-organize_albums1.py",
    "path": "github_repo/scripts/06_development_tools/development_utilities/update-file-organization-in-organize_albums1.py",
    "category": "06_development_tools",
    "type": "organization",
    "lines": 2,
    "size": 41,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [],
    "preview": "45b437252556203cd3a4431bbc2f5c6592e2bc58\n",
    "last_modified": "2025-09-13T04:20:03.057518"
  },
  {
    "id": "2551",
    "name": "devices.py",
    "path": "github_repo/scripts/06_development_tools/development_utilities/devices.py",
    "category": "06_development_tools",
    "type": "utility",
    "lines": 123,
    "size": 3529,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "random"
    ],
    "preview": "import random\n\nAPP_VERSION = \"136.0.0.34.124\"\nVERSION_CODE = \"208061712\"\nDEVICES = {\n    \"one_plus_7\": {\n        \"app_version\": APP_VERSION,\n        \"android_version\": \"29\",\n        \"android_release\": \"10.0\",\n        \"dpi\": \"420dpi\",\n        \"resolution\": \"1080x2340\",\n        \"manufacturer\": \"OnePlus\",\n        \"device\": \"GM1903\",\n        \"model\": \"OnePlus7\",\n        \"cpu\": \"qcom\",\n        \"version_code\": VERSION_CODE,\n    },\n    \"one_plus_3\": {\n        \"app_version\": APP_VERSION,\n        \"android_version\": \"28\",",
    "last_modified": "2025-03-28T18:35:47"
  },
  {
    "id": "2552",
    "name": "backup_installations.py",
    "path": "github_repo/scripts/06_development_tools/development_utilities/backup_installations.py",
    "category": "06_development_tools",
    "type": "setup",
    "lines": 45,
    "size": 1307,
    "docstring": "",
    "keywords": [],
    "functions": [
      "run_command"
    ],
    "classes": [],
    "imports": [
      "csv",
      "os",
      "subprocess"
    ],
    "preview": "import csv\nimport os\nimport subprocess\n\n\ndef run_command(command):\n    try:\n        result = subprocess.check_output(command, shell=True, universal_newlines=True)\n        return result.strip().split(\"\\n\")\n    except subprocess.CalledProcessError as e:\n        return [str(e)]\n\n\n# Define the commands to gather information\ncommands = {\n    \"Python Installations\": [\n        \"ls /usr/local/bin/python*\",\n        \"ls /usr/bin/python*\",\n        \"ls /Library/Frameworks/Python.framework/Versions/\",\n    ],",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2553",
    "name": "config_20250430201601.py",
    "path": "github_repo/scripts/06_development_tools/development_utilities/config_20250430201601.py",
    "category": "06_development_tools",
    "type": "setup",
    "lines": 3,
    "size": 35,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [],
    "preview": "# config.py\nSOURCE_DIRECTORY = \"/\"\n",
    "last_modified": "2025-04-30T20:16:01.553254"
  },
  {
    "id": "2554",
    "name": "redis_cache.py",
    "path": "github_repo/scripts/06_development_tools/development_utilities/redis_cache.py",
    "category": "06_development_tools",
    "type": "utility",
    "lines": 46,
    "size": 1371,
    "docstring": "",
    "keywords": [],
    "functions": [
      "__init__",
      "get",
      "set",
      "delete",
      "clear",
      "close"
    ],
    "classes": [
      "RedisCache"
    ],
    "imports": [
      "__future__",
      "datetime",
      "typing",
      "pip._vendor.cachecontrol.cache",
      "redis"
    ],
    "preview": "# SPDX-FileCopyrightText: 2015 Eric Larson\n#\n# SPDX-License-Identifier: Apache-2.0\nfrom __future__ import annotations\n\nfrom datetime import datetime, timezone\nfrom typing import TYPE_CHECKING\n\nfrom pip._vendor.cachecontrol.cache import BaseCache\n\nif TYPE_CHECKING:\n    from redis import Redis\n\n\nclass RedisCache(BaseCache):\n    def __init__(self, conn: Redis[bytes]) -> None:\n        self.conn = conn\n\n    def get(self, key: str) -> bytes | None:\n        return self.conn.get(key)",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2555",
    "name": "scheme.py",
    "path": "github_repo/scripts/06_development_tools/development_utilities/scheme.py",
    "category": "06_development_tools",
    "type": "web_tools",
    "lines": 31,
    "size": 737,
    "docstring": "For types associated with installation schemes.\n\nFor a general overview of available schemes and their context, see\nhttps://docs.python.org/3/install/index.html#alternate-installation.",
    "keywords": [],
    "functions": [
      "__init__"
    ],
    "classes": [
      "Scheme"
    ],
    "imports": [],
    "preview": "\"\"\"\nFor types associated with installation schemes.\n\nFor a general overview of available schemes and their context, see\nhttps://docs.python.org/3/install/index.html#alternate-installation.\n\"\"\"\n\nSCHEME_KEYS = [\"platlib\", \"purelib\", \"headers\", \"scripts\", \"data\"]\n\n\nclass Scheme:\n    \"\"\"A Scheme holds paths which are used as the base directories for\n    artifacts associated with a Python package.\n    \"\"\"\n\n    __slots__ = SCHEME_KEYS\n\n    def __init__(\n        self,\n        platlib: str,",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2556",
    "name": "base.py",
    "path": "github_repo/scripts/06_development_tools/development_utilities/base.py",
    "category": "06_development_tools",
    "type": "utility",
    "lines": 17,
    "size": 563,
    "docstring": "",
    "keywords": [],
    "functions": [
      "resolve",
      "get_installation_order"
    ],
    "classes": [
      "BaseResolver"
    ],
    "imports": [
      "typing",
      "pip._internal.req.req_install",
      "pip._internal.req.req_set"
    ],
    "preview": "from typing import Callable, List, Optional\n\nfrom pip._internal.req.req_install import InstallRequirement\nfrom pip._internal.req.req_set import RequirementSet\n\nInstallRequirementProvider = Callable[[str, Optional[InstallRequirement]], InstallRequirement]\n\n\nclass BaseResolver:\n    def resolve(\n        self, root_reqs: List[InstallRequirement], check_supported_wheels: bool\n    ) -> RequirementSet:\n        raise NotImplementedError()\n\n    def get_installation_order(self, req_set: RequirementSet) -> List[InstallRequirement]:\n        raise NotImplementedError()\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2557",
    "name": "quickstart.py",
    "path": "github_repo/scripts/06_development_tools/development_utilities/quickstart.py",
    "category": "06_development_tools",
    "type": "utility",
    "lines": 37,
    "size": 1028,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "instapy"
    ],
    "preview": "# imports\nfrom instapy import InstaPy, smart_run\n\n# login credentials\ninsta_username = \"\"\ninsta_password = \"\"\n\ncomments = [\n    \"Nice shot! @{}\",\n    \"I love your profile! @{}\",\n    \"Your feed is an inspiration :thumbsup:\",\n    \"Just incredible :open_mouth:\",\n    \"What camera did you use @{}?\",\n    \"Love your posts @{}\",\n    \"Looks awesome @{}\",\n    \"Getting inspired by you @{}\",\n    \":raised_hands: Yes!\",\n    \"I can feel your passion @{} :muscle:\",\n]\n",
    "last_modified": "2025-09-13T05:53:49.242462"
  },
  {
    "id": "2558",
    "name": "deepseek_python_20250608130223.py",
    "path": "github_repo/scripts/06_development_tools/development_utilities/deepseek_python_20250608130223.py",
    "category": "06_development_tools",
    "type": "utility",
    "lines": 2,
    "size": 7,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [],
    "preview": "format\n",
    "last_modified": "2025-09-13T05:53:27.376531"
  },
  {
    "id": "2559",
    "name": "ext.py",
    "path": "github_repo/scripts/06_development_tools/development_utilities/ext.py",
    "category": "06_development_tools",
    "type": "utility",
    "lines": 185,
    "size": 5972,
    "docstring": "",
    "keywords": [],
    "functions": [
      "__new__",
      "__init__",
      "__repr__",
      "__eq__",
      "__ne__",
      "__hash__",
      "from_bytes",
      "to_bytes",
      "from_unix",
      "to_unix"
    ],
    "classes": [
      "ExtType",
      "Timestamp"
    ],
    "imports": [
      "datetime",
      "struct",
      "sys",
      "collections"
    ],
    "preview": "# coding: utf-8\nimport datetime\nimport struct\nimport sys\nfrom collections import namedtuple\n\nPY2 = sys.version_info[0] == 2\n\nif PY2:\n    int_types = (int, long)\n    _utc = None\nelse:\n    int_types = int\n    try:\n        _utc = datetime.timezone.utc\n    except AttributeError:\n        _utc = datetime.timezone(datetime.timedelta(0))\n\n\nclass ExtType(namedtuple(\"ExtType\", \"code data\")):",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2560",
    "name": "colors.py",
    "path": "github_repo/scripts/06_development_tools/development_utilities/colors.py",
    "category": "06_development_tools",
    "type": "utility",
    "lines": 81,
    "size": 1418,
    "docstring": "",
    "keywords": [],
    "functions": [
      "randomize",
      "randomize1",
      "randomize2",
      "randomize3",
      "yellow",
      "cyan",
      "red",
      "white",
      "green",
      "magento"
    ],
    "classes": [
      "get_colors"
    ],
    "imports": [
      "random"
    ],
    "preview": "#!/usr/bin/python3\n# Created By ybenel\nfrom random import randint\n\nlist = [\n    \"\\033[1;33m\",\n    \"\\033[1;34m\",\n    \"\\033[1;30m\",\n    \"\\033[1;36m\",\n    \"\\033[1;31m\",\n    \"\\033[35m\",\n    \"\\033[95m\",\n    \"\\033[96m\",\n    \"\\033[39m\",\n    \"\\033[38;5;82m\",\n    \"\\033[38;5;198m\",\n    \"\\033[38;5;208m\",\n    \"\\033[38;5;167m\",\n    \"\\033[38;5;91m\",\n    \"\\033[38;5;210m\",",
    "last_modified": "2025-03-28T18:35:48.836972"
  },
  {
    "id": "2561",
    "name": "auth.py",
    "path": "github_repo/scripts/03_automation_platforms/youtube_automation/auth.py",
    "category": "03_automation_platforms",
    "type": "utility",
    "lines": 35,
    "size": 953,
    "docstring": "",
    "keywords": [],
    "functions": [
      "get_authenticated_service"
    ],
    "classes": [],
    "imports": [
      "httplib2",
      "googleapiclient.discovery",
      "oauth2client.client",
      "oauth2client.file",
      "oauth2client.tools",
      "constants",
      "utils"
    ],
    "preview": "import httplib2\nfrom googleapiclient.discovery import Resource, build\nfrom oauth2client.client import flow_from_clientsecrets\nfrom oauth2client.file import Storage\nfrom oauth2client.tools import run_flow\n\nfrom .constants import (\n    CLIENT_SECRETS_FILE,\n    MISSING_CLIENT_SECRETS_MESSAGE,\n    YOUTUBE_API_SCOPES,\n    YOUTUBE_API_SERVICE_NAME,\n    YOUTUBE_API_VERSION,\n)\nfrom .utils import get_local_path\n\n\ndef get_authenticated_service() -> Resource:\n    flow = flow_from_clientsecrets(\n        get_local_path(CLIENT_SECRETS_FILE),\n        scope=YOUTUBE_API_SCOPES,",
    "last_modified": "2025-09-13T05:53:47.043447"
  },
  {
    "id": "2562",
    "name": "set_github_vars.py",
    "path": "github_repo/scripts/03_automation_platforms/youtube_automation/set_github_vars.py",
    "category": "03_automation_platforms",
    "type": "utility",
    "lines": 44,
    "size": 1141,
    "docstring": "",
    "keywords": [],
    "functions": [
      "get_os",
      "get_cpu_architecture",
      "extract_app_version",
      "write_to_github_env"
    ],
    "classes": [],
    "imports": [
      "os",
      "platform",
      "subprocess"
    ],
    "preview": "import os\nimport platform\nimport subprocess\n\n\ndef get_os():\n    return platform.system()\n\n\ndef get_cpu_architecture():\n    # e.g. x86_64, arm64\n    return platform.machine()\n\n\ndef extract_app_version():\n    result = subprocess.run([\"youtube-bulk-upload\", \"--version\"], capture_output=True, text=True)\n    version = result.stdout.strip().split()[-1]\n    return version\n\n",
    "last_modified": "2025-09-13T05:53:46.179727"
  },
  {
    "id": "2563",
    "name": "test_auth.py",
    "path": "github_repo/scripts/03_automation_platforms/youtube_automation/test_auth.py",
    "category": "03_automation_platforms",
    "type": "testing",
    "lines": 21,
    "size": 636,
    "docstring": "",
    "keywords": [
      "testing"
    ],
    "functions": [
      "test_authentication"
    ],
    "classes": [],
    "imports": [
      "google_auth_oauthlib.flow"
    ],
    "preview": "import google_auth_oauthlib.flow\n\nCLIENT_SECRETS_FILE = \"client_secret.json\"  # Ensure this file is in the same folder\nSCOPES = [\"https://www.googleapis.com/auth/youtube.upload\"]\n\n\ndef test_authentication():\n    \"\"\"Tests OAuth authentication with YouTube.\"\"\"\n    try:\n        flow = google_auth_oauthlib.flow.InstalledAppFlow.from_client_secrets_file(\n            CLIENT_SECRETS_FILE, SCOPES\n        )\n        credentials = flow.run_local_server(port=0)\n        print(\"\u2705 Authentication successful!\")\n    except Exception as e:\n        print(\"\u274c Authentication failed:\", str(e))\n\n\nif __name__ == \"__main__\":\n    test_authentication()",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2564",
    "name": "bypass.py",
    "path": "github_repo/scripts/03_automation_platforms/youtube_automation/bypass.py",
    "category": "03_automation_platforms",
    "type": "organization",
    "lines": 127,
    "size": 4551,
    "docstring": "MIT License\n\nCopyright (c) 2021-2022 MShawon\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.",
    "keywords": [],
    "functions": [
      "ensure_click",
      "personalization",
      "bypass_consent",
      "click_popup",
      "bypass_popup",
      "bypass_other_popup"
    ],
    "classes": [],
    "imports": [
      "random",
      "time",
      "selenium",
      "selenium.common.exceptions",
      "selenium.webdriver.chrome.service",
      "selenium.webdriver.common.by",
      "selenium.webdriver.common.keys",
      "selenium.webdriver.support",
      "selenium.webdriver.support.ui"
    ],
    "preview": "\"\"\"\nMIT License\n\nCopyright (c) 2021-2022 MShawon\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,",
    "last_modified": "2025-09-13T05:54:11.615935"
  },
  {
    "id": "2565",
    "name": "APIHandler.py",
    "path": "github_repo/scripts/03_automation_platforms/youtube_automation/APIHandler.py",
    "category": "03_automation_platforms",
    "type": "web_tools",
    "lines": 59,
    "size": 2259,
    "docstring": "",
    "keywords": [
      "web_tools"
    ],
    "functions": [
      "get_yt_playlist_size",
      "get_new_twitch_token",
      "get_twitch_game_id"
    ],
    "classes": [
      "APIHandler"
    ],
    "imports": [
      "logging",
      "requests",
      "src.utils",
      "config"
    ],
    "preview": "import logging\n\nimport requests\nimport src.utils as utils\n\nimport config\n\n\nclass APIHandler:\n    @staticmethod\n    def get_yt_playlist_size(playlist_id: str) -> int:\n        logging.info(\"Getting amount of playlist items\")\n        try:\n            config.YT_API_KEY\n        except AttributeError:\n            logging.warning(f\"No YT_API_KEY provided in config.py -> return playlist_size of 0\")\n            return 0\n        url = f\"https://www.googleapis.com/youtube/v3/playlistItems\"\n        payload = {\"part\": \"id\", \"playlistId\": playlist_id, \"key\": config.YT_API_KEY}\n        resp = requests.get(url, params=payload, headers={})",
    "last_modified": "2025-09-13T05:53:45.320054"
  },
  {
    "id": "2566",
    "name": "constants.py",
    "path": "github_repo/scripts/03_automation_platforms/youtube_automation/constants.py",
    "category": "03_automation_platforms",
    "type": "utility",
    "lines": 66,
    "size": 2318,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "os",
      "httplib2"
    ],
    "preview": "import os\n\nimport httplib2\n\n# Explicitly tell the underlying HTTP transport library not to retry, since\n# we are handling retry logic ourselves.\nhttplib2.RETRIES = 1\n\n# Maximum number of times to retry before giving up.\nMAX_RETRIES = 10\n\n# Always retry when these exceptions are raised.\nRETRIABLE_EXCEPTIONS = (httplib2.HttpLib2Error, IOError)\n\n# Always retry when an apiclient.errors.HttpError with one of these status\n# codes is raised.\nRETRIABLE_STATUS_CODES = [500, 502, 503, 504]\n\n# The CLIENT_SECRETS_FILE variable specifies the name of a file that contains\n# the OAuth 2.0 information for this application, including its client_id and",
    "last_modified": "2025-05-04T22:47:12"
  },
  {
    "id": "2567",
    "name": "yt_plists_to_csv.py",
    "path": "github_repo/scripts/03_automation_platforms/youtube_automation/yt_plists_to_csv.py",
    "category": "03_automation_platforms",
    "type": "utility",
    "lines": 49,
    "size": 1083,
    "docstring": "",
    "keywords": [],
    "functions": [
      "main"
    ],
    "classes": [],
    "imports": [
      "csv",
      "os",
      "re",
      "sys"
    ],
    "preview": "#!/usr/bin/python\n# Copyright 2018 stringCode ltd.\n# Licensed under the Apache License, Version 2.0\n# http://www.apache.org/licenses/LICENSE-2.0\n\nimport csv\nimport os\nimport re\nimport sys\n\n\"\"\"YouTube playlist name & url extractor\n\nExtracts names and urls form YouTube playlist html page. Writes them to csv.\n\"\"\"\n\ndef main():\n    # Handle args\n    args = sys.argv[1:]\n    if not args:\n        print 'usage: file'",
    "last_modified": "2025-03-28T18:36:55"
  },
  {
    "id": "2568",
    "name": "secret.py",
    "path": "github_repo/scripts/03_automation_platforms/youtube_automation/secret.py",
    "category": "03_automation_platforms",
    "type": "utility",
    "lines": 4,
    "size": 176,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [],
    "preview": "SPOTIFY_CLIENT_ID = \"28b20556906f4b75874c4ae98320c81d\"\nSPOTIFY_CLIENT_SECRET = \"c7033fd14e1247cfb9eef73874dd2365\"\nYOUTUBE_CLIENT_ID = \"AIzaSyAAWcCkndgVzQBgbV0hk2UbRE0a1uO-5H8\"\n",
    "last_modified": "2022-12-30T18:09:58"
  },
  {
    "id": "2569",
    "name": "proxies.py",
    "path": "github_repo/scripts/03_automation_platforms/youtube_automation/proxies.py",
    "category": "03_automation_platforms",
    "type": "organization",
    "lines": 148,
    "size": 4259,
    "docstring": "MIT License\n\nCopyright (c) 2021-2022 MShawon\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.",
    "keywords": [
      "web_tools"
    ],
    "functions": [
      "gather_proxy",
      "load_proxy",
      "scrape_api",
      "check_proxy"
    ],
    "classes": [],
    "imports": [
      "sys",
      "random",
      "requests",
      "colors"
    ],
    "preview": "\"\"\"\nMIT License\n\nCopyright (c) 2021-2022 MShawon\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,",
    "last_modified": "2025-05-04T23:27:45"
  },
  {
    "id": "2570",
    "name": "yt_plists_to_csv 2.py",
    "path": "github_repo/scripts/03_automation_platforms/youtube_automation/yt_plists_to_csv 2.py",
    "category": "03_automation_platforms",
    "type": "utility",
    "lines": 49,
    "size": 1083,
    "docstring": "",
    "keywords": [],
    "functions": [
      "main"
    ],
    "classes": [],
    "imports": [
      "csv",
      "os",
      "re",
      "sys"
    ],
    "preview": "#!/usr/bin/python\n# Copyright 2018 stringCode ltd.\n# Licensed under the Apache License, Version 2.0\n# http://www.apache.org/licenses/LICENSE-2.0\n\nimport csv\nimport os\nimport re\nimport sys\n\n\"\"\"YouTube playlist name & url extractor\n\nExtracts names and urls form YouTube playlist html page. Writes them to csv.\n\"\"\"\n\ndef main():\n    # Handle args\n    args = sys.argv[1:]\n    if not args:\n        print 'usage: file'",
    "last_modified": "2025-08-06T14:07:36.329219"
  },
  {
    "id": "2571",
    "name": "proxy_harvester.py_02.py",
    "path": "github_repo/scripts/03_automation_platforms/proxy_harvester.py_consolidated/proxy_harvester.py_02.py",
    "category": "03_automation_platforms",
    "type": "utility",
    "lines": 39,
    "size": 1025,
    "docstring": "",
    "keywords": [
      "web_tools"
    ],
    "functions": [
      "find_proxies"
    ],
    "classes": [],
    "imports": [
      "asyncio",
      "libs.utils",
      "proxybroker",
      "requests"
    ],
    "preview": "import asyncio\n\nfrom libs.utils import ask_question, print_error, print_status, print_success\nfrom proxybroker import Broker\nfrom requests import get\n\n\nasync def show(proxies, proxy_list):\n    while len(proxy_list) < 50:\n        proxy = await proxies.get()\n        if proxy is None:\n            break\n\n        print_success(\n            \"[\" + str(len(proxy_list) + 1) + \"/50]\",\n            \"Proxy found:\",\n            proxy.as_json()[\"host\"] + \":\" + str(proxy.as_json()[\"port\"]),\n        )\n\n        proxy_list.append(proxy.as_json()[\"host\"] + \":\" + str(proxy.as_json()[\"port\"]))",
    "last_modified": "2025-09-13T05:53:40.916815"
  },
  {
    "id": "2572",
    "name": "proxy_harvester.py.py",
    "path": "github_repo/scripts/03_automation_platforms/proxy_harvester.py_consolidated/proxy_harvester.py.py",
    "category": "03_automation_platforms",
    "type": "utility",
    "lines": 151,
    "size": 6694,
    "docstring": "",
    "keywords": [
      "web_tools"
    ],
    "functions": [
      "find_proxies"
    ],
    "classes": [],
    "imports": [
      "os",
      "random",
      "time",
      "os",
      "requests",
      "colorama",
      "colorama",
      "libs.animation"
    ],
    "preview": "#!/usr/bin/env python3\nimport os  # line:4\nimport random  # line:8\nimport time  # line:7\nfrom os import _exit  # line:5\n\nimport requests  # line:2\nfrom colorama import *  # line:3\nfrom colorama import Back, Fore, Style  # line:6\nfrom libs.animation import colorText  # line:9\n\ninit(convert=True)  # line:10\nos.system(\"cls\" if os.name == \"nt\" else \"clear\")  # line:11\nrhttps = requests.get(\n    \"https://api.proxyscrape.com/?request=displayproxies&proxytype=https&timeout=7000&country=ALL&anonymity=elite&ssl=no\"\n)  # line:12\nrhttp = requests.get(\n    \"https://api.proxyscrape.com/?request=displayproxies&proxytype=http&timeout=7000&country=ALL&anonymity=elite&ssl=no\"\n)  # line:13\nrs4 = requests.get(\"https://www.proxy-list.download/api/v1/get?type=socks4\")  # line:14",
    "last_modified": "2025-05-04T23:28:20"
  },
  {
    "id": "2573",
    "name": "config.py.py",
    "path": "github_repo/scripts/03_automation_platforms/config.py_consolidated/config.py.py",
    "category": "03_automation_platforms",
    "type": "setup",
    "lines": 36,
    "size": 1320,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "json",
      "loguru",
      "llm_engineering.settings",
      "sagemaker.compute_resource_requirements.resource_requirements"
    ],
    "preview": "import json\n\nfrom loguru import logger\n\ntry:\n    from sagemaker.compute_resource_requirements.resource_requirements import ResourceRequirements\nexcept ModuleNotFoundError:\n    logger.warning(\n        \"Couldn't load SageMaker imports. Run 'poetry install --with aws' to support AWS.\"\n    )\n\nfrom llm_engineering.settings import settings\n\nhugging_face_deploy_config = {\n    \"HF_MODEL_ID\": settings.HF_MODEL_ID,\n    \"HUGGING_FACE_HUB_TOKEN\": settings.HUGGINGFACE_ACCESS_TOKEN,\n    \"SM_NUM_GPUS\": json.dumps(settings.SM_NUM_GPUS),  # Number of GPU used per replica\n    \"MAX_INPUT_LENGTH\": json.dumps(settings.MAX_INPUT_LENGTH),  # Max length of input text\n    \"MAX_TOTAL_TOKENS\": json.dumps(\n        settings.MAX_TOTAL_TOKENS",
    "last_modified": "2025-09-13T05:53:41.920509"
  },
  {
    "id": "2574",
    "name": "config.py_02.py",
    "path": "github_repo/scripts/03_automation_platforms/config.py_consolidated/config.py_02.py",
    "category": "03_automation_platforms",
    "type": "setup",
    "lines": 6,
    "size": 307,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [],
    "preview": "# Obtain Praw credentails and add your client id and secret below in their respected fields\n# For help try visiting: https://www.jcchouinard.com/get-reddit-api-credentials-with-praw/\n#              and: reddit.com/prefs/apps\n\nPRAW_CONFIG = {\"client_id\": \"\", \"client_secret\": \"\", \"user_agent\": \"user-agent\"}\n",
    "last_modified": "2025-09-13T05:53:51.719499"
  },
  {
    "id": "2575",
    "name": "like_timeline_feed.py",
    "path": "github_repo/scripts/03_automation_platforms/api_integrations/like_timeline_feed.py",
    "category": "03_automation_platforms",
    "type": "utility",
    "lines": 17,
    "size": 242,
    "docstring": "instabot example\n\nWorkflow:\n    Like rescent medias from your timeline feed.",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "os",
      "sys",
      "instabot"
    ],
    "preview": "\"\"\"\ninstabot example\n\nWorkflow:\n    Like rescent medias from your timeline feed.\n\"\"\"\n\nimport os\nimport sys\n\nsys.path.append(os.path.join(sys.path[0], \"../\"))\nfrom instabot import Bot  # noqa: E402\n\nbot = Bot()\nbot.login()\nbot.like_timeline()\n",
    "last_modified": "2025-05-04T23:28:20"
  },
  {
    "id": "2576",
    "name": "like_medias_by_location.py",
    "path": "github_repo/scripts/03_automation_platforms/api_integrations/like_medias_by_location.py",
    "category": "03_automation_platforms",
    "type": "utility",
    "lines": 99,
    "size": 3008,
    "docstring": "instabot example\n\nWorkflow:\n    Like medias by location.",
    "keywords": [],
    "functions": [
      "like_location_feed"
    ],
    "classes": [],
    "imports": [
      "argparse",
      "codecs",
      "os",
      "sys",
      "tqdm",
      "instabot"
    ],
    "preview": "# coding=utf-8\n\"\"\"\ninstabot example\n\nWorkflow:\n    Like medias by location.\n\"\"\"\n\nimport argparse\nimport codecs\nimport os\nimport sys\n\nfrom tqdm import tqdm\n\nstdout = sys.stdout\nsys.stdout = codecs.getwriter(\"utf8\")(sys.stdout)\n\nsys.path.append(os.path.join(sys.path[0], \"../\"))\nfrom instabot import Bot  # noqa: E402",
    "last_modified": "2025-09-13T05:54:55.301798"
  },
  {
    "id": "2577",
    "name": "chaos_scheduler.py",
    "path": "github_repo/scripts/03_automation_platforms/api_integrations/chaos_scheduler.py",
    "category": "03_automation_platforms",
    "type": "utility",
    "lines": 20,
    "size": 640,
    "docstring": "",
    "keywords": [],
    "functions": [
      "__init__",
      "_generate_quantum_seed",
      "schedule_operation"
    ],
    "classes": [
      "ChaosScheduler"
    ],
    "imports": [
      "hashlib",
      "random",
      "datetime"
    ],
    "preview": "import hashlib\nimport random\nfrom datetime import datetime\n\n\nclass ChaosScheduler:\n    def __init__(self, seed=None):\n        self.seed = seed or self._generate_quantum_seed()\n\n    def _generate_quantum_seed(self):\n        ts = str(datetime.now().timestamp()).encode()\n        return int(hashlib.sha3_256(ts).hexdigest(), 16) % 2**32\n\n    def schedule_operation(self, operation, criticality=3):\n        random.seed(self.seed)\n        chaos_level = (criticality * 0.15) + (random.random() * 0.35)\n        if random.random() < chaos_level:\n            raise RuntimeError(f\"Chaos failure (Level {chaos_level:.2f})\")\n        return operation()\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2578",
    "name": "bot_cache.py",
    "path": "github_repo/scripts/03_automation_platforms/api_integrations/bot_cache.py",
    "category": "03_automation_platforms",
    "type": "utility",
    "lines": 15,
    "size": 347,
    "docstring": "",
    "keywords": [],
    "functions": [
      "__init__",
      "__repr__"
    ],
    "classes": [
      "BotCache"
    ],
    "imports": [
      "instabot.singleton"
    ],
    "preview": "from instabot.singleton import Singleton\n\n\nclass BotCache(object):\n    __metaclass__ = Singleton\n\n    def __init__(self):\n        self.following = None\n        self.followers = None\n        self.user_infos = {}  # User info cache\n        self.usernames = {}  # `username` to `user_id` mapping\n\n    def __repr__(self):\n        return self.__dict__\n",
    "last_modified": "2025-03-28T18:35:47"
  },
  {
    "id": "2579",
    "name": "labels.py",
    "path": "github_repo/scripts/03_automation_platforms/api_integrations/labels.py",
    "category": "03_automation_platforms",
    "type": "web_tools",
    "lines": 232,
    "size": 6833,
    "docstring": "webencodings.labels\n~~~~~~~~~~~~~~~~~~~\n\nMap encoding labels to their name.\n\n:copyright: Copyright 2012 by Simon Sapin\n:license: BSD, see LICENSE for details.",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [],
    "preview": "\"\"\"\n\nwebencodings.labels\n~~~~~~~~~~~~~~~~~~~\n\nMap encoding labels to their name.\n\n:copyright: Copyright 2012 by Simon Sapin\n:license: BSD, see LICENSE for details.\n\n\"\"\"\n\n# XXX Do not edit!\n# This file is automatically generated by mklabels.py\n\nLABELS = {\n    \"unicode-1-1-utf-8\": \"utf-8\",\n    \"utf-8\": \"utf-8\",\n    \"utf8\": \"utf-8\",\n    \"866\": \"ibm866\",",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2580",
    "name": "like_users.py",
    "path": "github_repo/scripts/03_automation_platforms/api_integrations/like_users.py",
    "category": "03_automation_platforms",
    "type": "utility",
    "lines": 28,
    "size": 710,
    "docstring": "instabot example\n\nWorkflow:\n    Like last medias by users.",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "argparse",
      "os",
      "sys",
      "instabot"
    ],
    "preview": "\"\"\"\ninstabot example\n\nWorkflow:\n    Like last medias by users.\n\"\"\"\n\nimport argparse\nimport os\nimport sys\n\nsys.path.append(os.path.join(sys.path[0], \"../\"))\nfrom instabot import Bot  # noqa: E402\n\nparser = argparse.ArgumentParser(add_help=True)\nparser.add_argument(\"-u\", type=str, help=\"username\")\nparser.add_argument(\"-p\", type=str, help=\"password\")\nparser.add_argument(\"-l\", type=int, help=\"limit\")\nparser.add_argument(\"-proxy\", type=str, help=\"proxy\")\nparser.add_argument(\"users\", type=str, nargs=\"+\", help=\"users\")",
    "last_modified": "2025-03-28T18:35:47"
  },
  {
    "id": "2581",
    "name": "like_and_follow_media_likers.py",
    "path": "github_repo/scripts/03_automation_platforms/api_integrations/like_and_follow_media_likers.py",
    "category": "03_automation_platforms",
    "type": "utility",
    "lines": 44,
    "size": 1041,
    "docstring": "instabot example\n\nWorkflow:\n    Like and follow you last media likers.",
    "keywords": [],
    "functions": [
      "like_and_follow",
      "like_and_follow_media_likers"
    ],
    "classes": [],
    "imports": [
      "argparse",
      "os",
      "random",
      "sys",
      "time",
      "tqdm",
      "instabot"
    ],
    "preview": "\"\"\"\ninstabot example\n\nWorkflow:\n    Like and follow you last media likers.\n\"\"\"\n\nimport argparse\nimport os\nimport random\nimport sys\nimport time\n\nfrom tqdm import tqdm\n\nsys.path.append(os.path.join(sys.path[0], \"../\"))\nfrom instabot import Bot  # noqa: E402\n\n\ndef like_and_follow(bot, user_id, nlikes=3):",
    "last_modified": "2025-03-28T18:35:47"
  },
  {
    "id": "2582",
    "name": "interact_DM.py",
    "path": "github_repo/scripts/03_automation_platforms/api_integrations/interact_DM.py",
    "category": "03_automation_platforms",
    "type": "utility",
    "lines": 58,
    "size": 1493,
    "docstring": "instabot example\n\nWorkflow:\n    read and reply your DM",
    "keywords": [],
    "functions": [
      "choice"
    ],
    "classes": [],
    "imports": [
      "argparse",
      "os",
      "sys",
      "instabot"
    ],
    "preview": "# -*- coding: utf-8 -*-\n\"\"\"\ninstabot example\n\nWorkflow:\n    read and reply your DM\n\"\"\"\n\nimport argparse\nimport os\nimport sys\n\nsys.path.append(os.path.join(sys.path[0], \"../\"))\nfrom instabot import Bot  # noqa: E402\n\ntry:\n    input = raw_input\nexcept NameError:\n    pass\n",
    "last_modified": "2025-03-28T18:35:47"
  },
  {
    "id": "2583",
    "name": "like_and_follow_your_last_media_likers.py",
    "path": "github_repo/scripts/03_automation_platforms/api_integrations/like_and_follow_your_last_media_likers.py",
    "category": "03_automation_platforms",
    "type": "utility",
    "lines": 49,
    "size": 1229,
    "docstring": "instabot example\n\nWorkflow:\n    Like and follow likers of last medias from your timeline feed.",
    "keywords": [],
    "functions": [
      "like_and_follow",
      "like_and_follow_media_likers",
      "like_and_follow_your_feed_likers"
    ],
    "classes": [],
    "imports": [
      "argparse",
      "os",
      "random",
      "sys",
      "time",
      "tqdm",
      "instabot"
    ],
    "preview": "\"\"\"\ninstabot example\n\nWorkflow:\n    Like and follow likers of last medias from your timeline feed.\n\"\"\"\n\nimport argparse\nimport os\nimport random\nimport sys\nimport time\n\nfrom tqdm import tqdm\n\nsys.path.append(os.path.join(sys.path[0], \"../\"))\nfrom instabot import Bot  # noqa: E402\n\n\ndef like_and_follow(bot, user_id, nlikes=3):",
    "last_modified": "2025-03-28T18:35:47"
  },
  {
    "id": "2584",
    "name": "like_example.py",
    "path": "github_repo/scripts/03_automation_platforms/api_integrations/like_example.py",
    "category": "03_automation_platforms",
    "type": "utility",
    "lines": 35,
    "size": 555,
    "docstring": "instabot example\n\nWorkflow:\n1) likes your timeline feed\n2) likes user's feed\n\nNotes:\n1) You should pass user_id, not username",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "os",
      "sys",
      "instabot"
    ],
    "preview": "\"\"\"\ninstabot example\n\nWorkflow:\n1) likes your timeline feed\n2) likes user's feed\n\nNotes:\n1) You should pass user_id, not username\n\"\"\"\n\nimport os\nimport sys\n\nsys.path.append(os.path.join(sys.path[0], \"../\"))\nfrom instabot import Bot  # noqa: E402\n\nbot = Bot()\nbot.login()\n",
    "last_modified": "2025-03-28T18:35:47"
  },
  {
    "id": "2585",
    "name": "super_simple_setting_for_crontab.py",
    "path": "github_repo/scripts/03_automation_platforms/api_integrations/super_simple_setting_for_crontab.py",
    "category": "03_automation_platforms",
    "type": "utility",
    "lines": 66,
    "size": 1518,
    "docstring": "This template is written by @Edhim\n\nWhat does this quickstart script aim to do?\n- I am using simple settings for my personal account with a crontab each 3H,\nit's been working since 5 months with no problem.",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "instapy"
    ],
    "preview": "\"\"\"\nThis template is written by @Edhim\n\nWhat does this quickstart script aim to do?\n- I am using simple settings for my personal account with a crontab each 3H,\nit's been working since 5 months with no problem.\n\"\"\"\n\nfrom instapy import InstaPy, smart_run\n\n# get a session!\nsession = InstaPy(username=\"\", password=\"\")\n\n# let's go! :>\nwith smart_run(session):\n    # settings\n    session.set_relationship_bounds(\n        enabled=False,\n        potency_ratio=-1.21,\n        delimit_by_numbers=True,",
    "last_modified": "2025-03-28T18:36:56"
  },
  {
    "id": "2586",
    "name": "save_users_followers_into_file.py",
    "path": "github_repo/scripts/03_automation_platforms/api_integrations/save_users_followers_into_file.py",
    "category": "03_automation_platforms",
    "type": "organization",
    "lines": 30,
    "size": 780,
    "docstring": "instabot example\n\nWorkflow:\n    Save users' followers into a file.",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "argparse",
      "os",
      "sys",
      "instabot"
    ],
    "preview": "\"\"\"\ninstabot example\n\nWorkflow:\n    Save users' followers into a file.\n\"\"\"\n\nimport argparse\nimport os\nimport sys\n\nsys.path.append(os.path.join(sys.path[0], \"../\"))\nfrom instabot import Bot, utils  # noqa: E402\n\nparser = argparse.ArgumentParser(add_help=False)\nparser.add_argument(\"-u\", type=str, help=\"username\")\nparser.add_argument(\"-p\", type=str, help=\"password\")\nparser.add_argument(\"-proxy\", type=str, help=\"proxy\")\nparser.add_argument(\"filename\", type=str, help=\"filename\")\nparser.add_argument(\"users\", type=str, nargs=\"+\", help=\"users\")",
    "last_modified": "2025-03-28T18:35:47"
  },
  {
    "id": "2587",
    "name": "help_text.py",
    "path": "github_repo/scripts/03_automation_platforms/api_integrations/help_text.py",
    "category": "03_automation_platforms",
    "type": "utility",
    "lines": 85,
    "size": 2549,
    "docstring": "",
    "keywords": [],
    "functions": [
      "GetExpiryDate"
    ],
    "classes": [],
    "imports": [
      "logging",
      "os",
      "sqlite3",
      "pyrogram",
      "translation",
      "helper_funcs.chat_base",
      "sample_config",
      "config"
    ],
    "preview": "import logging\n\nlogging.basicConfig(\n    level=logging.DEBUG, format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\n)\nlogger = logging.getLogger(__name__)\n\nimport os\nimport sqlite3\n\n# the secret configuration specific things\nif bool(os.environ.get(\"WEBHOOK\", False)):\n    from sample_config import Config\nelse:\n    from config import Config\n\n# the Strings used for this \"thing\"\nimport pyrogram\nfrom translation import Translation\n",
    "last_modified": "2025-05-06T04:35:15.024583"
  },
  {
    "id": "2588",
    "name": "like_your_last_media_likers.py",
    "path": "github_repo/scripts/03_automation_platforms/api_integrations/like_your_last_media_likers.py",
    "category": "03_automation_platforms",
    "type": "utility",
    "lines": 37,
    "size": 899,
    "docstring": "instabot example\nWorkflow:\n    Like likers of last medias from your timeline feed.",
    "keywords": [],
    "functions": [
      "like_media_likers",
      "like_your_feed_likers"
    ],
    "classes": [],
    "imports": [
      "argparse",
      "os",
      "sys",
      "tqdm",
      "instabot"
    ],
    "preview": "\"\"\"\ninstabot example\nWorkflow:\n    Like likers of last medias from your timeline feed.\n\"\"\"\n\nimport argparse\nimport os\nimport sys\n\nfrom tqdm import tqdm\n\nsys.path.append(os.path.join(sys.path[0], \"../\"))\nfrom instabot import Bot  # noqa: E402\n\n\ndef like_media_likers(bot, media, nlikes=3):\n    for user in tqdm(bot.get_media_likers(media), desc=\"Media likers\"):\n        bot.like_user(user, nlikes)\n    return True",
    "last_modified": "2025-03-28T18:35:47"
  },
  {
    "id": "2589",
    "name": "save_unfollowers_into_file.py",
    "path": "github_repo/scripts/03_automation_platforms/api_integrations/save_unfollowers_into_file.py",
    "category": "03_automation_platforms",
    "type": "organization",
    "lines": 38,
    "size": 893,
    "docstring": "instabot example\n\nWorkflow:\n    Save user' unfollowers into a file.",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "argparse",
      "os",
      "sys",
      "instabot"
    ],
    "preview": "\"\"\"\ninstabot example\n\nWorkflow:\n    Save user' unfollowers into a file.\n\"\"\"\n\nimport argparse\nimport os\nimport sys\n\nsys.path.append(os.path.join(sys.path[0], \"../\"))\nfrom instabot import Bot, utils  # noqa: E402\n\nparser = argparse.ArgumentParser(add_help=False)\nparser.add_argument(\"-u\", type=str, help=\"username\")\nparser.add_argument(\"-p\", type=str, help=\"password\")\nparser.add_argument(\"-proxy\", type=str, help=\"proxy\")\nargs = parser.parse_args()\n",
    "last_modified": "2025-03-28T18:35:47"
  },
  {
    "id": "2590",
    "name": "follow_users_from_file.py",
    "path": "github_repo/scripts/03_automation_platforms/api_integrations/follow_users_from_file.py",
    "category": "03_automation_platforms",
    "type": "organization",
    "lines": 33,
    "size": 845,
    "docstring": "instabot example\n\nWorkflow:\n    Take users from input file and follow them.\n    The file should contain one username per line!",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "argparse",
      "os",
      "sys",
      "instabot"
    ],
    "preview": "\"\"\"\ninstabot example\n\nWorkflow:\n    Take users from input file and follow them.\n    The file should contain one username per line!\n\"\"\"\n\nimport argparse\nimport os\nimport sys\n\nsys.path.append(os.path.join(sys.path[0], \"../\"))\nfrom instabot import Bot  # noqa: E402\n\nparser = argparse.ArgumentParser(add_help=True)\nparser.add_argument(\"-u\", type=str, help=\"username\")\nparser.add_argument(\"-p\", type=str, help=\"password\")\nparser.add_argument(\"-proxy\", type=str, help=\"proxy\")\nparser.add_argument(\"filepath\", type=str, help=\"filepath\")",
    "last_modified": "2025-03-28T18:35:47"
  },
  {
    "id": "2591",
    "name": "whitelist_generator.py",
    "path": "github_repo/scripts/03_automation_platforms/api_integrations/whitelist_generator.py",
    "category": "03_automation_platforms",
    "type": "utility",
    "lines": 37,
    "size": 967,
    "docstring": "instabot example\n\nWhitelist generator: generates a list of users which\nwill not be unfollowed.",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "os",
      "random",
      "sys",
      "instabot"
    ],
    "preview": "\"\"\"\ninstabot example\n\nWhitelist generator: generates a list of users which\nwill not be unfollowed.\n\"\"\"\n\nimport os\nimport random\nimport sys\n\nsys.path.append(os.path.join(sys.path[0], \"../../\"))\nfrom instabot import Bot  # noqa: E402\n\nbot = Bot()\nbot.login()\n\nprint(\n    \"This script will generate whitelist.txt file with users\"\n    \"who will not be unfollowed by bot. \"",
    "last_modified": "2025-03-28T18:35:47"
  },
  {
    "id": "2592",
    "name": "conf.py",
    "path": "github_repo/scripts/03_automation_platforms/api_integrations/conf.py",
    "category": "03_automation_platforms",
    "type": "utility",
    "lines": 38,
    "size": 1204,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [],
    "preview": "# Configuration file for the Sphinx documentation builder.\n#\n# For the full list of built-in configuration values, see the documentation:\n# https://www.sphinx-doc.org/en/master/usage/configuration.html\n\n# -- Project information -----------------------------------------------------\n# https://www.sphinx-doc.org/en/master/usage/configuration.html#project-information\n\nproject = 'Creative Automation Docs'\ncopyright = '2025, Steven'\nauthor = 'Steven'\nrelease = '1'\n\n# -- General configuration ---------------------------------------------------\n# https://www.sphinx-doc.org/en/master/usage/configuration.html#general-configuration\n\nextensions = [\n    'myst_parser',  # For Markdown\n    'sphinx.ext.autodoc',  # For Python code\n    'sphinx.ext.napoleon',  # For Google-style docstrings",
    "last_modified": "2025-10-08T06:41:23"
  },
  {
    "id": "2593",
    "name": "ollama_gui.py",
    "path": "github_repo/scripts/03_automation_platforms/api_integrations/ollama_gui.py",
    "category": "03_automation_platforms",
    "type": "utility",
    "lines": 115,
    "size": 4130,
    "docstring": "",
    "keywords": [],
    "functions": [
      "run_command",
      "__init__",
      "on_run_clicked"
    ],
    "classes": [
      "OllamaGUI"
    ],
    "imports": [
      "os",
      "shlex",
      "tkinter",
      "subprocess",
      "tkinter"
    ],
    "preview": "#!/usr/bin/env python3\nimport os\nimport shlex\nimport tkinter as tk\nfrom subprocess import PIPE, STDOUT, CalledProcessError, Popen\nfrom tkinter import messagebox, scrolledtext, ttk\n\n\n# --------------------------------------------------\n# 1. Helper Function: Run a shell command & capture output\n# --------------------------------------------------\ndef run_command(cmd: str) -> str:\n    \"\"\"\n    Execute the given shell command string and return combined stdout/stderr.\n    If the command fails, return the error message.\n    \"\"\"\n    try:\n        # Use shlex.split to handle quoted arguments safely\n        process = Popen(shlex.split(cmd), stdout=PIPE, stderr=STDOUT, text=True)\n        output, _ = process.communicate()",
    "last_modified": "2025-09-06T12:24:11.827019"
  },
  {
    "id": "2594",
    "name": "block_bots.py",
    "path": "github_repo/scripts/03_automation_platforms/api_integrations/block_bots.py",
    "category": "03_automation_platforms",
    "type": "utility",
    "lines": 35,
    "size": 904,
    "docstring": "instabot example\n\nWorkflow:\n    Block bots. That makes them unfollow you -> You have clear account.",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "argparse",
      "os",
      "sys",
      "instabot"
    ],
    "preview": "\"\"\"\ninstabot example\n\nWorkflow:\n    Block bots. That makes them unfollow you -> You have clear account.\n\"\"\"\n\nimport argparse\nimport os\nimport sys\n\nsys.path.append(os.path.join(sys.path[0], \"../\"))\nfrom instabot import Bot  # noqa: E402\n\nparser = argparse.ArgumentParser(add_help=True)\nparser.add_argument(\"-u\", type=str, help=\"username\")\nparser.add_argument(\"-p\", type=str, help=\"password\")\nparser.add_argument(\"-proxy\", type=str, help=\"proxy\")\nargs = parser.parse_args()\n",
    "last_modified": "2025-03-28T18:35:47"
  },
  {
    "id": "2595",
    "name": "sphinxext.py",
    "path": "github_repo/scripts/03_automation_platforms/api_integrations/sphinxext.py",
    "category": "03_automation_platforms",
    "type": "utility",
    "lines": 236,
    "size": 7140,
    "docstring": "pygments.sphinxext\n~~~~~~~~~~~~~~~~~~\n\nSphinx extension to generate automatic documentation of lexers,\nformatters and filters.\n\n:copyright: Copyright 2006-2023 by the Pygments team, see AUTHORS.\n:license: BSD, see LICENSE for details.",
    "keywords": [],
    "functions": [
      "setup",
      "run",
      "document_lexers_overview",
      "document_lexers",
      "document_formatters",
      "document_filters",
      "format_link",
      "write_row",
      "write_seperator"
    ],
    "classes": [
      "PygmentsDoc"
    ],
    "imports": [
      "sys",
      "docutils",
      "docutils.parsers.rst",
      "docutils.statemachine",
      "sphinx.util.nodes",
      "pip._vendor.pygments.lexers",
      "pip._vendor.pygments.lexers._mapping",
      "pip._vendor.pygments.lexers._mapping",
      "pip._vendor.pygments.formatters",
      "pip._vendor.pygments.filters"
    ],
    "preview": "\"\"\"\npygments.sphinxext\n~~~~~~~~~~~~~~~~~~\n\nSphinx extension to generate automatic documentation of lexers,\nformatters and filters.\n\n:copyright: Copyright 2006-2023 by the Pygments team, see AUTHORS.\n:license: BSD, see LICENSE for details.\n\"\"\"\n\nimport sys\n\nfrom docutils import nodes\nfrom docutils.parsers.rst import Directive\nfrom docutils.statemachine import ViewList\nfrom sphinx.util.nodes import nested_parse_with_titles\n\nMODULEDOC = \"\"\"\n.. module:: %s",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2596",
    "name": "archive_medias.py",
    "path": "github_repo/scripts/03_automation_platforms/api_integrations/archive_medias.py",
    "category": "03_automation_platforms",
    "type": "utility",
    "lines": 35,
    "size": 749,
    "docstring": "instabot example\n\nWorkflow:\n    Archive medias.",
    "keywords": [],
    "functions": [
      "archive_medias"
    ],
    "classes": [],
    "imports": [
      "argparse",
      "os",
      "sys",
      "tqdm",
      "instabot"
    ],
    "preview": "\"\"\"\ninstabot example\n\nWorkflow:\n    Archive medias.\n\"\"\"\n\nimport argparse\nimport os\nimport sys\n\nfrom tqdm import tqdm\n\nsys.path.append(os.path.join(sys.path[0], \"../\"))\nfrom instabot import Bot  # noqa: E402\n\n\ndef archive_medias(bot, medias):\n    for media in tqdm(medias, desc=\"Medias\"):\n        bot.archive(media)",
    "last_modified": "2025-03-28T18:35:47"
  },
  {
    "id": "2597",
    "name": "infinity_feedliker.py",
    "path": "github_repo/scripts/03_automation_platforms/api_integrations/infinity_feedliker.py",
    "category": "03_automation_platforms",
    "type": "setup",
    "lines": 30,
    "size": 623,
    "docstring": "instabot example\n\nWorkflow:\n    Like rescent medias from your timeline feed.",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "argparse",
      "os",
      "sys",
      "time",
      "instabot"
    ],
    "preview": "\"\"\"\ninstabot example\n\nWorkflow:\n    Like rescent medias from your timeline feed.\n\"\"\"\n\nimport argparse\nimport os\nimport sys\nimport time\n\nsys.path.append(os.path.join(sys.path[0], \"../\"))\nfrom instabot import Bot  # noqa: E402\n\nparser = argparse.ArgumentParser(add_help=True)\nparser.add_argument(\"-u\", type=str, help=\"username\")\nparser.add_argument(\"-p\", type=str, help=\"password\")\nparser.add_argument(\"-proxy\", type=str, help=\"proxy\")\nargs = parser.parse_args()",
    "last_modified": "2025-03-28T18:35:47"
  },
  {
    "id": "2598",
    "name": "save_users_following_into_file.py",
    "path": "github_repo/scripts/03_automation_platforms/api_integrations/save_users_following_into_file.py",
    "category": "03_automation_platforms",
    "type": "organization",
    "lines": 30,
    "size": 780,
    "docstring": "instabot example\n\nWorkflow:\n    Save users' following into a file.",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "argparse",
      "os",
      "sys",
      "instabot"
    ],
    "preview": "\"\"\"\ninstabot example\n\nWorkflow:\n    Save users' following into a file.\n\"\"\"\n\nimport argparse\nimport os\nimport sys\n\nsys.path.append(os.path.join(sys.path[0], \"../\"))\nfrom instabot import Bot, utils  # noqa: E402\n\nparser = argparse.ArgumentParser(add_help=False)\nparser.add_argument(\"-u\", type=str, help=\"username\")\nparser.add_argument(\"-p\", type=str, help=\"password\")\nparser.add_argument(\"-proxy\", type=str, help=\"proxy\")\nparser.add_argument(\"filename\", type=str, help=\"filename\")\nparser.add_argument(\"users\", type=str, nargs=\"+\", help=\"users\")",
    "last_modified": "2025-03-28T18:35:47"
  },
  {
    "id": "2599",
    "name": "mistral_ai_api_quickstart copy.py",
    "path": "github_repo/scripts/03_automation_platforms/api_integrations/mistral_ai_api_quickstart copy.py",
    "category": "03_automation_platforms",
    "type": "web_tools",
    "lines": 38,
    "size": 1038,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "mistralai"
    ],
    "preview": "# -*- coding: utf-8 -*-\n\"\"\"Mistral AI API quickstart.ipynb\n\nAutomatically generated by Colab.\n\nOriginal file is located at\n    https://colab.research.google.com/drive/1lh2Uc6h2BRkSVXhzyewGHRjhXWvkz9Rq\n\n# Getting started with Mistral AI API\n\"\"\"\n\n! pip install mistralai\n\n\"\"\"Our API is currently available through [La Plateforme](https://console.mistral.ai/). You need to activate payments on your account to enable your API keys. After a few moments, you will be able to use our `chat` endpoint:\"\"\"\n\nfrom mistralai import Mistral\napi_key = \"z7opEhiZF9aPgHsKCgMW9G3axhFq5qiO\"\nmodel = \"mistral-large-latest\"\n\nclient = Mistral(api_key=api_key)",
    "last_modified": "2025-09-07T01:16:31.677939"
  },
  {
    "id": "2600",
    "name": "__init__.py",
    "path": "github_repo/scripts/03_automation_platforms/api_integrations/__init__.py",
    "category": "03_automation_platforms",
    "type": "setup",
    "lines": 4,
    "size": 40,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "bot"
    ],
    "preview": "from .bot import Bot\n\n__all__ = [\"Bot\"]\n",
    "last_modified": "2022-11-05T04:33:59"
  },
  {
    "id": "2601",
    "name": "comment_hashtags.py",
    "path": "github_repo/scripts/03_automation_platforms/api_integrations/comment_hashtags.py",
    "category": "03_automation_platforms",
    "type": "organization",
    "lines": 34,
    "size": 793,
    "docstring": "instabot example\n\nDependencies:\n    You must have a file with comments to post.\n    The file should have one comment per line.\n\nNotes:\n    You can change file and add there your comments.",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "os",
      "sys",
      "instabot"
    ],
    "preview": "\"\"\"\ninstabot example\n\nDependencies:\n    You must have a file with comments to post.\n    The file should have one comment per line.\n\nNotes:\n    You can change file and add there your comments.\n\"\"\"\n\nimport os\nimport sys\n\nsys.path.append(os.path.join(sys.path[0], \"../../\"))\nfrom instabot import Bot  # noqa: E402\n\nif len(sys.argv) < 3:\n    print(\"USAGE: Pass a path to the file with comments \" \"and a hashtag to comment\")\n    print(\"Example: %s comments_emoji.txt dog cat\" % sys.argv[0])",
    "last_modified": "2025-03-28T18:35:47"
  },
  {
    "id": "2602",
    "name": "get_hashtags_from_keywords.py",
    "path": "github_repo/scripts/03_automation_platforms/api_integrations/get_hashtags_from_keywords.py",
    "category": "03_automation_platforms",
    "type": "utility",
    "lines": 68,
    "size": 1591,
    "docstring": "",
    "keywords": [],
    "functions": [
      "dispositions"
    ],
    "classes": [],
    "imports": [
      "__future__",
      "os",
      "sys",
      "tqdm",
      "instabot",
      "itertools"
    ],
    "preview": "from __future__ import unicode_literals\n\nimport os\nimport sys\n\nfrom tqdm import tqdm\n\nsys.path.append(os.path.join(sys.path[0], \"../\"))\nfrom instabot import Bot  # noqa: E402\n\nif len(sys.argv) < 2:\n    print(\n        \"Please provide keywords separated by space\\n\"\n        \"Usage:\\n\"\n        \"python get_hashtags_from_keywords.py keyword1 keyword2 etc\"\n    )\n    exit()\n\nbot = Bot()\nbot.login()",
    "last_modified": "2025-03-28T18:35:47"
  },
  {
    "id": "2603",
    "name": "mistral_ai_api_quickstart.py",
    "path": "github_repo/scripts/03_automation_platforms/api_integrations/mistral_ai_api_quickstart.py",
    "category": "03_automation_platforms",
    "type": "web_tools",
    "lines": 38,
    "size": 1038,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "mistralai"
    ],
    "preview": "# -*- coding: utf-8 -*-\n\"\"\"Mistral AI API quickstart.ipynb\n\nAutomatically generated by Colab.\n\nOriginal file is located at\n    https://colab.research.google.com/drive/1lh2Uc6h2BRkSVXhzyewGHRjhXWvkz9Rq\n\n# Getting started with Mistral AI API\n\"\"\"\n\n! pip install mistralai\n\n\"\"\"Our API is currently available through [La Plateforme](https://console.mistral.ai/). You need to activate payments on your account to enable your API keys. After a few moments, you will be able to use our `chat` endpoint:\"\"\"\n\nfrom mistralai import Mistral\napi_key = \"z7opEhiZF9aPgHsKCgMW9G3axhFq5qiO\"\nmodel = \"mistral-large-latest\"\n\nclient = Mistral(api_key=api_key)",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2604",
    "name": "unarchive_your_medias.py",
    "path": "github_repo/scripts/03_automation_platforms/api_integrations/unarchive_your_medias.py",
    "category": "03_automation_platforms",
    "type": "utility",
    "lines": 19,
    "size": 269,
    "docstring": "instabot example\n\nWorkflow:\n1) Unarchives your last medias",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "os",
      "sys",
      "instabot"
    ],
    "preview": "\"\"\"\ninstabot example\n\nWorkflow:\n1) Unarchives your last medias\n\n\"\"\"\n\nimport os\nimport sys\n\nsys.path.append(os.path.join(sys.path[0], \"../\"))\nfrom instabot import Bot  # noqa: E402\n\nbot = Bot()\nbot.login()\nmedias = bot.get_archived_medias()\nbot.unarchive_medias(medias)\n",
    "last_modified": "2025-03-28T18:35:47"
  },
  {
    "id": "2605",
    "name": "ultimate.py",
    "path": "github_repo/scripts/03_automation_platforms/api_integrations/ultimate.py",
    "category": "03_automation_platforms",
    "type": "analysis",
    "lines": 44,
    "size": 1383,
    "docstring": "ULTIMATE SCRIPT\n\nIt uses data written in files:\n    * follow_followers.txt\n    * follow_following.txt\n    * like_hashtags.txt\n    * like_users.txt\nand do the job. This bot can be run 24/7.",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "os",
      "sys",
      "instabot"
    ],
    "preview": "\"\"\"\nULTIMATE SCRIPT\n\nIt uses data written in files:\n    * follow_followers.txt\n    * follow_following.txt\n    * like_hashtags.txt\n    * like_users.txt\nand do the job. This bot can be run 24/7.\n\"\"\"\n\nimport os\nimport sys\n\nsys.path.append(os.path.join(sys.path[0], \"../../\"))\nfrom instabot import Bot  # noqa: E402\n\nbot = Bot()\nbot.login()\n",
    "last_modified": "2025-03-28T18:35:47"
  },
  {
    "id": "2606",
    "name": "botInstagram.py",
    "path": "github_repo/scripts/03_automation_platforms/api_integrations/botInstagram.py",
    "category": "03_automation_platforms",
    "type": "web_tools",
    "lines": 53,
    "size": 1198,
    "docstring": "Created in 07/2020\n@Author: Paulo https://github.com/alpdias",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "os",
      "platform",
      "art",
      "botComment",
      "botDraw",
      "botLike",
      "botStories"
    ],
    "preview": "# -*- coding: utf-8 -*-\n\n\"\"\"\nCreated in 07/2020\n@Author: Paulo https://github.com/alpdias\n\"\"\"\n\n# imported libraries\nimport os\nimport platform\n\nimport art\nimport botComment\nimport botDraw\nimport botLike\nimport botStories\n\nmySystem = platform.system()  # which operating system is running\n\nwhile True:",
    "last_modified": "2025-09-13T05:54:06.948910"
  },
  {
    "id": "2607",
    "name": "animation.py",
    "path": "github_repo/scripts/03_automation_platforms/api_integrations/animation.py",
    "category": "03_automation_platforms",
    "type": "utility",
    "lines": 103,
    "size": 4311,
    "docstring": "",
    "keywords": [],
    "functions": [
      "load_animation",
      "animation_bar",
      "starting_bot",
      "colorText"
    ],
    "classes": [],
    "imports": [
      "os",
      "sys",
      "time"
    ],
    "preview": "import os  # line:3\nimport sys  # line:2\nimport time  # line:1\n\n\ndef load_animation():  # line:6\n    OOO0OOOO0O00O0OOO = \"starting your console application...\"  # line:7\n    OOO0O0000O0OOO00O = len(OOO0OOOO0O00O0OOO)  # line:8\n    O0OO0OO00O0OOOOO0 = \"|/-\\\\\"  # line:10\n    O00OO00000O000O0O = 0  # line:11\n    OOO0O0O00OOO0OOO0 = 0  # line:12\n    OO0OOO00O00OO00O0 = 0  # line:13\n    while OOO0O0O00OOO0OOO0 != 100:  # line:15\n        time.sleep(0.075)  # line:17\n        O0O000O0O0O0O0OOO = list(OOO0OOOO0O00O0OOO)  # line:18\n        OOOOO00O0OO00OOO0 = ord(O0O000O0O0O0O0OOO[OO0OOO00O00OO00O0])  # line:19\n        OOOO0O00O00O0000O = 0  # line:20\n        if OOOOO00O0OO00OOO0 != 32 and OOOOO00O0OO00OOO0 != 46:  # line:21\n            if OOOOO00O0OO00OOO0 > 90:  # line:22\n                OOOO0O00O00O0000O = OOOOO00O0OO00OOO0 - 32  # line:23",
    "last_modified": "2025-09-13T05:53:30.538659"
  },
  {
    "id": "2608",
    "name": "filter_private_profiles.py",
    "path": "github_repo/scripts/03_automation_platforms/api_integrations/filter_private_profiles.py",
    "category": "03_automation_platforms",
    "type": "organization",
    "lines": 19,
    "size": 447,
    "docstring": "instabot filtering private users example\n\nWorkflow:\n    Try to follow a private user with the bot and see how it\n    filters that user out.",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "os",
      "sys",
      "instabot"
    ],
    "preview": "\"\"\"\ninstabot filtering private users example\n\nWorkflow:\n    Try to follow a private user with the bot and see how it\n    filters that user out.\n\"\"\"\n\nimport os\nimport sys\n\nsys.path.append(os.path.join(sys.path[0], \"../\"))\nfrom instabot import Bot  # noqa: E402\n\nbot = Bot(filter_users=True, filter_private_users=True)\nbot.login()\nprivate_user_input = input(\"\\n Enter a private user: \")\nbot.follow(bot.get_user_id_from_username(private_user_input))\n",
    "last_modified": "2025-05-04T23:28:20"
  },
  {
    "id": "2609",
    "name": "delete_sagemaker_endpoint.py",
    "path": "github_repo/scripts/03_automation_platforms/api_integrations/delete_sagemaker_endpoint.py",
    "category": "03_automation_platforms",
    "type": "utility",
    "lines": 77,
    "size": 2403,
    "docstring": "",
    "keywords": [],
    "functions": [
      "delete_endpoint_and_config"
    ],
    "classes": [],
    "imports": [
      "loguru",
      "llm_engineering.settings",
      "boto3",
      "botocore.exceptions"
    ],
    "preview": "from loguru import logger\n\ntry:\n    import boto3\n    from botocore.exceptions import ClientError\nexcept ModuleNotFoundError:\n    logger.warning(\n        \"Couldn't load AWS or SageMaker imports. Run 'poetry install --with aws' to support AWS.\"\n    )\n\n\nfrom llm_engineering.settings import settings\n\n\ndef delete_endpoint_and_config(endpoint_name) -> None:\n    \"\"\"\n    Deletes an AWS SageMaker endpoint and its associated configuration.\n    Args:\n    endpoint_name (str): The name of the SageMaker endpoint to delete.\n    Returns:",
    "last_modified": "2025-09-13T05:53:41.894767"
  },
  {
    "id": "2610",
    "name": "unfollow_everyone.py",
    "path": "github_repo/scripts/03_automation_platforms/api_integrations/unfollow_everyone.py",
    "category": "03_automation_platforms",
    "type": "utility",
    "lines": 24,
    "size": 541,
    "docstring": "instabot example\n\nWorkflow:\n    1) unfollows every from your account.",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "argparse",
      "os",
      "sys",
      "instabot"
    ],
    "preview": "\"\"\"\ninstabot example\n\nWorkflow:\n    1) unfollows every from your account.\n\"\"\"\n\nimport argparse\nimport os\nimport sys\n\nsys.path.append(os.path.join(sys.path[0], \"../\"))\nfrom instabot import Bot  # noqa: E402\n\nparser = argparse.ArgumentParser(add_help=True)\nparser.add_argument(\"-u\", type=str, help=\"username\")\nparser.add_argument(\"-p\", type=str, help=\"password\")\nparser.add_argument(\"-proxy\", type=str, help=\"proxy\")\nargs = parser.parse_args()\n",
    "last_modified": "2025-05-04T23:28:20"
  },
  {
    "id": "2611",
    "name": "awsqueue.py",
    "path": "github_repo/scripts/03_automation_platforms/api_integrations/awsqueue.py",
    "category": "03_automation_platforms",
    "type": "utility",
    "lines": 36,
    "size": 1046,
    "docstring": "Talks to AWS",
    "keywords": [],
    "functions": [
      "__init__",
      "get_messages",
      "send_message"
    ],
    "classes": [
      "Awsqueue"
    ],
    "imports": [
      "json",
      "logging",
      "boto3"
    ],
    "preview": "\"Talks to AWS\"\n\nimport json\nimport logging\n\nimport boto3\n\n\nclass Awsqueue(object):\n    \"Calling an aws queue\"\n\n    def __init__(self, queue_url):\n        self.queue_url = queue_url\n        self.logger = logging.getLogger(__name__)\n\n    def get_messages(self):\n        \"Gets the next few messages\"\n        self.logger.info(\"Loading messages\")\n        try:\n            sqs = boto3.resource(\"sqs\")",
    "last_modified": "2025-05-04T23:27:55"
  },
  {
    "id": "2612",
    "name": "follow_user_followers.py",
    "path": "github_repo/scripts/03_automation_platforms/api_integrations/follow_user_followers.py",
    "category": "03_automation_platforms",
    "type": "utility",
    "lines": 33,
    "size": 807,
    "docstring": "instabot example\n\nWorkflow:\n    Follow user's followers by username.",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "argparse",
      "os",
      "sys",
      "instabot"
    ],
    "preview": "\"\"\"\ninstabot example\n\nWorkflow:\n    Follow user's followers by username.\n\"\"\"\n\nimport argparse\nimport os\nimport sys\n\nsys.path.append(os.path.join(sys.path[0], \"../\"))\nfrom instabot import Bot  # noqa: E402\n\nparser = argparse.ArgumentParser(add_help=True)\nparser.add_argument(\"-u\", type=str, help=\"username\")\nparser.add_argument(\"-p\", type=str, help=\"password\")\nparser.add_argument(\"-proxy\", type=str, help=\"proxy\")\nparser.add_argument(\"users\", type=str, nargs=\"+\", help=\"users\")\nargs = parser.parse_args()",
    "last_modified": "2025-03-28T18:35:47"
  },
  {
    "id": "2613",
    "name": "bot_state.py",
    "path": "github_repo/scripts/03_automation_platforms/api_integrations/bot_state.py",
    "category": "03_automation_platforms",
    "type": "utility",
    "lines": 69,
    "size": 1587,
    "docstring": "",
    "keywords": [],
    "functions": [
      "__init__",
      "__repr__"
    ],
    "classes": [
      "BotState"
    ],
    "imports": [
      "datetime",
      "instabot.singleton"
    ],
    "preview": "import datetime\n\nfrom instabot.singleton import Singleton\n\n\nclass BotState(object):\n    __metaclass__ = Singleton\n\n    def __init__(self):\n        self.start_time = datetime.datetime.now()\n        self.total = dict.fromkeys(\n            [\n                \"likes\",\n                \"unlikes\",\n                \"follows\",\n                \"unfollows\",\n                \"comments\",\n                \"blocks\",\n                \"unblocks\",\n                \"messages\",",
    "last_modified": "2025-03-28T18:35:47"
  },
  {
    "id": "2614",
    "name": "bot.py",
    "path": "github_repo/scripts/03_automation_platforms/api_integrations/bot.py",
    "category": "03_automation_platforms",
    "type": "utility",
    "lines": 35,
    "size": 840,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "logging",
      "os",
      "pyrogram",
      "sample_config",
      "config"
    ],
    "preview": "import logging\n\nlogging.basicConfig(\n    level=logging.DEBUG, format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\n)\nlogger = logging.getLogger(__name__)\n\nimport os\n\n# the secret configuration specific things\nif bool(os.environ.get(\"WEBHOOK\", False)):\n    from sample_config import Config\nelse:\n    from config import Config\n\nimport pyrogram\n\nlogging.getLogger(\"pyrogram\").setLevel(logging.WARNING)\n\n",
    "last_modified": "2025-05-06T04:35:15.015944"
  },
  {
    "id": "2615",
    "name": "like_and_follow_last_user_media_likers.py",
    "path": "github_repo/scripts/03_automation_platforms/api_integrations/like_and_follow_last_user_media_likers.py",
    "category": "03_automation_platforms",
    "type": "utility",
    "lines": 34,
    "size": 896,
    "docstring": "instabot example\n\nWorkflow:\n    Like and follow users who liked the last media of input users.",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "argparse",
      "os",
      "sys",
      "tqdm",
      "instabot"
    ],
    "preview": "\"\"\"\ninstabot example\n\nWorkflow:\n    Like and follow users who liked the last media of input users.\n\"\"\"\n\nimport argparse\nimport os\nimport sys\n\nfrom tqdm import tqdm\n\nsys.path.append(os.path.join(sys.path[0], \"../\"))\nfrom instabot import Bot  # noqa: E402\n\nparser = argparse.ArgumentParser(add_help=True)\nparser.add_argument(\"-u\", type=str, help=\"username\")\nparser.add_argument(\"-p\", type=str, help=\"password\")\nparser.add_argument(\"-proxy\", type=str, help=\"proxy\")",
    "last_modified": "2025-03-28T18:35:47"
  },
  {
    "id": "2616",
    "name": "reply_to_media_comments.py",
    "path": "github_repo/scripts/03_automation_platforms/api_integrations/reply_to_media_comments.py",
    "category": "03_automation_platforms",
    "type": "utility",
    "lines": 92,
    "size": 3008,
    "docstring": "instabot example\n\nWorkflow:\n    If media is commented, reply to comments\n    if you didn't reply yet to that user.",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "__future__",
      "argparse",
      "os",
      "sys",
      "tqdm",
      "instabot"
    ],
    "preview": "\"\"\"\ninstabot example\n\nWorkflow:\n    If media is commented, reply to comments\n    if you didn't reply yet to that user.\n\"\"\"\n\nfrom __future__ import unicode_literals\n\nimport argparse\nimport os\nimport sys\n\nfrom tqdm import tqdm\n\nsys.path.append(os.path.join(sys.path[0], \"../../\"))\nfrom instabot import Bot  # noqa: E402\n\nparser = argparse.ArgumentParser(add_help=True)",
    "last_modified": "2025-09-13T05:54:54.980397"
  },
  {
    "id": "2617",
    "name": "like_user_followers.py",
    "path": "github_repo/scripts/03_automation_platforms/api_integrations/like_user_followers.py",
    "category": "03_automation_platforms",
    "type": "utility",
    "lines": 27,
    "size": 657,
    "docstring": "instabot example\n\nWorkflow:\n    Like user's, follower's media by user_id.",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "argparse",
      "os",
      "sys",
      "instabot"
    ],
    "preview": "\"\"\"\ninstabot example\n\nWorkflow:\n    Like user's, follower's media by user_id.\n\"\"\"\n\nimport argparse\nimport os\nimport sys\n\nsys.path.append(os.path.join(sys.path[0], \"../\"))\nfrom instabot import Bot  # noqa: E402\n\nparser = argparse.ArgumentParser(add_help=True)\nparser.add_argument(\"-u\", type=str, help=\"username\")\nparser.add_argument(\"-p\", type=str, help=\"password\")\nparser.add_argument(\"-proxy\", type=str, help=\"proxy\")\nparser.add_argument(\"users\", type=str, nargs=\"+\", help=\"users\")\nargs = parser.parse_args()",
    "last_modified": "2025-03-28T18:35:47"
  },
  {
    "id": "2618",
    "name": "build_site.py",
    "path": "github_repo/scripts/03_automation_platforms/api_integrations/build_site.py",
    "category": "03_automation_platforms",
    "type": "utility",
    "lines": 88,
    "size": 2772,
    "docstring": "",
    "keywords": [],
    "functions": [
      "load_index",
      "build_html",
      "main"
    ],
    "classes": [],
    "imports": [
      "__future__",
      "json",
      "datetime",
      "pathlib",
      "typing"
    ],
    "preview": "from __future__ import annotations\n\nimport json\nfrom datetime import datetime\nfrom pathlib import Path\nfrom typing import List, Dict, Any\n\nROOT = Path(__file__).resolve().parents[1]\n\n\ndef load_index() -> List[Dict[str, Any]]:\n    data = json.loads((ROOT / \"python_index.json\").read_text(encoding=\"utf-8\"))\n    # sort by project then name\n    data.sort(key=lambda d: (d.get(\"project\", \"\"), d.get(\"path\", \"\")))\n    return data\n\n\ndef build_html(items: List[Dict[str, Any]]) -> str:\n    ts = datetime.now().strftime(\"%Y-%m-%d %H:%M\")\n    rows = []",
    "last_modified": "2025-09-13T06:01:14.790179"
  },
  {
    "id": "2619",
    "name": "collect_stats.py",
    "path": "github_repo/scripts/03_automation_platforms/api_integrations/collect_stats.py",
    "category": "03_automation_platforms",
    "type": "organization",
    "lines": 32,
    "size": 789,
    "docstring": "instabot example\n\nCollects the information about your account\nevery hour in username.tsv file.",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "argparse",
      "os",
      "sys",
      "time",
      "instabot"
    ],
    "preview": "\"\"\"\ninstabot example\n\nCollects the information about your account\nevery hour in username.tsv file.\n\"\"\"\n\nimport argparse\nimport os\nimport sys\nimport time\n\nsys.path.append(os.path.join(sys.path[0], \"../\"))\nfrom instabot import Bot  # noqa: E402\n\nparser = argparse.ArgumentParser(add_help=True)\nparser.add_argument(\"-u\", type=str, help=\"username\")\nparser.add_argument(\"-p\", type=str, help=\"password\")\nparser.add_argument(\"-proxy\", type=str, help=\"proxy\")\nparser.add_argument(\"user\", type=str, nargs=\"*\", help=\"user\")",
    "last_modified": "2025-03-28T18:35:47"
  },
  {
    "id": "2620",
    "name": "welcome_message.py",
    "path": "github_repo/scripts/03_automation_platforms/api_integrations/welcome_message.py",
    "category": "03_automation_platforms",
    "type": "utility",
    "lines": 82,
    "size": 2289,
    "docstring": "instabot example\nWorkflow:\nWelcome message for new followers.",
    "keywords": [],
    "functions": [
      "get_recent_followers",
      "main"
    ],
    "classes": [],
    "imports": [
      "argparse",
      "datetime",
      "os",
      "sys",
      "time",
      "instabot"
    ],
    "preview": "\"\"\"\ninstabot example\nWorkflow:\nWelcome message for new followers.\n\"\"\"\n\nimport argparse\nimport datetime\nimport os\nimport sys\nimport time\n\nsys.path.append(os.path.join(sys.path[0], \"../\"))\nfrom instabot import Bot  # noqa: E402\n\nRETRY_DELAY = 60\nDELAY = 30 * 60\n\n\ndef get_recent_followers(bot, from_time):",
    "last_modified": "2025-09-13T05:54:56.030467"
  },
  {
    "id": "2621",
    "name": "infinity_hashtags_follower.py",
    "path": "github_repo/scripts/03_automation_platforms/api_integrations/infinity_hashtags_follower.py",
    "category": "03_automation_platforms",
    "type": "setup",
    "lines": 33,
    "size": 780,
    "docstring": "instabot example\n\nWorkflow:\n    Follow users who post medias with hashtag.",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "argparse",
      "os",
      "sys",
      "time",
      "instabot"
    ],
    "preview": "\"\"\"\ninstabot example\n\nWorkflow:\n    Follow users who post medias with hashtag.\n\"\"\"\n\nimport argparse\nimport os\nimport sys\nimport time\n\nsys.path.append(os.path.join(sys.path[0], \"../\"))\nfrom instabot import Bot  # noqa: E402\n\nparser = argparse.ArgumentParser(add_help=True)\nparser.add_argument(\"-u\", type=str, help=\"username\")\nparser.add_argument(\"-p\", type=str, help=\"password\")\nparser.add_argument(\"-proxy\", type=str, help=\"proxy\")\nparser.add_argument(\"hashtags\", type=str, nargs=\"+\", help=\"hashtags\")",
    "last_modified": "2025-03-28T18:35:47"
  },
  {
    "id": "2622",
    "name": "follow_users_by_hashtag.py",
    "path": "github_repo/scripts/03_automation_platforms/api_integrations/follow_users_by_hashtag.py",
    "category": "03_automation_platforms",
    "type": "utility",
    "lines": 28,
    "size": 694,
    "docstring": "instabot example\n\nWorkflow:\n    Follow users who post medias with hashtag.",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "argparse",
      "os",
      "sys",
      "instabot"
    ],
    "preview": "\"\"\"\ninstabot example\n\nWorkflow:\n    Follow users who post medias with hashtag.\n\"\"\"\n\nimport argparse\nimport os\nimport sys\n\nsys.path.append(os.path.join(sys.path[0], \"../\"))\nfrom instabot import Bot  # noqa: E402\n\nparser = argparse.ArgumentParser(add_help=True)\nparser.add_argument(\"-u\", type=str, help=\"username\")\nparser.add_argument(\"-p\", type=str, help=\"password\")\nparser.add_argument(\"-proxy\", type=str, help=\"proxy\")\nparser.add_argument(\"hashtags\", type=str, nargs=\"+\", help=\"hashtags\")\nargs = parser.parse_args()",
    "last_modified": "2025-03-28T18:35:47"
  },
  {
    "id": "2623",
    "name": "black_white_lists.py",
    "path": "github_repo/scripts/03_automation_platforms/api_integrations/black_white_lists.py",
    "category": "03_automation_platforms",
    "type": "organization",
    "lines": 38,
    "size": 969,
    "docstring": "instabot example\n\nWorkflow:\n    1) Reads user_ids from blacklist and whitelist\n    2) likes several last medias by users in your timeline\n\nNotes:\n    blacklist and whitelist files should contain user_ids - each one on the\n    separate line.\n    Example:\n        1234125\n        1234124512",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "argparse",
      "os",
      "sys",
      "tqdm",
      "instabot"
    ],
    "preview": "\"\"\"\ninstabot example\n\nWorkflow:\n    1) Reads user_ids from blacklist and whitelist\n    2) likes several last medias by users in your timeline\n\nNotes:\n    blacklist and whitelist files should contain user_ids - each one on the\n    separate line.\n    Example:\n        1234125\n        1234124512\n\"\"\"\n\nimport argparse\nimport os\nimport sys\n\nfrom tqdm import tqdm",
    "last_modified": "2025-03-28T18:35:47"
  },
  {
    "id": "2624",
    "name": "extract_streams.py",
    "path": "github_repo/scripts/03_automation_platforms/api_integrations/extract_streams.py",
    "category": "03_automation_platforms",
    "type": "utility",
    "lines": 76,
    "size": 2493,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "logging",
      "asyncio",
      "os",
      "time",
      "pyrogram",
      "translation",
      "helper_funcs.chat_base",
      "helper_funcs.display_progress",
      "sample_config",
      "config"
    ],
    "preview": "import logging\n\nlogging.basicConfig(\n    level=logging.DEBUG, format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\n)\nlogger = logging.getLogger(__name__)\n\nimport asyncio\nimport os\nimport time\n\n# the secret configuration specific things\nif bool(os.environ.get(\"WEBHOOK\", False)):\n    from sample_config import Config\nelse:\n    from config import Config\n\n# the Strings used for this \"thing\"\nimport pyrogram\nfrom translation import Translation",
    "last_modified": "2025-05-06T04:35:15.023095"
  },
  {
    "id": "2625",
    "name": "unfollow_non_followers.py",
    "path": "github_repo/scripts/03_automation_platforms/api_integrations/unfollow_non_followers.py",
    "category": "03_automation_platforms",
    "type": "utility",
    "lines": 24,
    "size": 550,
    "docstring": "instabot example\n\nWorkflow:\n    1) unfollows users that don't follow you.",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "argparse",
      "os",
      "sys",
      "instabot"
    ],
    "preview": "\"\"\"\ninstabot example\n\nWorkflow:\n    1) unfollows users that don't follow you.\n\"\"\"\n\nimport argparse\nimport os\nimport sys\n\nsys.path.append(os.path.join(sys.path[0], \"../\"))\nfrom instabot import Bot  # noqa: E402\n\nparser = argparse.ArgumentParser(add_help=True)\nparser.add_argument(\"-u\", type=str, help=\"username\")\nparser.add_argument(\"-p\", type=str, help=\"password\")\nparser.add_argument(\"-proxy\", type=str, help=\"proxy\")\nargs = parser.parse_args()\n",
    "last_modified": "2025-05-04T23:28:20"
  },
  {
    "id": "2626",
    "name": "follow_last_user_media_likers.py",
    "path": "github_repo/scripts/03_automation_platforms/api_integrations/follow_last_user_media_likers.py",
    "category": "03_automation_platforms",
    "type": "utility",
    "lines": 33,
    "size": 878,
    "docstring": "instabot example\n\nWorkflow:\n    Follow users who liked the last media of input users.",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "argparse",
      "os",
      "sys",
      "tqdm",
      "instabot"
    ],
    "preview": "\"\"\"\ninstabot example\n\nWorkflow:\n    Follow users who liked the last media of input users.\n\"\"\"\n\nimport argparse\nimport os\nimport sys\n\nfrom tqdm import tqdm\n\nsys.path.append(os.path.join(sys.path[0], \"../\"))\nfrom instabot import Bot  # noqa: E402\n\nparser = argparse.ArgumentParser(add_help=True)\nparser.add_argument(\"-u\", type=str, help=\"username\")\nparser.add_argument(\"-p\", type=str, help=\"password\")\nparser.add_argument(\"-proxy\", type=str, help=\"proxy\")",
    "last_modified": "2025-03-28T18:35:47"
  },
  {
    "id": "2627",
    "name": "follow_user_following.py",
    "path": "github_repo/scripts/03_automation_platforms/api_integrations/follow_user_following.py",
    "category": "03_automation_platforms",
    "type": "utility",
    "lines": 27,
    "size": 644,
    "docstring": "instabot example\n\nWorkflow:\n    Follow user's following by username.",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "argparse",
      "os",
      "sys",
      "instabot"
    ],
    "preview": "\"\"\"\ninstabot example\n\nWorkflow:\n    Follow user's following by username.\n\"\"\"\n\nimport argparse\nimport os\nimport sys\n\nsys.path.append(os.path.join(sys.path[0], \"../\"))\nfrom instabot import Bot  # noqa: E402\n\nparser = argparse.ArgumentParser(add_help=True)\nparser.add_argument(\"-u\", type=str, help=\"username\")\nparser.add_argument(\"-p\", type=str, help=\"password\")\nparser.add_argument(\"-proxy\", type=str, help=\"proxy\")\nparser.add_argument(\"users\", type=str, nargs=\"+\", help=\"users\")\nargs = parser.parse_args()",
    "last_modified": "2025-03-28T18:35:47"
  },
  {
    "id": "2628",
    "name": "like_users_from_file.py",
    "path": "github_repo/scripts/03_automation_platforms/api_integrations/like_users_from_file.py",
    "category": "03_automation_platforms",
    "type": "organization",
    "lines": 34,
    "size": 889,
    "docstring": "instabot example\n\nWorkflow:\n    Take users from input file and like them.\n    The file should contain one username per line!",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "argparse",
      "os",
      "sys",
      "instabot"
    ],
    "preview": "\"\"\"\ninstabot example\n\nWorkflow:\n    Take users from input file and like them.\n    The file should contain one username per line!\n\"\"\"\n\nimport argparse\nimport os\nimport sys\n\nsys.path.append(os.path.join(sys.path[0], \"../\"))\nfrom instabot import Bot  # noqa: E402\n\nparser = argparse.ArgumentParser(add_help=True)\nparser.add_argument(\"-u\", type=str, help=\"username\")\nparser.add_argument(\"-p\", type=str, help=\"password\")\nparser.add_argument(\"-proxy\", type=str, help=\"proxy\")\nparser.add_argument(\"filepath\", type=str, help=\"filepath\")",
    "last_modified": "2025-03-28T18:35:47"
  },
  {
    "id": "2629",
    "name": "filter_blacklist_hashtag_medias.py",
    "path": "github_repo/scripts/03_automation_platforms/api_integrations/filter_blacklist_hashtag_medias.py",
    "category": "03_automation_platforms",
    "type": "utility",
    "lines": 27,
    "size": 660,
    "docstring": "instabot filters out the media with your set blacklist hashtags\n\nWorkflow:\n    Try to follow a media with your blacklist hashtag in the\n    description and see how bot filters it out.",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "os",
      "sys",
      "instabot"
    ],
    "preview": "\"\"\"\ninstabot filters out the media with your set blacklist hashtags\n\nWorkflow:\n    Try to follow a media with your blacklist hashtag in the\n    description and see how bot filters it out.\n\"\"\"\n\nimport os\nimport sys\n\nsys.path.append(os.path.join(sys.path[0], \"../\"))\nfrom instabot import Bot  # noqa: E402\n\nblacklist_hashtag_input = input(\"\\n Enter a blacklist hashtag: \")\n\nbot = Bot(\n    filter_users=True,\n    filter_private_users=True,\n    filter_previously_followed=True,",
    "last_modified": "2025-03-28T18:35:47"
  },
  {
    "id": "2630",
    "name": "download_stories.py",
    "path": "github_repo/scripts/03_automation_platforms/api_integrations/download_stories.py",
    "category": "03_automation_platforms",
    "type": "youtube",
    "lines": 19,
    "size": 553,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "argparse",
      "os",
      "sys",
      "instabot"
    ],
    "preview": "import argparse\nimport os\nimport sys\n\nsys.path.append(os.path.join(sys.path[0], \"../\"))\nfrom instabot import Bot  # noqa: E402\n\nparser = argparse.ArgumentParser(add_help=True)\nparser.add_argument(\"-u\", type=str, help=\"username\")\nparser.add_argument(\"-p\", type=str, help=\"password\")\nparser.add_argument(\"-proxy\", type=str, help=\"proxy\")\nparser.add_argument(\"-story_username\", type=str, help=\"story_username\")\nargs = parser.parse_args()\n\nbot = Bot()\nbot.login(username=args.u, password=args.p, proxy=args.proxy)\n\nbot.download_stories(args.story_username)\n",
    "last_modified": "2025-03-28T18:35:47"
  },
  {
    "id": "2631",
    "name": "delete_all_posts.py",
    "path": "github_repo/scripts/03_automation_platforms/api_integrations/delete_all_posts.py",
    "category": "03_automation_platforms",
    "type": "organization",
    "lines": 25,
    "size": 582,
    "docstring": "instabot example\n\nWorkflow:\n    delete all posts in profile.",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "argparse",
      "os",
      "sys",
      "instabot"
    ],
    "preview": "\"\"\"\ninstabot example\n\nWorkflow:\n    delete all posts in profile.\n\"\"\"\n\nimport argparse\nimport os\nimport sys\n\nsys.path.append(os.path.join(sys.path[0], \"../\"))\nfrom instabot import Bot  # noqa: E402\n\nparser = argparse.ArgumentParser(add_help=True)\nparser.add_argument(\"-u\", type=str, help=\"username\")\nparser.add_argument(\"-p\", type=str, help=\"password\")\nparser.add_argument(\"-proxy\", type=str, help=\"proxy\")\nargs = parser.parse_args()\n",
    "last_modified": "2025-03-28T18:35:47"
  },
  {
    "id": "2632",
    "name": "brand.py",
    "path": "github_repo/scripts/03_automation_platforms/api_integrations/brand.py",
    "category": "03_automation_platforms",
    "type": "utility",
    "lines": 26,
    "size": 689,
    "docstring": "",
    "keywords": [],
    "functions": [
      "load"
    ],
    "classes": [
      "BrandTemplate"
    ],
    "imports": [
      "__future__",
      "dataclasses",
      "json",
      "typing"
    ],
    "preview": "from __future__ import annotations\n\nimport dataclasses\nimport json\nfrom typing import Any, Dict\n\n\n@dataclasses.dataclass\nclass BrandTemplate:\n    font: str = \"Arial\"\n    caption_case: str = \"sentence\"  # sentence|upper|lower|title\n    color: str = \"#FFFFFF\"\n    stroke_color: str = \"#000000\"\n    stroke_width: int = 2\n    position: str = \"bottom\"  # top|bottom\n    margin_px: int = 40\n    safe_area_pct: float = 0.08\n\n    @staticmethod\n    def load(path: str) -> \"BrandTemplate\":",
    "last_modified": "2025-09-11T13:27:06.809466"
  },
  {
    "id": "2633",
    "name": "hashes.py",
    "path": "github_repo/scripts/03_automation_platforms/api_integrations/hashes.py",
    "category": "03_automation_platforms",
    "type": "utility",
    "lines": 152,
    "size": 5118,
    "docstring": "",
    "keywords": [],
    "functions": [
      "__init__",
      "__and__",
      "digest_count",
      "is_hash_allowed",
      "check_against_chunks",
      "_raise",
      "check_against_file",
      "check_against_path",
      "has_one_of",
      "__bool__"
    ],
    "classes": [
      "Hashes",
      "MissingHashes"
    ],
    "imports": [
      "hashlib",
      "typing",
      "pip._internal.exceptions",
      "pip._internal.utils.misc",
      "hashlib",
      "typing"
    ],
    "preview": "import hashlib\nfrom typing import TYPE_CHECKING, BinaryIO, Dict, Iterable, List, Optional\n\nfrom pip._internal.exceptions import HashMismatch, HashMissing, InstallationError\nfrom pip._internal.utils.misc import read_chunks\n\nif TYPE_CHECKING:\n    from hashlib import _Hash\n\n    # NoReturn introduced in 3.6.2; imported only for type checking to maintain\n    # pip compatibility with older patch versions of Python 3.6\n    from typing import NoReturn\n\n\n# The recommended hash algo of the moment. Change this whenever the state of\n# the art changes; it won't hurt backward compatibility.\nFAVORITE_HASH = \"sha256\"\n\n\n# Names of hashlib algorithms allowed by the --hash option and ``pip hash``",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2634",
    "name": "unlike_users.py",
    "path": "github_repo/scripts/03_automation_platforms/api_integrations/unlike_users.py",
    "category": "03_automation_platforms",
    "type": "utility",
    "lines": 27,
    "size": 632,
    "docstring": "instabot example\n\nWorkflow:\n    Unlike last media's of users.",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "argparse",
      "os",
      "sys",
      "instabot"
    ],
    "preview": "\"\"\"\ninstabot example\n\nWorkflow:\n    Unlike last media's of users.\n\"\"\"\n\nimport argparse\nimport os\nimport sys\n\nsys.path.append(os.path.join(sys.path[0], \"../\"))\nfrom instabot import Bot  # noqa: E402\n\nparser = argparse.ArgumentParser(add_help=True)\nparser.add_argument(\"-u\", type=str, help=\"username\")\nparser.add_argument(\"-p\", type=str, help=\"password\")\nparser.add_argument(\"-proxy\", type=str, help=\"proxy\")\nparser.add_argument(\"users\", type=str, nargs=\"+\", help=\"users\")\nargs = parser.parse_args()",
    "last_modified": "2025-03-28T18:35:47"
  },
  {
    "id": "2635",
    "name": "like_location_feed.py",
    "path": "github_repo/scripts/03_automation_platforms/api_integrations/like_location_feed.py",
    "category": "03_automation_platforms",
    "type": "utility",
    "lines": 20,
    "size": 613,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "argparse",
      "os",
      "sys",
      "instabot"
    ],
    "preview": "import argparse\nimport os\nimport sys\n\nsys.path.append(os.path.join(sys.path[0], \"../\"))\nfrom instabot import Bot  # noqa: E402\n\nparser = argparse.ArgumentParser(add_help=True)\nparser.add_argument(\"-u\", type=str, help=\"username\")\nparser.add_argument(\"-p\", type=str, help=\"password\")\nparser.add_argument(\"-proxy\", type=str, help=\"proxy\")\nparser.add_argument(\"-location\", type=str, help=\"location\")\nparser.add_argument(\"-amount\", type=str, help=\"amount\")\nargs = parser.parse_args()\n\nbot = Bot()\nbot.login(username=args.u, password=args.p, proxy=args.proxy)\n\nbot.like_location_feed(args.location, amount=args.amount)\n",
    "last_modified": "2025-03-28T18:35:47"
  },
  {
    "id": "2636",
    "name": "like_user_following.py",
    "path": "github_repo/scripts/03_automation_platforms/api_integrations/like_user_following.py",
    "category": "03_automation_platforms",
    "type": "utility",
    "lines": 27,
    "size": 658,
    "docstring": "instabot example\n\nWorkflow:\n    Like user's, following's media by user_id.",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "argparse",
      "os",
      "sys",
      "instabot"
    ],
    "preview": "\"\"\"\ninstabot example\n\nWorkflow:\n    Like user's, following's media by user_id.\n\"\"\"\n\nimport argparse\nimport os\nimport sys\n\nsys.path.append(os.path.join(sys.path[0], \"../\"))\nfrom instabot import Bot  # noqa: E402\n\nparser = argparse.ArgumentParser(add_help=True)\nparser.add_argument(\"-u\", type=str, help=\"username\")\nparser.add_argument(\"-p\", type=str, help=\"password\")\nparser.add_argument(\"-proxy\", type=str, help=\"proxy\")\nparser.add_argument(\"users\", type=str, nargs=\"+\", help=\"users\")\nargs = parser.parse_args()",
    "last_modified": "2025-03-28T18:35:47"
  },
  {
    "id": "2637",
    "name": "comment_medias_by_location.py",
    "path": "github_repo/scripts/03_automation_platforms/api_integrations/comment_medias_by_location.py",
    "category": "03_automation_platforms",
    "type": "utility",
    "lines": 95,
    "size": 3017,
    "docstring": "instabot example\n\nWorkflow:\n    Comment medias by location.",
    "keywords": [],
    "functions": [
      "comment_location_feed"
    ],
    "classes": [],
    "imports": [
      "argparse",
      "codecs",
      "os",
      "sys",
      "tqdm",
      "instabot"
    ],
    "preview": "# coding=utf-8\n\"\"\"\ninstabot example\n\nWorkflow:\n    Comment medias by location.\n\"\"\"\n\nimport argparse\nimport codecs\nimport os\nimport sys\n\nfrom tqdm import tqdm\n\nstdout = sys.stdout\nsys.stdout = codecs.getwriter(\"utf8\")(sys.stdout)\n\nsys.path.append(os.path.join(sys.path[0], \"../\"))\nfrom instabot import Bot  # noqa: E402",
    "last_modified": "2025-09-13T05:54:55.039536"
  },
  {
    "id": "2638",
    "name": "comment_your_feed.py",
    "path": "github_repo/scripts/03_automation_platforms/api_integrations/comment_your_feed.py",
    "category": "03_automation_platforms",
    "type": "organization",
    "lines": 36,
    "size": 817,
    "docstring": "instabot example\n\nDependencies:\n    You must have a file with comments to post.\n    The file should have one comment per line.\n\nWorkflow:\n    1) Get your timeline medias\n    2) Comment them with random comments from file.\n\nNotes:\n    You can change file and add there your comments.",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "os",
      "sys",
      "instabot"
    ],
    "preview": "\"\"\"\ninstabot example\n\nDependencies:\n    You must have a file with comments to post.\n    The file should have one comment per line.\n\nWorkflow:\n    1) Get your timeline medias\n    2) Comment them with random comments from file.\n\nNotes:\n    You can change file and add there your comments.\n\"\"\"\n\nimport os\nimport sys\n\nsys.path.append(os.path.join(sys.path[0], \"../../\"))\nfrom instabot import Bot  # noqa: E402",
    "last_modified": "2025-03-28T18:35:47"
  },
  {
    "id": "2639",
    "name": "get_followers_or_followings_to_file.py",
    "path": "github_repo/scripts/03_automation_platforms/api_integrations/get_followers_or_followings_to_file.py",
    "category": "03_automation_platforms",
    "type": "organization",
    "lines": 91,
    "size": 2451,
    "docstring": "instabot example\nWorkflow:\n    Get total or filtered followers or followings to file.",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "argparse",
      "os",
      "sys",
      "instabot"
    ],
    "preview": "\"\"\"\ninstabot example\nWorkflow:\n    Get total or filtered followers or followings to file.\n\"\"\"\n\nimport argparse\nimport os\nimport sys\n\nsys.path.append(os.path.join(sys.path[0], \"../\"))\nfrom instabot import Bot  # noqa: E402\n\nparser = argparse.ArgumentParser(add_help=True)\n# login arguments\nparser.add_argument(\"-u\", type=str, help=\"username\")\nparser.add_argument(\"-p\", type=str, help=\"password\")\nparser.add_argument(\"-proxy\", type=str, help=\"proxy\")\n# required arguments\nparser.add_argument(",
    "last_modified": "2025-03-28T18:35:47"
  },
  {
    "id": "2640",
    "name": "logo.py_02.py",
    "path": "github_repo/scripts/03_automation_platforms/logo.py_consolidated/logo.py_02.py",
    "category": "03_automation_platforms",
    "type": "utility",
    "lines": 24,
    "size": 2236,
    "docstring": "",
    "keywords": [],
    "functions": [
      "print_logo"
    ],
    "classes": [],
    "imports": [
      "os",
      "random",
      "colorama"
    ],
    "preview": "import os\nfrom random import choice\n\nfrom colorama import Back, Fore, Style\n\nlogo = \"\"\"\n\n        \u2588\u2588\u2557 \u2588\u2588\u2588\u2588\u2588\u2588\u2557     \u2588\u2588\u2588\u2557   \u2588\u2588\u2588\u2557 \u2588\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557    \u2588\u2588\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2588\u2588\u2588\u2588\u2557  \u2588\u2588\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\n        \u2588\u2588\u2551\u2588\u2588\u2554\u2550\u2550\u2550\u2550\u255d     \u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2588\u2588\u2551\u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2557\u2588\u2588\u2554\u2550\u2550\u2550\u2550\u255d\u2588\u2588\u2554\u2550\u2550\u2550\u2550\u255d    \u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2557\u2588\u2588\u2554\u2550\u2550\u2550\u2550\u255d\u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2557\u2588\u2588\u2554\u2550\u2550\u2550\u2588\u2588\u2557\u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2557\u255a\u2550\u2550\u2588\u2588\u2554\u2550\u2550\u255d\n        \u2588\u2588\u2551\u2588\u2588\u2551  \u2588\u2588\u2588\u2557    \u2588\u2588\u2554\u2588\u2588\u2588\u2588\u2554\u2588\u2588\u2551\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2551\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557    \u2588\u2588\u2588\u2588\u2588\u2588\u2554\u255d\u2588\u2588\u2588\u2588\u2588\u2557  \u2588\u2588\u2588\u2588\u2588\u2588\u2554\u255d\u2588\u2588\u2551   \u2588\u2588\u2551\u2588\u2588\u2588\u2588\u2588\u2588\u2554\u255d   \u2588\u2588\u2551   \n        \u2588\u2588\u2551\u2588\u2588\u2551   \u2588\u2588\u2551    \u2588\u2588\u2551\u255a\u2588\u2588\u2554\u255d\u2588\u2588\u2551\u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2551\u255a\u2550\u2550\u2550\u2550\u2588\u2588\u2551\u255a\u2550\u2550\u2550\u2550\u2588\u2588\u2551    \u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2557\u2588\u2588\u2554\u2550\u2550\u255d  \u2588\u2588\u2554\u2550\u2550\u2550\u255d \u2588\u2588\u2551   \u2588\u2588\u2551\u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2557   \u2588\u2588\u2551   \n        \u2588\u2588\u2551\u255a\u2588\u2588\u2588\u2588\u2588\u2588\u2554\u255d    \u2588\u2588\u2551 \u255a\u2550\u255d \u2588\u2588\u2551\u2588\u2588\u2551  \u2588\u2588\u2551\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2551\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2551    \u2588\u2588\u2551  \u2588\u2588\u2551\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2551     \u255a\u2588\u2588\u2588\u2588\u2588\u2588\u2554\u255d\u2588\u2588\u2551  \u2588\u2588\u2551   \u2588\u2588\u2551   \n        \u255a\u2550\u255d \u255a\u2550\u2550\u2550\u2550\u2550\u255d     \u255a\u2550\u255d     \u255a\u2550\u255d\u255a\u2550\u255d  \u255a\u2550\u255d\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u255d\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u255d    \u255a\u2550\u255d  \u255a\u2550\u255d\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u255d\u255a\u2550\u255d      \u255a\u2550\u2550\u2550\u2550\u2550\u255d \u255a\u2550\u255d  \u255a\u2550\u255d   \u255a\u2550\u255d   \n                                                                                                          \"\"\".replace(\n    \"\u2588\", f\"{Fore.WHITE}\u2588{Fore.BLACK}\"\n)\n\n\ndef print_logo():\n    os.system(f\"title Instagram Mass Report Bot Made by Rdimo#6969\")",
    "last_modified": "2025-05-04T23:28:20"
  },
  {
    "id": "2641",
    "name": "logo.py.py",
    "path": "github_repo/scripts/03_automation_platforms/logo.py_consolidated/logo.py.py",
    "category": "03_automation_platforms",
    "type": "utility",
    "lines": 29,
    "size": 3654,
    "docstring": "",
    "keywords": [],
    "functions": [
      "print_logo"
    ],
    "classes": [],
    "imports": [
      "libs.animation"
    ],
    "preview": "# coding=utf-8\n#!/usr/bin/env python3\n\nfrom libs.animation import colorText\n\nlogo = \"\"\"\n\n[[black-bright-background]][[red]] \u2588\u2588\u2593 [[green]]\u2588\u2588\u2588\u2584    \u2588  [[blue]] \u2588\u2588\u2588\u2588\u2588\u2588 [[magenta]]\u2584\u2584\u2584\u2588\u2588\u2588\u2588\u2588\u2593 [[cyan]]\u2584\u2584\u2584         [[red]] \u2588\u2588\u2580\u2588\u2588\u2588  [[green]]\u2593\u2588\u2588\u2588\u2588\u2588 [[blue]] \u2588\u2588\u2593\u2588\u2588\u2588   [[magenta]]\u2592\u2588\u2588\u2588\u2588\u2588   [[cyan]]\u2588\u2588\u2580\u2588\u2588\u2588  [[yellow]]\u2584\u2584\u2584\u2588\u2588\u2588\u2588\u2588\u2593[[reset]]\n[[black-bright-background]][[red]]\u2593\u2588\u2588\u2592 [[green]]\u2588\u2588 \u2580\u2588   \u2588 \u2592[[blue]]\u2588\u2588    \u2592 [[magenta]]\u2593  \u2588\u2588\u2592 \u2593\u2592\u2592[[cyan]]\u2588\u2588\u2588\u2588\u2584       [[red]]\u2593\u2588\u2588 \u2592 \u2588\u2588\u2592[[green]]\u2593\u2588   \u2580 [[blue]]\u2593\u2588\u2588\u2591  \u2588\u2588\u2592\u2592[[magenta]]\u2588\u2588\u2592  \u2588\u2588\u2592\u2593[[cyan]]\u2588\u2588 \u2592 \u2588\u2588\u2592[[yellow]]\u2593  \u2588\u2588\u2592 \u2593\u2592[[reset]]\n[[black-bright-background]][[red]]\u2592\u2588\u2588\u2592\u2593[[green]]\u2588\u2588  \u2580\u2588 \u2588\u2588\u2592\u2591[[blue]] \u2593\u2588\u2588\u2584   [[magenta]]\u2592 \u2593\u2588\u2588\u2591 \u2592\u2591\u2592[[cyan]]\u2588\u2588  \u2580\u2588\u2584     [[red]]\u2593\u2588\u2588 \u2591\u2584\u2588 \u2592[[green]]\u2592\u2588\u2588\u2588   [[blue]]\u2593\u2588\u2588\u2591 \u2588\u2588\u2593\u2592\u2592[[magenta]]\u2588\u2588\u2591  \u2588\u2588\u2592\u2593[[cyan]]\u2588\u2588 \u2591\u2584\u2588 \u2592[[yellow]]\u2592 \u2593\u2588\u2588\u2591 \u2592\u2591[[reset]]\n[[black-bright-background]][[red]]\u2591\u2588\u2588\u2591\u2593[[green]]\u2588\u2588\u2592  \u2590\u258c\u2588\u2588\u2592 [[blue]] \u2592   \u2588\u2588\u2592[[magenta]]\u2591 \u2593\u2588\u2588\u2593 \u2591 \u2591[[cyan]]\u2588\u2588\u2584\u2584\u2584\u2584\u2588\u2588    [[red]]\u2592\u2588\u2588\u2580\u2580\u2588\u2584  [[green]]\u2592\u2593\u2588  \u2584 [[blue]]\u2592\u2588\u2588\u2584\u2588\u2593\u2592 \u2592\u2592[[magenta]]\u2588\u2588   \u2588\u2588\u2591\u2592[[cyan]]\u2588\u2588\u2580\u2580\u2588\u2584  [[yellow]]\u2591 \u2593\u2588\u2588\u2593 \u2591 [[reset]]\n[[black-bright-background]][[red]]\u2591\u2588\u2588\u2591\u2592[[green]]\u2588\u2588\u2591   \u2593\u2588\u2588\u2591\u2592[[blue]]\u2588\u2588\u2588\u2588\u2588\u2588\u2592\u2592[[magenta]]  \u2592\u2588\u2588\u2592 \u2591  [[cyan]]\u2593\u2588   \u2593\u2588\u2588\u2592   [[red]]\u2591\u2588\u2588\u2593 \u2592\u2588\u2588\u2592[[green]]\u2591\u2592\u2588\u2588\u2588\u2588\u2592[[blue]]\u2592\u2588\u2588\u2592 \u2591  \u2591\u2591[[magenta]] \u2588\u2588\u2588\u2588\u2593\u2592\u2591\u2591[[cyan]]\u2588\u2588\u2593 \u2592\u2588\u2588\u2592[[yellow]]  \u2592\u2588\u2588\u2592 \u2591 [[reset]]\n[[black-bright-background]][[red]]\u2591\u2593  \u2591[[green]] \u2592\u2591   \u2592 \u2592 \u2592[[blue]] \u2592\u2593\u2592 \u2592 \u2591[[magenta]]  \u2592 \u2591\u2591    [[cyan]]\u2592\u2592   \u2593\u2592\u2588\u2591   [[red]]\u2591 \u2592\u2593 \u2591\u2592\u2593\u2591[[green]]\u2591\u2591 \u2592\u2591 \u2591[[blue]]\u2592\u2593\u2592\u2591 \u2591  \u2591\u2591[[magenta]] \u2592\u2591\u2592\u2591\u2592\u2591 \u2591[[cyan]] \u2592\u2593 \u2591\u2592\u2593\u2591[[yellow]]  \u2592 \u2591\u2591   [[reset]]\n[[black-bright-background]][[red]] \u2592 \u2591\u2591[[green]] \u2591\u2591   \u2591 \u2592\u2591\u2591[[blue]] \u2591\u2592  \u2591 \u2591[[magenta]]    \u2591     [[cyan]] \u2592   \u2592\u2592 \u2591   [[red]]  \u2591\u2592 \u2591 \u2592\u2591[[green]] \u2591 \u2591  \u2591[[blue]]\u2591\u2592 \u2591      [[magenta]] \u2591 \u2592 \u2592\u2591  [[cyan]] \u2591\u2592 \u2591 \u2592\u2591[[yellow]]    \u2591    [[reset]]\n[[black-bright-background]][[red]] \u2592 \u2591 [[green]]  \u2591   \u2591 \u2591 \u2591[[blue]]  \u2591  \u2591  [[magenta]]  \u2591       [[cyan]] \u2591   \u2592      [[red]]  \u2591\u2591   \u2591 [[green]]   \u2591   [[blue]]\u2591\u2591       \u2591[[magenta]] \u2591 \u2591 \u2592   [[cyan]] \u2591\u2591   \u2591 [[yellow]]  \u2591      [[reset]]\n[[black-bright-background]][[red]] \u2591   [[green]]        \u2591  [[blue]]     \u2591  [[magenta]]          [[cyan]]     \u2591  \u2591   [[red]]   \u2591     [[green]]   \u2591  \u2591[[blue]]          [[magenta]]   \u2591 \u2591   [[cyan]]  \u2591     [[yellow]]         [[reset]]\n                                                                                                  \n\n                                           [[black-bright-background]][[white]]Codded By Crevil[[reset]]\n                                            [[black]]Version :- 2.01[[reset]]",
    "last_modified": "2025-03-28T18:35:46"
  },
  {
    "id": "2642",
    "name": "backlinker.py",
    "path": "github_repo/scripts/03_automation_platforms/web_automation/backlinker.py",
    "category": "03_automation_platforms",
    "type": "utility",
    "lines": 42,
    "size": 1394,
    "docstring": "",
    "keywords": [
      "web_tools"
    ],
    "functions": [],
    "classes": [],
    "imports": [
      "json",
      "re",
      "sys",
      "requests"
    ],
    "preview": "import json\nimport re\nimport sys\n\nimport requests\n\ntry:\n    print(\n        \"\"\"\n            _ ____             _    _ _       _             \n           | |  _ \\           | |  | (_)     | |            \n _   _ _ __| | |_) | __ _  ___| | _| |_ _ __ | | _____ _ __ \n| | | | '__| |  _ < / _` |/ __| |/ / | | '_ \\| |/ / _ \\ '__|\n| |_| | |  | | |_) | (_| | (__|   <| | | | | |   <  __/ |   \n \\__,_|_|  |_|____/ \\__,_|\\___|_|\\_\\_|_|_| |_|_|\\_\\___|_|   \n                                              H4-cklinker - wmdark.com     \n  \"\"\"\n    )\n    if sys.version_info.major == 3:\n        site = input(\" => Backlink Kasilcak Site\\t: \")",
    "last_modified": "2025-09-13T05:53:29.852027"
  },
  {
    "id": "2643",
    "name": "myinfo.py",
    "path": "github_repo/scripts/03_automation_platforms/web_automation/myinfo.py",
    "category": "03_automation_platforms",
    "type": "utility",
    "lines": 17,
    "size": 505,
    "docstring": "",
    "keywords": [
      "web_tools"
    ],
    "functions": [],
    "classes": [],
    "imports": [
      "json",
      "requests"
    ],
    "preview": "import json\n\nimport requests\n\nurl = \"https://cloud.leonardo.ai/api/rest/v1/generations/user/f7bb8476-e3f0-4f1f-9a06-4600866cc49c?offset=0&limit=1000\"\nheaders = {\n    \"accept\": \"application/json\",\n    \"authorization\": \"Bearer de7c9cb8-022f-42f8-8bf7-a8f9caadfaee\",\n}\n\nresponse = requests.get(url, headers=headers)\nif response.status_code == 200:\n    data = response.json()\n    print(json.dumps(data, indent=4))  # Pretty print the JSON data\nelse:\n    print(f\"Failed to fetch data: {response.status_code}\")\n",
    "last_modified": "2025-05-04T22:47:12.937472"
  },
  {
    "id": "2644",
    "name": "wrapper.py",
    "path": "github_repo/scripts/03_automation_platforms/web_automation/wrapper.py",
    "category": "03_automation_platforms",
    "type": "utility",
    "lines": 43,
    "size": 1416,
    "docstring": "",
    "keywords": [],
    "functions": [
      "CacheControl"
    ],
    "classes": [],
    "imports": [
      "__future__",
      "typing",
      "pip._vendor.cachecontrol.adapter",
      "pip._vendor.cachecontrol.cache",
      "pip._vendor",
      "pip._vendor.cachecontrol.cache",
      "pip._vendor.cachecontrol.controller",
      "pip._vendor.cachecontrol.heuristics",
      "pip._vendor.cachecontrol.serialize"
    ],
    "preview": "# SPDX-FileCopyrightText: 2015 Eric Larson\n#\n# SPDX-License-Identifier: Apache-2.0\nfrom __future__ import annotations\n\nfrom typing import TYPE_CHECKING, Collection\n\nfrom pip._vendor.cachecontrol.adapter import CacheControlAdapter\nfrom pip._vendor.cachecontrol.cache import DictCache\n\nif TYPE_CHECKING:\n    from pip._vendor import requests\n    from pip._vendor.cachecontrol.cache import BaseCache\n    from pip._vendor.cachecontrol.controller import CacheController\n    from pip._vendor.cachecontrol.heuristics import BaseHeuristic\n    from pip._vendor.cachecontrol.serialize import Serializer\n\n\ndef CacheControl(\n    sess: requests.Session,",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2645",
    "name": "translate.py",
    "path": "github_repo/scripts/03_automation_platforms/web_automation/translate.py",
    "category": "03_automation_platforms",
    "type": "utility",
    "lines": 223,
    "size": 6026,
    "docstring": "",
    "keywords": [],
    "functions": [
      "__init__",
      "translate",
      "__init__",
      "__repr__",
      "__init__",
      "detect"
    ],
    "classes": [
      "TranslationError(Exception)",
      "Translator(object)",
      "DetectionError(Exception)",
      "Language(object)",
      "LanguageDetector(object)"
    ],
    "imports": [
      "urllib",
      "browser",
      "json",
      "simplejson"
    ],
    "preview": "#!/usr/bin/python\n#\n# Peteris Krumins (peter@catonmat.net)\n# http://www.catonmat.net  --  good coders code, great reuse\n#\n# http://www.catonmat.net/blog/python-library-for-google-translate/\n#\n# Code is licensed under MIT license.\n#\n\nfrom urllib import quote_plus\n\nfrom browser import Browser, BrowserError\n\ntry:\n    import json\nexcept:\n    import simplejson as json\n\n",
    "last_modified": "2025-05-04T23:28:20.727612"
  },
  {
    "id": "2646",
    "name": "git_downloader.py",
    "path": "github_repo/scripts/03_automation_platforms/web_automation/git_downloader.py",
    "category": "03_automation_platforms",
    "type": "youtube",
    "lines": 94,
    "size": 3128,
    "docstring": "",
    "keywords": [
      "youtube",
      "web_tools"
    ],
    "functions": [
      "download_repository",
      "download_files",
      "compare_files",
      "main"
    ],
    "classes": [],
    "imports": [
      "hashlib",
      "os",
      "requests",
      "git"
    ],
    "preview": "import hashlib\nimport os\n\nimport requests\nfrom git import Repo\n\n# Define the URL of the GitHub repository\nurl = \"https://github.com/ichoake/python\"\n\n\ndef download_repository(path):\n    \"\"\"Clone the repository using Git.\"\"\"\n    try:\n        Repo.clone_from(url, path)\n        print(\"Repository cloned successfully.\")\n        return True\n    except Exception as e:\n        print(f\"Error cloning repository: {e}\")\n        return False\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2647",
    "name": "compat.py",
    "path": "github_repo/scripts/03_automation_platforms/web_automation/compat.py",
    "category": "03_automation_platforms",
    "type": "utility",
    "lines": 64,
    "size": 1884,
    "docstring": "Stuff that differs in different Python versions and platform\ndistributions.",
    "keywords": [],
    "functions": [
      "has_tls",
      "get_path_uid"
    ],
    "classes": [],
    "imports": [
      "logging",
      "os",
      "sys",
      "pip._vendor.urllib3.util",
      "_ssl"
    ],
    "preview": "\"\"\"Stuff that differs in different Python versions and platform\ndistributions.\"\"\"\n\nimport logging\nimport os\nimport sys\n\n__all__ = [\"get_path_uid\", \"stdlib_pkgs\", \"WINDOWS\"]\n\n\nlogger = logging.getLogger(__name__)\n\n\ndef has_tls() -> bool:\n    try:\n        import _ssl  # noqa: F401  # ignore unused\n\n        return True\n    except ImportError:\n        pass",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2648",
    "name": "filewrapper.py",
    "path": "github_repo/scripts/03_automation_platforms/web_automation/filewrapper.py",
    "category": "03_automation_platforms",
    "type": "organization",
    "lines": 116,
    "size": 4240,
    "docstring": "",
    "keywords": [],
    "functions": [
      "__init__",
      "__getattr__",
      "__is_fp_closed",
      "_close",
      "read",
      "_safe_read"
    ],
    "classes": [
      "CallbackFileWrapper"
    ],
    "imports": [
      "__future__",
      "mmap",
      "tempfile",
      "typing",
      "http.client"
    ],
    "preview": "# SPDX-FileCopyrightText: 2015 Eric Larson\n#\n# SPDX-License-Identifier: Apache-2.0\nfrom __future__ import annotations\n\nimport mmap\nfrom tempfile import NamedTemporaryFile\nfrom typing import TYPE_CHECKING, Any, Callable\n\nif TYPE_CHECKING:\n    from http.client import HTTPResponse\n\n\nclass CallbackFileWrapper:\n    \"\"\"\n    Small wrapper around a fp object which will tee everything read into a\n    buffer, and when that file is closed it will execute a callback with the\n    contents of that buffer.\n\n    All attributes are proxied to the underlying file object.",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2649",
    "name": "follow_unfollow_and_send_telegram_msg.py",
    "path": "github_repo/scripts/03_automation_platforms/web_automation/follow_unfollow_and_send_telegram_msg.py",
    "category": "03_automation_platforms",
    "type": "web_tools",
    "lines": 167,
    "size": 4805,
    "docstring": "This template is written by @Mehran\n\nWhat does this quickstart script aim to do?\n- My quickstart is just for follow/unfollow users.\n\nNOTES:\n- It uses schedulers to trigger activities in chosen hours and also, sends me\n  messages through Telegram API.",
    "keywords": [
      "web_tools"
    ],
    "functions": [
      "get_session",
      "follow",
      "unfollow",
      "xunfollow"
    ],
    "classes": [],
    "imports": [
      "time",
      "traceback",
      "datetime",
      "requests",
      "schedule",
      "instapy"
    ],
    "preview": "\"\"\"\nThis template is written by @Mehran\n\nWhat does this quickstart script aim to do?\n- My quickstart is just for follow/unfollow users.\n\nNOTES:\n- It uses schedulers to trigger activities in chosen hours and also, sends me\n  messages through Telegram API.\n\"\"\"\n\n# -*- coding: UTF-8 -*-\nimport time\nimport traceback\nfrom datetime import datetime\n\nimport requests\nimport schedule\nfrom instapy import InstaPy, smart_run\n",
    "last_modified": "2025-09-13T05:53:49.337466"
  },
  {
    "id": "2650",
    "name": "heuristics.py",
    "path": "github_repo/scripts/03_automation_platforms/web_automation/heuristics.py",
    "category": "03_automation_platforms",
    "type": "utility",
    "lines": 155,
    "size": 4828,
    "docstring": "",
    "keywords": [],
    "functions": [
      "expire_after",
      "datetime_to_header",
      "warning",
      "update_headers",
      "apply",
      "update_headers",
      "__init__",
      "update_headers",
      "warning",
      "update_headers"
    ],
    "classes": [
      "BaseHeuristic",
      "OneDayCache",
      "ExpiresAfter",
      "LastModified"
    ],
    "imports": [
      "__future__",
      "calendar",
      "time",
      "datetime",
      "email.utils",
      "typing",
      "pip._vendor.urllib3"
    ],
    "preview": "# SPDX-FileCopyrightText: 2015 Eric Larson\n#\n# SPDX-License-Identifier: Apache-2.0\nfrom __future__ import annotations\n\nimport calendar\nimport time\nfrom datetime import datetime, timedelta, timezone\nfrom email.utils import formatdate, parsedate, parsedate_tz\nfrom typing import TYPE_CHECKING, Any, Mapping\n\nif TYPE_CHECKING:\n    from pip._vendor.urllib3 import HTTPResponse\n\nTIME_FMT = \"%a, %d %b %Y %H:%M:%S GMT\"\n\n\ndef expire_after(delta: timedelta, date: datetime | None = None) -> datetime:\n    date = date or datetime.now(timezone.utc)\n    return date + delta",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2651",
    "name": "xmlrpc.py",
    "path": "github_repo/scripts/03_automation_platforms/web_automation/xmlrpc.py",
    "category": "03_automation_platforms",
    "type": "utility",
    "lines": 60,
    "size": 1823,
    "docstring": "xmlrpclib.Transport implementation",
    "keywords": [
      "web_tools"
    ],
    "functions": [
      "__init__",
      "request"
    ],
    "classes": [
      "PipXmlrpcTransport"
    ],
    "imports": [
      "logging",
      "urllib.parse",
      "xmlrpc.client",
      "typing",
      "pip._internal.exceptions",
      "pip._internal.network.session",
      "pip._internal.network.utils",
      "xmlrpc.client",
      "_typeshed"
    ],
    "preview": "\"\"\"xmlrpclib.Transport implementation\"\"\"\n\nimport logging\nimport urllib.parse\nimport xmlrpc.client\nfrom typing import TYPE_CHECKING, Tuple\n\nfrom pip._internal.exceptions import NetworkConnectionError\nfrom pip._internal.network.session import PipSession\nfrom pip._internal.network.utils import raise_for_status\n\nif TYPE_CHECKING:\n    from xmlrpc.client import _HostType, _Marshallable\n\n    from _typeshed import SizedBuffer\n\nlogger = logging.getLogger(__name__)\n\n\nclass PipXmlrpcTransport(xmlrpc.client.Transport):",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2652",
    "name": "mywork.py",
    "path": "github_repo/scripts/03_automation_platforms/web_automation/mywork.py",
    "category": "03_automation_platforms",
    "type": "utility",
    "lines": 62,
    "size": 1904,
    "docstring": "",
    "keywords": [
      "web_tools"
    ],
    "functions": [],
    "classes": [],
    "imports": [
      "csv",
      "json",
      "requests"
    ],
    "preview": "import csv\nimport json\n\nimport requests\n\n# Define the URL and headers\nurl = \"https://cloud.leonardo.ai/api/rest/v1/generations/user/f7bb8476-e3f0-4f1f-9a06-4600866cc49c?offset=0&limit=1000\"\nheaders = {\n    \"accept\": \"application/json\",\n    \"authorization\": \"Bearer de7c9cb8-022f-42f8-8bf7-a8f9caadfaee\",\n}\n\ntry:\n    # Make the GET request\n    response = requests.get(url, headers=headers)\n    response.raise_for_status()  # Raises an HTTPError for bad responses\n\n    # Load JSON data from the response\n    data = response.json()\n",
    "last_modified": "2025-05-04T22:47:12.937776"
  },
  {
    "id": "2653",
    "name": "adapter.py",
    "path": "github_repo/scripts/03_automation_platforms/web_automation/adapter.py",
    "category": "03_automation_platforms",
    "type": "utility",
    "lines": 156,
    "size": 6304,
    "docstring": "",
    "keywords": [
      "web_tools"
    ],
    "functions": [
      "__init__",
      "send",
      "build_response",
      "close",
      "_update_chunk_length"
    ],
    "classes": [
      "CacheControlAdapter"
    ],
    "imports": [
      "__future__",
      "functools",
      "types",
      "zlib",
      "typing",
      "pip._vendor.cachecontrol.cache",
      "pip._vendor.cachecontrol.controller",
      "pip._vendor.cachecontrol.filewrapper",
      "pip._vendor.requests.adapters",
      "pip._vendor.cachecontrol.cache"
    ],
    "preview": "# SPDX-FileCopyrightText: 2015 Eric Larson\n#\n# SPDX-License-Identifier: Apache-2.0\nfrom __future__ import annotations\n\nimport functools\nimport types\nimport zlib\nfrom typing import TYPE_CHECKING, Any, Collection, Mapping\n\nfrom pip._vendor.cachecontrol.cache import DictCache\nfrom pip._vendor.cachecontrol.controller import PERMANENT_REDIRECT_STATUSES, CacheController\nfrom pip._vendor.cachecontrol.filewrapper import CallbackFileWrapper\nfrom pip._vendor.requests.adapters import HTTPAdapter\n\nif TYPE_CHECKING:\n    from pip._vendor.cachecontrol.cache import BaseCache\n    from pip._vendor.cachecontrol.heuristics import BaseHeuristic\n    from pip._vendor.cachecontrol.serialize import Serializer\n    from pip._vendor.requests import PreparedRequest, Response",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2654",
    "name": "approve_thread_requests.py",
    "path": "github_repo/scripts/03_automation_platforms/web_automation/approve_thread_requests.py",
    "category": "03_automation_platforms",
    "type": "web_tools",
    "lines": 24,
    "size": 555,
    "docstring": "instabot example\n\nWorkflow:\n    1) approve incoming message requests.",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "argparse",
      "os",
      "sys",
      "instabot"
    ],
    "preview": "\"\"\"\ninstabot example\n\nWorkflow:\n    1) approve incoming message requests.\n\"\"\"\n\nimport argparse\nimport os\nimport sys\n\nsys.path.append(os.path.join(sys.path[0], \"../\"))\nfrom instabot import Bot  # noqa: E402\n\nparser = argparse.ArgumentParser(add_help=True)\nparser.add_argument(\"-u\", type=str, help=\"username\")\nparser.add_argument(\"-p\", type=str, help=\"password\")\nparser.add_argument(\"-proxy\", type=str, help=\"proxy\")\nargs = parser.parse_args()\n",
    "last_modified": "2025-05-04T23:28:20"
  },
  {
    "id": "2655",
    "name": "check_modules.py",
    "path": "github_repo/scripts/03_automation_platforms/web_automation/check_modules.py",
    "category": "03_automation_platforms",
    "type": "utility",
    "lines": 48,
    "size": 1173,
    "docstring": "",
    "keywords": [
      "web_tools"
    ],
    "functions": [
      "check_modules"
    ],
    "classes": [],
    "imports": [
      "sys",
      "warnings",
      "colorama",
      "requests",
      "colorama",
      "asyncio",
      "proxybroker",
      "warnings"
    ],
    "preview": "import sys\n\n\ndef check_modules():\n    try:\n        import requests\n    except:\n        print_error(\"'requests' module not found!\")\n        print_status(\"run install_requirements.bat to install the modules\")\n        sys.exiprint_errort(0)\n\n    try:\n        import colorama\n    except Exception as e:\n        print_error(\"'colorama' package not installed!\")\n        print_status(\"run install_requirements.bat to install the modules\")\n        print(e)\n        sys.exit(0)\n\n    try:",
    "last_modified": "2025-03-28T18:35:48"
  },
  {
    "id": "2656",
    "name": "sdist.py",
    "path": "github_repo/scripts/03_automation_platforms/web_automation/sdist.py",
    "category": "03_automation_platforms",
    "type": "utility",
    "lines": 149,
    "size": 6603,
    "docstring": "",
    "keywords": [],
    "functions": [
      "build_tracker_id",
      "get_metadata_distribution",
      "prepare_distribution_metadata",
      "_prepare_build_backend",
      "_get_build_requires_wheel",
      "_get_build_requires_editable",
      "_install_build_reqs",
      "_raise_conflicts",
      "_raise_missing_reqs"
    ],
    "classes": [
      "SourceDistribution"
    ],
    "imports": [
      "logging",
      "typing",
      "pip._internal.build_env",
      "pip._internal.distributions.base",
      "pip._internal.exceptions",
      "pip._internal.index.package_finder",
      "pip._internal.metadata",
      "pip._internal.utils.subprocess"
    ],
    "preview": "import logging\nfrom typing import Iterable, Optional, Set, Tuple\n\nfrom pip._internal.build_env import BuildEnvironment\nfrom pip._internal.distributions.base import AbstractDistribution\nfrom pip._internal.exceptions import InstallationError\nfrom pip._internal.index.package_finder import PackageFinder\nfrom pip._internal.metadata import BaseDistribution\nfrom pip._internal.utils.subprocess import runner_with_spinner_message\n\nlogger = logging.getLogger(__name__)\n\n\nclass SourceDistribution(AbstractDistribution):\n    \"\"\"Represents a source distribution.\n\n    The preparation step for these needs metadata for the packages to be\n    generated, either using PEP 517 or using the legacy `setup.py egg_info`.\n    \"\"\"\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2657",
    "name": "dalle.py",
    "path": "github_repo/scripts/03_automation_platforms/web_automation/dalle.py",
    "category": "03_automation_platforms",
    "type": "utility",
    "lines": 27,
    "size": 908,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "csv"
    ],
    "preview": "import csv\n\n# Define the input and output file paths\ninput_file_path = \"dalle.txt\"  # Make sure this points to your actual input file path\noutput_file_path = \"output_urls_info.csv\"  # The CSV file to save the output\n\n# Initialize lists to store URLs and descriptions\ndata = []\n\n# Open and read the input file\nwith open(input_file_path, \"r\") as file:\n    lines = file.readlines()\n\n# Iterate over the lines in the file\nfor i in range(0, len(lines), 2):  # Iterate by pairs (URL, description)\n    url = lines[i].strip()\n    description = lines[i + 1].strip() if i + 1 < len(lines) else \"\"\n    data.append([url, description])\n\n# Write the data to a CSV file",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2658",
    "name": "browser.py",
    "path": "github_repo/scripts/03_automation_platforms/web_automation/browser.py",
    "category": "03_automation_platforms",
    "type": "utility",
    "lines": 107,
    "size": 4599,
    "docstring": "",
    "keywords": [],
    "functions": [
      "__init__",
      "connect",
      "http_open",
      "__init__",
      "get_page",
      "set_random_user_agent"
    ],
    "classes": [
      "BrowserError(Exception)",
      "PoolHTTPConnection(httplib.HTTPConnection)",
      "PoolHTTPHandler(urllib2.HTTPHandler)",
      "Browser(object)"
    ],
    "imports": [
      "random",
      "socket",
      "urllib",
      "httplib",
      "urllib2"
    ],
    "preview": "#!/usr/bin/python\n#\n# Peteris Krumins (peter@catonmat.net)\n# http://www.catonmat.net  --  good coders code, great reuse\n#\n# http://www.catonmat.net/blog/python-library-for-google-search/\n#\n# Code is licensed under MIT license.\n#\n\nimport random\nimport socket\nimport urllib\n\nimport httplib\nimport urllib2\n\nBROWSERS = (\n    # Top most popular browsers in my access.log on 2009.02.12\n    # tail -50000 access.log |",
    "last_modified": "2025-05-04T23:28:20.735922"
  },
  {
    "id": "2659",
    "name": "sufflecsv.py",
    "path": "github_repo/scripts/03_automation_platforms/web_automation/sufflecsv.py",
    "category": "03_automation_platforms",
    "type": "utility",
    "lines": 34,
    "size": 860,
    "docstring": "",
    "keywords": [
      "web_tools"
    ],
    "functions": [
      "shuffle_csv"
    ],
    "classes": [],
    "imports": [
      "csv",
      "random",
      "io",
      "requests"
    ],
    "preview": "import csv\nimport random\nfrom io import StringIO\n\nimport requests\n\n\ndef shuffle_csv(url):\n    # Download the CSV data from the URL\n    response = requests.get(url)\n    response.raise_for_status()  # Check for errors\n    csv_data = response.text\n\n    # Read the CSV data\n    csv_file = StringIO(csv_data)\n    reader = csv.reader(csv_file)\n    data = list(reader)\n\n    # Shuffle the data\n    random.shuffle(data)",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2660",
    "name": "setup.py",
    "path": "github_repo/scripts/03_automation_platforms/web_automation/setup.py",
    "category": "03_automation_platforms",
    "type": "setup",
    "lines": 6,
    "size": 194,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [],
    "preview": "# This is the file that will run the setup before we run the main program.\n# This has not been developed yet\n\n# TODO DOWNLOAD THE SELENIUM DRIVER FOR THE USER\n# ALONG WITH OTHER PYTHON PACKAGES\n",
    "last_modified": "2025-03-28T18:35:47.782369"
  },
  {
    "id": "2661",
    "name": "requirements.py",
    "path": "github_repo/scripts/03_automation_platforms/web_automation/requirements.py",
    "category": "03_automation_platforms",
    "type": "utility",
    "lines": 147,
    "size": 4764,
    "docstring": "",
    "keywords": [],
    "functions": [
      "__init__",
      "__str__",
      "__repr__"
    ],
    "classes": [
      "InvalidRequirement",
      "Requirement"
    ],
    "imports": [
      "re",
      "string",
      "urllib.parse",
      "typing",
      "typing",
      "typing",
      "pip._vendor.pyparsing",
      "pip._vendor.pyparsing",
      "pip._vendor.pyparsing",
      "markers"
    ],
    "preview": "# This file is dual licensed under the terms of the Apache License, Version\n# 2.0, and the BSD License. See the LICENSE file in the root of this repository\n# for complete details.\n\nimport re\nimport string\nimport urllib.parse\nfrom typing import List\nfrom typing import Optional as TOptional\nfrom typing import Set\n\nfrom pip._vendor.pyparsing import Combine\nfrom pip._vendor.pyparsing import Literal as L  # noqa\nfrom pip._vendor.pyparsing import (\n    Optional,\n    ParseException,\n    Regex,\n    Word,\n    ZeroOrMore,\n    originalTextFor,",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2662",
    "name": "rank_provider.py",
    "path": "github_repo/scripts/03_automation_platforms/web_automation/rank_provider.py",
    "category": "03_automation_platforms",
    "type": "utility",
    "lines": 257,
    "size": 7414,
    "docstring": "",
    "keywords": [],
    "functions": [
      "__init__",
      "get_rank",
      "__init__",
      "get_rank",
      "__init__",
      "get_rank",
      "_compute_ch_new",
      "_compute_ch",
      "_mix",
      "_wadd"
    ],
    "classes": [
      "RankProvider",
      "AlexaTrafficRank",
      "GooglePageRank"
    ],
    "imports": [
      "re",
      "struct",
      "sys",
      "urllib",
      "xml.etree.ElementTree",
      "httplib",
      "urllib2"
    ],
    "preview": "import re\nimport struct\nimport sys\nimport urllib\nimport xml.etree.ElementTree\n\nimport httplib\nimport urllib2\n\n\nclass RankProvider(object):\n    \"\"\"Abstract class for obtaining the page rank (popularity)\n    from a provider such as Google or Alexa.\n\n    \"\"\"\n\n    def __init__(self, host, proxy=None, timeout=30):\n        \"\"\"Keyword arguments:\n        host -- toolbar host address\n        proxy -- address of proxy server. Default: None",
    "last_modified": "2025-09-13T05:54:17.959882"
  },
  {
    "id": "2663",
    "name": "settings.py",
    "path": "github_repo/scripts/03_automation_platforms/web_automation/settings.py",
    "category": "03_automation_platforms",
    "type": "utility",
    "lines": 175,
    "size": 5564,
    "docstring": "",
    "keywords": [],
    "functions": [
      "crawl",
      "check",
      "crawl_and_check",
      "check_vars",
      "check_toml",
      "get_check_value"
    ],
    "classes": [],
    "imports": [
      "re",
      "pathlib",
      "typing",
      "toml",
      "rich.console",
      "utils.console"
    ],
    "preview": "import re\nfrom pathlib import Path\nfrom typing import Dict, Tuple\n\nimport toml\nfrom rich.console import Console\n\nfrom utils.console import handle_input\n\nconsole = Console()\nconfig = dict  # autocomplete\n\n\ndef crawl(obj: dict, func=lambda x, y: print(x, y, end=\"\\n\"), path=None):\n    if path is None:  # path Default argument value is mutable\n        path = []\n    for key in obj.keys():\n        if type(obj[key]) is dict:\n            crawl(obj[key], func, path + [key])\n            continue",
    "last_modified": "2025-09-13T05:54:00.429473"
  },
  {
    "id": "2664",
    "name": "google_url_scrapper.py",
    "path": "github_repo/scripts/03_automation_platforms/web_automation/google_url_scrapper.py",
    "category": "03_automation_platforms",
    "type": "utility",
    "lines": 54,
    "size": 1513,
    "docstring": "",
    "keywords": [
      "web_tools"
    ],
    "functions": [
      "__init__",
      "scrape",
      "MajesticSEO_API"
    ],
    "classes": [
      "google_url_scrapper"
    ],
    "imports": [
      "datetime",
      "os",
      "random",
      "sys",
      "time",
      "requests",
      "xgoogle.search"
    ],
    "preview": "import datetime\nimport os\nimport random\nimport sys\nimport time\n\nimport requests\nfrom xgoogle.search import GoogleSearch, SearchError\n\n\nclass google_url_scrapper:\n    def __init__(self):        \n        self.urls = []\n        self.seo = ''\n\n    def scrape(self, keyword, pages=2):\n        try:\n            gs = GoogleSearch(keyword)\n            gs.results_per_page = 10\n            gs.page = 0",
    "last_modified": "2025-05-04T23:28:20.714689"
  },
  {
    "id": "2665",
    "name": "lazy_wheel.py",
    "path": "github_repo/scripts/03_automation_platforms/web_automation/lazy_wheel.py",
    "category": "03_automation_platforms",
    "type": "web_tools",
    "lines": 208,
    "size": 7623,
    "docstring": "Lazy ZIP over HTTP",
    "keywords": [
      "web_tools"
    ],
    "functions": [
      "dist_from_wheel_url",
      "__init__",
      "mode",
      "name",
      "seekable",
      "close",
      "closed",
      "read",
      "readable",
      "seek"
    ],
    "classes": [
      "HTTPRangeRequestUnsupported",
      "LazyZipOverHTTP"
    ],
    "imports": [
      "bisect",
      "contextlib",
      "tempfile",
      "typing",
      "zipfile",
      "pip._internal.metadata",
      "pip._internal.network.session",
      "pip._internal.network.utils",
      "pip._vendor.packaging.utils",
      "pip._vendor.requests.models"
    ],
    "preview": "\"\"\"Lazy ZIP over HTTP\"\"\"\n\n__all__ = [\"HTTPRangeRequestUnsupported\", \"dist_from_wheel_url\"]\n\nfrom bisect import bisect_left, bisect_right\nfrom contextlib import contextmanager\nfrom tempfile import NamedTemporaryFile\nfrom typing import Any, Dict, Generator, List, Optional, Tuple\nfrom zipfile import BadZipFile, ZipFile\n\nfrom pip._internal.metadata import BaseDistribution, MemoryWheel, get_wheel_distribution\nfrom pip._internal.network.session import PipSession\nfrom pip._internal.network.utils import HEADERS, raise_for_status, response_chunks\nfrom pip._vendor.packaging.utils import canonicalize_name\nfrom pip._vendor.requests.models import CONTENT_CHUNK_SIZE, Response\n\n\nclass HTTPRangeRequestUnsupported(Exception):\n    pass\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2666",
    "name": "main.py",
    "path": "github_repo/scripts/03_automation_platforms/web_automation/main.py",
    "category": "03_automation_platforms",
    "type": "utility",
    "lines": 81,
    "size": 2388,
    "docstring": "",
    "keywords": [],
    "functions": [
      "doesnt_exist"
    ],
    "classes": [],
    "imports": [
      "os",
      "time",
      "selenium",
      "selenium.common.exceptions",
      "webdriver_manager.chrome"
    ],
    "preview": "import os\nimport time\n\nfrom selenium import webdriver\nfrom selenium.common.exceptions import NoSuchElementException\nfrom webdriver_manager.chrome import ChromeDriverManager as CM\n\nHOW_MANY = int(input(\"How many comments you want to like (0-20):\"))\n\nwhile HOW_MANY > 20:\n    print(\"Cant like more than 20 comments, please choose a smaller number!\")\n    HOW_MANY = int(input(\"How many comments you want to like (0-20):\"))\n\n\noptions = webdriver.ChromeOptions()\noptions.add_argument(\"--log-level=3\")\noptions.add_argument(f\"--user-data-dir={os.getcwd()}\\\\profile\")\nmobile_emulation = {\n    \"userAgent\": \"Mozilla/5.0 (Linux; Android 4.2.1; en-us; Nexus 5 Build/JOP40D) AppleWebKit/535.19 (KHTML, like Gecko) Chrome/90.0.1025.166 Mobile Safari/535.19\"\n}",
    "last_modified": "2025-05-04T23:28:20.677246"
  },
  {
    "id": "2667",
    "name": "urls.py",
    "path": "github_repo/scripts/03_automation_platforms/web_automation/urls.py",
    "category": "03_automation_platforms",
    "type": "utility",
    "lines": 59,
    "size": 1723,
    "docstring": "",
    "keywords": [],
    "functions": [
      "get_url_scheme",
      "path_to_url",
      "url_to_path"
    ],
    "classes": [],
    "imports": [
      "os",
      "string",
      "urllib.parse",
      "urllib.request",
      "typing",
      "compat"
    ],
    "preview": "import os\nimport string\nimport urllib.parse\nimport urllib.request\nfrom typing import Optional\n\nfrom .compat import WINDOWS\n\n\ndef get_url_scheme(url: str) -> Optional[str]:\n    if \":\" not in url:\n        return None\n    return url.split(\":\", 1)[0].lower()\n\n\ndef path_to_url(path: str) -> str:\n    \"\"\"\n    Convert a path to a file: URL.  The path will be made absolute and have\n    quoted path parts.\n    \"\"\"",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2668",
    "name": "test_bot.py",
    "path": "github_repo/scripts/03_automation_platforms/web_automation/test_bot.py",
    "category": "03_automation_platforms",
    "type": "testing",
    "lines": 140,
    "size": 4283,
    "docstring": "",
    "keywords": [
      "testing",
      "web_tools"
    ],
    "functions": [
      "setup",
      "prepare_api",
      "test_login",
      "test_generate_uuid",
      "test_set_user",
      "test_reset_counters",
      "mockreturn",
      "mockreturn_login"
    ],
    "classes": [
      "TestBot",
      "TestBotAPI"
    ],
    "imports": [
      "json",
      "requests",
      "instabot",
      "unittest.mock",
      "mock",
      "uuid"
    ],
    "preview": "import json\n\nimport requests\nfrom instabot import Bot\n\ntry:\n    from unittest.mock import Mock, patch\nexcept ImportError:\n    from mock import Mock, patch\n\n\nclass TestBot:\n    def setup(self):\n        self.USER_ID = 1234567\n        self.USERNAME = \"test_username\"\n        self.PASSWORD = \"test_password\"\n        self.FULLNAME = \"test_full_name\"\n        self.TOKEN = \"abcdef123456\"\n        self.bot = Bot(\n            max_likes_per_day=1000,",
    "last_modified": "2025-09-13T05:54:58.221758"
  },
  {
    "id": "2669",
    "name": "follow_requests.py",
    "path": "github_repo/scripts/03_automation_platforms/web_automation/follow_requests.py",
    "category": "03_automation_platforms",
    "type": "web_tools",
    "lines": 25,
    "size": 741,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "argparse",
      "os",
      "sys",
      "instabot"
    ],
    "preview": "import argparse\nimport os\nimport sys\n\nsys.path.append(os.path.join(sys.path[0], \"../\"))\nfrom instabot import Bot  # noqa: E402\n\nparser = argparse.ArgumentParser(add_help=True)\nparser.add_argument(\"-u\", type=str, help=\"username\")\nparser.add_argument(\"-p\", type=str, help=\"password\")\nparser.add_argument(\"-proxy\", type=str, help=\"proxy\")\nparser.add_argument(\"-story_username\", type=str, help=\"story_username\")\nargs = parser.parse_args()\n\nbot = Bot()\nbot.login(username=args.u, password=args.p, proxy=args.proxy)\n\n# (The following functions apply if you have a private account)\n\n# Approve users that requested to follow you",
    "last_modified": "2025-03-28T18:35:47"
  },
  {
    "id": "2670",
    "name": "test_netlify_uploader.py",
    "path": "github_repo/scripts/03_automation_platforms/web_automation/test_netlify_uploader.py",
    "category": "03_automation_platforms",
    "type": "youtube",
    "lines": 105,
    "size": 4005,
    "docstring": "",
    "keywords": [
      "testing"
    ],
    "functions": [
      "setUp",
      "test_netlify_without_location",
      "test_get_authorization_token",
      "test_get_netlify_site_id",
      "test_deploy_to_netlify",
      "test_upload_gallery"
    ],
    "classes": [
      "NetlifyUploaderTestCase"
    ],
    "imports": [
      "json",
      "os",
      "unittest",
      "unittest",
      "unittest.mock",
      "simplegallery.upload.variants.netlify_uploader",
      "simplegallery.upload.uploader_factory",
      "testfixtures"
    ],
    "preview": "import json\nimport os\nimport unittest\nfrom unittest import mock\nfrom unittest.mock import Mock\n\nimport simplegallery.upload.variants.netlify_uploader as netlify\nfrom simplegallery.upload.uploader_factory import get_uploader\nfrom testfixtures import TempDirectory\n\n\nclass NetlifyUploaderTestCase(unittest.TestCase):\n    def setUp(self) -> None:\n        self.uploader = get_uploader(\"netlify\")\n\n    def test_netlify_without_location(self):\n        self.assertTrue(self.uploader.check_location(\"\"))\n\n    @mock.patch(\"webbrowser.open\")\n    def test_get_authorization_token(self, webbrowser_open):",
    "last_modified": "2025-09-13T05:53:53.233150"
  },
  {
    "id": "2671",
    "name": "news.py",
    "path": "github_repo/scripts/03_automation_platforms/web_automation/news.py",
    "category": "03_automation_platforms",
    "type": "utility",
    "lines": 33,
    "size": 990,
    "docstring": "",
    "keywords": [
      "web_tools"
    ],
    "functions": [
      "__init__",
      "getnews"
    ],
    "classes": [
      "NEWS"
    ],
    "imports": [
      "logging",
      "requests",
      "utilities.const"
    ],
    "preview": "import logging\n\nimport requests\nfrom utilities.const import LOG_PATH, NEWS_FETCH_LIMIT\n\n# Configure logging\nlogging.basicConfig(\n    level=logging.INFO,\n    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n    filename=LOG_PATH,\n)\n\n\nclass NEWS:\n    def __init__(self, news_url):\n        self.news_fetch_limit = NEWS_FETCH_LIMIT\n        self.url = news_url\n\n    def getnews(self):\n        try:",
    "last_modified": "2025-03-28T18:37:11.566049"
  },
  {
    "id": "2672",
    "name": "about.py.py",
    "path": "github_repo/scripts/03_automation_platforms/about.py_consolidated/about.py.py",
    "category": "03_automation_platforms",
    "type": "utility",
    "lines": 36,
    "size": 1334,
    "docstring": "",
    "keywords": [],
    "functions": [
      "about_msg"
    ],
    "classes": [],
    "imports": [
      "os",
      "webbrowser",
      "libs.animation"
    ],
    "preview": "import os  # line:2\nimport webbrowser  # line:3\n\nfrom libs.animation import colorText  # line:1\n\n\ndef about_msg():  # line:4\n    print(\n        colorText(\n            \"\"\"\n\n[[red]] [ 1 ] [[reset]] [[cyan]] Github - https://github.com/crevils\n[[red]] [ 2 ] [[reset]] [[cyan]] Youtube - https://github.com/crevil\n[[red]] [ 3 ] [[reset]] [[cyan]] Telegram - https://t.me/HackerExploits\n[[red]] [ 4 ] [[reset]] [[cyan]] Instagram - https://instagram.com/_crevil\n[[red]] [ 5 ] [[reset]] [[cyan]] EXIT \n\n    \"\"\"\n        )\n    )  # line:13",
    "last_modified": "2025-09-13T05:53:30.471732"
  },
  {
    "id": "2673",
    "name": "about.py_02.py",
    "path": "github_repo/scripts/03_automation_platforms/about.py_consolidated/about.py_02.py",
    "category": "03_automation_platforms",
    "type": "utility",
    "lines": 31,
    "size": 898,
    "docstring": "",
    "keywords": [],
    "functions": [
      "about_msg"
    ],
    "classes": [],
    "imports": [
      "os",
      "webbrowser",
      "libs.animation"
    ],
    "preview": "import os  # line:2\nimport webbrowser  # line:3\n\nfrom libs.animation import colorText  # line:1\n\n\ndef about_msg():  # line:4\n    print(\n        colorText(\n            \"\"\"\n\n\n    \"\"\"\n        )\n    )  # line:13\n    OOO000OO00000O00O = input(\" Select - \")  # line:14\n    if int(OOO000OO00000O00O) == 1:  # line:15\n        webbrowser.open(\"\")  # line:16\n        about_msg()  # line:17\n    if int(OOO000OO00000O00O) == 2:  # line:18",
    "last_modified": "2025-09-13T05:53:30.879501"
  },
  {
    "id": "2674",
    "name": "utils.py_02.py",
    "path": "github_repo/scripts/03_automation_platforms/utils.py_consolidated/utils.py_02.py",
    "category": "03_automation_platforms",
    "type": "utility",
    "lines": 69,
    "size": 1660,
    "docstring": "",
    "keywords": [
      "web_tools"
    ],
    "functions": [
      "print_success",
      "print_error",
      "print_status",
      "ask_question",
      "parse_proxy_file"
    ],
    "classes": [],
    "imports": [
      "random",
      "re",
      "os",
      "sys",
      "colorama",
      "requests"
    ],
    "preview": "import random\nimport re\nfrom os import path\nfrom sys import exit\n\nfrom colorama import Back, Fore, Style\nfrom requests import get\n\n\ndef print_success(message, *argv):\n    print(Fore.GREEN + \"[OK] \" + Style.RESET_ALL + Style.BRIGHT, end=\"\")\n    print(message, end=\" \")\n    for arg in argv:\n        print(arg, end=\" \")\n    print(\"\")\n\n\ndef print_error(message, *argv):\n    print(Fore.RED + \"[ERR] \" + Style.RESET_ALL + Style.BRIGHT, end=\"\")\n    print(message, end=\" \")",
    "last_modified": "2025-05-04T23:28:20"
  },
  {
    "id": "2675",
    "name": "utils.py.py",
    "path": "github_repo/scripts/03_automation_platforms/utils.py_consolidated/utils.py.py",
    "category": "03_automation_platforms",
    "type": "utility",
    "lines": 42,
    "size": 1566,
    "docstring": "",
    "keywords": [],
    "functions": [
      "__init__",
      "endpoint_config_exists",
      "endpoint_exists"
    ],
    "classes": [
      "ResourceManager"
    ],
    "imports": [
      "loguru",
      "llm_engineering.settings",
      "boto3",
      "botocore.exceptions"
    ],
    "preview": "from loguru import logger\n\ntry:\n    import boto3\n    from botocore.exceptions import ClientError\nexcept ModuleNotFoundError:\n    logger.warning(\n        \"Couldn't load AWS or SageMaker imports. Run 'poetry install --with aws' to support AWS.\"\n    )\n\nfrom llm_engineering.settings import settings\n\n\nclass ResourceManager:\n    def __init__(self) -> None:\n        self.sagemaker_client = boto3.client(\n            \"sagemaker\",\n            region_name=settings.AWS_REGION,\n            aws_access_key_id=settings.AWS_ACCESS_KEY,\n            aws_secret_access_key=settings.AWS_SECRET_KEY,",
    "last_modified": "2025-09-13T05:53:42.487557"
  },
  {
    "id": "2676",
    "name": "id.py",
    "path": "github_repo/scripts/03_automation_platforms/social_media_automation/id.py",
    "category": "03_automation_platforms",
    "type": "utility",
    "lines": 13,
    "size": 290,
    "docstring": "",
    "keywords": [],
    "functions": [
      "id"
    ],
    "classes": [],
    "imports": [
      "re",
      "utils.console"
    ],
    "preview": "import re\n\nfrom utils.console import print_substep\n\n\ndef id(reddit_obj: dict):\n    \"\"\"\n    This function takes a reddit object and returns the post id\n    \"\"\"\n    id = re.sub(r\"[^\\w\\s-]\", \"\", reddit_obj[\"thread_id\"])\n    print_substep(f\"Thread ID is {id}\", style=\"bold blue\")\n    return id\n",
    "last_modified": "2025-05-04T22:47:11.890812"
  },
  {
    "id": "2677",
    "name": "test_bot_comment.py",
    "path": "github_repo/scripts/03_automation_platforms/social_media_automation/test_bot_comment.py",
    "category": "03_automation_platforms",
    "type": "testing",
    "lines": 116,
    "size": 4117,
    "docstring": "",
    "keywords": [
      "testing"
    ],
    "functions": [
      "test_comment_feedback",
      "test_comment"
    ],
    "classes": [
      "TestBotGet"
    ],
    "imports": [
      "pytest",
      "responses",
      "instabot.api.config",
      "test_bot",
      "test_variables",
      "unittest.mock",
      "mock"
    ],
    "preview": "import pytest\nimport responses\nfrom instabot.api.config import API_URL\n\nfrom .test_bot import TestBot\nfrom .test_variables import TEST_CAPTION_ITEM, TEST_COMMENT_ITEM\n\ntry:\n    from unittest.mock import patch\nexcept ImportError:\n    from mock import patch\n\n\nclass TestBotGet(TestBot):\n    @responses.activate\n    @pytest.mark.parametrize(\n        \"blocked_actions_protection,blocked_actions\",\n        [(True, True), (True, False), (False, True), (False, False)],\n    )\n    @patch(\"time.sleep\", return_value=None)",
    "last_modified": "2025-09-13T05:54:58.284333"
  },
  {
    "id": "2678",
    "name": "database.py",
    "path": "github_repo/scripts/03_automation_platforms/social_media_automation/database.py",
    "category": "03_automation_platforms",
    "type": "analysis",
    "lines": 300,
    "size": 9403,
    "docstring": "",
    "keywords": [],
    "functions": [
      "startDatabase",
      "initDatabase",
      "beginDatabaseConnection",
      "addFoundClip",
      "getFoundClips",
      "addFilter",
      "getAllSavedFilters",
      "getSavedFilterByName",
      "getFilterNames",
      "getFilterClipCount"
    ],
    "classes": [],
    "imports": [
      "pickle",
      "datetime",
      "mysql.connector",
      "settings",
      "mysql.connector"
    ],
    "preview": "import pickle\nfrom datetime import date\n\nimport mysql.connector\nimport settings\nfrom mysql.connector import pooling\n\ncurrent_date = date.today()\nconnection_pool = None\n\n\ndef startDatabase():\n    beginDatabaseConnection()\n    initDatabase()\n\n\ndef initDatabase():\n    global connection_pool\n    connection_object = connection_pool.get_connection()\n    cursor = connection_object.cursor()",
    "last_modified": "2025-09-13T05:53:32.019246"
  },
  {
    "id": "2679",
    "name": "multi_script_CLI.py",
    "path": "github_repo/scripts/03_automation_platforms/social_media_automation/multi_script_CLI.py",
    "category": "03_automation_platforms",
    "type": "utility",
    "lines": 664,
    "size": 18308,
    "docstring": "",
    "keywords": [],
    "functions": [
      "initial_checker",
      "read_input",
      "setting_input",
      "parameter_setting",
      "username_adder",
      "get_adder",
      "hashtag_adder",
      "competitor_adder",
      "blacklist_adder",
      "whitelist_adder"
    ],
    "classes": [],
    "imports": [
      "__future__",
      "getpass",
      "os",
      "random",
      "sys",
      "time",
      "tqdm",
      "instabot"
    ],
    "preview": "#!/usr/bin/python\n# - * - coding: utf-8 - * -\nfrom __future__ import unicode_literals\n\nimport getpass\nimport os\nimport random\nimport sys\nimport time\n\nfrom tqdm import tqdm\n\nsys.path.append(os.path.join(sys.path[0], \"../\"))\nfrom instabot import Bot\n\n\ndef initial_checker():\n    files = [hashtag_file, users_file, whitelist, blacklist, comment, setting_file]\n    # files = [setting_file]\n    try:",
    "last_modified": "2025-09-13T05:54:55.647239"
  },
  {
    "id": "2680",
    "name": "config-example.py",
    "path": "github_repo/scripts/03_automation_platforms/social_media_automation/config-example.py",
    "category": "03_automation_platforms",
    "type": "setup",
    "lines": 9,
    "size": 175,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [],
    "preview": "# Reddit Praw\npraw_client_id = \"xxxxxx\"\npraw_client_secret = \"xxxxxx\"\npraw_user_agent = \"xxxxxx\"\n\n# Amazon Polly\naws_access_key_id = \"xxxxxx\"\naws_secret_access_key = \"xxxxxx\"\n",
    "last_modified": "2025-03-28T18:35:46.755522"
  },
  {
    "id": "2681",
    "name": "from_link.py",
    "path": "github_repo/scripts/03_automation_platforms/social_media_automation/from_link.py",
    "category": "03_automation_platforms",
    "type": "utility",
    "lines": 49,
    "size": 1203,
    "docstring": "",
    "keywords": [],
    "functions": [
      "main"
    ],
    "classes": [],
    "imports": [
      "argparse",
      "sys",
      "praw",
      "config"
    ],
    "preview": "# Program takes a reddit post link\n# Then returns the top 5 comments from the post\n#\n\n\nimport argparse  # command line argument parser\nimport sys\n\nimport praw\n\nsys.path.append(\"../\")\nimport config\n\n\ndef main() -> int:\n\n    # Creating an argument parser and an argument for the link\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"link\", help=\"the link of the post\")\n    args = parser.parse_args()  # Collecting arguments",
    "last_modified": "2025-05-04T23:28:21.790812"
  },
  {
    "id": "2682",
    "name": "cleanup.py",
    "path": "github_repo/scripts/03_automation_platforms/social_media_automation/cleanup.py",
    "category": "03_automation_platforms",
    "type": "organization",
    "lines": 21,
    "size": 422,
    "docstring": "",
    "keywords": [
      "organization"
    ],
    "functions": [
      "_listdir",
      "cleanup"
    ],
    "classes": [],
    "imports": [
      "os",
      "shutil",
      "os.path"
    ],
    "preview": "import os\nimport shutil\nfrom os.path import exists\n\n\ndef _listdir(d):  # listdir with full path\n    return [os.path.join(d, f) for f in os.listdir(d)]\n\n\ndef cleanup(reddit_id) -> int:\n    \"\"\"Deletes all temporary assets in assets/temp\n\n    Returns:\n        int: How many files were deleted\n    \"\"\"\n    directory = f\"../assets/temp/{reddit_id}/\"\n    if exists(directory):\n        shutil.rmtree(directory)\n\n        return 1",
    "last_modified": "2025-05-04T22:47:11.888656"
  },
  {
    "id": "2683",
    "name": "instaclient.py",
    "path": "github_repo/scripts/03_automation_platforms/social_media_automation/instaclient.py",
    "category": "03_automation_platforms",
    "type": "utility",
    "lines": 204,
    "size": 7337,
    "docstring": "",
    "keywords": [
      "web_tools"
    ],
    "functions": [
      "__init__",
      "SetDefaultHeaders",
      "IsCookiesOK",
      "GetAndUpdate",
      "PostAndUpdate",
      "Connect",
      "Login",
      "Spam"
    ],
    "classes": [
      "InstaClient"
    ],
    "imports": [
      "random",
      "libs.utils",
      "requests"
    ],
    "preview": "from random import choice\n\nfrom libs.utils import CheckPublicIP, IsProxyWorking, PrintError, PrintStatus, PrintSuccess\nfrom requests import Session\n\nUSER_AGENTS = [\n    \"Mozilla/5.0 (Android 4.4; Mobile; rv:41.0) Gecko/41.0 Firefox/41.0\",\n    \"Mozilla/5.0 (Android 4.4; Tablet; rv:41.0) Gecko/41.0 Firefox/41.0\",\n    \"Mozilla/5.0 (Windows NT x.y; rv:10.0) Gecko/20100101 Firefox/10.0\",\n    \"Mozilla/5.0 (X11; Linux i686; rv:10.0) Gecko/20100101 Firefox/10.0\",\n    \"Mozilla/5.0 (X11; Linux x86_64; rv:10.0) Gecko/20100101 Firefox/10.0\",\n    \"Mozilla/5.0 (Android 4.4; Mobile; rv:41.0) Gecko/41.0 Firefox/41.0\",\n]\n\n\nclass InstaClient:\n    def __init__(self, user, password, ip, port):\n        self.isproxyok = True\n        self.ip = ip\n        self.port = port",
    "last_modified": "2025-09-13T05:53:28.033763"
  },
  {
    "id": "2684",
    "name": "scrape100.py",
    "path": "github_repo/scripts/03_automation_platforms/social_media_automation/scrape100.py",
    "category": "03_automation_platforms",
    "type": "web_tools",
    "lines": 16,
    "size": 365,
    "docstring": "",
    "keywords": [
      "web_tools"
    ],
    "functions": [],
    "classes": [],
    "imports": [
      "requests",
      "bs4"
    ],
    "preview": "import requests\nfrom bs4 import BeautifulSoup\n\nurl = requests.get(\"https://redditmetrics.com/top\")\n\nsoup = BeautifulSoup(url.text, \"html.parser\")\n\n\nwith open(\"sb.txt\", \"w\") as f:\n    for subreddit in soup.find_all(\"a\"):\n        try:\n            if \"/r\" in subreddit.string:\n                f.write(subreddit.string[3:] + \"\\n\")\n        except:\n            TypeError\n",
    "last_modified": "2025-05-04T23:28:25.649247"
  },
  {
    "id": "2685",
    "name": "autodownloader.py",
    "path": "github_repo/scripts/03_automation_platforms/social_media_automation/autodownloader.py",
    "category": "03_automation_platforms",
    "type": "youtube",
    "lines": 67,
    "size": 2273,
    "docstring": "",
    "keywords": [
      "youtube"
    ],
    "functions": [
      "__init__",
      "startAutoMode",
      "startDownloading",
      "startFinding",
      "stop",
      "findClips",
      "downloadClips"
    ],
    "classes": [
      "AutoDownloader"
    ],
    "imports": [
      "threading",
      "time",
      "database",
      "tiktok"
    ],
    "preview": "from threading import Thread\nfrom time import sleep\n\nimport database\nimport tiktok\n\n\nclass AutoDownloader:\n    def __init__(self, window, downloadqueue):\n        self.window = window\n        self.autoDownloadQueue = downloadqueue\n        self.clipIndex = 0\n        self.auto = False\n\n    def startAutoMode(self):\n        self.auto = True\n        self.findClips()\n\n    def startDownloading(self):\n        self.downloadClips()",
    "last_modified": "2025-09-13T05:53:31.618362"
  },
  {
    "id": "2686",
    "name": "bot_block.py",
    "path": "github_repo/scripts/03_automation_platforms/social_media_automation/bot_block.py",
    "category": "03_automation_platforms",
    "type": "utility",
    "lines": 67,
    "size": 2025,
    "docstring": "",
    "keywords": [],
    "functions": [
      "block",
      "unblock",
      "block_users",
      "unblock_users",
      "block_bots"
    ],
    "classes": [],
    "imports": [
      "random",
      "tqdm"
    ],
    "preview": "import random\n\nfrom tqdm import tqdm\n\n\ndef block(self, user_id):\n    user_id = self.convert_to_user_id(user_id)\n    if self.check_not_bot(user_id):\n        return True\n    if not self.reached_limit(\"blocks\"):\n        self.delay(\"block\")\n        if self.api.block(user_id):\n            self.total[\"blocks\"] += 1\n            return True\n    else:\n        self.logger.info(\"Out of blocks for today.\")\n    return False\n\n\ndef unblock(self, user_id):",
    "last_modified": "2025-09-13T05:54:57.121697"
  },
  {
    "id": "2687",
    "name": "Instagram Report Bot2.py",
    "path": "github_repo/scripts/03_automation_platforms/social_media_automation/Instagram Report Bot2.py",
    "category": "03_automation_platforms",
    "type": "utility",
    "lines": 102,
    "size": 2267,
    "docstring": "",
    "keywords": [],
    "functions": [
      "getOptions"
    ],
    "classes": [],
    "imports": [
      "argparse",
      "sys",
      "time",
      "pyautogui",
      "webbot"
    ],
    "preview": "import argparse\nimport sys\nimport time\n\nimport pyautogui\nfrom webbot import *\n\n\n# To parse the arguments\ndef getOptions(args=sys.argv[1:]):\n\n    parser = argparse.ArgumentParser(\n        description=\"This bot helps users to mass report accounts with clickbaits or objectionable material.\"\n    )\n    parser.add_argument(\"-u\", \"--username\", type=str, default=\"\", help=\"Username to report.\")\n    parser.add_argument(\n        \"-f\",\n        \"--file\",\n        type=str,\n        default=\"acc.txt\",",
    "last_modified": "2025-09-13T05:53:30.312819"
  },
  {
    "id": "2688",
    "name": "reddit_scraper.py",
    "path": "github_repo/scripts/03_automation_platforms/social_media_automation/reddit_scraper.py",
    "category": "03_automation_platforms",
    "type": "web_tools",
    "lines": 44,
    "size": 1182,
    "docstring": "",
    "keywords": [
      "youtube"
    ],
    "functions": [
      "download_vid",
      "reddit_scraper"
    ],
    "classes": [],
    "imports": [
      "os",
      "praw",
      "redvid",
      "config"
    ],
    "preview": "import os\n\nimport praw\nfrom redvid import Downloader\n\nimport config\n\n\ndef download_vid(url, directory):  # Download reddit vid given URL and directory\n    download = Downloader(url, max_q=True)\n    download.path = directory\n    download.download()\n    print(os.listdir(directory))\n\n\ndef reddit_scraper(subreddit):  # pulls out top reddit posts\n    print(\"Logging into Reddit...\")\n\n    red = praw.Reddit(\n        client_id=config.reddit_login[\"client_id\"],",
    "last_modified": "2025-09-11T13:27:03.985868"
  },
  {
    "id": "2689",
    "name": "art.py",
    "path": "github_repo/scripts/03_automation_platforms/social_media_automation/art.py",
    "category": "03_automation_platforms",
    "type": "web_tools",
    "lines": 25,
    "size": 450,
    "docstring": "Created in 07/2020\n@Author: Paulo https://github.com/alpdias",
    "keywords": [],
    "functions": [
      "artName"
    ],
    "classes": [],
    "imports": [
      "time",
      "pyfiglet"
    ],
    "preview": "# -*- coding: utf-8 -*-\n\n\"\"\"\nCreated in 07/2020\n@Author: Paulo https://github.com/alpdias\n\"\"\"\n\n# imported libraries\nfrom time import sleep\n\nfrom pyfiglet import Figlet\n\n\ndef artName(timeSleep=0):\n    \"\"\"\n    -> function to print text in ascii art\\\n    \\n:param timeSleep: art loading time\\\n    \\n:return: ascii art\\\n    \"\"\"\n",
    "last_modified": "2025-05-04T23:28:20.977701"
  },
  {
    "id": "2690",
    "name": "ex.py",
    "path": "github_repo/scripts/03_automation_platforms/social_media_automation/ex.py",
    "category": "03_automation_platforms",
    "type": "utility",
    "lines": 29,
    "size": 568,
    "docstring": "",
    "keywords": [],
    "functions": [
      "main"
    ],
    "classes": [],
    "imports": [
      "sys",
      "praw",
      "config"
    ],
    "preview": "import sys\n\nimport praw\n\nsys.path.append(\"../\")\nimport config\n\n\n# Main driver function for software\ndef main() -> int:\n\n    # Creating a reddit api instance\n    reddit = praw.Reddit(\n        client_id=config.PRAW_CONFIG[\"client_id\"],\n        client_secret=config.PRAW_CONFIG[\"client_secret\"],\n        user_agent=config.PRAW_CONFIG[\"user_agent\"],\n    )\n\n    # Looping a subreddit\n    for submission in reddit.subreddit(\"learnpython\").hot(limit=10):",
    "last_modified": "2025-05-04T23:28:21.795450"
  },
  {
    "id": "2691",
    "name": "GTTS.py",
    "path": "github_repo/scripts/03_automation_platforms/social_media_automation/GTTS.py",
    "category": "03_automation_platforms",
    "type": "utility",
    "lines": 23,
    "size": 443,
    "docstring": "",
    "keywords": [],
    "functions": [
      "__init__",
      "run",
      "randomvoice"
    ],
    "classes": [
      "GTTS"
    ],
    "imports": [
      "random",
      "gtts",
      "utils"
    ],
    "preview": "import random\n\nfrom gtts import gTTS\n\nfrom utils import settings\n\n\nclass GTTS:\n    def __init__(self):\n        self.max_chars = 5000\n        self.voices = []\n\n    def run(self, text, filepath):\n        tts = gTTS(\n            text=text,\n            lang=settings.config[\"reddit\"][\"thread\"][\"post_lang\"] or \"en\",\n            slow=False,\n        )\n        tts.save(filepath)\n",
    "last_modified": "2025-09-11T13:27:01.858790"
  },
  {
    "id": "2692",
    "name": "chat_base.py",
    "path": "github_repo/scripts/05_data_management/file_organization/chat_base.py",
    "category": "05_data_management",
    "type": "utility",
    "lines": 33,
    "size": 764,
    "docstring": "",
    "keywords": [],
    "functions": [
      "TRChatBase"
    ],
    "classes": [],
    "imports": [
      "logging",
      "os",
      "chatbase",
      "pyrogram",
      "translation",
      "sample_config",
      "config"
    ],
    "preview": "import logging\n\nlogging.basicConfig(\n    level=logging.DEBUG, format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\n)\nlogger = logging.getLogger(__name__)\n\nimport os\n\n# the secret configuration specific things\nif bool(os.environ.get(\"WEBHOOK\", False)):\n    from sample_config import Config\nelse:\n    from config import Config\n\n# the Strings used for this \"thing\"\n# the Telegram trackings\nfrom chatbase import Message\nfrom pyrogram import Client, Filters\nfrom translation import Translation",
    "last_modified": "2025-05-06T04:35:15.016401"
  },
  {
    "id": "2693",
    "name": "group_files_with_date.py",
    "path": "github_repo/scripts/05_data_management/file_organization/group_files_with_date.py",
    "category": "05_data_management",
    "type": "organization",
    "lines": 43,
    "size": 1613,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "csv",
      "os",
      "collections",
      "datetime"
    ],
    "preview": "import csv\nimport os\nfrom collections import defaultdict\nfrom datetime import datetime\n\n# Prompt the user for the directory\ndirectory = input(\"Enter the directory containing the files: \").strip()\n\n# Get the root folder name\nroot_folder = os.path.basename(os.path.normpath(directory))\ncurrent_date = datetime.now().strftime(\"%m%d%y\")\noutput_csv = os.path.join(directory, f\"{root_folder}-{current_date}.csv\")\n\n# Step 1: Group files by song title (ignoring version indicators)\nfile_groups = defaultdict(list)\n\ntry:\n    # Check if the directory exists\n    if not os.path.isdir(directory):\n        raise FileNotFoundError(f\"Directory '{directory}' does not exist.\")",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2694",
    "name": "config_loader.py",
    "path": "github_repo/scripts/05_data_management/file_organization/config_loader.py",
    "category": "05_data_management",
    "type": "setup",
    "lines": 19,
    "size": 349,
    "docstring": "",
    "keywords": [],
    "functions": [
      "load_config",
      "load_settings"
    ],
    "classes": [],
    "imports": [
      "json",
      "pathlib"
    ],
    "preview": "import json\nfrom pathlib import Path\n\nCONFIG_DIR = Path(__file__).parent.parent / \"config\"\n\n\ndef load_config():\n    with open(CONFIG_DIR / \"exclusions.json\") as f:\n        return json.load(f)\n\n\ndef load_settings():\n    with open(CONFIG_DIR / \"settings.json\") as f:\n        return json.load(f)\n\n\nEXCLUSIONS = load_config()\nSETTINGS = load_settings()\n",
    "last_modified": "2025-05-04T22:47:12.611439"
  },
  {
    "id": "2695",
    "name": "intranges.py",
    "path": "github_repo/scripts/05_data_management/file_organization/intranges.py",
    "category": "05_data_management",
    "type": "utility",
    "lines": 58,
    "size": 1898,
    "docstring": "Given a list of integers, made up of (hopefully) a small number of long runs\nof consecutive integers, compute a representation of the form\n((start1, end1), (start2, end2) ...). Then answer the question \"was x present\nin the original list?\" in time O(log(# runs)).",
    "keywords": [],
    "functions": [
      "intranges_from_list",
      "_encode_range",
      "_decode_range",
      "intranges_contain"
    ],
    "classes": [],
    "imports": [
      "bisect",
      "typing"
    ],
    "preview": "\"\"\"\nGiven a list of integers, made up of (hopefully) a small number of long runs\nof consecutive integers, compute a representation of the form\n((start1, end1), (start2, end2) ...). Then answer the question \"was x present\nin the original list?\" in time O(log(# runs)).\n\"\"\"\n\nimport bisect\nfrom typing import List, Tuple\n\n\ndef intranges_from_list(list_: List[int]) -> Tuple[int, ...]:\n    \"\"\"Represent a list of integers as a sequence of ranges:\n    ((start_0, end_0), (start_1, end_1), ...), such that the original\n    integers are exactly those x such that start_i <= x < end_i for some i.\n\n    Ranges are encoded as single integers (start << 32 | end), not as tuples.\n    \"\"\"\n\n    sorted_list = sorted(list_)",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2696",
    "name": "config.py",
    "path": "github_repo/scripts/05_data_management/file_organization/config.py",
    "category": "05_data_management",
    "type": "setup",
    "lines": 16,
    "size": 412,
    "docstring": "",
    "keywords": [],
    "functions": [
      "parse_config"
    ],
    "classes": [],
    "imports": [
      "os.path",
      "ConfigParser",
      "configparser"
    ],
    "preview": "try:\n    from ConfigParser import ConfigParser\nexcept ImportError as e:\n    from configparser import ConfigParser\n\nfrom os.path import abspath, dirname, join\n\nconfig = ConfigParser()\nconfig_path = join(abspath(dirname(dirname(__file__))), \"config\", \"config.cfg\")\nconfig.read(config_path)\n\n\ndef parse_config(app_name=\"local\"):\n    \"\"\"Parse config for the given app name\"\"\"\n    return dict(config.items(app_name))\n",
    "last_modified": "2025-05-04T23:28:19.982710"
  },
  {
    "id": "2697",
    "name": "pyproject.py",
    "path": "github_repo/scripts/05_data_management/file_organization/pyproject.py",
    "category": "05_data_management",
    "type": "utility",
    "lines": 178,
    "size": 7135,
    "docstring": "",
    "keywords": [],
    "functions": [
      "_is_list_of_str",
      "make_pyproject_path",
      "load_pyproject_toml"
    ],
    "classes": [],
    "imports": [
      "importlib.util",
      "os",
      "collections",
      "typing",
      "pip._internal.exceptions",
      "pip._vendor",
      "pip._vendor.packaging.requirements"
    ],
    "preview": "import importlib.util\nimport os\nfrom collections import namedtuple\nfrom typing import Any, List, Optional\n\nfrom pip._internal.exceptions import (\n    InstallationError,\n    InvalidPyProjectBuildRequires,\n    MissingPyProjectBuildRequires,\n)\nfrom pip._vendor import tomli\nfrom pip._vendor.packaging.requirements import InvalidRequirement, Requirement\n\n\ndef _is_list_of_str(obj: Any) -> bool:\n    return isinstance(obj, list) and all(isinstance(item, str) for item in obj)\n\n\ndef make_pyproject_path(unpacked_source_directory: str) -> str:\n    return os.path.join(unpacked_source_directory, \"pyproject.toml\")",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2698",
    "name": "pdf-merge.py",
    "path": "github_repo/scripts/05_data_management/file_organization/pdf-merge.py",
    "category": "05_data_management",
    "type": "utility",
    "lines": 28,
    "size": 686,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "os",
      "PyPDF2"
    ],
    "preview": "import os\n\nfrom PyPDF2 import PdfMerger\n\n# Directory containing the PDF files\npdf_dir = \"/Users/steven/Documents/project2025/book\"\n\n# Output file name\noutput_file = \"merged_document.pdf\"\n\n# Create a PdfMerger object\nmerger = PdfMerger()\n\n# Iterate through the files in the directory in order from ch1.pdf to ch30.pdf\nfor i in range(1, 31):\n    pdf_file = f\"ch{i}.pdf\"\n    pdf_path = os.path.join(pdf_dir, pdf_file)\n    if os.path.isfile(pdf_path):\n        merger.append(pdf_path)\n    else:",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2699",
    "name": "files_io.py",
    "path": "github_repo/scripts/05_data_management/file_organization/files_io.py",
    "category": "05_data_management",
    "type": "organization",
    "lines": 32,
    "size": 979,
    "docstring": "",
    "keywords": [],
    "functions": [
      "read",
      "write"
    ],
    "classes": [
      "JsonFileManager"
    ],
    "imports": [
      "json",
      "pathlib"
    ],
    "preview": "import json\nfrom pathlib import Path\n\n\nclass JsonFileManager:\n    @classmethod\n    def read(cls, filename: str | Path) -> list:\n        file_path: Path = Path(filename)\n\n        try:\n            with file_path.open(\"r\") as file:\n                return json.load(file)\n        except FileNotFoundError:\n            raise FileNotFoundError(f\"File '{file_path=}' does not exist.\") from None\n        except json.JSONDecodeError as e:\n            raise json.JSONDecodeError(\n                msg=f\"File '{file_path=}' is not properly formatted as JSON.\",\n                doc=e.doc,\n                pos=e.pos,\n            ) from None",
    "last_modified": "2025-05-04T22:47:11.568362"
  },
  {
    "id": "2700",
    "name": "find_icloud_duplicates.py",
    "path": "github_repo/scripts/05_data_management/file_organization/find_icloud_duplicates.py",
    "category": "05_data_management",
    "type": "utility",
    "lines": 58,
    "size": 1721,
    "docstring": "",
    "keywords": [],
    "functions": [
      "hash_file",
      "find_duplicates",
      "save_duplicates_to_csv"
    ],
    "classes": [],
    "imports": [
      "csv",
      "hashlib",
      "os",
      "collections"
    ],
    "preview": "import csv\nimport hashlib\nimport os\nfrom collections import defaultdict\n\nICLOUD_DIR = \"/Users/steven/Library/Mobile Documents/com~apple~CloudDocs\"\nDUPLICATES_CSV = \"icloud_duplicates_report.csv\"\n\n\ndef hash_file(path):\n    hasher = hashlib.sha256()\n    try:\n        with open(path, \"rb\") as f:\n            while chunk := f.read(8192):\n                hasher.update(chunk)\n        return hasher.hexdigest()\n    except Exception as e:\n        print(f\"Error reading {path}: {e}\")\n        return None\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2701",
    "name": "target_python.py",
    "path": "github_repo/scripts/05_data_management/file_organization/target_python.py",
    "category": "05_data_management",
    "type": "utility",
    "lines": 117,
    "size": 4218,
    "docstring": "",
    "keywords": [],
    "functions": [
      "__init__",
      "format_given",
      "get_sorted_tags",
      "get_unsorted_tags"
    ],
    "classes": [
      "TargetPython"
    ],
    "imports": [
      "sys",
      "typing",
      "pip._internal.utils.compatibility_tags",
      "pip._internal.utils.misc",
      "pip._vendor.packaging.tags"
    ],
    "preview": "import sys\nfrom typing import List, Optional, Set, Tuple\n\nfrom pip._internal.utils.compatibility_tags import get_supported, version_info_to_nodot\nfrom pip._internal.utils.misc import normalize_version_info\nfrom pip._vendor.packaging.tags import Tag\n\n\nclass TargetPython:\n    \"\"\"\n    Encapsulates the properties of a Python interpreter one is targeting\n    for a package install, download, etc.\n    \"\"\"\n\n    __slots__ = [\n        \"_given_py_version_info\",\n        \"abis\",\n        \"implementation\",\n        \"platforms\",\n        \"py_version\",",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2702",
    "name": "organize.py",
    "path": "github_repo/scripts/05_data_management/file_organization/organize.py",
    "category": "05_data_management",
    "type": "organization",
    "lines": 29,
    "size": 1065,
    "docstring": "",
    "keywords": [],
    "functions": [
      "copy_files_with_structure"
    ],
    "classes": [],
    "imports": [
      "csv",
      "os",
      "shutil"
    ],
    "preview": "import csv\nimport os\nimport shutil\n\n\ndef copy_files_with_structure(csv_file_path, destination_base_path):\n    with open(csv_file_path, \"r\") as csv_file:\n        reader = csv.DictReader(csv_file)\n        for row in reader:\n            original_path = row[\"Original Path\"]\n            destination_path = os.path.join(destination_base_path, original_path.lstrip(os.sep))\n\n            # Create the destination directory if it doesn't exist\n            destination_dir = os.path.dirname(destination_path)\n            if not os.path.exists(destination_dir):\n                os.makedirs(destination_dir)\n\n            # Copy the file to the destination\n            shutil.copy2(original_path, destination_path)\n            print(f\"Copied {original_path} to {destination_path}\")",
    "last_modified": "2025-09-13T05:53:48.520011"
  },
  {
    "id": "2703",
    "name": "zip-t0-csv.py",
    "path": "github_repo/scripts/05_data_management/file_organization/zip-t0-csv.py",
    "category": "05_data_management",
    "type": "utility",
    "lines": 62,
    "size": 1864,
    "docstring": "",
    "keywords": [],
    "functions": [
      "main"
    ],
    "classes": [],
    "imports": [
      "csv",
      "os",
      "zipfile",
      "datetime"
    ],
    "preview": "import csv\nimport os\nimport zipfile\nfrom datetime import datetime\n\n\ndef main():\n    # Prompt for folder path\n    zip_folder = input(\"\ud83d\udcc2 Enter the full path to the folder containing ZIP files: \").strip()\n\n    if not os.path.isdir(zip_folder):\n        print(f\"\u274c Error: '{zip_folder}' is not a valid directory.\")\n        return\n\n    # Output CSV name (saved in the current script directory)\n    output_csv = \"zip_contents_inventory.csv\"\n\n    # Scan for .zip files\n    zip_files = [\n        os.path.join(zip_folder, f) for f in os.listdir(zip_folder) if f.lower().endswith(\".zip\")",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2704",
    "name": "editable_legacy.py",
    "path": "github_repo/scripts/05_data_management/file_organization/editable_legacy.py",
    "category": "05_data_management",
    "type": "setup",
    "lines": 47,
    "size": 1282,
    "docstring": "Legacy editable installation process, i.e. `setup.py develop`.",
    "keywords": [],
    "functions": [
      "install_editable"
    ],
    "classes": [],
    "imports": [
      "logging",
      "typing",
      "pip._internal.build_env",
      "pip._internal.utils.logging",
      "pip._internal.utils.setuptools_build",
      "pip._internal.utils.subprocess"
    ],
    "preview": "\"\"\"Legacy editable installation process, i.e. `setup.py develop`.\"\"\"\n\nimport logging\nfrom typing import Optional, Sequence\n\nfrom pip._internal.build_env import BuildEnvironment\nfrom pip._internal.utils.logging import indent_log\nfrom pip._internal.utils.setuptools_build import make_setuptools_develop_args\nfrom pip._internal.utils.subprocess import call_subprocess\n\nlogger = logging.getLogger(__name__)\n\n\ndef install_editable(\n    *,\n    global_options: Sequence[str],\n    prefix: Optional[str],\n    home: Optional[str],\n    use_user_site: bool,\n    name: str,",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2705",
    "name": "zip_inventory_to_csv.py",
    "path": "github_repo/scripts/05_data_management/file_organization/zip_inventory_to_csv.py",
    "category": "05_data_management",
    "type": "utility",
    "lines": 62,
    "size": 1864,
    "docstring": "",
    "keywords": [],
    "functions": [
      "main"
    ],
    "classes": [],
    "imports": [
      "csv",
      "os",
      "zipfile",
      "datetime"
    ],
    "preview": "import csv\nimport os\nimport zipfile\nfrom datetime import datetime\n\n\ndef main():\n    # Prompt for folder path\n    zip_folder = input(\"\ud83d\udcc2 Enter the full path to the folder containing ZIP files: \").strip()\n\n    if not os.path.isdir(zip_folder):\n        print(f\"\u274c Error: '{zip_folder}' is not a valid directory.\")\n        return\n\n    # Output CSV name (saved in the current script directory)\n    output_csv = \"zip_contents_inventory.csv\"\n\n    # Scan for .zip files\n    zip_files = [\n        os.path.join(zip_folder, f) for f in os.listdir(zip_folder) if f.lower().endswith(\".zip\")",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2706",
    "name": "posttextparser.py",
    "path": "github_repo/scripts/05_data_management/file_organization/posttextparser.py",
    "category": "05_data_management",
    "type": "utility",
    "lines": 36,
    "size": 844,
    "docstring": "",
    "keywords": [],
    "functions": [
      "posttextparser"
    ],
    "classes": [],
    "imports": [
      "os",
      "re",
      "time",
      "typing",
      "spacy",
      "utils.console",
      "utils.voice"
    ],
    "preview": "import os\nimport re\nimport time\nfrom typing import List\n\nimport spacy\n\nfrom utils.console import print_step\nfrom utils.voice import sanitize_text\n\n\n# working good\ndef posttextparser(obj, *, tried: bool = False) -> List[str]:\n    text: str = re.sub(\"\\n\", \" \", obj)\n    try:\n        nlp = spacy.load(\"en_core_web_sm\")\n    except OSError as e:\n        if not tried:\n            os.system(\"python -m spacy download en_core_web_sm\")\n            time.sleep(5)",
    "last_modified": "2025-09-11T13:27:01.781766"
  },
  {
    "id": "2707",
    "name": "download.py",
    "path": "github_repo/scripts/05_data_management/file_organization/download.py",
    "category": "05_data_management",
    "type": "youtube",
    "lines": 148,
    "size": 5335,
    "docstring": "",
    "keywords": [],
    "functions": [
      "add_options",
      "run"
    ],
    "classes": [
      "DownloadCommand"
    ],
    "imports": [
      "logging",
      "os",
      "optparse",
      "typing",
      "pip._internal.cli",
      "pip._internal.cli.cmdoptions",
      "pip._internal.cli.req_command",
      "pip._internal.cli.status_codes",
      "pip._internal.operations.build.build_tracker",
      "pip._internal.req.req_install"
    ],
    "preview": "import logging\nimport os\nfrom optparse import Values\nfrom typing import List\n\nfrom pip._internal.cli import cmdoptions\nfrom pip._internal.cli.cmdoptions import make_target_python\nfrom pip._internal.cli.req_command import RequirementCommand, with_cleanup\nfrom pip._internal.cli.status_codes import SUCCESS\nfrom pip._internal.operations.build.build_tracker import get_build_tracker\nfrom pip._internal.req.req_install import check_legacy_setup_py_options\nfrom pip._internal.utils.misc import ensure_dir, normalize_path, write_output\nfrom pip._internal.utils.temp_dir import TempDirectory\n\nlogger = logging.getLogger(__name__)\n\n\nclass DownloadCommand(RequirementCommand):\n    \"\"\"\n    Download packages from:",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2708",
    "name": "filesystem.py",
    "path": "github_repo/scripts/05_data_management/file_organization/filesystem.py",
    "category": "05_data_management",
    "type": "organization",
    "lines": 153,
    "size": 5121,
    "docstring": "",
    "keywords": [
      "testing"
    ],
    "functions": [
      "check_path_owner",
      "adjacent_tmp_file",
      "test_writable_dir",
      "_test_writable_dir_win",
      "find_files",
      "file_size",
      "format_file_size",
      "directory_size",
      "format_directory_size"
    ],
    "classes": [],
    "imports": [
      "fnmatch",
      "os",
      "os.path",
      "random",
      "sys",
      "contextlib",
      "tempfile",
      "typing",
      "pip._internal.utils.compat",
      "pip._internal.utils.misc"
    ],
    "preview": "import fnmatch\nimport os\nimport os.path\nimport random\nimport sys\nfrom contextlib import contextmanager\nfrom tempfile import NamedTemporaryFile\nfrom typing import Any, BinaryIO, Generator, List, Union, cast\n\nfrom pip._internal.utils.compat import get_path_uid\nfrom pip._internal.utils.misc import format_size\nfrom pip._vendor.tenacity import retry, stop_after_delay, wait_fixed\n\n\ndef check_path_owner(path: str) -> bool:\n    # If we don't have a way to check the effective uid of this process, then\n    # we'll just assume that we own the directory.\n    if sys.platform == \"win32\" or not hasattr(os, \"geteuid\"):\n        return True\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2709",
    "name": "__init__.py",
    "path": "github_repo/scripts/05_data_management/file_organization/__init__.py",
    "category": "05_data_management",
    "type": "setup",
    "lines": 45,
    "size": 836,
    "docstring": "",
    "keywords": [],
    "functions": [
      "main"
    ],
    "classes": [],
    "imports": [
      "__future__",
      "gettext",
      "os.path",
      "sys",
      "colors",
      "downloader",
      "info",
      "version"
    ],
    "preview": "#!/usr/bin/python\n# Updated In 00/04/2020\n# Created By ybenel\nfrom __future__ import unicode_literals\n\nimport gettext\nimport os.path\nimport sys\n\nfrom .colors import get_colors\nfrom .downloader import download\nfrom .info import (\n    __appname__,\n    __author__,\n    __contact__,\n    __description__,\n    __descriptionfull__,\n    __license__,\n    __licensefull__,\n    __projecturl__,",
    "last_modified": "2025-09-13T05:53:58.081429"
  },
  {
    "id": "2710",
    "name": "test_aws_uploader.py",
    "path": "github_repo/scripts/05_data_management/file_organization/test_aws_uploader.py",
    "category": "05_data_management",
    "type": "youtube",
    "lines": 70,
    "size": 2139,
    "docstring": "",
    "keywords": [
      "testing"
    ],
    "functions": [
      "test_no_location",
      "test_upload_gallery"
    ],
    "classes": [
      "AWSUploaderTestCase"
    ],
    "imports": [
      "os",
      "subprocess",
      "unittest",
      "unittest",
      "simplegallery.upload.uploader_factory",
      "testfixtures"
    ],
    "preview": "import os\nimport subprocess\nimport unittest\nfrom unittest import mock\n\nfrom simplegallery.upload.uploader_factory import get_uploader\nfrom testfixtures import TempDirectory\n\n\nclass AWSUploaderTestCase(unittest.TestCase):\n    def test_no_location(self):\n        uploader = get_uploader(\"aws\")\n        self.assertFalse(uploader.check_location(\"\"))\n\n    @mock.patch(\"subprocess.run\")\n    def test_upload_gallery(self, subprocess_run):\n        subprocess_run.return_value = subprocess.CompletedProcess([], returncode=0)\n\n        with TempDirectory() as tempdir:\n            # Setup mock file and uploader",
    "last_modified": "2025-09-06T12:30:29.549951"
  },
  {
    "id": "2711",
    "name": "category_flake8.py",
    "path": "github_repo/scripts/05_data_management/file_organization/category_flake8.py",
    "category": "05_data_management",
    "type": "utility",
    "lines": 32,
    "size": 881,
    "docstring": "",
    "keywords": [],
    "functions": [
      "categorize_flake8_output",
      "display_issues"
    ],
    "classes": [],
    "imports": [
      "re",
      "collections"
    ],
    "preview": "import re\nfrom collections import defaultdict\n\n\ndef categorize_flake8_output(file_path):\n    with open(file_path, \"r\") as file:\n        lines = file.readlines()\n\n    issues = defaultdict(list)\n\n    for line in lines:\n        match = re.match(r\"(./[^:]+):(\\d+):\\d+: (\\w\\d+) (.+)\", line)\n        if match:\n            file_name, line_number, code, message = match.groups()\n            issues[code].append((file_name, line_number, message))\n\n    return issues\n\n\ndef display_issues(issues):",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2712",
    "name": "organize copy.py",
    "path": "github_repo/scripts/05_data_management/file_organization/organize copy.py",
    "category": "05_data_management",
    "type": "organization",
    "lines": 29,
    "size": 1065,
    "docstring": "",
    "keywords": [],
    "functions": [
      "copy_files_with_structure"
    ],
    "classes": [],
    "imports": [
      "csv",
      "os",
      "shutil"
    ],
    "preview": "import csv\nimport os\nimport shutil\n\n\ndef copy_files_with_structure(csv_file_path, destination_base_path):\n    with open(csv_file_path, \"r\") as csv_file:\n        reader = csv.DictReader(csv_file)\n        for row in reader:\n            original_path = row[\"Original Path\"]\n            destination_path = os.path.join(destination_base_path, original_path.lstrip(os.sep))\n\n            # Create the destination directory if it doesn't exist\n            destination_dir = os.path.dirname(destination_path)\n            if not os.path.exists(destination_dir):\n                os.makedirs(destination_dir)\n\n            # Copy the file to the destination\n            shutil.copy2(original_path, destination_path)\n            print(f\"Copied {original_path} to {destination_path}\")",
    "last_modified": "2025-09-13T05:55:10.821579"
  },
  {
    "id": "2713",
    "name": "mercurial.py",
    "path": "github_repo/scripts/05_data_management/file_organization/mercurial.py",
    "category": "05_data_management",
    "type": "utility",
    "lines": 163,
    "size": 5233,
    "docstring": "",
    "keywords": [],
    "functions": [
      "get_base_rev_args",
      "fetch_new",
      "switch",
      "update",
      "get_remote_url",
      "get_revision",
      "get_requirement_revision",
      "is_commit_id_equal",
      "get_subdirectory",
      "get_repository_root"
    ],
    "classes": [
      "Mercurial"
    ],
    "imports": [
      "configparser",
      "logging",
      "os",
      "typing",
      "pip._internal.exceptions",
      "pip._internal.utils.misc",
      "pip._internal.utils.subprocess",
      "pip._internal.utils.urls",
      "pip._internal.vcs.versioncontrol"
    ],
    "preview": "import configparser\nimport logging\nimport os\nfrom typing import List, Optional, Tuple\n\nfrom pip._internal.exceptions import BadCommand, InstallationError\nfrom pip._internal.utils.misc import HiddenText, display_path\nfrom pip._internal.utils.subprocess import make_command\nfrom pip._internal.utils.urls import path_to_url\nfrom pip._internal.vcs.versioncontrol import (\n    RevOptions,\n    VersionControl,\n    find_path_to_project_root_from_repo_root,\n    vcs,\n)\n\nlogger = logging.getLogger(__name__)\n\n\nclass Mercurial(VersionControl):",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2714",
    "name": "appdirs.py",
    "path": "github_repo/scripts/05_data_management/file_organization/appdirs.py",
    "category": "05_data_management",
    "type": "testing",
    "lines": 53,
    "size": 1665,
    "docstring": "This code wraps the vendored appdirs module to so the return values are\ncompatible for the current pip code base.\n\nThe intention is to rewrite current usages gradually, keeping the tests pass,\nand eventually drop this after all usages are changed.",
    "keywords": [],
    "functions": [
      "user_cache_dir",
      "_macos_user_config_dir",
      "user_config_dir",
      "site_config_dirs"
    ],
    "classes": [],
    "imports": [
      "os",
      "sys",
      "typing",
      "pip._vendor"
    ],
    "preview": "\"\"\"\nThis code wraps the vendored appdirs module to so the return values are\ncompatible for the current pip code base.\n\nThe intention is to rewrite current usages gradually, keeping the tests pass,\nand eventually drop this after all usages are changed.\n\"\"\"\n\nimport os\nimport sys\nfrom typing import List\n\nfrom pip._vendor import platformdirs as _appdirs\n\n\ndef user_cache_dir(appname: str) -> str:\n    return _appdirs.user_cache_dir(appname, appauthor=False)\n\n\ndef _macos_user_config_dir(appname: str, roaming: bool = True) -> str:",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2715",
    "name": "flake8.py",
    "path": "github_repo/scripts/05_data_management/file_organization/flake8.py",
    "category": "05_data_management",
    "type": "utility",
    "lines": 32,
    "size": 881,
    "docstring": "",
    "keywords": [],
    "functions": [
      "categorize_flake8_output",
      "display_issues"
    ],
    "classes": [],
    "imports": [
      "re",
      "collections"
    ],
    "preview": "import re\nfrom collections import defaultdict\n\n\ndef categorize_flake8_output(file_path):\n    with open(file_path, \"r\") as file:\n        lines = file.readlines()\n\n    issues = defaultdict(list)\n\n    for line in lines:\n        match = re.match(r\"(./[^:]+):(\\d+):\\d+: (\\w\\d+) (.+)\", line)\n        if match:\n            file_name, line_number, code, message = match.groups()\n            issues[code].append((file_name, line_number, message))\n\n    return issues\n\n\ndef display_issues(issues):",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2716",
    "name": "utils.py",
    "path": "github_repo/scripts/05_data_management/file_organization/utils.py",
    "category": "05_data_management",
    "type": "utility",
    "lines": 31,
    "size": 778,
    "docstring": "",
    "keywords": [],
    "functions": [
      "resource_path",
      "get_local_path",
      "save_local_file",
      "load_local_file"
    ],
    "classes": [],
    "imports": [
      "sys",
      "pathlib"
    ],
    "preview": "import sys\nfrom pathlib import Path\n\nif getattr(sys, \"frozen\", False):\n    APP_PATH = Path(sys.executable).resolve().parent\n    RESOURCE_PATH = Path(sys._MEIPASS).resolve()\nelse:\n    APP_PATH = Path(sys.argv[0]).resolve().parent\n    RESOURCE_PATH = APP_PATH\n\n\ndef resource_path(path: str) -> Path:\n    return RESOURCE_PATH.joinpath(path).resolve()\n\n\ndef get_local_path(path: str) -> Path:\n    return APP_PATH.joinpath(path).resolve()\n\n\ndef save_local_file(file_path: str, content: str):",
    "last_modified": "2025-05-04T22:47:12"
  },
  {
    "id": "2717",
    "name": "rag.py",
    "path": "github_repo/scripts/05_data_management/file_organization/rag.py",
    "category": "05_data_management",
    "type": "utility",
    "lines": 25,
    "size": 794,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "langchain.globals",
      "llm_engineering.application.rag.retriever",
      "llm_engineering.infrastructure.opik_utils",
      "loguru"
    ],
    "preview": "from langchain.globals import set_verbose\nfrom llm_engineering.application.rag.retriever import ContextRetriever\nfrom llm_engineering.infrastructure.opik_utils import configure_opik\nfrom loguru import logger\n\nif __name__ == \"__main__\":\n    configure_opik()\n    set_verbose(True)\n\n    query = \"\"\"\n        My name is Paul Iusztin.\n        \n        Could you draft a LinkedIn post discussing RAG systems?\n        I'm particularly interested in:\n            - how RAG works\n            - how it is integrated with vector DBs and large language models (LLMs).\n        \"\"\"\n\n    retriever = ContextRetriever(mock=False)\n    documents = retriever.search(query, k=9)",
    "last_modified": "2025-05-06T04:35:14.991564"
  },
  {
    "id": "2718",
    "name": "inspect_local.py",
    "path": "github_repo/scripts/05_data_management/file_organization/inspect_local.py",
    "category": "05_data_management",
    "type": "utility",
    "lines": 92,
    "size": 3187,
    "docstring": "",
    "keywords": [],
    "functions": [
      "add_options",
      "run",
      "_dist_to_dict"
    ],
    "classes": [
      "InspectCommand"
    ],
    "imports": [
      "logging",
      "optparse",
      "typing",
      "pip",
      "pip._internal.cli",
      "pip._internal.cli.req_command",
      "pip._internal.cli.status_codes",
      "pip._internal.metadata",
      "pip._internal.utils.compat",
      "pip._internal.utils.urls"
    ],
    "preview": "import logging\nfrom optparse import Values\nfrom typing import Any, Dict, List\n\nfrom pip import __version__\nfrom pip._internal.cli import cmdoptions\nfrom pip._internal.cli.req_command import Command\nfrom pip._internal.cli.status_codes import SUCCESS\nfrom pip._internal.metadata import BaseDistribution, get_environment\nfrom pip._internal.utils.compat import stdlib_pkgs\nfrom pip._internal.utils.urls import path_to_url\nfrom pip._vendor.packaging.markers import default_environment\nfrom pip._vendor.rich import print_json\n\nlogger = logging.getLogger(__name__)\n\n\nclass InspectCommand(Command):\n    \"\"\"\n    Inspect the content of a Python environment and produce a report in JSON format.",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2719",
    "name": "merge_markdown.py",
    "path": "github_repo/scripts/05_data_management/file_organization/merge_markdown.py",
    "category": "05_data_management",
    "type": "utility",
    "lines": 57,
    "size": 1744,
    "docstring": "",
    "keywords": [],
    "functions": [
      "read_file",
      "write_file",
      "find_duplicates",
      "merge_files",
      "main"
    ],
    "classes": [],
    "imports": [
      "difflib",
      "os"
    ],
    "preview": "import difflib\nimport os\n\nMARKDOWN_DIR = \"/Users/steven/avatararts/mark/2/markdown_files\"\n\n\ndef read_file(file_path):\n    with open(file_path, \"r\") as file:\n        return file.readlines()\n\n\ndef write_file(file_path, content):\n    with open(file_path, \"w\") as file:\n        file.writelines(content)\n\n\ndef find_duplicates(markdown_files):\n    duplicates = []\n    checked = set()\n    for i, file1 in enumerate(markdown_files):",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2720",
    "name": "markers.py",
    "path": "github_repo/scripts/05_data_management/file_organization/markers.py",
    "category": "05_data_management",
    "type": "utility",
    "lines": 295,
    "size": 8463,
    "docstring": "",
    "keywords": [],
    "functions": [
      "_coerce_parse_result",
      "_format_marker",
      "_eval_op",
      "_get_env",
      "_evaluate_markers",
      "format_full_version",
      "default_environment",
      "__init__",
      "__str__",
      "__repr__"
    ],
    "classes": [
      "InvalidMarker",
      "UndefinedComparison",
      "UndefinedEnvironmentName",
      "Node",
      "Variable",
      "Value",
      "Op",
      "Undefined",
      "Marker"
    ],
    "imports": [
      "operator",
      "os",
      "platform",
      "sys",
      "typing",
      "pip._vendor.pyparsing",
      "pip._vendor.pyparsing",
      "pip._vendor.pyparsing",
      "specifiers"
    ],
    "preview": "# This file is dual licensed under the terms of the Apache License, Version\n# 2.0, and the BSD License. See the LICENSE file in the root of this repository\n# for complete details.\n\nimport operator\nimport os\nimport platform\nimport sys\nfrom typing import Any, Callable, Dict, List, Optional, Tuple, Union\n\nfrom pip._vendor.pyparsing import Forward, Group\nfrom pip._vendor.pyparsing import Literal as L  # noqa: N817\nfrom pip._vendor.pyparsing import (\n    ParseException,\n    ParseResults,\n    QuotedString,\n    ZeroOrMore,\n    stringEnd,\n    stringStart,\n)",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2721",
    "name": "base_command.py",
    "path": "github_repo/scripts/05_data_management/file_organization/base_command.py",
    "category": "05_data_management",
    "type": "utility",
    "lines": 234,
    "size": 8703,
    "docstring": "Base Command class, and related routines",
    "keywords": [],
    "functions": [
      "__init__",
      "add_options",
      "handle_pip_version_check",
      "run",
      "parse_args",
      "main",
      "_main",
      "intercepts_unhandled_exc",
      "exc_logging_wrapper"
    ],
    "classes": [
      "Command"
    ],
    "imports": [
      "functools",
      "logging",
      "logging.config",
      "optparse",
      "os",
      "sys",
      "traceback",
      "optparse",
      "typing",
      "pip._internal.cli"
    ],
    "preview": "\"\"\"Base Command class, and related routines\"\"\"\n\nimport functools\nimport logging\nimport logging.config\nimport optparse\nimport os\nimport sys\nimport traceback\nfrom optparse import Values\nfrom typing import Any, Callable, List, Optional, Tuple\n\nfrom pip._internal.cli import cmdoptions\nfrom pip._internal.cli.command_context import CommandContextMixIn\nfrom pip._internal.cli.parser import ConfigOptionParser, UpdatingDefaultsHelpFormatter\nfrom pip._internal.cli.status_codes import (\n    ERROR,\n    PREVIOUS_BUILD_DIR_ERROR,\n    UNKNOWN_ERROR,\n    VIRTUALENV_NOT_FOUND,",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2722",
    "name": "file_utils.py",
    "path": "github_repo/scripts/05_data_management/file_organization/file_utils.py",
    "category": "05_data_management",
    "type": "organization",
    "lines": 14,
    "size": 290,
    "docstring": "Common file operations",
    "keywords": [],
    "functions": [
      "ensure_dir",
      "get_file_size"
    ],
    "classes": [],
    "imports": [
      "os",
      "pathlib"
    ],
    "preview": "\"\"\"\nCommon file operations\n\"\"\"\nimport os\nfrom pathlib import Path\n\ndef ensure_dir(path):\n    \"\"\"Ensure directory exists.\"\"\"\n    Path(path).mkdir(parents=True, exist_ok=True)\n\ndef get_file_size(file_path):\n    \"\"\"Get file size in MB.\"\"\"\n    return os.path.getsize(file_path) / (1024 * 1024)\n",
    "last_modified": "2025-10-09T05:27:15.569261"
  },
  {
    "id": "2723",
    "name": "errors.py",
    "path": "github_repo/scripts/05_data_management/file_organization/errors.py",
    "category": "05_data_management",
    "type": "utility",
    "lines": 40,
    "size": 1186,
    "docstring": "",
    "keywords": [],
    "functions": [
      "check_syntax",
      "check_directory"
    ],
    "classes": [],
    "imports": [
      "ast",
      "os"
    ],
    "preview": "import ast\nimport os\n\n\ndef check_syntax(file_path):\n    with open(file_path, \"r\") as file:\n        try:\n            ast.parse(file.read(), filename=file_path)\n        except SyntaxError as e:\n            return f\"{file_path}: {e}\"\n    return None\n\n\ndef check_directory(directory):\n    error_files = []\n    for root, _, files in os.walk(directory):\n        for file in files:\n            if file.endswith(\".py\"):\n                file_path = os.path.join(root, file)\n                error_message = check_syntax(file_path)",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2724",
    "name": "copy_my.py",
    "path": "github_repo/scripts/05_data_management/file_organization/copy_my.py",
    "category": "05_data_management",
    "type": "utility",
    "lines": 31,
    "size": 1131,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "csv",
      "os",
      "shutil"
    ],
    "preview": "import csv\nimport os\nimport shutil\n\n# Prompt the user for the path to the CSV file\ncsv_file_path = input(\"Enter the path to the CSV file: \")\n\n# Destination directory\ndestination_directory = \"/Volumes/oG-bAk/steven\"\n\n# Check if destination directory exists, if not create it\nif not os.path.exists(destination_directory):\n    os.makedirs(destination_directory)\n\n# Read the CSV file\nwith open(csv_file_path, mode=\"r\") as file:\n    csv_reader = csv.reader(file)\n    for row in csv_reader:\n        file_path = row[-1].strip()  # Assuming the file paths are in the last column\n        if os.path.isfile(file_path):",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2725",
    "name": "display_progress.py",
    "path": "github_repo/scripts/05_data_management/file_organization/display_progress.py",
    "category": "05_data_management",
    "type": "utility",
    "lines": 82,
    "size": 2655,
    "docstring": "",
    "keywords": [],
    "functions": [
      "humanbytes",
      "TimeFormatter"
    ],
    "classes": [],
    "imports": [
      "logging",
      "math",
      "os",
      "time",
      "translation",
      "sample_config",
      "config"
    ],
    "preview": "import logging\n\nlogging.basicConfig(\n    level=logging.DEBUG, format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\n)\nlogger = logging.getLogger(__name__)\n\nimport math\nimport os\nimport time\n\n# the secret configuration specific things\nif bool(os.environ.get(\"WEBHOOK\", False)):\n    from sample_config import Config\nelse:\n    from config import Config\n\n# the Strings used for this \"thing\"\nfrom translation import Translation\n",
    "last_modified": "2025-09-13T05:53:43.395861"
  },
  {
    "id": "2726",
    "name": "groff.py",
    "path": "github_repo/scripts/05_data_management/file_organization/groff.py",
    "category": "05_data_management",
    "type": "utility",
    "lines": 165,
    "size": 4994,
    "docstring": "pygments.formatters.groff\n~~~~~~~~~~~~~~~~~~~~~~~~~\n\nFormatter for groff output.\n\n:copyright: Copyright 2006-2023 by the Pygments team, see AUTHORS.\n:license: BSD, see LICENSE for details.",
    "keywords": [],
    "functions": [
      "__init__",
      "_make_styles",
      "_define_colors",
      "_write_lineno",
      "_wrap_line",
      "_escape_chars",
      "format_unencoded"
    ],
    "classes": [
      "GroffFormatter"
    ],
    "imports": [
      "math",
      "pip._vendor.pygments.formatter",
      "pip._vendor.pygments.util"
    ],
    "preview": "\"\"\"\npygments.formatters.groff\n~~~~~~~~~~~~~~~~~~~~~~~~~\n\nFormatter for groff output.\n\n:copyright: Copyright 2006-2023 by the Pygments team, see AUTHORS.\n:license: BSD, see LICENSE for details.\n\"\"\"\n\nimport math\n\nfrom pip._vendor.pygments.formatter import Formatter\nfrom pip._vendor.pygments.util import get_bool_opt, get_int_opt\n\n__all__ = [\"GroffFormatter\"]\n\n\nclass GroffFormatter(Formatter):\n    \"\"\"",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2727",
    "name": "installation_report.py",
    "path": "github_repo/scripts/05_data_management/file_organization/installation_report.py",
    "category": "05_data_management",
    "type": "setup",
    "lines": 54,
    "size": 2787,
    "docstring": "",
    "keywords": [],
    "functions": [
      "__init__",
      "_install_req_to_dict",
      "to_dict"
    ],
    "classes": [
      "InstallationReport"
    ],
    "imports": [
      "typing",
      "pip",
      "pip._internal.req.req_install",
      "pip._vendor.packaging.markers"
    ],
    "preview": "from typing import Any, Dict, Sequence\n\nfrom pip import __version__\nfrom pip._internal.req.req_install import InstallRequirement\nfrom pip._vendor.packaging.markers import default_environment\n\n\nclass InstallationReport:\n    def __init__(self, install_requirements: Sequence[InstallRequirement]):\n        self._install_requirements = install_requirements\n\n    @classmethod\n    def _install_req_to_dict(cls, ireq: InstallRequirement) -> Dict[str, Any]:\n        assert ireq.download_info, f\"No download_info for {ireq}\"\n        res = {\n            # PEP 610 json for the download URL. download_info.archive_info.hashes may\n            # be absent when the requirement was installed from the wheel cache\n            # and the cache entry was populated by an older pip version that did not\n            # record origin.json.\n            \"download_info\": ireq.download_info.to_dict(),",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2728",
    "name": "__main__.py",
    "path": "github_repo/scripts/05_data_management/file_organization/__main__.py",
    "category": "05_data_management",
    "type": "utility",
    "lines": 19,
    "size": 395,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "__future__",
      "sys",
      "PrNdOwN",
      "os.path"
    ],
    "preview": "#!/usr/bin/env python\n# Date 31/08/2021\n# Author ybenel\nfrom __future__ import unicode_literals\n\nimport sys\n\nif __package__ is None and not hasattr(sys, \"frozen\"):\n    # direct call of __main__.py\n    import os.path\n\n    PATH = os.path.realpath(os.path.abspath(__file__))\n    sys.path.append(os.path.dirname(os.path.dirname(PATH)))\n\nimport PrNdOwN\n\nif __name__ == \"__main__\":\n    PrNdOwN.main()\n",
    "last_modified": "2025-05-04T23:28:20.850052"
  },
  {
    "id": "2729",
    "name": "cmd_logs.py",
    "path": "github_repo/scripts/05_data_management/file_organization/cmd_logs.py",
    "category": "05_data_management",
    "type": "utility",
    "lines": 20,
    "size": 371,
    "docstring": "",
    "keywords": [],
    "functions": [
      "cls",
      "initLog",
      "info",
      "log"
    ],
    "classes": [],
    "imports": [
      "os",
      "colorama"
    ],
    "preview": "import os\n\nfrom colorama import *\n\n\ndef cls() -> None:\n    os.system(\"cls\" if os.name == \"nt\" else \"clear\")\n\n\ndef initLog() -> None:\n    init()\n\n\ndef info(text: str) -> None:\n    print(f\"[{Style.BRIGHT}{Fore.YELLOW}i{Style.RESET_ALL}]{text}\")\n\n\ndef log(text, success=True) -> None:\n    print(f\"[{f'{Fore.GREEN}V' if success else f'{Fore.RED}X'}{Style.RESET_ALL}]{text}\")\n",
    "last_modified": "2025-03-28T18:37:01"
  },
  {
    "id": "2730",
    "name": "banner.py",
    "path": "github_repo/scripts/05_data_management/file_organization/banner.py",
    "category": "05_data_management",
    "type": "utility",
    "lines": 157,
    "size": 3548,
    "docstring": "",
    "keywords": [],
    "functions": [
      "clear",
      "banner",
      "banner2",
      "banner3",
      "buggy",
      "banner4"
    ],
    "classes": [],
    "imports": [
      "os",
      "sys",
      "threading",
      "random",
      "time",
      "colors"
    ],
    "preview": "#!/usr/bin/python3\n# Created By ybenel\n# Updated In 09/04/2020\nimport os\nimport sys\nimport threading\nfrom random import shuffle\nfrom time import sleep as sl\n\nfrom .colors import get_colors\n\nsys.path.insert(1, \"ascii\")\n\n# Global Variables\nplatform = sys.platform\n\n\ndef clear():\n    if sys.platform == \"win32\":\n        os.system(\"cls\")",
    "last_modified": "2025-05-04T23:28:20.852595"
  },
  {
    "id": "2731",
    "name": "copy2.py",
    "path": "github_repo/scripts/05_data_management/file_organization/copy2.py",
    "category": "05_data_management",
    "type": "utility",
    "lines": 54,
    "size": 2215,
    "docstring": "",
    "keywords": [],
    "functions": [
      "copy_files_with_logging"
    ],
    "classes": [],
    "imports": [
      "csv",
      "os",
      "shutil"
    ],
    "preview": "import csv\nimport os\nimport shutil\n\n\n# Function to copy files while preserving folder structure and logging each copied file\ndef copy_files_with_logging(csv_file, destination_root):\n    log_file = \"copy_log.txt\"\n    with (\n        open(csv_file, newline=\"\") as csvfile,\n        open(log_file, \"a\") as log,\n    ):  # Use 'a' to append to the log file\n        reader = csv.reader(csvfile)\n        for row in reader:\n            if row:  # Ensuring the row is not empty\n                src_file_path = row[0]\n                try:\n                    # Check if the source file exists\n                    if not os.path.exists(src_file_path):\n                        error_message = f\"Source file does not exist: {src_file_path}\"",
    "last_modified": "2025-09-13T05:53:54.343117"
  },
  {
    "id": "2732",
    "name": "opik_utils.py",
    "path": "github_repo/scripts/05_data_management/file_organization/opik_utils.py",
    "category": "05_data_management",
    "type": "utility",
    "lines": 33,
    "size": 1054,
    "docstring": "",
    "keywords": [],
    "functions": [
      "configure_opik"
    ],
    "classes": [],
    "imports": [
      "os",
      "opik",
      "llm_engineering",
      "loguru",
      "opik.configurator.configure"
    ],
    "preview": "import os\n\nimport opik\nfrom llm_engineering import settings\nfrom loguru import logger\nfrom opik.configurator.configure import OpikConfigurator\n\n\ndef configure_opik() -> None:\n    if settings.COMET_API_KEY and settings.COMET_PROJECT:\n        try:\n            client = OpikConfigurator(api_key=settings.COMET_API_KEY)\n            default_workspace = client._get_default_workspace()\n        except Exception:\n            logger.warning(\n                \"Default workspace not found. Setting workspace to None and enabling interactive mode.\"\n            )\n            default_workspace = None\n\n        os.environ[\"OPIK_PROJECT_NAME\"] = settings.COMET_PROJECT",
    "last_modified": "2025-05-06T04:35:14.987238"
  },
  {
    "id": "2733",
    "name": "initialise.py",
    "path": "github_repo/scripts/05_data_management/file_organization/initialise.py",
    "category": "05_data_management",
    "type": "setup",
    "lines": 120,
    "size": 3284,
    "docstring": "",
    "keywords": [],
    "functions": [
      "_wipe_internal_state_for_tests",
      "reset_all",
      "init",
      "deinit",
      "just_fix_windows_console",
      "colorama_text",
      "reinit",
      "wrap_stream"
    ],
    "classes": [],
    "imports": [
      "atexit",
      "contextlib",
      "sys",
      "ansitowin32"
    ],
    "preview": "# Copyright Jonathan Hartley 2013. BSD 3-Clause license, see LICENSE file.\nimport atexit\nimport contextlib\nimport sys\n\nfrom .ansitowin32 import AnsiToWin32\n\n\ndef _wipe_internal_state_for_tests():\n    global orig_stdout, orig_stderr\n    orig_stdout = None\n    orig_stderr = None\n\n    global wrapped_stdout, wrapped_stderr\n    wrapped_stdout = None\n    wrapped_stderr = None\n\n    global atexit_done\n    atexit_done = False\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2734",
    "name": "colors.py",
    "path": "github_repo/scripts/05_data_management/file_organization/colors.py",
    "category": "05_data_management",
    "type": "organization",
    "lines": 40,
    "size": 1332,
    "docstring": "MIT License\n\nCopyright (c) 2021-2022 MShawon\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.",
    "keywords": [],
    "functions": [],
    "classes": [
      "bcolors"
    ],
    "imports": [
      "os"
    ],
    "preview": "\"\"\"\nMIT License\n\nCopyright (c) 2021-2022 MShawon\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,",
    "last_modified": "2025-05-04T23:27:45"
  },
  {
    "id": "2735",
    "name": "chat_base.py",
    "path": "05_data_management/file_organization/chat_base.py",
    "category": "05_data_management",
    "type": "utility",
    "lines": 33,
    "size": 764,
    "docstring": "",
    "keywords": [],
    "functions": [
      "TRChatBase"
    ],
    "classes": [],
    "imports": [
      "logging",
      "os",
      "chatbase",
      "pyrogram",
      "translation",
      "sample_config",
      "config"
    ],
    "preview": "import logging\n\nlogging.basicConfig(\n    level=logging.DEBUG, format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\n)\nlogger = logging.getLogger(__name__)\n\nimport os\n\n# the secret configuration specific things\nif bool(os.environ.get(\"WEBHOOK\", False)):\n    from sample_config import Config\nelse:\n    from config import Config\n\n# the Strings used for this \"thing\"\n# the Telegram trackings\nfrom chatbase import Message\nfrom pyrogram import Client, Filters\nfrom translation import Translation",
    "last_modified": "2025-05-06T04:35:15.016401"
  },
  {
    "id": "2736",
    "name": "group_files_with_date.py",
    "path": "05_data_management/file_organization/group_files_with_date.py",
    "category": "05_data_management",
    "type": "organization",
    "lines": 43,
    "size": 1613,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "csv",
      "os",
      "collections",
      "datetime"
    ],
    "preview": "import csv\nimport os\nfrom collections import defaultdict\nfrom datetime import datetime\n\n# Prompt the user for the directory\ndirectory = input(\"Enter the directory containing the files: \").strip()\n\n# Get the root folder name\nroot_folder = os.path.basename(os.path.normpath(directory))\ncurrent_date = datetime.now().strftime(\"%m%d%y\")\noutput_csv = os.path.join(directory, f\"{root_folder}-{current_date}.csv\")\n\n# Step 1: Group files by song title (ignoring version indicators)\nfile_groups = defaultdict(list)\n\ntry:\n    # Check if the directory exists\n    if not os.path.isdir(directory):\n        raise FileNotFoundError(f\"Directory '{directory}' does not exist.\")",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2737",
    "name": "config_loader.py",
    "path": "05_data_management/file_organization/config_loader.py",
    "category": "05_data_management",
    "type": "setup",
    "lines": 19,
    "size": 349,
    "docstring": "",
    "keywords": [],
    "functions": [
      "load_config",
      "load_settings"
    ],
    "classes": [],
    "imports": [
      "json",
      "pathlib"
    ],
    "preview": "import json\nfrom pathlib import Path\n\nCONFIG_DIR = Path(__file__).parent.parent / \"config\"\n\n\ndef load_config():\n    with open(CONFIG_DIR / \"exclusions.json\") as f:\n        return json.load(f)\n\n\ndef load_settings():\n    with open(CONFIG_DIR / \"settings.json\") as f:\n        return json.load(f)\n\n\nEXCLUSIONS = load_config()\nSETTINGS = load_settings()\n",
    "last_modified": "2025-05-04T22:47:12.611439"
  },
  {
    "id": "2738",
    "name": "intranges.py",
    "path": "05_data_management/file_organization/intranges.py",
    "category": "05_data_management",
    "type": "utility",
    "lines": 58,
    "size": 1898,
    "docstring": "Given a list of integers, made up of (hopefully) a small number of long runs\nof consecutive integers, compute a representation of the form\n((start1, end1), (start2, end2) ...). Then answer the question \"was x present\nin the original list?\" in time O(log(# runs)).",
    "keywords": [],
    "functions": [
      "intranges_from_list",
      "_encode_range",
      "_decode_range",
      "intranges_contain"
    ],
    "classes": [],
    "imports": [
      "bisect",
      "typing"
    ],
    "preview": "\"\"\"\nGiven a list of integers, made up of (hopefully) a small number of long runs\nof consecutive integers, compute a representation of the form\n((start1, end1), (start2, end2) ...). Then answer the question \"was x present\nin the original list?\" in time O(log(# runs)).\n\"\"\"\n\nimport bisect\nfrom typing import List, Tuple\n\n\ndef intranges_from_list(list_: List[int]) -> Tuple[int, ...]:\n    \"\"\"Represent a list of integers as a sequence of ranges:\n    ((start_0, end_0), (start_1, end_1), ...), such that the original\n    integers are exactly those x such that start_i <= x < end_i for some i.\n\n    Ranges are encoded as single integers (start << 32 | end), not as tuples.\n    \"\"\"\n\n    sorted_list = sorted(list_)",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2739",
    "name": "config.py",
    "path": "05_data_management/file_organization/config.py",
    "category": "05_data_management",
    "type": "setup",
    "lines": 16,
    "size": 412,
    "docstring": "",
    "keywords": [],
    "functions": [
      "parse_config"
    ],
    "classes": [],
    "imports": [
      "os.path",
      "ConfigParser",
      "configparser"
    ],
    "preview": "try:\n    from ConfigParser import ConfigParser\nexcept ImportError as e:\n    from configparser import ConfigParser\n\nfrom os.path import abspath, dirname, join\n\nconfig = ConfigParser()\nconfig_path = join(abspath(dirname(dirname(__file__))), \"config\", \"config.cfg\")\nconfig.read(config_path)\n\n\ndef parse_config(app_name=\"local\"):\n    \"\"\"Parse config for the given app name\"\"\"\n    return dict(config.items(app_name))\n",
    "last_modified": "2025-05-04T23:28:19.982710"
  },
  {
    "id": "2740",
    "name": "pyproject.py",
    "path": "05_data_management/file_organization/pyproject.py",
    "category": "05_data_management",
    "type": "utility",
    "lines": 178,
    "size": 7135,
    "docstring": "",
    "keywords": [],
    "functions": [
      "_is_list_of_str",
      "make_pyproject_path",
      "load_pyproject_toml"
    ],
    "classes": [],
    "imports": [
      "importlib.util",
      "os",
      "collections",
      "typing",
      "pip._internal.exceptions",
      "pip._vendor",
      "pip._vendor.packaging.requirements"
    ],
    "preview": "import importlib.util\nimport os\nfrom collections import namedtuple\nfrom typing import Any, List, Optional\n\nfrom pip._internal.exceptions import (\n    InstallationError,\n    InvalidPyProjectBuildRequires,\n    MissingPyProjectBuildRequires,\n)\nfrom pip._vendor import tomli\nfrom pip._vendor.packaging.requirements import InvalidRequirement, Requirement\n\n\ndef _is_list_of_str(obj: Any) -> bool:\n    return isinstance(obj, list) and all(isinstance(item, str) for item in obj)\n\n\ndef make_pyproject_path(unpacked_source_directory: str) -> str:\n    return os.path.join(unpacked_source_directory, \"pyproject.toml\")",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2741",
    "name": "pdf-merge.py",
    "path": "05_data_management/file_organization/pdf-merge.py",
    "category": "05_data_management",
    "type": "utility",
    "lines": 28,
    "size": 686,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "os",
      "PyPDF2"
    ],
    "preview": "import os\n\nfrom PyPDF2 import PdfMerger\n\n# Directory containing the PDF files\npdf_dir = \"/Users/steven/Documents/project2025/book\"\n\n# Output file name\noutput_file = \"merged_document.pdf\"\n\n# Create a PdfMerger object\nmerger = PdfMerger()\n\n# Iterate through the files in the directory in order from ch1.pdf to ch30.pdf\nfor i in range(1, 31):\n    pdf_file = f\"ch{i}.pdf\"\n    pdf_path = os.path.join(pdf_dir, pdf_file)\n    if os.path.isfile(pdf_path):\n        merger.append(pdf_path)\n    else:",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2742",
    "name": "files_io.py",
    "path": "05_data_management/file_organization/files_io.py",
    "category": "05_data_management",
    "type": "organization",
    "lines": 32,
    "size": 979,
    "docstring": "",
    "keywords": [],
    "functions": [
      "read",
      "write"
    ],
    "classes": [
      "JsonFileManager"
    ],
    "imports": [
      "json",
      "pathlib"
    ],
    "preview": "import json\nfrom pathlib import Path\n\n\nclass JsonFileManager:\n    @classmethod\n    def read(cls, filename: str | Path) -> list:\n        file_path: Path = Path(filename)\n\n        try:\n            with file_path.open(\"r\") as file:\n                return json.load(file)\n        except FileNotFoundError:\n            raise FileNotFoundError(f\"File '{file_path=}' does not exist.\") from None\n        except json.JSONDecodeError as e:\n            raise json.JSONDecodeError(\n                msg=f\"File '{file_path=}' is not properly formatted as JSON.\",\n                doc=e.doc,\n                pos=e.pos,\n            ) from None",
    "last_modified": "2025-05-04T22:47:11.568362"
  },
  {
    "id": "2743",
    "name": "find_icloud_duplicates.py",
    "path": "05_data_management/file_organization/find_icloud_duplicates.py",
    "category": "05_data_management",
    "type": "utility",
    "lines": 58,
    "size": 1721,
    "docstring": "",
    "keywords": [],
    "functions": [
      "hash_file",
      "find_duplicates",
      "save_duplicates_to_csv"
    ],
    "classes": [],
    "imports": [
      "csv",
      "hashlib",
      "os",
      "collections"
    ],
    "preview": "import csv\nimport hashlib\nimport os\nfrom collections import defaultdict\n\nICLOUD_DIR = \"/Users/steven/Library/Mobile Documents/com~apple~CloudDocs\"\nDUPLICATES_CSV = \"icloud_duplicates_report.csv\"\n\n\ndef hash_file(path):\n    hasher = hashlib.sha256()\n    try:\n        with open(path, \"rb\") as f:\n            while chunk := f.read(8192):\n                hasher.update(chunk)\n        return hasher.hexdigest()\n    except Exception as e:\n        print(f\"Error reading {path}: {e}\")\n        return None\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2744",
    "name": "target_python.py",
    "path": "05_data_management/file_organization/target_python.py",
    "category": "05_data_management",
    "type": "utility",
    "lines": 117,
    "size": 4218,
    "docstring": "",
    "keywords": [],
    "functions": [
      "__init__",
      "format_given",
      "get_sorted_tags",
      "get_unsorted_tags"
    ],
    "classes": [
      "TargetPython"
    ],
    "imports": [
      "sys",
      "typing",
      "pip._internal.utils.compatibility_tags",
      "pip._internal.utils.misc",
      "pip._vendor.packaging.tags"
    ],
    "preview": "import sys\nfrom typing import List, Optional, Set, Tuple\n\nfrom pip._internal.utils.compatibility_tags import get_supported, version_info_to_nodot\nfrom pip._internal.utils.misc import normalize_version_info\nfrom pip._vendor.packaging.tags import Tag\n\n\nclass TargetPython:\n    \"\"\"\n    Encapsulates the properties of a Python interpreter one is targeting\n    for a package install, download, etc.\n    \"\"\"\n\n    __slots__ = [\n        \"_given_py_version_info\",\n        \"abis\",\n        \"implementation\",\n        \"platforms\",\n        \"py_version\",",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2745",
    "name": "organize.py",
    "path": "05_data_management/file_organization/organize.py",
    "category": "05_data_management",
    "type": "organization",
    "lines": 29,
    "size": 1065,
    "docstring": "",
    "keywords": [],
    "functions": [
      "copy_files_with_structure"
    ],
    "classes": [],
    "imports": [
      "csv",
      "os",
      "shutil"
    ],
    "preview": "import csv\nimport os\nimport shutil\n\n\ndef copy_files_with_structure(csv_file_path, destination_base_path):\n    with open(csv_file_path, \"r\") as csv_file:\n        reader = csv.DictReader(csv_file)\n        for row in reader:\n            original_path = row[\"Original Path\"]\n            destination_path = os.path.join(destination_base_path, original_path.lstrip(os.sep))\n\n            # Create the destination directory if it doesn't exist\n            destination_dir = os.path.dirname(destination_path)\n            if not os.path.exists(destination_dir):\n                os.makedirs(destination_dir)\n\n            # Copy the file to the destination\n            shutil.copy2(original_path, destination_path)\n            print(f\"Copied {original_path} to {destination_path}\")",
    "last_modified": "2025-09-13T05:53:48.520011"
  },
  {
    "id": "2746",
    "name": "zip-t0-csv.py",
    "path": "05_data_management/file_organization/zip-t0-csv.py",
    "category": "05_data_management",
    "type": "utility",
    "lines": 62,
    "size": 1864,
    "docstring": "",
    "keywords": [],
    "functions": [
      "main"
    ],
    "classes": [],
    "imports": [
      "csv",
      "os",
      "zipfile",
      "datetime"
    ],
    "preview": "import csv\nimport os\nimport zipfile\nfrom datetime import datetime\n\n\ndef main():\n    # Prompt for folder path\n    zip_folder = input(\"\ud83d\udcc2 Enter the full path to the folder containing ZIP files: \").strip()\n\n    if not os.path.isdir(zip_folder):\n        print(f\"\u274c Error: '{zip_folder}' is not a valid directory.\")\n        return\n\n    # Output CSV name (saved in the current script directory)\n    output_csv = \"zip_contents_inventory.csv\"\n\n    # Scan for .zip files\n    zip_files = [\n        os.path.join(zip_folder, f) for f in os.listdir(zip_folder) if f.lower().endswith(\".zip\")",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2747",
    "name": "editable_legacy.py",
    "path": "05_data_management/file_organization/editable_legacy.py",
    "category": "05_data_management",
    "type": "setup",
    "lines": 47,
    "size": 1282,
    "docstring": "Legacy editable installation process, i.e. `setup.py develop`.",
    "keywords": [],
    "functions": [
      "install_editable"
    ],
    "classes": [],
    "imports": [
      "logging",
      "typing",
      "pip._internal.build_env",
      "pip._internal.utils.logging",
      "pip._internal.utils.setuptools_build",
      "pip._internal.utils.subprocess"
    ],
    "preview": "\"\"\"Legacy editable installation process, i.e. `setup.py develop`.\"\"\"\n\nimport logging\nfrom typing import Optional, Sequence\n\nfrom pip._internal.build_env import BuildEnvironment\nfrom pip._internal.utils.logging import indent_log\nfrom pip._internal.utils.setuptools_build import make_setuptools_develop_args\nfrom pip._internal.utils.subprocess import call_subprocess\n\nlogger = logging.getLogger(__name__)\n\n\ndef install_editable(\n    *,\n    global_options: Sequence[str],\n    prefix: Optional[str],\n    home: Optional[str],\n    use_user_site: bool,\n    name: str,",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2748",
    "name": "zip_inventory_to_csv.py",
    "path": "05_data_management/file_organization/zip_inventory_to_csv.py",
    "category": "05_data_management",
    "type": "utility",
    "lines": 62,
    "size": 1864,
    "docstring": "",
    "keywords": [],
    "functions": [
      "main"
    ],
    "classes": [],
    "imports": [
      "csv",
      "os",
      "zipfile",
      "datetime"
    ],
    "preview": "import csv\nimport os\nimport zipfile\nfrom datetime import datetime\n\n\ndef main():\n    # Prompt for folder path\n    zip_folder = input(\"\ud83d\udcc2 Enter the full path to the folder containing ZIP files: \").strip()\n\n    if not os.path.isdir(zip_folder):\n        print(f\"\u274c Error: '{zip_folder}' is not a valid directory.\")\n        return\n\n    # Output CSV name (saved in the current script directory)\n    output_csv = \"zip_contents_inventory.csv\"\n\n    # Scan for .zip files\n    zip_files = [\n        os.path.join(zip_folder, f) for f in os.listdir(zip_folder) if f.lower().endswith(\".zip\")",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2749",
    "name": "posttextparser.py",
    "path": "05_data_management/file_organization/posttextparser.py",
    "category": "05_data_management",
    "type": "utility",
    "lines": 36,
    "size": 844,
    "docstring": "",
    "keywords": [],
    "functions": [
      "posttextparser"
    ],
    "classes": [],
    "imports": [
      "os",
      "re",
      "time",
      "typing",
      "spacy",
      "utils.console",
      "utils.voice"
    ],
    "preview": "import os\nimport re\nimport time\nfrom typing import List\n\nimport spacy\n\nfrom utils.console import print_step\nfrom utils.voice import sanitize_text\n\n\n# working good\ndef posttextparser(obj, *, tried: bool = False) -> List[str]:\n    text: str = re.sub(\"\\n\", \" \", obj)\n    try:\n        nlp = spacy.load(\"en_core_web_sm\")\n    except OSError as e:\n        if not tried:\n            os.system(\"python -m spacy download en_core_web_sm\")\n            time.sleep(5)",
    "last_modified": "2025-09-11T13:27:01.781766"
  },
  {
    "id": "2750",
    "name": "download.py",
    "path": "05_data_management/file_organization/download.py",
    "category": "05_data_management",
    "type": "youtube",
    "lines": 148,
    "size": 5335,
    "docstring": "",
    "keywords": [],
    "functions": [
      "add_options",
      "run"
    ],
    "classes": [
      "DownloadCommand"
    ],
    "imports": [
      "logging",
      "os",
      "optparse",
      "typing",
      "pip._internal.cli",
      "pip._internal.cli.cmdoptions",
      "pip._internal.cli.req_command",
      "pip._internal.cli.status_codes",
      "pip._internal.operations.build.build_tracker",
      "pip._internal.req.req_install"
    ],
    "preview": "import logging\nimport os\nfrom optparse import Values\nfrom typing import List\n\nfrom pip._internal.cli import cmdoptions\nfrom pip._internal.cli.cmdoptions import make_target_python\nfrom pip._internal.cli.req_command import RequirementCommand, with_cleanup\nfrom pip._internal.cli.status_codes import SUCCESS\nfrom pip._internal.operations.build.build_tracker import get_build_tracker\nfrom pip._internal.req.req_install import check_legacy_setup_py_options\nfrom pip._internal.utils.misc import ensure_dir, normalize_path, write_output\nfrom pip._internal.utils.temp_dir import TempDirectory\n\nlogger = logging.getLogger(__name__)\n\n\nclass DownloadCommand(RequirementCommand):\n    \"\"\"\n    Download packages from:",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2751",
    "name": "filesystem.py",
    "path": "05_data_management/file_organization/filesystem.py",
    "category": "05_data_management",
    "type": "organization",
    "lines": 153,
    "size": 5121,
    "docstring": "",
    "keywords": [
      "testing"
    ],
    "functions": [
      "check_path_owner",
      "adjacent_tmp_file",
      "test_writable_dir",
      "_test_writable_dir_win",
      "find_files",
      "file_size",
      "format_file_size",
      "directory_size",
      "format_directory_size"
    ],
    "classes": [],
    "imports": [
      "fnmatch",
      "os",
      "os.path",
      "random",
      "sys",
      "contextlib",
      "tempfile",
      "typing",
      "pip._internal.utils.compat",
      "pip._internal.utils.misc"
    ],
    "preview": "import fnmatch\nimport os\nimport os.path\nimport random\nimport sys\nfrom contextlib import contextmanager\nfrom tempfile import NamedTemporaryFile\nfrom typing import Any, BinaryIO, Generator, List, Union, cast\n\nfrom pip._internal.utils.compat import get_path_uid\nfrom pip._internal.utils.misc import format_size\nfrom pip._vendor.tenacity import retry, stop_after_delay, wait_fixed\n\n\ndef check_path_owner(path: str) -> bool:\n    # If we don't have a way to check the effective uid of this process, then\n    # we'll just assume that we own the directory.\n    if sys.platform == \"win32\" or not hasattr(os, \"geteuid\"):\n        return True\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2752",
    "name": "__init__.py",
    "path": "05_data_management/file_organization/__init__.py",
    "category": "05_data_management",
    "type": "setup",
    "lines": 45,
    "size": 836,
    "docstring": "",
    "keywords": [],
    "functions": [
      "main"
    ],
    "classes": [],
    "imports": [
      "__future__",
      "gettext",
      "os.path",
      "sys",
      "colors",
      "downloader",
      "info",
      "version"
    ],
    "preview": "#!/usr/bin/python\n# Updated In 00/04/2020\n# Created By ybenel\nfrom __future__ import unicode_literals\n\nimport gettext\nimport os.path\nimport sys\n\nfrom .colors import get_colors\nfrom .downloader import download\nfrom .info import (\n    __appname__,\n    __author__,\n    __contact__,\n    __description__,\n    __descriptionfull__,\n    __license__,\n    __licensefull__,\n    __projecturl__,",
    "last_modified": "2025-09-13T05:53:58.081429"
  },
  {
    "id": "2753",
    "name": "test_aws_uploader.py",
    "path": "05_data_management/file_organization/test_aws_uploader.py",
    "category": "05_data_management",
    "type": "youtube",
    "lines": 70,
    "size": 2139,
    "docstring": "",
    "keywords": [
      "testing"
    ],
    "functions": [
      "test_no_location",
      "test_upload_gallery"
    ],
    "classes": [
      "AWSUploaderTestCase"
    ],
    "imports": [
      "os",
      "subprocess",
      "unittest",
      "unittest",
      "simplegallery.upload.uploader_factory",
      "testfixtures"
    ],
    "preview": "import os\nimport subprocess\nimport unittest\nfrom unittest import mock\n\nfrom simplegallery.upload.uploader_factory import get_uploader\nfrom testfixtures import TempDirectory\n\n\nclass AWSUploaderTestCase(unittest.TestCase):\n    def test_no_location(self):\n        uploader = get_uploader(\"aws\")\n        self.assertFalse(uploader.check_location(\"\"))\n\n    @mock.patch(\"subprocess.run\")\n    def test_upload_gallery(self, subprocess_run):\n        subprocess_run.return_value = subprocess.CompletedProcess([], returncode=0)\n\n        with TempDirectory() as tempdir:\n            # Setup mock file and uploader",
    "last_modified": "2025-09-06T12:30:29.549951"
  },
  {
    "id": "2754",
    "name": "category_flake8.py",
    "path": "05_data_management/file_organization/category_flake8.py",
    "category": "05_data_management",
    "type": "utility",
    "lines": 32,
    "size": 881,
    "docstring": "",
    "keywords": [],
    "functions": [
      "categorize_flake8_output",
      "display_issues"
    ],
    "classes": [],
    "imports": [
      "re",
      "collections"
    ],
    "preview": "import re\nfrom collections import defaultdict\n\n\ndef categorize_flake8_output(file_path):\n    with open(file_path, \"r\") as file:\n        lines = file.readlines()\n\n    issues = defaultdict(list)\n\n    for line in lines:\n        match = re.match(r\"(./[^:]+):(\\d+):\\d+: (\\w\\d+) (.+)\", line)\n        if match:\n            file_name, line_number, code, message = match.groups()\n            issues[code].append((file_name, line_number, message))\n\n    return issues\n\n\ndef display_issues(issues):",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2755",
    "name": "organize copy.py",
    "path": "05_data_management/file_organization/organize copy.py",
    "category": "05_data_management",
    "type": "organization",
    "lines": 29,
    "size": 1065,
    "docstring": "",
    "keywords": [],
    "functions": [
      "copy_files_with_structure"
    ],
    "classes": [],
    "imports": [
      "csv",
      "os",
      "shutil"
    ],
    "preview": "import csv\nimport os\nimport shutil\n\n\ndef copy_files_with_structure(csv_file_path, destination_base_path):\n    with open(csv_file_path, \"r\") as csv_file:\n        reader = csv.DictReader(csv_file)\n        for row in reader:\n            original_path = row[\"Original Path\"]\n            destination_path = os.path.join(destination_base_path, original_path.lstrip(os.sep))\n\n            # Create the destination directory if it doesn't exist\n            destination_dir = os.path.dirname(destination_path)\n            if not os.path.exists(destination_dir):\n                os.makedirs(destination_dir)\n\n            # Copy the file to the destination\n            shutil.copy2(original_path, destination_path)\n            print(f\"Copied {original_path} to {destination_path}\")",
    "last_modified": "2025-09-13T05:55:10.821579"
  },
  {
    "id": "2756",
    "name": "mercurial.py",
    "path": "05_data_management/file_organization/mercurial.py",
    "category": "05_data_management",
    "type": "utility",
    "lines": 163,
    "size": 5233,
    "docstring": "",
    "keywords": [],
    "functions": [
      "get_base_rev_args",
      "fetch_new",
      "switch",
      "update",
      "get_remote_url",
      "get_revision",
      "get_requirement_revision",
      "is_commit_id_equal",
      "get_subdirectory",
      "get_repository_root"
    ],
    "classes": [
      "Mercurial"
    ],
    "imports": [
      "configparser",
      "logging",
      "os",
      "typing",
      "pip._internal.exceptions",
      "pip._internal.utils.misc",
      "pip._internal.utils.subprocess",
      "pip._internal.utils.urls",
      "pip._internal.vcs.versioncontrol"
    ],
    "preview": "import configparser\nimport logging\nimport os\nfrom typing import List, Optional, Tuple\n\nfrom pip._internal.exceptions import BadCommand, InstallationError\nfrom pip._internal.utils.misc import HiddenText, display_path\nfrom pip._internal.utils.subprocess import make_command\nfrom pip._internal.utils.urls import path_to_url\nfrom pip._internal.vcs.versioncontrol import (\n    RevOptions,\n    VersionControl,\n    find_path_to_project_root_from_repo_root,\n    vcs,\n)\n\nlogger = logging.getLogger(__name__)\n\n\nclass Mercurial(VersionControl):",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2757",
    "name": "appdirs.py",
    "path": "05_data_management/file_organization/appdirs.py",
    "category": "05_data_management",
    "type": "testing",
    "lines": 53,
    "size": 1665,
    "docstring": "This code wraps the vendored appdirs module to so the return values are\ncompatible for the current pip code base.\n\nThe intention is to rewrite current usages gradually, keeping the tests pass,\nand eventually drop this after all usages are changed.",
    "keywords": [],
    "functions": [
      "user_cache_dir",
      "_macos_user_config_dir",
      "user_config_dir",
      "site_config_dirs"
    ],
    "classes": [],
    "imports": [
      "os",
      "sys",
      "typing",
      "pip._vendor"
    ],
    "preview": "\"\"\"\nThis code wraps the vendored appdirs module to so the return values are\ncompatible for the current pip code base.\n\nThe intention is to rewrite current usages gradually, keeping the tests pass,\nand eventually drop this after all usages are changed.\n\"\"\"\n\nimport os\nimport sys\nfrom typing import List\n\nfrom pip._vendor import platformdirs as _appdirs\n\n\ndef user_cache_dir(appname: str) -> str:\n    return _appdirs.user_cache_dir(appname, appauthor=False)\n\n\ndef _macos_user_config_dir(appname: str, roaming: bool = True) -> str:",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2758",
    "name": "flake8.py",
    "path": "05_data_management/file_organization/flake8.py",
    "category": "05_data_management",
    "type": "utility",
    "lines": 32,
    "size": 881,
    "docstring": "",
    "keywords": [],
    "functions": [
      "categorize_flake8_output",
      "display_issues"
    ],
    "classes": [],
    "imports": [
      "re",
      "collections"
    ],
    "preview": "import re\nfrom collections import defaultdict\n\n\ndef categorize_flake8_output(file_path):\n    with open(file_path, \"r\") as file:\n        lines = file.readlines()\n\n    issues = defaultdict(list)\n\n    for line in lines:\n        match = re.match(r\"(./[^:]+):(\\d+):\\d+: (\\w\\d+) (.+)\", line)\n        if match:\n            file_name, line_number, code, message = match.groups()\n            issues[code].append((file_name, line_number, message))\n\n    return issues\n\n\ndef display_issues(issues):",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2759",
    "name": "utils.py",
    "path": "05_data_management/file_organization/utils.py",
    "category": "05_data_management",
    "type": "utility",
    "lines": 31,
    "size": 778,
    "docstring": "",
    "keywords": [],
    "functions": [
      "resource_path",
      "get_local_path",
      "save_local_file",
      "load_local_file"
    ],
    "classes": [],
    "imports": [
      "sys",
      "pathlib"
    ],
    "preview": "import sys\nfrom pathlib import Path\n\nif getattr(sys, \"frozen\", False):\n    APP_PATH = Path(sys.executable).resolve().parent\n    RESOURCE_PATH = Path(sys._MEIPASS).resolve()\nelse:\n    APP_PATH = Path(sys.argv[0]).resolve().parent\n    RESOURCE_PATH = APP_PATH\n\n\ndef resource_path(path: str) -> Path:\n    return RESOURCE_PATH.joinpath(path).resolve()\n\n\ndef get_local_path(path: str) -> Path:\n    return APP_PATH.joinpath(path).resolve()\n\n\ndef save_local_file(file_path: str, content: str):",
    "last_modified": "2025-05-04T22:47:12"
  },
  {
    "id": "2760",
    "name": "rag.py",
    "path": "05_data_management/file_organization/rag.py",
    "category": "05_data_management",
    "type": "utility",
    "lines": 25,
    "size": 794,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "langchain.globals",
      "llm_engineering.application.rag.retriever",
      "llm_engineering.infrastructure.opik_utils",
      "loguru"
    ],
    "preview": "from langchain.globals import set_verbose\nfrom llm_engineering.application.rag.retriever import ContextRetriever\nfrom llm_engineering.infrastructure.opik_utils import configure_opik\nfrom loguru import logger\n\nif __name__ == \"__main__\":\n    configure_opik()\n    set_verbose(True)\n\n    query = \"\"\"\n        My name is Paul Iusztin.\n        \n        Could you draft a LinkedIn post discussing RAG systems?\n        I'm particularly interested in:\n            - how RAG works\n            - how it is integrated with vector DBs and large language models (LLMs).\n        \"\"\"\n\n    retriever = ContextRetriever(mock=False)\n    documents = retriever.search(query, k=9)",
    "last_modified": "2025-05-06T04:35:14.991564"
  },
  {
    "id": "2761",
    "name": "inspect_local.py",
    "path": "05_data_management/file_organization/inspect_local.py",
    "category": "05_data_management",
    "type": "utility",
    "lines": 92,
    "size": 3187,
    "docstring": "",
    "keywords": [],
    "functions": [
      "add_options",
      "run",
      "_dist_to_dict"
    ],
    "classes": [
      "InspectCommand"
    ],
    "imports": [
      "logging",
      "optparse",
      "typing",
      "pip",
      "pip._internal.cli",
      "pip._internal.cli.req_command",
      "pip._internal.cli.status_codes",
      "pip._internal.metadata",
      "pip._internal.utils.compat",
      "pip._internal.utils.urls"
    ],
    "preview": "import logging\nfrom optparse import Values\nfrom typing import Any, Dict, List\n\nfrom pip import __version__\nfrom pip._internal.cli import cmdoptions\nfrom pip._internal.cli.req_command import Command\nfrom pip._internal.cli.status_codes import SUCCESS\nfrom pip._internal.metadata import BaseDistribution, get_environment\nfrom pip._internal.utils.compat import stdlib_pkgs\nfrom pip._internal.utils.urls import path_to_url\nfrom pip._vendor.packaging.markers import default_environment\nfrom pip._vendor.rich import print_json\n\nlogger = logging.getLogger(__name__)\n\n\nclass InspectCommand(Command):\n    \"\"\"\n    Inspect the content of a Python environment and produce a report in JSON format.",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2762",
    "name": "merge_markdown.py",
    "path": "05_data_management/file_organization/merge_markdown.py",
    "category": "05_data_management",
    "type": "utility",
    "lines": 57,
    "size": 1744,
    "docstring": "",
    "keywords": [],
    "functions": [
      "read_file",
      "write_file",
      "find_duplicates",
      "merge_files",
      "main"
    ],
    "classes": [],
    "imports": [
      "difflib",
      "os"
    ],
    "preview": "import difflib\nimport os\n\nMARKDOWN_DIR = \"/Users/steven/avatararts/mark/2/markdown_files\"\n\n\ndef read_file(file_path):\n    with open(file_path, \"r\") as file:\n        return file.readlines()\n\n\ndef write_file(file_path, content):\n    with open(file_path, \"w\") as file:\n        file.writelines(content)\n\n\ndef find_duplicates(markdown_files):\n    duplicates = []\n    checked = set()\n    for i, file1 in enumerate(markdown_files):",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2763",
    "name": "markers.py",
    "path": "05_data_management/file_organization/markers.py",
    "category": "05_data_management",
    "type": "utility",
    "lines": 295,
    "size": 8463,
    "docstring": "",
    "keywords": [],
    "functions": [
      "_coerce_parse_result",
      "_format_marker",
      "_eval_op",
      "_get_env",
      "_evaluate_markers",
      "format_full_version",
      "default_environment",
      "__init__",
      "__str__",
      "__repr__"
    ],
    "classes": [
      "InvalidMarker",
      "UndefinedComparison",
      "UndefinedEnvironmentName",
      "Node",
      "Variable",
      "Value",
      "Op",
      "Undefined",
      "Marker"
    ],
    "imports": [
      "operator",
      "os",
      "platform",
      "sys",
      "typing",
      "pip._vendor.pyparsing",
      "pip._vendor.pyparsing",
      "pip._vendor.pyparsing",
      "specifiers"
    ],
    "preview": "# This file is dual licensed under the terms of the Apache License, Version\n# 2.0, and the BSD License. See the LICENSE file in the root of this repository\n# for complete details.\n\nimport operator\nimport os\nimport platform\nimport sys\nfrom typing import Any, Callable, Dict, List, Optional, Tuple, Union\n\nfrom pip._vendor.pyparsing import Forward, Group\nfrom pip._vendor.pyparsing import Literal as L  # noqa: N817\nfrom pip._vendor.pyparsing import (\n    ParseException,\n    ParseResults,\n    QuotedString,\n    ZeroOrMore,\n    stringEnd,\n    stringStart,\n)",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2764",
    "name": "base_command.py",
    "path": "05_data_management/file_organization/base_command.py",
    "category": "05_data_management",
    "type": "utility",
    "lines": 234,
    "size": 8703,
    "docstring": "Base Command class, and related routines",
    "keywords": [],
    "functions": [
      "__init__",
      "add_options",
      "handle_pip_version_check",
      "run",
      "parse_args",
      "main",
      "_main",
      "intercepts_unhandled_exc",
      "exc_logging_wrapper"
    ],
    "classes": [
      "Command"
    ],
    "imports": [
      "functools",
      "logging",
      "logging.config",
      "optparse",
      "os",
      "sys",
      "traceback",
      "optparse",
      "typing",
      "pip._internal.cli"
    ],
    "preview": "\"\"\"Base Command class, and related routines\"\"\"\n\nimport functools\nimport logging\nimport logging.config\nimport optparse\nimport os\nimport sys\nimport traceback\nfrom optparse import Values\nfrom typing import Any, Callable, List, Optional, Tuple\n\nfrom pip._internal.cli import cmdoptions\nfrom pip._internal.cli.command_context import CommandContextMixIn\nfrom pip._internal.cli.parser import ConfigOptionParser, UpdatingDefaultsHelpFormatter\nfrom pip._internal.cli.status_codes import (\n    ERROR,\n    PREVIOUS_BUILD_DIR_ERROR,\n    UNKNOWN_ERROR,\n    VIRTUALENV_NOT_FOUND,",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2765",
    "name": "file_utils.py",
    "path": "05_data_management/file_organization/file_utils.py",
    "category": "05_data_management",
    "type": "organization",
    "lines": 14,
    "size": 290,
    "docstring": "Common file operations",
    "keywords": [],
    "functions": [
      "ensure_dir",
      "get_file_size"
    ],
    "classes": [],
    "imports": [
      "os",
      "pathlib"
    ],
    "preview": "\"\"\"\nCommon file operations\n\"\"\"\nimport os\nfrom pathlib import Path\n\ndef ensure_dir(path):\n    \"\"\"Ensure directory exists.\"\"\"\n    Path(path).mkdir(parents=True, exist_ok=True)\n\ndef get_file_size(file_path):\n    \"\"\"Get file size in MB.\"\"\"\n    return os.path.getsize(file_path) / (1024 * 1024)\n",
    "last_modified": "2025-10-09T05:27:15.569261"
  },
  {
    "id": "2766",
    "name": "errors.py",
    "path": "05_data_management/file_organization/errors.py",
    "category": "05_data_management",
    "type": "utility",
    "lines": 40,
    "size": 1186,
    "docstring": "",
    "keywords": [],
    "functions": [
      "check_syntax",
      "check_directory"
    ],
    "classes": [],
    "imports": [
      "ast",
      "os"
    ],
    "preview": "import ast\nimport os\n\n\ndef check_syntax(file_path):\n    with open(file_path, \"r\") as file:\n        try:\n            ast.parse(file.read(), filename=file_path)\n        except SyntaxError as e:\n            return f\"{file_path}: {e}\"\n    return None\n\n\ndef check_directory(directory):\n    error_files = []\n    for root, _, files in os.walk(directory):\n        for file in files:\n            if file.endswith(\".py\"):\n                file_path = os.path.join(root, file)\n                error_message = check_syntax(file_path)",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2767",
    "name": "copy_my.py",
    "path": "05_data_management/file_organization/copy_my.py",
    "category": "05_data_management",
    "type": "utility",
    "lines": 31,
    "size": 1131,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "csv",
      "os",
      "shutil"
    ],
    "preview": "import csv\nimport os\nimport shutil\n\n# Prompt the user for the path to the CSV file\ncsv_file_path = input(\"Enter the path to the CSV file: \")\n\n# Destination directory\ndestination_directory = \"/Volumes/oG-bAk/steven\"\n\n# Check if destination directory exists, if not create it\nif not os.path.exists(destination_directory):\n    os.makedirs(destination_directory)\n\n# Read the CSV file\nwith open(csv_file_path, mode=\"r\") as file:\n    csv_reader = csv.reader(file)\n    for row in csv_reader:\n        file_path = row[-1].strip()  # Assuming the file paths are in the last column\n        if os.path.isfile(file_path):",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2768",
    "name": "display_progress.py",
    "path": "05_data_management/file_organization/display_progress.py",
    "category": "05_data_management",
    "type": "utility",
    "lines": 82,
    "size": 2655,
    "docstring": "",
    "keywords": [],
    "functions": [
      "humanbytes",
      "TimeFormatter"
    ],
    "classes": [],
    "imports": [
      "logging",
      "math",
      "os",
      "time",
      "translation",
      "sample_config",
      "config"
    ],
    "preview": "import logging\n\nlogging.basicConfig(\n    level=logging.DEBUG, format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\n)\nlogger = logging.getLogger(__name__)\n\nimport math\nimport os\nimport time\n\n# the secret configuration specific things\nif bool(os.environ.get(\"WEBHOOK\", False)):\n    from sample_config import Config\nelse:\n    from config import Config\n\n# the Strings used for this \"thing\"\nfrom translation import Translation\n",
    "last_modified": "2025-09-13T05:53:43.395861"
  },
  {
    "id": "2769",
    "name": "groff.py",
    "path": "05_data_management/file_organization/groff.py",
    "category": "05_data_management",
    "type": "utility",
    "lines": 165,
    "size": 4994,
    "docstring": "pygments.formatters.groff\n~~~~~~~~~~~~~~~~~~~~~~~~~\n\nFormatter for groff output.\n\n:copyright: Copyright 2006-2023 by the Pygments team, see AUTHORS.\n:license: BSD, see LICENSE for details.",
    "keywords": [],
    "functions": [
      "__init__",
      "_make_styles",
      "_define_colors",
      "_write_lineno",
      "_wrap_line",
      "_escape_chars",
      "format_unencoded"
    ],
    "classes": [
      "GroffFormatter"
    ],
    "imports": [
      "math",
      "pip._vendor.pygments.formatter",
      "pip._vendor.pygments.util"
    ],
    "preview": "\"\"\"\npygments.formatters.groff\n~~~~~~~~~~~~~~~~~~~~~~~~~\n\nFormatter for groff output.\n\n:copyright: Copyright 2006-2023 by the Pygments team, see AUTHORS.\n:license: BSD, see LICENSE for details.\n\"\"\"\n\nimport math\n\nfrom pip._vendor.pygments.formatter import Formatter\nfrom pip._vendor.pygments.util import get_bool_opt, get_int_opt\n\n__all__ = [\"GroffFormatter\"]\n\n\nclass GroffFormatter(Formatter):\n    \"\"\"",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2770",
    "name": "installation_report.py",
    "path": "05_data_management/file_organization/installation_report.py",
    "category": "05_data_management",
    "type": "setup",
    "lines": 54,
    "size": 2787,
    "docstring": "",
    "keywords": [],
    "functions": [
      "__init__",
      "_install_req_to_dict",
      "to_dict"
    ],
    "classes": [
      "InstallationReport"
    ],
    "imports": [
      "typing",
      "pip",
      "pip._internal.req.req_install",
      "pip._vendor.packaging.markers"
    ],
    "preview": "from typing import Any, Dict, Sequence\n\nfrom pip import __version__\nfrom pip._internal.req.req_install import InstallRequirement\nfrom pip._vendor.packaging.markers import default_environment\n\n\nclass InstallationReport:\n    def __init__(self, install_requirements: Sequence[InstallRequirement]):\n        self._install_requirements = install_requirements\n\n    @classmethod\n    def _install_req_to_dict(cls, ireq: InstallRequirement) -> Dict[str, Any]:\n        assert ireq.download_info, f\"No download_info for {ireq}\"\n        res = {\n            # PEP 610 json for the download URL. download_info.archive_info.hashes may\n            # be absent when the requirement was installed from the wheel cache\n            # and the cache entry was populated by an older pip version that did not\n            # record origin.json.\n            \"download_info\": ireq.download_info.to_dict(),",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2771",
    "name": "__main__.py",
    "path": "05_data_management/file_organization/__main__.py",
    "category": "05_data_management",
    "type": "utility",
    "lines": 19,
    "size": 395,
    "docstring": "",
    "keywords": [],
    "functions": [],
    "classes": [],
    "imports": [
      "__future__",
      "sys",
      "PrNdOwN",
      "os.path"
    ],
    "preview": "#!/usr/bin/env python\n# Date 31/08/2021\n# Author ybenel\nfrom __future__ import unicode_literals\n\nimport sys\n\nif __package__ is None and not hasattr(sys, \"frozen\"):\n    # direct call of __main__.py\n    import os.path\n\n    PATH = os.path.realpath(os.path.abspath(__file__))\n    sys.path.append(os.path.dirname(os.path.dirname(PATH)))\n\nimport PrNdOwN\n\nif __name__ == \"__main__\":\n    PrNdOwN.main()\n",
    "last_modified": "2025-05-04T23:28:20.850052"
  },
  {
    "id": "2772",
    "name": "cmd_logs.py",
    "path": "05_data_management/file_organization/cmd_logs.py",
    "category": "05_data_management",
    "type": "utility",
    "lines": 20,
    "size": 371,
    "docstring": "",
    "keywords": [],
    "functions": [
      "cls",
      "initLog",
      "info",
      "log"
    ],
    "classes": [],
    "imports": [
      "os",
      "colorama"
    ],
    "preview": "import os\n\nfrom colorama import *\n\n\ndef cls() -> None:\n    os.system(\"cls\" if os.name == \"nt\" else \"clear\")\n\n\ndef initLog() -> None:\n    init()\n\n\ndef info(text: str) -> None:\n    print(f\"[{Style.BRIGHT}{Fore.YELLOW}i{Style.RESET_ALL}]{text}\")\n\n\ndef log(text, success=True) -> None:\n    print(f\"[{f'{Fore.GREEN}V' if success else f'{Fore.RED}X'}{Style.RESET_ALL}]{text}\")\n",
    "last_modified": "2025-03-28T18:37:01"
  },
  {
    "id": "2773",
    "name": "banner.py",
    "path": "05_data_management/file_organization/banner.py",
    "category": "05_data_management",
    "type": "utility",
    "lines": 157,
    "size": 3548,
    "docstring": "",
    "keywords": [],
    "functions": [
      "clear",
      "banner",
      "banner2",
      "banner3",
      "buggy",
      "banner4"
    ],
    "classes": [],
    "imports": [
      "os",
      "sys",
      "threading",
      "random",
      "time",
      "colors"
    ],
    "preview": "#!/usr/bin/python3\n# Created By ybenel\n# Updated In 09/04/2020\nimport os\nimport sys\nimport threading\nfrom random import shuffle\nfrom time import sleep as sl\n\nfrom .colors import get_colors\n\nsys.path.insert(1, \"ascii\")\n\n# Global Variables\nplatform = sys.platform\n\n\ndef clear():\n    if sys.platform == \"win32\":\n        os.system(\"cls\")",
    "last_modified": "2025-05-04T23:28:20.852595"
  },
  {
    "id": "2774",
    "name": "copy2.py",
    "path": "05_data_management/file_organization/copy2.py",
    "category": "05_data_management",
    "type": "utility",
    "lines": 54,
    "size": 2215,
    "docstring": "",
    "keywords": [],
    "functions": [
      "copy_files_with_logging"
    ],
    "classes": [],
    "imports": [
      "csv",
      "os",
      "shutil"
    ],
    "preview": "import csv\nimport os\nimport shutil\n\n\n# Function to copy files while preserving folder structure and logging each copied file\ndef copy_files_with_logging(csv_file, destination_root):\n    log_file = \"copy_log.txt\"\n    with (\n        open(csv_file, newline=\"\") as csvfile,\n        open(log_file, \"a\") as log,\n    ):  # Use 'a' to append to the log file\n        reader = csv.reader(csvfile)\n        for row in reader:\n            if row:  # Ensuring the row is not empty\n                src_file_path = row[0]\n                try:\n                    # Check if the source file exists\n                    if not os.path.exists(src_file_path):\n                        error_message = f\"Source file does not exist: {src_file_path}\"",
    "last_modified": "2025-09-13T05:53:54.343117"
  },
  {
    "id": "2775",
    "name": "opik_utils.py",
    "path": "05_data_management/file_organization/opik_utils.py",
    "category": "05_data_management",
    "type": "utility",
    "lines": 33,
    "size": 1054,
    "docstring": "",
    "keywords": [],
    "functions": [
      "configure_opik"
    ],
    "classes": [],
    "imports": [
      "os",
      "opik",
      "llm_engineering",
      "loguru",
      "opik.configurator.configure"
    ],
    "preview": "import os\n\nimport opik\nfrom llm_engineering import settings\nfrom loguru import logger\nfrom opik.configurator.configure import OpikConfigurator\n\n\ndef configure_opik() -> None:\n    if settings.COMET_API_KEY and settings.COMET_PROJECT:\n        try:\n            client = OpikConfigurator(api_key=settings.COMET_API_KEY)\n            default_workspace = client._get_default_workspace()\n        except Exception:\n            logger.warning(\n                \"Default workspace not found. Setting workspace to None and enabling interactive mode.\"\n            )\n            default_workspace = None\n\n        os.environ[\"OPIK_PROJECT_NAME\"] = settings.COMET_PROJECT",
    "last_modified": "2025-05-06T04:35:14.987238"
  },
  {
    "id": "2776",
    "name": "initialise.py",
    "path": "05_data_management/file_organization/initialise.py",
    "category": "05_data_management",
    "type": "setup",
    "lines": 120,
    "size": 3284,
    "docstring": "",
    "keywords": [],
    "functions": [
      "_wipe_internal_state_for_tests",
      "reset_all",
      "init",
      "deinit",
      "just_fix_windows_console",
      "colorama_text",
      "reinit",
      "wrap_stream"
    ],
    "classes": [],
    "imports": [
      "atexit",
      "contextlib",
      "sys",
      "ansitowin32"
    ],
    "preview": "# Copyright Jonathan Hartley 2013. BSD 3-Clause license, see LICENSE file.\nimport atexit\nimport contextlib\nimport sys\n\nfrom .ansitowin32 import AnsiToWin32\n\n\ndef _wipe_internal_state_for_tests():\n    global orig_stdout, orig_stderr\n    orig_stdout = None\n    orig_stderr = None\n\n    global wrapped_stdout, wrapped_stderr\n    wrapped_stdout = None\n    wrapped_stderr = None\n\n    global atexit_done\n    atexit_done = False\n",
    "last_modified": "2025-10-08T06:33:23"
  },
  {
    "id": "2777",
    "name": "colors.py",
    "path": "05_data_management/file_organization/colors.py",
    "category": "05_data_management",
    "type": "organization",
    "lines": 40,
    "size": 1332,
    "docstring": "MIT License\n\nCopyright (c) 2021-2022 MShawon\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.",
    "keywords": [],
    "functions": [],
    "classes": [
      "bcolors"
    ],
    "imports": [
      "os"
    ],
    "preview": "\"\"\"\nMIT License\n\nCopyright (c) 2021-2022 MShawon\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,",
    "last_modified": "2025-05-04T23:27:45"
  }
];
        const categories = {
  "07_experimental": {
    "name": "Experimental",
    "count": 12,
    "types": [
      "video_processing",
      "utility",
      "setup",
      "analysis",
      "web_tools",
      "organization"
    ]
  },
  "00_shared_libraries": {
    "name": "Shared Libraries",
    "count": 4,
    "types": [
      "utility"
    ]
  },
  "01_core_ai_analysis": {
    "name": "AI Analysis & Transcription",
    "count": 1152,
    "types": [
      "image_processing",
      "transcription",
      "video_processing",
      "utility",
      "setup",
      "analysis",
      "youtube",
      "audio_processing",
      "web_tools",
      "organization",
      "testing"
    ]
  },
  "02_media_processing": {
    "name": "Media Processing",
    "count": 1048,
    "types": [
      "image_processing",
      "transcription",
      "video_processing",
      "utility",
      "setup",
      "analysis",
      "youtube",
      "audio_processing",
      "web_tools",
      "organization",
      "testing"
    ]
  },
  "06_development_tools": {
    "name": "Development Tools",
    "count": 213,
    "types": [
      "video_processing",
      "utility",
      "setup",
      "youtube",
      "web_tools",
      "analysis",
      "organization",
      "testing"
    ]
  },
  "03_automation_platforms": {
    "name": "Platform Automation",
    "count": 262,
    "types": [
      "utility",
      "setup",
      "analysis",
      "web_tools",
      "youtube",
      "organization",
      "testing"
    ]
  },
  "05_data_management": {
    "name": "Data Management",
    "count": 86,
    "types": [
      "utility",
      "setup",
      "youtube",
      "organization",
      "testing"
    ]
  }
};
        
        // Modern Code Browser with enhanced UI
        class ModernCodeBrowser {{
            constructor() {{
                this.files = filesData;
                this.categories = categories;
                this.filteredFiles = [...this.files];
                this.sortOrder = 'name';
                this.isLoading = false;
                
                this.init();
            }}
            
            init() {{
                this.setupEventListeners();
                this.updateStats();
                this.renderFiles();
            }}
            
            setupEventListeners() {{
                // Search input
                const searchInput = document.getElementById('searchInput');
                searchInput.addEventListener('input', (e) => {{
                    this.filterFiles();
                }});
                
                // Category filter
                const categoryFilter = document.getElementById('categoryFilter');
                categoryFilter.addEventListener('change', (e) => {{
                    this.filterFiles();
                }});
                
                // Type filter
                const typeFilter = document.getElementById('typeFilter');
                typeFilter.addEventListener('change', (e) => {{
                    this.filterFiles();
                }});
                
                // Sort button
                const sortBtn = document.getElementById('sortBtn');
                sortBtn.addEventListener('click', (e) => {{
                    this.toggleSort();
                }});
                
                // Modal close
                const closeModal = document.getElementById('closeModal');
                closeModal.addEventListener('click', (e) => {{
                    this.closeModal();
                }});
                
                // Close modal on backdrop click
                const modal = document.getElementById('codeModal');
                modal.addEventListener('click', (e) => {{
                    if (e.target === modal) {{
                        this.closeModal();
                    }}
                }});
                
                // Close modal on escape key
                document.addEventListener('keydown', (e) => {{
                    if (e.key === 'Escape') {{
                        this.closeModal();
                    }}
                }});
            }}
            
            filterFiles() {{
                const searchTerm = document.getElementById('searchInput').value.toLowerCase();
                const categoryFilter = document.getElementById('categoryFilter').value;
                const typeFilter = document.getElementById('typeFilter').value;
                
                this.filteredFiles = this.files.filter(file => {{
                    const matchesSearch = !searchTerm || 
                        file.name.toLowerCase().includes(searchTerm) ||
                        file.docstring.toLowerCase().includes(searchTerm) ||
                        file.keywords.some(kw => kw.toLowerCase().includes(searchTerm)) ||
                        file.functions.some(fn => fn.toLowerCase().includes(searchTerm));
                    
                    const matchesCategory = !categoryFilter || file.category === categoryFilter;
                    const matchesType = !typeFilter || file.type === typeFilter;
                    
                    return matchesSearch && matchesCategory && matchesType;
                }});
                
                this.renderFiles();
                this.updateStats();
            }}
            
            toggleSort() {{
                const sortBtn = document.getElementById('sortBtn');
                
                if (this.sortOrder === 'name') {{
                    this.sortOrder = 'lines';
                    sortBtn.textContent = 'Sort by Lines';
                }} else if (this.sortOrder === 'lines') {{
                    this.sortOrder = 'size';
                    sortBtn.textContent = 'Sort by Size';
                }} else {{
                    this.sortOrder = 'name';
                    sortBtn.textContent = 'Sort A-Z';
                }}
                
                this.sortFiles();
                this.renderFiles();
            }}
            
            sortFiles() {{
                this.filteredFiles.sort((a, b) => {{
                    switch (this.sortOrder) {{
                        case 'name':
                            return a.name.localeCompare(b.name);
                        case 'lines':
                            return b.lines - a.lines;
                        case 'size':
                            return b.size - a.size;
                        default:
                            return 0;
                    }}
                }});
            }}
            
            renderFiles() {{
                const grid = document.getElementById('codeGrid');
                
                // Clear existing cards
                grid.innerHTML = '';
                
                if (this.filteredFiles.length === 0) {{
                    grid.innerHTML = '<div class="loading">No files found matching your criteria.</div>';
                    return;
                }}
                
                // Create cards for filtered files
                this.filteredFiles.forEach(file => {{
                    const card = this.createFileCard(file);
                    grid.appendChild(card);
                }});
                
                // Add animation delay to cards
                const cards = grid.querySelectorAll('.code-card');
                cards.forEach((card, index) => {{
                    card.style.animationDelay = `${{index * 0.1}}s`;
                }});
            }}
            
            createFileCard(file) {{
                const card = document.createElement('div');
                card.className = 'code-card';
                card.setAttribute('data-category', file.category);
                card.setAttribute('data-type', file.type);
                card.setAttribute('data-name', file.name.toLowerCase());
                card.setAttribute('data-keywords', file.keywords.join(' ').toLowerCase());
                
                const typeIcon = this.getTypeIcon(file.type);
                const sizeKb = (file.size / 1024).toFixed(1);
                const keywordsHtml = file.keywords.slice(0, 5).map(kw => 
                    `<span class="keyword">${{kw}}</span>`
                ).join('');
                
                const functionsText = file.functions.length > 0 ? 
                    file.functions.slice(0, 3).join(', ') : 'No functions';
                
                const preview = this.escapeHtml(file.preview.substring(0, 400));
                
                card.innerHTML = `
                    <div class="card-header">
                        <div class="file-icon">${{typeIcon}}</div>
                        <div class="file-info">
                            <h3 class="file-name">${{file.name}}</h3>
                            <p class="file-path">${{file.path}}</p>
                        </div>
                        <div class="file-stats">
                            <span class="stat-badge">${{file.lines}} lines</span>
                            <span class="stat-badge">${{sizeKb}} KB</span>
                        </div>
                    </div>
                    
                    <div class="card-body">
                        <div class="file-description">
                            ${{file.docstring ? this.escapeHtml(file.docstring.substring(0, 200)) : 'No description available'}}
                        </div>
                        
                        <div class="keywords">
                            ${{keywordsHtml}}
                        </div>
                        
                        <div class="functions">
                            ${{functionsText}}
                        </div>
                    </div>
                    
                    <div class="card-footer">
                        <div class="preview-code">
                            <pre><code class="language-python">${{preview}}</code></pre>
                        </div>
                        <button class="view-code-btn" onclick="modernCodeBrowser.openCodeModal('${{file.id}}')">
                            View Full Code
                        </button>
                    </div>
                `;
                
                return card;
            }}
            
            openCodeModal(fileId) {{
                const file = this.files.find(f => f.id === fileId);
                if (!file) return;
                
                const modal = document.getElementById('codeModal');
                const title = document.getElementById('modalTitle');
                const path = document.getElementById('modalPath');
                const lines = document.getElementById('modalLines');
                const size = document.getElementById('modalSize');
                const type = document.getElementById('modalType');
                const code = document.getElementById('modalCode');
                
                title.textContent = file.name;
                path.textContent = file.path;
                lines.textContent = file.lines;
                size.textContent = `${{(file.size / 1024).toFixed(1)}} KB`;
                type.textContent = file.type;
                code.textContent = file.preview;
                
                // Highlight syntax
                if (window.Prism) {{
                    Prism.highlightElement(code);
                }}
                
                modal.style.display = 'block';
                document.body.style.overflow = 'hidden';
            }}
            
            closeModal() {{
                const modal = document.getElementById('codeModal');
                modal.style.display = 'none';
                document.body.style.overflow = 'auto';
            }}
            
            updateStats() {{
                document.getElementById('totalCount').textContent = this.files.length;
                document.getElementById('visibleCount').textContent = this.filteredFiles.length;
                document.getElementById('categoriesCount').textContent = Object.keys(this.categories).length;
            }}
            
            getTypeIcon(type) {{
                const icons = {{
                    'transcription': 'üé§',
                    'analysis': 'üìä',
                    'youtube': 'üì∫',
                    'image_processing': 'üñºÔ∏è',
                    'video_processing': 'üé¨',
                    'audio_processing': 'üîä',
                    'web_tools': 'üåê',
                    'data_processing': 'üìà',
                    'testing': 'üß™',
                    'setup': '‚öôÔ∏è',
                    'organization': 'üìÅ',
                    'utility': 'üîß'
                }};
                return icons[type] || 'üêç';
            }}
            
            escapeHtml(text) {{
                const div = document.createElement('div');
                div.textContent = text;
                return div.innerHTML;
            }}
        }}

        // Global functions for onclick handlers
        function openCodeModal(fileId) {{
            if (window.modernCodeBrowser) {{
                window.modernCodeBrowser.openCodeModal(fileId);
            }}
        }}

        // Initialize when DOM is loaded
        document.addEventListener('DOMContentLoaded', () => {{
            window.modernCodeBrowser = new ModernCodeBrowser();
        }});
    </script>
</body>
</html>