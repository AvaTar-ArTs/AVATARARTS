# AI Manager Consolidation Guide

## üì¶ What Was Consolidated

### Ollama Scripts (Merged)
```
OLD:                                    NEW:
‚îú‚îÄ‚îÄ ollama_gui.py                       ‚îÇ
‚îú‚îÄ‚îÄ ollama-run.py                       ‚îú‚îÄ‚Üí unified_ai_manager.py (GUI + CLI)
‚îî‚îÄ‚îÄ ollama-run_from_utilities.py         ‚îÇ
    (all 3 had overlapping functionality)

OLD: ollama_test.py                     ‚îú‚îÄ‚Üí Built into unified_ai_manager.py
    (testing/status features)
```

### OpenAI Scripts (Consolidated)
```
OLD:                                    NEW:
‚îú‚îÄ‚îÄ openai_client.py                    ‚îÇ
‚îú‚îÄ‚îÄ openai-cli.py                       ‚îú‚îÄ‚Üí unified_ai_manager.py (integrated)
‚îî‚îÄ‚îÄ openai.py                           ‚îÇ
    (all provided OpenAI access)

OLD: openpy.py                          ‚îú‚îÄ‚Üí Still available as utility
    (Python encoding utility)           ‚îÇ   (standalone, no consolidation needed)
```

### New Structure
```
~/Documents/python/
‚îú‚îÄ‚îÄ unified_ai_manager.py         ‚Üê MAIN: Full-featured manager
‚îú‚îÄ‚îÄ ai                            ‚Üê CLI: Quick command wrapper
‚îú‚îÄ‚îÄ AI_CONSOLIDATION_GUIDE.md     ‚Üê You are here
‚îú‚îÄ‚îÄ OLLAMA_QUICKSTART.md          ‚Üê Ollama setup guide
‚îú‚îÄ‚îÄ ollama_model_manager.py       ‚Üê Model deployment tool
‚îî‚îÄ‚îÄ AI_CONTENT/
    ‚îî‚îÄ‚îÄ content_creation/
        ‚îú‚îÄ‚îÄ ollama_gui.py         ‚Üê [LEGACY - use unified_ai_manager instead]
        ‚îú‚îÄ‚îÄ openai-cli.py         ‚Üê [LEGACY - use unified_ai_manager instead]
        ‚îî‚îÄ‚îÄ ... (old files archived)
```

---

## üöÄ How to Use the New Unified System

### Option 1: Interactive Mode (Most User-Friendly)
```bash
python3 ~/Documents/python/unified_ai_manager.py
```
**Choose from menu:**
- Chat with Ollama
- Ask OpenAI
- View status
- Launch GUI

### Option 2: GUI Mode (Best for Frequent Use)
```bash
python3 ~/Documents/python/unified_ai_manager.py gui
```
**Features:**
- Tabbed interface (Ollama | OpenAI | Status)
- Real-time responses
- Model management
- Status dashboard

### Option 3: CLI Mode (Fastest for Scripts/Automation)
```bash
# Chat with Ollama
python3 ~/Documents/python/unified_ai_manager.py ollama "Your prompt"

# Ask OpenAI
python3 ~/Documents/python/unified_ai_manager.py openai "Your question"

# Check status
python3 ~/Documents/python/unified_ai_manager.py status

# Launch GUI
python3 ~/Documents/python/unified_ai_manager.py gui
```

### Option 4: Quick CLI Wrapper (Fastest)
```bash
# Make executable
chmod +x ~/Documents/python/ai

# Use as command
python3 ~/Documents/python/ai ollama "Your prompt"
python3 ~/Documents/python/ai openai "Your question"
python3 ~/Documents/python/ai status
python3 ~/Documents/python/ai gui
```

### Option 5: Create Shell Alias (One-Time Setup)
```bash
# Add to ~/.zshrc or ~/.bashrc
alias ai="python3 ~/Documents/python/ai"
alias ai-manager="python3 ~/Documents/python/unified_ai_manager.py"

# Then use directly:
ai ollama "Your prompt"
ai openai "Your question"
ai-manager gui
```

---

## üîß Setup Instructions

### Step 1: Install Dependencies
```bash
pip3 install requests openai
```

### Step 2: Configure OpenAI (Optional)
```bash
# Option A: Environment variable
export OPENAI_API_KEY="your-key-here"

# Option B: Add to ~/.zshrc or ~/.bashrc
echo 'export OPENAI_API_KEY="your-key-here"' >> ~/.zshrc
source ~/.zshrc

# Option C: Create ~/.env file
OPENAI_API_KEY=your-key-here
OPENAI_MODEL=gpt-4o
```

### Step 3: Start Ollama (if using Ollama)
```bash
# Terminal 1: Start Ollama service
ollama serve

# Terminal 2: Run manager
python3 ~/Documents/python/unified_ai_manager.py
```

### Step 4: Make CLI Executable (Optional)
```bash
chmod +x ~/Documents/python/ai
```

---

## üìä Feature Comparison

| Feature | Old | New |
|---------|-----|-----|
| Ollama Chat | ‚úÖ | ‚úÖ (Enhanced) |
| OpenAI Chat | ‚úÖ | ‚úÖ (Integrated) |
| GUI Interface | ‚úÖ | ‚úÖ (Tabbed) |
| CLI Support | ‚ùå | ‚úÖ (Full) |
| Status Reporting | ‚úÖ | ‚úÖ (Detailed) |
| Error Handling | ‚ö†Ô∏è | ‚úÖ (Production-grade) |
| Logging | ‚ùå | ‚úÖ (~/.cache/ai_manager.log) |
| Configuration Management | ‚ùå | ‚úÖ (JSON config) |
| Code Reusability | ‚ùå | ‚úÖ (Modular classes) |
| Performance | Good | Better (async support) |

---

## üîÑ Migration from Old Scripts

### If You Were Using `ollama_gui.py`
**Before:**
```bash
python3 ~/Documents/python/AI_CONTENT/content_creation/ollama_gui.py
```

**After:**
```bash
python3 ~/Documents/python/unified_ai_manager.py gui
```

### If You Were Using `openai-cli.py`
**Before:**
```bash
python3 ~/Documents/python/AI_CONTENT/content_creation/openai-cli.py "Your question"
```

**After:**
```bash
python3 ~/Documents/python/unified_ai_manager.py openai "Your question"
# Or:
python3 ~/Documents/python/ai openai "Your question"
```

### If You Were Using `ollama_test.py`
**Before:**
```bash
python3 ~/Documents/python/AI_CONTENT/content_creation/ollama_test.py
```

**After:**
```bash
python3 ~/Documents/python/unified_ai_manager.py status
```

---

## üíª Integration with Your Automation Bots

### Example: Use in AUTOMATION_BOTS

```python
# In your bot script
import sys
sys.path.insert(0, '/Users/steven/Documents/python')

from unified_ai_manager import UnifiedAIManager

manager = UnifiedAIManager()

# Chat with Ollama (local, fast)
response = manager.ollama_chat("Classify this message as positive/negative: 'Great product!'")

# Ask OpenAI (powerful, when needed)
response = manager.openai_ask("Write a tweet about this: 'New feature released'")

# Check status
if manager.ollama.available:
    print("Ollama is ready!")
else:
    print("Ollama is down!")
```

### Example: Use in AI_CONTENT

```python
# In content generation script
from unified_ai_manager import UnifiedAIManager

manager = UnifiedAIManager()

def generate_content(topic):
    # Use local model for speed
    return manager.ollama_chat(f"Write creative content about: {topic}")

def polish_content(content):
    # Use OpenAI for quality when needed
    return manager.openai_ask(f"Improve this content: {content}")
```

---

## üö® Troubleshooting

### "Ollama is not running"
```bash
# Terminal 1: Start Ollama
ollama serve

# Terminal 2: Run manager
python3 ~/Documents/python/unified_ai_manager.py status
```

### "OPENAI_API_KEY not set"
```bash
# Add to ~/.zshrc or ~/.bashrc
export OPENAI_API_KEY="sk-..."
source ~/.zshrc

# Verify
echo $OPENAI_API_KEY
```

### "No Ollama models available"
```bash
# Pull a model
ollama pull qwen3-coder:30b

# Check models
python3 ~/Documents/python/unified_ai_manager.py ollama list
```

### "requests module not found"
```bash
pip3 install requests
```

---

## üìö Documentation

- **Setup:** See `OLLAMA_QUICKSTART.md`
- **Model Management:** See `ollama_model_manager.py`
- **Curl Reference:** See `OLLAMA_CURL_REFERENCE.md`

---

## ‚úÖ What Changed

### Improvements
- ‚úÖ **Removed duplication** - 3 Ollama GUIs ‚Üí 1 unified interface
- ‚úÖ **Added CLI mode** - No GUI needed for scripts/automation
- ‚úÖ **Better error handling** - Production-grade exceptions
- ‚úÖ **Configuration management** - Persistent settings
- ‚úÖ **Logging** - Debug issues in ~/.cache/ai_manager.log
- ‚úÖ **Modular design** - Easy to import into your projects
- ‚úÖ **Tabbed GUI** - Cleaner interface for both tools

### Backward Compatibility
- Old files still exist in `AI_CONTENT/content_creation/` (for reference)
- All functionality preserved
- No breaking changes to your existing code

---

## üéØ Recommended Usage by Role

### For Development/Testing
```bash
python3 ~/Documents/python/unified_ai_manager.py
# Interactive menu
```

### For Automation Bots
```python
from unified_ai_manager import UnifiedAIManager
manager = UnifiedAIManager()
response = manager.ollama_chat(prompt)
```

### For Content Creation
```bash
python3 ~/Documents/python/unified_ai_manager.py gui
# Keep GUI open, switch between Ollama/OpenAI tabs
```

### For Quick Commands
```bash
ai ollama "Your prompt"
ai openai "Your question"
```

---

## üîê Security Notes

- API keys stored in environment variables (not in code)
- Log file at `~/.cache/ai_manager.log` (review for sensitive data)
- Configuration stored at `~/.ai_manager/config.json` (keep private)

---

**Consolidation Date:** October 26, 2025
**Status:** Ready for production
